{"audio_filepath": "lec001_001.wav", "duration": 407.681, "text": "\nhello everyone  welcome to lecture one of csseven fifteen  which is the course on deep learning \nin  today\u2019s lecture  is  going  to be a bit  non technical  we are  not going to  cover any\ntechnical  concepts  or  we  only  going  to   talk   about  a  brief  or  partial   history  of  deep\nlearning \nso  we hear the terms artificial neural networks  artificial neurons quite often these days \nand i just wanted you take you through the journey of where does all these originate\nfrom   and  this  history   contains  several  spans   across   several  fields    not  just   computer\nscience we will start with biology then talk about something in physics then eventually\ncome to computer science and so on  so  with that let us start \n\n\nso  just some acknowledgments and disclaimers  i have taken lot of this material from\nthe first people  which i have mentioned on the bullet and there might still be some errors\nbecause its dates as back as one thousand  eight hundred and seventy one  so  maybe i have got some of the facts wrong  so  feel\nfree to contact me if you think some of these portions need to be corrected and it would\nbe good if you could provide me appropriate references for these corrections \n\fso  let us start with the first chapter  which is on biological neurons as i said its spans\nseveral fields  will start with biology \n\n\nand we will first talk about the brain and neurons within the brain  so   way back in\none thousand  eight hundred and seventy one  one thousand  eight hundred and seventy three around that time  joseph von gerlach actually proposed that the nervous\nsystem our nervous system is a single continuous network as opposed to a network of\nmany discrete cells \nso  his idea was that this is one gigantic cell sitting in our nervous system and it is not a\nnetwork of discrete cells  and this theory was known as the reticular theory \n\f\n\nand around the same time there was the some breakthrough or some progress in staining\ntechniques  where camillo golgi discovered that a chemical reaction that would allow\nyou to examine the neurons or the nervous tissue \nso  he was looking at this nervous tissue using some staining technique and by looking at\nwhat you see in this figure on the right hand side the yellow figure  even he concluded\nthat this is just once single cell and not a network of discrete cells  so  he was again a\nproponent of reticular theory  so  this is about camillo golgi \n\n\n\fand then interestingly santiago cajal he used the same technique which golgi proposed \nand he studied the same tissue and he came up with the conclusion that this is not a\nsingle cell this is actually a collection of various discrete cells  which together forms a\nnetwork  so  it is a network of things as opposed to a single cell there  so  that is what\nhis theory was  and this was eventually came to be known as the neuron doctrine \n\n\nalthough this was not a consolidated in the form of a doctrine by cajal  that was done by\nthis gentleman  so  he coined the term neuron  so  now  today when you think about art\nhere   about   artificial   neural   networks   or   artificial   neurons    the   term   neuron   actually\noriginated way back in one thousand  eight hundred and ninety one and this gentleman was responsible for coining that \nand he was also responsible for consolidating the neuron doctrine   so   already as you\nsaw on the previous slide cajal had proposed it  but then over the years many people\nbought   this   idea   and   this   guy   was   responsible   for   consolidating   that   into   a   neuron\ndoctrine  interestingly he is not only responsible for coining the term neuron  he is also\nresponsible for coining the term chromosome  so  two very important terms were coined\nby this one person \nso  now here is a question  so  around one thousand  nine hundred and six  when it was time to give the nobel prize in\nmedicine  what do you think which of these two proponents  say there are two theories\none is reticular theory  which is a single cell and then there is this neuron doctrine which\nis a collection of cells or collection of neurons  that a nervous system is a collection of\n\fneurons  so  what do you think which of these two guys who are proponents of these two\ndifferent theories  who would have got the actual nobel prize for medicine \n\n\nso  interestingly it was given to both of them  so   till one thousand  nine hundred and six in fact  way later till one thousand  nine hundred and fifty\nalso this debate was not completely set settled and then the committee said both of these\nare interesting pieces of work  we yet do not know what really actual what the situation\nis actually  but these conflicting ideas have a place together  and so  the nobel prize was\nactually given to both of them  and this led to a history of a some kind of controversies\nbetween these two scientists and so on \n\f\n\nand this debate surprisingly was settled way later in one thousand  nine hundred and fifty and not by progress in biology\nas such but by progress in a different field \nso  this was with the advent of electron microscopy  so  now  it was able to see this at a\nmuch better scale and by looking at this under a microscope it was found that actually\nthere is a gap between these neurons and hence it is not a one single cell  it is actually a\ncollection or a network of cells with a clear gap between them or some connections\nbetween them  which are now known as synapses  so  this was when the debate was\nsettled \nso  now why am i talking about biology why am i telling you about biological neuron\nand so on  so  this is what we need to understand  so  there has always been interested in\nunderstanding how the human brain works from a biological perspective at least  and\naround this time the debate was more or less settled that we have this our brain is a\ncollection of many neurons and they interact with each other  to help us do a lot of\ncomplex processing that we do on a daily basis right from getting up in the morning and\ndeciding what do we want to do today  taking decisions performing computations and\nvarious complex things that our brain is capable of doing \nnow  the interest is in seeing if we understand how the brain works  can we make an\nartificial model for that  so  can we come up with something which can simulate how our\nbrain works and what is that model  and how do we make a computer do that or how do\n\fwe make a machine do that  so  that is why i started from biological neurons to take the\ninspiration from biology \n\f"}
{"audio_filepath": "lec001_002.wav", "duration": 813.24, "text": "\nwe will start talking about artificial intelligence  and this is titled as from the spring to\nthe winter of ai   so   i am going to talk about when was this boom in ai started  or\nwhen is that people started thinking and talking about ai seriously  and what eventually\nhappened to the initial boom and so \n\n\nso   let   us  start   with  one thousand  nine hundred and forty three   whereas    i   saying   that   there   was   a   lot   of   interest   in\nunderstanding  how does a human brain work  and then come up with a computational\nor   a   mathematical   model   of   that   so   mcculloch   and   pitts    one   of   them   was   a\nneuroscientist and the other one was a logician  no computer scientists or anything at that\npoint of time  \nand they came up with this extremely simplified model  that just as a brain takes a input\nfrom lot of factors  so now  suppose you want to decide whether you want to go out for a\nmovie or not  so  you would probably think about  do you really have any exams coming\nup that could be our factor xone  you could think about is a weather good to go out is it\n\fraining would it be difficult to go out at this point  would there be a lot of traffic is it a\nvery popular movie and hence tickets may not be available and so on  \nso  being kind of presses all this  information you might also look at things  like the\nreviews of the movie  or the imdb rating of the movie and so on  and based on all these\ncomplex inputs  it applies some function and then takes a decision yes or no that  i want\nto probably go for a movie  \nso  this is an overly simplified model of  how the brain works is  and what this model\nsays is that  you take inputs from various sources and based on that you come up with the\nbinary decision right  so  this is what they proposed in one thousand  nine hundred and forty three  so now  we have come to\nan artificial neuron  so  this is not a biological neuron this is how  you would implement\nit as a machine right  so  that was in one thousand  nine hundred and forty three  \n\n\nthen   along   and   then   this   kind   of   led   to   a   lot   of   boom   in   our   interest   in   artificial\nintelligence  and so on  and i guess  around one thousand  nine hundred and fifty six  in a conference  the term artificial\nintelligence   was   a   formally   coined    and   within   a   one   or   two   years   from   there   frank\nrosenberg   came   up   with   this   perceptron   model   of   doing   computations   or   what\nperceptron model of what an artificial neuron could be \nand we will talk about this in detail later on the course and not tell you what these things\nare as of now  just think of the a new model was proposed and this is what he had to say\n\fabout this model right  so  he said that  the perceptron may eventually be able to learn \nmake decisions and translate languages  do you find anything odd about this statement\nyeah  so  learn and make decisions make sense  but why translate languages  why is so\nspecific  why such a specific interest in languages \nso  that you have to connect back to history  so  this is also the period of the cold war\nand there was always  always a lot of interest  there was lot of research and translation\nwas actually fuelled by the world war and evens that happened after that  where  these\ncountries which were at loggerheads with each other  \nthey wanted to understand what the other country is doing  but they did not speak each\nother\u2019s language  that is why  there was a lot of interest from espionage point of view or\nfrom spying and so on to be able to translate languages  and hence  that specific require\nand lot of this research would have been funded from agencies  which are interested in\nthese things right  and the defence or war or something  \n\n\nso  and this work was largely done for the navy and this is an this is an extract from the\narticle written in new york times  way back in one thousand  nine hundred and fifty seven or fifty eight  where it was mentioned that\nthe embryo often this perceptron is an embryo of an electronic computer  that the navy\nexpects will be able to walk  talk  see  write  reproduce itself and be conscious of it is\nexistence   \n\fso  i am not quoting something from two thousand and seventeen or eighteen  this is way back in one thousand  nine hundred and fifty seven fifty eight  why i am\nthat is why i like the history part of it  so  recently there is a lot of boom or a lot of hype\naround  ai that ai will take over a lot of things will take our jobs  it might eventually  we\nmight be colonized by ai agents and so on  \nso  i just want to emphasize that  i do not know whether  that will happen or not  but this\nis not something new we have been talking about the promise of ai as far back  since \none thousand  nine hundred and fifty seven one thousand  nine hundred and fifty eight right  this not something new that people are talking about  now  it is always\nbeen there and to what extent  this promise will be fulfilled is yet to be seen  \nand of course  as compared to one thousand  nine hundred and fifty seven fifty eight  we have made a lot of progress in other fields\nwhich have enabled  ai to be much more successful than it was earlier for example  we\nhave much better compute power now  we have lots of data  now  and all thanks to the\ninternet  and other things that you can actually crawl tons and tons of data and then try to\nlearn something from a data or try to make the machine learn something from it \nso  we have made a lot of progress in other aspects where  which ai is now at a position \nwhere  it can really make a difference  but just wanted to say that  these are not things\nwhich   i   have   not   been   said   in   the   past    it   has   always   been   the   it   has   always   been\nconsidered to be very promising and perhaps a bit hyped also  so  that is about one thousand  nine hundred and fifty seven fifty eight  \n\n\n\fthen now  what we talk about  what is all the for the past eight to ten years  at least when\nwe talk about ai talking about deep learning  and that is what this course  is about\nlargely about deep learning  i am not saying that other and what deep learning is largely\nabout  if i want to tell you in a very layman nutshell term is  it is about a large number\nof artificial neurons connected to each other in layers and functioning towards achieving\ncertain goal \nso  this is like a schematic of what a deep neural network or a feed forward neural\nnetwork would look like  now this is again not something new which is up in the last eight\nto ten years  although  people have started discussing it a lot in the last eight to ten years \nlook at it way back in  one thousand  nine hundred and sixty five sixty eight opposed something which looked very much like a\nmodern deep neural network or a modern feed forward neural network  \nand in many circles  he is considered to be one of the founding fathers of modern deep\nlearning \n\n\nso  that is about that \n right  from one thousand  nine hundred and forty three to one thousand  nine hundred and sixty eight  it was mainly\nabout   the   springtime   for  ai   and   what   i   mean   by   that   that   everyone   was   showing\ninterest in that  the government was funding a lot of research in ai and people really\nthought that ai could deliver a lot of things on a lot of fronts \n for\nvarious applications  health care defence and so on  \n\fand then around  one thousand  nine hundred and sixty nine an interesting paper came out by these two gentlemen minsky\nand papert  which essentially outlined some limitations of the perceptron model and we\nwill talk about these limitations later on in the course  in the second or third lecture  but\nfor now i will not get into a details of that  but what it is said that it is possible that a\nperceptron cannot handle some very simple functions also  \nso  you are trying to make the perceptron learn some very complex functions  because \nthe way we decide how to watch a movie is a very complex function of the inputs that\nwe considered  but even a simple function like xor or is something which a perceptron\ncannot be used to model that is what this paper essentially showed  and this led to severe\ncriticism for ai and then  people started losing interest in ai and lot of government\nfunding actually subsided after one thousand  nine hundred and sixty nine  \n\n\nall the way to one thousand  nine hundred and eighty six actually this was the ai winter of connectionism  so  there was very\nlittle interest in connectionist ai  so  there are two types of ai one is symbolic ai and the\nother is connectionist ai  so whatever  we are going to study in this course about neural\nnetworks and all  that probably falls in connectionist ai paradigm and there was no\ninterest in this and people  i mean hard to get funding and so on for these seventeen to eighteen years  \n\f\n\nand that was largely triggered by this study that was done by minsky and papert and\ninterestingly they were also often misquoted and what they had actually  said in that\npapers  so  they had said a single perceptron cannot do it  they in fact  said that a multi \nlayer network of perceptrons can do it   but  no one focused on the second part that  a\nmulti layer   network   of   perceptron   people   started   pushing   the   idea   that   a   perceptron\ncannot do it  and hence  we should not be investigating it  and so  on right  so that is\nwhat happened for a long time and this known as the winter the first winter  \n\n\n\fthen around one thousand  nine hundred and eighty six actually came this algorithm which is known as back propagation \nagain this is an algorithm which we are going to cover in a lot of detail in the course in\nthe fourth or fiveth lecture  and this algorithm actually enables to train a deep neural network\nright  so  deep network of neurons is something that you can train using this algorithm  \nnow  this algorithm was actually popularized by at rumelhart and others in one thousand  nine hundred and eighty six  but it\nis not completely discovered by them  this was also around in various other fields  so  it\nwas there  in  i think in systems analysis or something like that  it was being used for\nother purposes in a different context and so on  and rumelhart other and others in one thousand  nine hundred and eighty six\nwere the first to kind of popularize it in the context of deep neural networks  \nand this was a very important discovery because  even today all the neural network  so\nmost of them are trained using back propagation right  and of course  there have been\nseveral other advances  but the core remains the same that you use back propagation to\ntrain a deep neural network right  so  something this was discovered almost thirty years\nback is still primarily used for training deep neural networks  that is why  this was a very\nimportant paper or breakthrough at that time  \n \n\nand   around   the   same   time  so   again   interestingly    so   back   propagation    is   used   in\nconjunction  with something known as gradient descent  which was again discovered\nway back in one thousand  eight hundred and forty seven by cauchy and he was interested in using this to compute the orbit of\nheavenly bodies \n\fthat is something that people care about at that time today of course  we use it for\nvarious   other   purposes   one   of   them   being   discovering   cats   and   videos   or   even   for\nmedical imaging or for describing  whether  certain have of cancer is being depicted in a\nx ray or things like that there is a lot of other purposes for which  deep neural networks\nenhance  and hence  back propagation gradient descent and other things are being used\nfor it  but again these are not very modern discoveries these are dated way back thirty years\nand even gradient descent is almost  one hundred and fifty years and so on  so  that is  what i wanted to\nemphasize  \n\n\nand around the same time in one thousand  nine hundred and ninety or one thousand  nine hundred and eighty nine there is this another interesting theorem\nwhich was proved which is known as the universal approximation theorem  and this is\nagain something that we will cover in the course in the third lecture or something like\nwhere we will talk about the power of a deep neural network \nso  again the importance of this or why this theorem was important will become clear\nlater and when we cover it in detail  but for now it is important to understand that  what\nthis theorem said is that if you have a deep neural network you could basically model all\ntypes of functions continuous functions to any desired precision  \nso  what it means in very layman terms is that  if the way you make decisions using a\nbunch of inputs is a very  very complex function of the input  then you can have a neural\n\fnetwork  which will be able to learn this function right in many laymen terms  that is\nwhat it means  \nand if i have to hype it up a bit or i have to say it in a very enthused and excited manner \ni would say that basically it says that  deed neural networks can be used for solving all\nkinds of machine learning problems  and that is roughly what it says  but with a pinch of\nsalt and a lot of caveats  but that is what it means at least in the context of this course \nso    this   is   all   around   one thousand  nine hundred and eighty nine   and   despite   this   happening   some   important   discoveries\ntowards   the   late   end   of   eighty\u2019s    which   was   back   propagation   universal   approximation\ntheorem  people were still not being able to use deep neural networks for really solving\nlarge practical problems  and a few challenges there was of course the compute power at\nthat time was not at a level where  it could support deep neural networks  \nwe do not have enough data for training deep neural networks and also in terms  of\ntechniques  while back propagation is a sound technique  it is to fail when you have\nreally deep neural network  so  when people try it training a very deep neural network\nthey found that the training does not really converge the system does not really learn\nanything and so on  and there were certain issues with using back propagation off the\nshelf at that time because of which it was not very successful \nso  again despite these slight boom around eighty six to ninety where some important discoveries\nwere made  and even follow up in  ninety two  ninety three and so on   there is still  not a real big hype\naround deep neural networks or artificial neural networks and at time again a slump a\nslow winter right up till two thousand and six  \n\f"}
{"audio_filepath": "lec001_003.wav", "duration": 467.008, "text": "\nwhen  this  deep  revival  happened   so  in  two thousand and six  a  very  important  study  was  or  a  very\nimportant contribution was made by hinton and salakhutdinov  \n\n\n \nsorry  if i have not pronounced it properly and they found that a method for training very\ndeep neural network effectively  now  again the details of these are not important   we\nwill be doing that in the course at some point  but what is the important take away here is\nthat while from one thousand  nine hundred and eighty nine to two thousand and six  we knew that there is an algorithm for training deep neural\nnetworks and they can potentially be used for solving a wide range of problems because\nthat   is   what   the   universal   approximation   theorem   said   but   the   problem   was   that   in\npractice we were not being able to use it for much  \nit was not easy to train these networks  but now with this technique there was revived\ninterest   and   hope   that   now   actually   can   train   very   deep   neural   networks   for   lot   of\npractical problems  this sparked off the interest again  and then  people started looking at\nall   such  of  thing   right  that  even  this  particular  study  which  was   done  in two thousand and six  will\n\factually be very simple to something done way back in ninety one ninety three and which again showed\nthat you can train a very deep neural network  but again due to several factors may be at\nthat time due to the computational requirements or the data requirements or whatever i\nam not too sure about that  it did not become so popular then  but by two thousand and six probably the\nstage was much better for these kind of networks or techniques to succeed   so  then it\nbecame popular in two thousand and six  \n\n\n \nthen    this   two thousand and six   to   two thousand and nine   people   started   gaining   more   and   more   insights   into   the\neffectiveness of this discovery made by  hinton and others which is unsupervised pre \ntraining    right    that   is   what   i   spoke   about   on   the   previous   slide   unsupervised   pre \ntraining  \nand they started getting more and more insights into how you can make deep neural\nnetworks really work  so  they came up with various techniques  some of which we are\ngoing to study in this course  so  this was about how do you initialise the network better \nwhat   is   the   better   optimization   algorithm   to   use    what   is   the   better   regularization\nalgorithm to use and so on  so  there were many things which were started coming out at\nthis period two thousand and six to two thousand and nine and by two thousand and nine  everyone started taking note of this and again deep\nneural networks of artificial neural networks started becoming popular  \nthat is when people realised that all this  all the negative things that were tied to it that\nyou are not able to train it well and so on  have slowly  people have started finding\n\fsolutions to get by those and maybe we should start again focusing on the potential of\ndeep neural networks and see if they can be used for large scale practical application  so \nthis two thousand and six to two thousand and nine was again a slow boom period were people were again trying to do a\nlot of work to popularize  deep neural  networks and get rid of some of the problems\nwhich existed in training them  \n\n\nnow   from   two thousand and nine   onwards   there   was   this   series   of   success   is   which   kind   of   caught\neveryone   which  made   everyone  to  stand up  and  take  notice   right  that  this  is   really\nworking for a  lot of practical  applications  starting  with handwriting  recognition   so \naround two thousand and nine  these guys won handwriting recognition competition in arabic and they did\nway better than the competitor systems using a deep neural network and then  this was a\nsuccess \n\f\n\n \nso  this was an handwriting recognition and then  there was speech  so  this shown that\nvarious   existing   systems    the   error   rate   of   these   system   could   be   seriously  be\nsignificantly   reduced   by   using   deep   neural   networks   or   plugging   in   a   deep   neural\nnetwork component to existing systems  right  so  this was handwriting and then  speech \n\n\nthen again some kind of pattern recognition which was on handwritten digit recognition\nfor mnist  this is a very popular data set which had been around since ninety eight and a new\nrecord was set on this data  so  this is the highest accuracy that was achieved on this data\n \n\fset around that time in two thousand and ten  sorry and this is also the time when gpus entered the same \nso  before that all of the stuff was being done on cpus   but  then people realised that\nvery deep neural networks require a lot of computation and lot of this computation can\nhappen very quickly on gpus as opposed to cpus  \nso  people started using gpus for training and that drastically reduced the training and\ninference   time   so   that   was   again   something   which   sparked   a   lot   of   interest    right\nbecause even though these were successful  they were taking a lot of time to train  but\nnow the gpus could even take care of that and this success continued  \n\n\nso   people   started   gaining   or   getting   success   in   other   fields   like   visual   pattern\nrecognition  so  this was a competition on recognising traffic sign boards and here again\na deep neural network did way better than its other competitors  \n \n\f\n\n \nand then  the most popular or one thing which made neural networks really popular was\nthis image net challenge which was around since two thousand and eight or two thousand and nine and before two thousand and twelve when\nthis  alexnet   was   one   of   the   participating   systems   in   this   competition    most   of   the\nsystems   were   non   neural   network   based   systems   and   this   competition   was   basically\nabout classifying a given image into one of thousand classes \nso  this could be an image of a bird or a dog or a human or car  truck and so on say you\nhave to identify the right class of the main object in the image  so  in two thousand and twelve this alexnet\nwhich was a deep neural network or a convolutional neural network based system was\nable to actually outperform all the other systems by a margin of sixty seven percent  so  the error\nfor this system was sixteen percent and this is a deep neural network because it had eight layers  \nthe next year this was improved further and something known as zf network propose\nwhich was again eight layers  but it did better than alexnet   the next year even  a  deeper\nnetwork with nineteen layers was proposed which did significantly better than alexnet  then \ngoogle entered the scene and they proposed something which is twenty two layers and again\nreduced the error  then microsoft joined in and they proposed something which had one hundred and fifty two\nlayers and the error that you see here is actually better than what humans do  \nso  even if a human was asked to label this image because of certain law  certain noise in\nthe image and so on  even a human is bound to make more errors than three six per cent  that\nmeans  even if you show hundred images to humans  he or she is bound to may go wrong\n\for more than three or four of these images right there is this system was able to get an\nerror of three six per cent over the large test set  \nso  this two thousand and twelve to two thousand and sixteen period were there was this continuous success on the image net\nchallenge   as   well   as   successes   in   other   fields   like   natural   language   processing \nhandwriting recognition  speech and so on  so  this is the period where now everyone\nstarted talking about deep learning and lot of company started investing in it   a  lot of\ntraditional systems which were not deep neural network based was now started  people\nstarted converting them to deep neural network based system  \nso  translation system  speed systems  image classification  object detection and so on \nthere   were   lot   of   success   in   all   these   fields   using   deep   neural   networks   and   this\nparticular thing that we are talking about which is image net and the success in this was\ndriven by something known as convolutional neural networks  \n\f"}
{"audio_filepath": "lec001_004.wav", "duration": 201.812, "text": "\n\ni will talk about the history of convolutional neural networks  and i call this part of\nhistory as cats and it will become obvious why i call it so \n\n\nso  around one thousand  nine hundred and fifty nine hubel and wiesel did this famous experiment they are still i think you\ncould see some videos of it on youtube  where there is this cat and there was a screen in\nfront of it and on the screen there were these lines being displayed at different locations\nand in different orientations  so   slanted  horizontal  vertical and so on and there are\nsome electrodes fitted to the cat and they were measuring trying to measure that which\nparts of brain actually respond to different visual stimuli \nlet us say if you show it stimulus at a certain location  does the different part of the brain\nfire and so on  so  and one of the things of outcomes of the study was that  that different\nneurons in brain fire to only different types of stimuli  it is not that all neurons in brain\nalways fire to any kind of visual stimuli that you give to them  \n\f\n\nso  this is essentially  roughly the idea behind convolutional  neural networks starting\nfrom something known as neocognitron  which was proposed way back in one thousand  nine hundred and eighty  you\ncould think of it as a very primitive convolutional neural network  i am sure that most of\nyou have now read about or heard about convolutional neural networks  but something\nvery similar to it was proposed way back in one thousand  nine hundred and eighty \n\n\nand what we know as the modern convolutional neural networks maybe i think yan li\nkun is someone who proposed them way back in one thousand  nine hundred and eighty nine  and he was interested in using\n\fthem for the task of handwritten digit recognition and this was again in the context of\npostal delivery services  so  lot of pin codes get written or phone numbers get written on\nthe postcards and there was a requirement to read them automatically  so  that they can\nbe the letters or postcards can be separated into different categories according to the\npostcard according to the postal code and so on right so or the pin code \nso  that is where this interest was there and one thousand  nine hundred and eighty nine was when this convolutional neural\nnetworks were first proposed or used for this task \n\n\nand then over the years  several improvements were done to that  and in one thousand  nine hundred and ninety eight this now\nhow   famous   data   set   the   mnist   data   set   which   is   used   for   teaching   deep   neural\nnetworks   courses  or  even  for initial  experiments  with  various   neural  network  based\nnetworks  this is one of the popular data sets  which is used in this field and this was\nagain released way back in one thousand  nine hundred and ninety eight and even today even for my course i use it for various\nassignments and so on \n\f\n\nso  it is interesting that an algorithm which was inspired by an experiment on cats is \ntoday used to detect cats in videos of course  among other various other things is just i\nam just jokingly saying this \n\f"}
{"audio_filepath": "lec001_005.wav", "duration": 157.496, "text": "\n\nso  this is what the progression was right that  in two thousand and six people started or the study by\nhinton and others led to the survival  and then people started realizing the deep neural\nnetworks and actually we use for lot of practical applications and actually beat a lot of\nexisting systems \nbut there are still some problems and we still need to make the system more robust \nfaster and even scale higher accuracies and so on \n\n\nso  in parallelly while there was lot of success happening from two thousand and twelve to  two thousand and sixteen  or even\ntwo thousand and ten to two thousand and sixteen  in parallel there will also a lot of research to find better optimization\nalgorithms which could lead to better convergence  better accuracies \nand again some of the older ideas which were proposed way back in one thousand  nine hundred and eighty three  now this is\nagain something that we will do in the course  so  most of the things that i am talking\nabout we are going to cover in the course  so  we are going to talk about the imagenet\n\fchallenge  we are going to talk about all those networks the winning networks that i had\nlisted there alex net  zf net  google net and so on \nwe are going to talk about nesterov gradient descent which is listed on the slide  and\nmany other better optimization methods which were proposed starting from two thousand and eleven  so \nthere was this parallel resource happening while people were getting a lot of success\nusing traditional neural networks  they are also interested in making them better and\nrobust and lead for lead to faster convergence and better accuracies and so on \nso  this led to a lot of interest in coming up with better optimization algorithms  and\nthere was a series of these proposed starting from two thousand and eleven  so  adagrad is again something\nthat we will do in the course  rms prop  adam eve and many more  so  many new\nalgorithms i have been proposed  and in parallel a lot of other regularization techniques\nor   weight   initialization   strategies   have   also   been   proposed   for   example    batch\nnormalization  or xavier initialization  and so on   so   these are all things which were\naimed at making neural networks perform even better or faster and even reach better\nsolutions or better accuracies and so on  this all that we are going to see in the course at\nsome point or the other \n\f"}
{"audio_filepath": "lec001_006.wav", "duration": 385.204, "text": "\nso  i  was talking  about successes in image   speech  pattern recognition  even natural\nlanguage processing and so on  so  one interesting thing here is about sequences  so  i\nwill talk about sequences now  \n\n\nsequences are everywhere when you are dealing with data   so   you have time series\nwhich is like say the stock market trends or any other kind of a series  time series  then\nyou have speech which is again a series of phonemes or you have music  you have text\nwhich is a series of words  you could even have videos which are the series of images \nright  one frame  each image  each frame can be considered to be an image and so on \nso  in speech data one peculiar characteristic of speech data is that every unit in the\nsequence interacts with other units  so  words on their own may not mean much   but\nwhen you put them together into a sentence  they all interact with each other and give\nmeaning to the sentence  right and the same can be said about music or speech or any\nkind of sequence data  so  all these elements of the sequence actually interact with each\nother  \n\fso  there was a need for models to capture this interaction and this is very important for\nnatural   language   processing   because   in   natural   language   processing    you   deal   with\nsequence of words or all your texts or sentences or documents or all sequences of words \nso  that is very important and the same in the case of speech also  \nso  if you take up any deep learning paper  nowadays it is very likely that you will come\nacross the term recurrent neural network or lstms which are long short term memory\ncells and so on \n\n\nso  this is also something which was proposed way back in one thousand  nine hundred and eighty six  \n \n\f\n\n \nso  a recurrent neural network is something which allows you to capture the interactions\nbetween the elements of your sequence  i had said at a very layman level  but of course \nyou are going to see this  in much more detail  in the course  and this  was  also not\nsomething new even though you hear about it a lot in the past  three  to four years  the first\nrecurrent neural network and what you see here is exactly a very similar to what we are\ngoing to cover in the course was proposed way back in jordan by jordan in one thousand  nine hundred and eighty six  \n\n\n \n\fits variant was proposed by elmen in one thousand  nine hundred and ninety  so  this is again not a very new idea  this\nhas existed for some time  but now there are various factors because of which it has been\npossible to now start using them for a lot of practical applications   as i said one  you\nhave a lot of compute time and the other you have a lot of data and the third is now the\ntraining has stabilized a lot because of these advances which i was talking about in terms\nof better optimization algorithms  better regularization  better weight initialization and so\non  \nso   it has become very easy to train these networks for real world problems at a large\nscale  so  that is why they have become very popular and hear about them on a regular\nbasis  but it is again something which was done way back  \n\n\nso  from one thousand  nine hundred and ninety nine to one thousand  nine hundred and ninety four  actually people also looking at various problems will be training\nneural networks and recurrent neural networks  and so that this problem which is known\nas exploding and the vanishing gradient problem which is again something that we will\nsee in the course in reasonable detail   we have this problem and it is very difficult to\ntrain   recurrent   neural   networks   for   longer   sequences    so    if   you   have   a   very   long\nsequence or a time series  you cannot really train a recurrent neural network to learn\nsomething from that  \n\f\n\n \nand to overcome these problems around one thousand  nine hundred and ninety seven   long short term memory cells were\nproposed and this is again something that we will cover in the course and this is now\nalmost de facto standard used for training for a lot of nlp work  lstm are used as one\nof   the   building   blocks   and   another   variants   of   lstms   which   are   known   as   gated\nrecurrent units and some other variants  \nso    this   is   also   not   something   new   even   though   they   have   become   very   popular\nnowadays like almost any article that you pick about to talk about  any article on deep\nlearning   that  pick  about   to talk   about  recurrent  neural   networks  or  lstms   or gated\nrecurrent units  this is not something which is new  \n\f\n\n \nlstms had come way back in one thousand  nine hundred and ninety seven   but again due to various compute and other issues\nwhich i said at that time  it is not  so  easy to use them   but  by two thousand and fourteen  because of these\nparallel progresses which i mentioned in terms of optimization regularization and so on \npeople are now able to use rnns lstms for large scale sequence to sequence problems\nand in particular a very important discovery at this time are very important model which\nwas proposed at this time which is attention mechanism which is used in a lot of deep\nneural   networks   nowadays   which   enabled   to   deal   with   a   lot   of   sequence   prediction\nproblems  \nfor example  translation where you have given one sequence in one language and you\nwant to generate the equivalent sequence in another language  so  this is known as a\nsequence to sequence translation problem   so   for that people proposed a sequence to\nsequence attention network and this was one of the key discoveries which then led to a\nlot of adaptation of or  adoption of deep neural networks for nlp  \na lot of research in nlp happened which was then driven by deep neutral networks  so \na lot of existing algorithms which are non neural network based algorithms which are\ntraditionally  used  for nlp was  slowly  replaced  by  these  deep  neural  network  based\nalgorithms  ok \n\f\n\nand   again   this   idea   of   attention   itself   is   something   that   was   explored   earlier   also\nsomewhere around one thousand  nine hundred and ninety one or so and it was something known as reinforcement learning\nwhich was used for learning this attention mechanism  what attention basically tells you\nis that if you have a large sequence and if you want to do something with this sequence \nwhat are the important entities of this sequence or elements of this sequence that you\nneed to focus on  so  this is again something that we will look at in detail in the course   \n\f"}
{"audio_filepath": "lec001_007.wav", "duration": 112.983, "text": "\nnow  since i mentioned  rl  so  we will go on to the next chapter  which was  now\nbecoming much more ambitious with what you can do with deep learning and people\nstarted beating humans at their own game quite literally \n\n\nso  there was this starting with atari games in two thousand and fifteen   where  resources from deep mind\nshow that you could train a deep neural network to play atari games and do much better\nthan what humans do  so  that is something that they were able to show on atari games\nand then  people started looking at other game \n\f\n\nso   then there was this go and this popular tournament and which  alphago which is\ndeep reinforcement learning based agent was actually able to beat the reigning champion \nat that time one of the best players of go at that time \n\n\nthen    even   at   poker   were   something   known   as   deepstack   which   is   again   a   deep\nreinforcement learning based agent which is able to beat eleven professional poker players at\nthis game \n\f\n\nthen  other games like defense of the ancients since on  which is a much more complex\nstrategy based game where again deep reinforcement learning based agents have shown a\nlot of success in beating top professional players on this game \n \n\f"}
{"audio_filepath": "lec001_008.wav", "duration": 284.462, "text": "\nso  this was all happening where deep learning now started showing a lot of promise in a \nlot of fields nlp  vision  speech and again this deep reinforcement learning and so on  \nwhich led to this complete madness starting from two thousand and thirteen \n\n\nwell almost for every application the traditional methods were then overwritten or kind\nof beaten by deep neural network based system  so  something like language modelling \nwhich has been around since probably one thousand  nine hundred and fiftys or so \n  now   the   reining   algorithm   or   the   better   algorithm   for   language   modelling   is   now\nsomething which is based on deep neural networks \n\f\n\nthen similarly for speech recognition  lot of work  a lot of probabilistic  lot of work\nbased   on  probabilistic   models   was   done   in   this   or   in   the   speech   area   or  the   speech\nliterature for the past thirty  forty years  and now all of that has been overcome by deep neural\nnetwork based solutions \n\n\nsame for machine translation  a lot of interest in this field  a lot of companies now have\ntheir   machine   translation   systems   based   on   deep   neural   networks   as   opposed   to   the\n\fearlier phrase based statistical machine translations or the probabilistic models  which\nwere used earlier \n\n\nsimilarly  for conversation modelling dialogue  a lot of new work started in dialogue post\na deep learning era  where people now realize that if you have a lot of sequences of\nconversations  you could actually try to train a deep neural network to learn from this\nsequence and have conversations  with humans  of course  you are nowhere close to\nhuman level conversations  we are very very far off from them  but in limited domains\nthese bots are showing some success now \n\f\n\nsame for question answering where you are given a question and you want to answer it \neither from a knowledge graph or from a document or from a image and so on \n\n\nand in the field of computer vision things like object detection  most of the reigning\nsystems   or   the   best   performing   systems    nowadays   are   deep   neural   network   based\nsystems  a lot of advances are being made on these systems over in the last few years \n\f\n\nsame for visual tracking where you want to track the same person in a video or image\ncaptioning  where you want to generate captions for images  for example  people upload\na lot of images on facebook \n\n\nand if you want to automatically caption them or imagine you are on a reselling site\nright  something like olx where you upload your furniture  and you do not provide a\ndescription from that  but can the machine already automatically generate a description\nfor it  so  it is easier for the human to read what that product is and so on \n\f\n\nso  similarly video captioning  i given a video anyone to caption the main activity which\nis happening in that video  all of these problems are being solved using deep learning\nbased   solutions    using   a   combination   of   something   known   as   feed   forward   neural\nnetworks or convolutional neural networks or recurrent neural networks and so on \n\n\nvisual  question answering  you are given an image  and a question and you want to\nanswer that question \n\f\n\nvideo question answering  answering questions from videos \n\n\nvideo summarizations  if you are given a large video and you want to generate a trailer  a\nsort of a trailer for that video contains  which kind is the most important frame for that\nvideo  even these systems are based on deep learning \n\f\n\nthen this  was all about classification  recognition  and so on   but  now  people  started\ngetting more ambitious that can  we humans are very good at creativity  so  can we use\nmachines   to   be   creative   right   to   generate   images   so    now   if   i   have   seen   a   lot   of\ncelebrity faces   can i generate new celebrity faces or if i have seen a lot of bedroom\nimages \nand i am if a fireman architect  now can i generate new bedroom images can i  can we\ntrain a machine to generate new bed bedroom images  so  a lot of phenomenal progress\nor work has happened in this field in the last four five years  starting with things like\ngenerative adversarial networks  variational autoencoders and so on \nand   people   are   now   starting   to   seriously   invest   into   creativity   that   how   to   make\nmachines creative  again we are far off from where the desired output  but there is still\nsignificant progress happening in this field generating audio \n\f\n\nso  that was about generating images  you can generate music also \n\n\nand this is again about generating images and so on \n\f"}
{"audio_filepath": "lec001_009.wav", "duration": 275.151, "text": "\nso  lot of fields have adopted deep learning now and lot of state of the art ai systems are \nbased on deep neural networks   but  now what is needed is after all thi s madness were \ndeep learning has taken over a lot of research areas  can we now bring in some sanity to \nthe proceeding  so  this is really a need for sanity \n\n\nand why i say that is that because there is this paradox of deep learning  so  there is this\ninteresting question that why does deep learning works so well  despite having a high\ncapacity \nso  the deep neural networks have a very high capacity which means that susceptible to\nover fitting  so  most of you would have done some course on machine learning   so \nthere you know that over fitting is bad because you are just memorizing the training data\nand then  you might not be able to do so well and at tested and over fitting happens when\nyour   model   has   a   high   capacity   so   even   though   deep   neural   networks   have   high\ncapacity  why are they doing so well  we will focus on this high capacity  but when we\n\ftalk about the universal approximation theorem and give some arguments for why deep\nneural networks have such a high capacity \nthe other thing is they have this numerical  instability  right   so   we spoke about these\nvanishing and exploding  gradients  and again  we will talk about  this  later  on in the\ncourse  so  despite this training difficulties why is it that deep neural networks performs\nso well and of course  they have this sharp minima which is again it could lead to over\nfitting    so   if   you   look   at   there   is   an  optimization   problem    it   is   not   a  neat   convex\noptimization problem  so  it is a non convex optimization problem  so  why does it still\ndo so well \nso  it is also not very robust  so  here is an example on the right hand side the figure that\nyou show  so  the first figure is actually of a panda and the machine is able to detect this\npanda with some fifty seven percent confidence  right  we have  trained a machine for a lot of\nanimal images  we have shown it a lot of animal images  at test time we show at this\nimage  the first image that you see on the right hand side and is able to classify this is a\npanda with fifty seven percent confidence  but now what i do is i add some very random noise \nso  that second image that you see with some very random pixels if i add it to this image \ni will get a new image \nso  every pixel in this image is added to this new noise image and i get the image which\nis see on the third  the third image that you see right to you and me or to any average\nhuman  this still looks like a panda  there is hardly any difference between this image\nand the original image  but now if you pass this to the machine  all of a sudden instead of\nrecognizing this is a panda  it starts to recognize it as a gibbon and that too with ninety nine\npercent confidence  so  why is it that they are not very robust and despite this not being\nvery robust  why are deep neural networks  so  successful  so   people are interested in\nthese questions and people have started asking these questions \nthere are no clear answers yet  but slowly and steadily there is an increasing emphasis\non explainability and theoretical justifications  so  it is not enough to say that your deep\nneural network works and gives you ninety nine percent accuracy   it is  also good to have an\nexplanation for why that happens is it that some components of the networks are really\nable to discriminate between certain patterns and so on  so  what is going on inside the\n\fnetwork which is actually making it work so well  right and hopefully this will bring in\nsome sanity to the proceedings \nso  instead of just saying that i apply deep learning to problem x and got  ninety percent\nsuccess  we will also make some kind of more sane arguments just to why this works and\nwhat   is   the  further   promise  of  this   and  thinks  like  that   so   that  is  roughly   a  quick\nhistorical recap of where deep learning started and where it is today  starting all the way\nback from advances in biology in one thousand  eight hundred and seventy one to recent advances till two thousand and seventeen and so on deep\nlearning  right and here are few url \n\n\nso  you could take a look at this for a lot of interesting applications of recurrent neural\nnetworks \n\f\n\nbunch of start ups which have come up in this space is working on very varied and\ninteresting problems  and here are all the references that i have used for this particular\n \npresentation \n\n\nso  that is where we end lecture one and i will see you again soon for lecture two \n\f"}
{"audio_filepath": "lec001_010.wav", "duration": 441.605, "text": "\nalgorithm and convergence  multilayer perceptrons \n  representation\npower of mlps\nso  welcome to lecture two of cs seven thousand and fifteen which is the course on deep learning  so  we will\ntalk  about mcculloch pitts  neuron  thresholding logic  perceptrons  and a learning\nalgorithm for perceptrons and talk about the convergence of this algorithm  and then we\nwill talk about multilayer network of perceptrons and finally  the representation power\nof perceptrons \nso   let  us   start  module  one   which   is   on  biological  neurons   so   remember  during   the\nhistory we had started all the way back  in the one thousand  eight hundred and eightys when we spoke about biological\nneurons   so   we will just start there spend a few minutes on it and then go on to the\ncomputational models which is mcculloch pitts neuron \n\n\nso  now this is a course on deep learning   so   we are going to talk about deep neural\nnetworks now the most fundamental unit of a deep neural network is something known\nas an artificial neuron \n\fand the question is  why is it called a neuron  where does the inspiration come from  so \nwe already know that the inspiration comes from biology and more specifically it comes\nfrom the brain  because we saw that way back in the one thousand  eight hundred and ninetys  this term neuron was coined\nfor neural processing units or the cells in our brain \nso  now before we move on to the computational neurons or the artificial neurons  we\nwill just see the biological neurons in a bit more detail and then we will move on from\nthere \n\n\nso  this  is  what a typical  biological  neuron looks  like   so   here actually  there are two\nneurons   this portion is called the dendrite  so it is used to receive inputs from all the\nother neurons \nso  that is the place where the input comes in  then remember we said that in one thousand  nine hundred and fiftys we\ndiscovered that these neurons are actually discrete cells and there is something which\nconnects them  so  that connection is called a synapse and it decides the strength of the\nconnection between these two neurons  so  there is an input  there is some strength to the\nconnection \nthen once this neuron receives inputs from various other neurons  it starts processing it \nso that is the central processing unit which is called the soma  and once it is done this\nprocessing it will  it is ready to send its output to other set of neurons  so  that output is\n\fcarried on by the axon  so  we have inputs  we have some weights attached to the input \nwe have some processing and then an output  so  that is what a typical biological neuron\nlooks like \n\n\nand let us see a very cartoonish illustration of how this works right  how the neuron\nworks   so   our sense organs interact with the outside world and then they pass on this\ninformation  to the neuron and then the neuron decides whether i need to take some\naction  in this case the action could be whether i it should laugh or not right  whether the\ninput is really funny enough to evoke laughter   so   if that happens this is known as\nsomething that the neuron has fired \n\f\n\nnow  of course  in reality it is not just a single neuron which does all this   there is a\nmassively parallel interconnected network of neurons  so  you see a massive network\nhere   now the neurons in the lower level site  so these neurons  they actually interact\nwith the sensory organs  they do some processing based on the inputs   so  they decide\nwhether i should fire or not \nand if they fire they transmit this information to the next set of neurons and this process\ncontinues till the information is relayed all the way back and then finally  you decide\nwhether you need to take any action or not  again in which this case it should be laughter \nso  that is how it works  and when i say massively parallel interconnected network i\nreally mean it  because there are ten raise to eleven which is roughly one hundred billion neurons\nin the brain \n\f\n\nnow  this massively parallel network also ensures that there is some division of work \nnow what do you mean by that is not that every neuron is responsible for taking care of\nwhether i should laugh or not or not every neuron is responsible for processing visual\ndata  some neurons may possess visual data  some neurons may possess speeds data and\nso on  so  there is this division of work  every neuron has a certain role to play  so for\nexample  in this cartoonish example that we took \n\n\n\fso  there might be this one neuron which fires if the visuals are funny right whatever you\nare seeing is funny  there will be one neuron which finds sheldons speech to be funny \nthe way he speaks   so  that might be funny and there might be another neuron which\nactually   finds   the   dialogue   content   to   be   funny   and   now   all   of   this   pass   on   the\ninformation to the next level and this guy would fire if at least two of these three inputs are\nfunny  so  that means  i have some threshold based on which i decide whether to react or\nnot  if it is really funny then only i laugh it  otherwise i will not laugh \n\n\nso  the  neurons  in the brain  as  was  obvious  in the previous  slide are  arranged  in a\nhierarchy and i will take a more realistic example  where we look at the visual cortex \nso    is   this   is   the   portion   of   the   brain    which   is   responsible   for   processing   visual\ninformation right  so  as you see here you have our retina from where the information\nstarts flowing  and it goes through various levels \nso  you see  you follow the arrows and you will see there are several levels  there is one\nlevel here  then another here another here and so on right  so  it is again as i was trying\nto illustrate in that cartoon the information is relayed through multiple layers  and then it\ngoes all the way back to the spinal cord which decides that  in this case i need to move\nthe muscle right \nso    that   is   what   is   being   decided   here   right    so    the   information   flows   through   a\nhierarchy of layers  and in this particular case i am going to focus on these three circled\n\flayers which are vone  vtwo and ait right  so  these actually form a hierarchy and let us see\nwhat this hierarchy does right \nso  at layer one you detect edges and corners  so  i am looking at you all  i just see some\ndots and some shapes  so that is what layer one recognizes  i just recognize some edges and\nsome dots and so on \n\n\nnow  layer two tries to group all of these together and come up with some meaningful\nfeature groups right  so  it realizes oh these two edges actually form the nose  these two\ndots actually form the eyes and these two edges actually form the mouth right  so  that is\nslightly higher level of processing that it is doing and then layer three further collects all this\nand leads to higher level objects right \nso  now it is realizing all these things put together is actually a human face right  so  you\nadd edges and circles or dots   then you had some feature groups and then the feature\ngroups combine into objects right  so  that is how this hierarchy processes \n\f\n\nso  here is a disclaimer  i understand very little about how the human brain works right\nand what you saw is a very oversimplified explanation of how the brain works right \nwhat i told you is there is an input a layer of networks which does a network  which has\nmany layers which does some processing and then you have an output right  that is the\nvery simplistic view that i gave you  this is an oversimplified version  but this version\nsuffices for everything that we need for this course right  this is not a biology or a neural\nprocessing course right  so  it is enough for this course   so   that is where we will end\nmodule one \n\f"}
{"audio_filepath": "lec001_011.wav", "duration": 808.689, "text": "\nmcculloch pitts neuron\nlet us start with module two  which is about mcculloch pitts neuron  \n\n\n \nso  as we are done this during the history lecture way back in one thousand  nine hundred and forty three  mcculloch and pitts \nthey proposed highly simplified computational model of the brain  so  now let us see\nwhat\u2019s the motivation  we know that our brain is capable of very complex processing \nit\u2019s capable of taking a lot of inputs from various sources and then  help us taking various\ndecisions and actions  now  what if you want a computer to do this  we want a module\nwhich is very similar to how the brain works or at least how we think the brain works\nwhich takes a lot of inputs and then does some processing and helps us take a decision  \nso  what they proposed is this model which will take a lot of inputs and these inputs are\nall binary  all these inputs that you see here  these inputs are fed to this mcculloch pitts\nneuron  which is an artificial neuron and it is divided into two parts   so   the first part\ncollects all the input   so   remember you had these dendrites which were taking all the\ninformation   from   everywhere   so   this   just   collects   all   the   information   and   then   the\n\fsecond part is aggregation   i  have collected a lot of information from all the sources \nnow  the second function will decide what this aggregation is and based on that it will\ntake a decision whether to fire or not  \nso  the output is again boolean  if it\u2019s zero  then neuron does not fire  if it\u2019s one  the neuron\nfires   so   let us take a concrete example  so  suppose  i  am trying to make a decision\nwhether i should watch a movie or not  so  xone could be is the genre of the movie thriller \nsimilarly  there could be another  variable say xn which says is the actor  matt damon \nso  these are all various such factors that i could take is the director christopher nolan \nthe music given by someone and so on  so  all these are probably factors which help me\ndecide whether i want to watch this movie or not and you want this neuron to help us\nmake that decision \n\n\n \nso  now  what is  happening here is  these all inputs  they can be either  excitatory  or\ninhibitory  so  let me tell you what inhibitory is first  so  you are taking input from a lot\nof sources  now  see one of these sources or one of these inputs is am i ill today  am i\ndown   with   fever    so   if   that   input   is   on   irrespective   of   who   the   actor    director   or\nwhatever is  i am not going to watch the movie right because i just cannot leave from my\nbed  so  these are known as inhibitory inputs irrespective of what else is on in your input\nfeatures  if this input is on  your output is always going to be zero  that means  the neuron\nis never going to fire  so  you could think of it as suppose my mood is not good today  i\n\fdo  not  feel  like  getting  up  or  if  i  injured  my  leg  or anything  right   if  any  of these\nconditions is on irrespective of what the other factors are  i am not going to watch the\nmovie  \nso  that is an inhibitory input and excitatory input are on the other hand is not something\nwhich will cause the neuron to fire on its own  but it combine with all the other inputs\nthat you have seen could cause the neuron to fire and how  so  this is how  so  these are\nall the inputs that your neuron is taking  all i am going to do is i am going to take a sum\nof these  \ni am going to take aggregation of all of these  so  what does this count actually give me\nthe number of inputs which are on  the number of inputs which are value one  that is all \nthis aggregate  this is a sum of all the one s in my input \nnow  this is what g does  this is a very simple function is taking a sum of my inputs \nnow  the function y takes this as the input  that means  it takes this sum as the input and\nif the sum is greater than a certain threshold  then it fires   if the sum is less than the\ncertain threshold  then it does not fire  so  again see what is happening here is it is same\nas now if you depend on the actor  director  and genre and so on and you fine  at least\ntwo of these three conditions are satisfied  at least i am happy with the actor and the\ndirector even though the genre is not something that i care about  \ni will watch the movie or you might be a very niche go movie watcher who only goes to\na movie if the actor matches your requirement  the director matches your requirement\nand  the  genre  and  the  music  and  everything  matches  your  requirement   so   you are\nthreshold  in that case  it should be high   so   this is how it is going to help you make\ndecisions  now  again a very simplified model and this is theta is called the thresholding\nparameter that is the value which decides whether the neuron is going to fire or not and\nthis over all thing is known as the thresholding logic  so  this is what a mcculloch pitts\nneuron looks like  \n\f\n\nnow  let us implement some boolean functions using this mp neuron  so  from now on i\nwill just called it mp neuron and we will try to implement some boolean functions using\nit  so  now  why are we interested in boolean functions  it is because we have overly\nsimplified the way we take decisions  we are saying that the way we take decisions is we\ntake a lot of boolean inputs is actor matt damon and genre thriller and so on and based\non that we produce a boolean output  \nso  an input is all booleans  so  we have xone to xn which are all booleans and your output\nis also boolean  so  that is a boolean function that you are trying to learn from x to y  is\nthat clear  you have  x  just happens to contain n different variables here  ok and lot of\ndecision problems you could cast in this framework  you can just imagine right whether\nto come for lecture  today or not   again is you could cast in it depending on various\nboolean inputs  \n\f\n\nthis is a very concise representation of the  mcculloch pitts neuron   what it says is it\ntakes a few boolean inputs and it has certain threshold  if the sum of these inputs crosses\nthis  threshold  then the neuron will fire otherwise it will not fire  that is the simple\nrepresentation of the m p neuron  now  suppose i am trying to learn the and function \nwhen would the and function fire \nall the inputs are on  so  what should be the value of the threshold in this case \nthree  everyone agrees  what about the or function \none  let us see a few more this function  so  let me tell you what this function is  so \nyou see this circle here  so  that means that this input is an inhibitory input  if that is on \nthen the neuron is not going to fire   that is how i am representing it   so  now  tell me\nwhat should the threshold for this be  it is not so hard  \nsee if xtwo is on  it is not going to fire  so  you have four rows  zero zero zero one one zero one one  so  two of\nthose are ruled out and it is not going to fire  now  out of the remaining two  when do\nyou wanted to fire \nso  what should be the threshold \none   now  what about this function  zero or three  three  is not even a valid option   zero   everyone\nagrees to that  and what about this  zero  so  you get this  so  now if you have a certain\n\fnumber of input variables and the function that you are trying to model the decision that\nyou are trying to make is a boolean function  then you could represent using these mp\nneurons whether all boolean functions can be represented in this way or not that is still\nnot clear  i am just showed you some good examples  we will come to the bad examples\nlater on  here is the question  \n\n\nso  can any boolean function be represented using a mcculloch pitts neuron  so  before\nanswering this  question  we will see a bit  of a geometric  interpretation  of what mp\nneuron is actually trying to do \n\f\n\nso   let us take or function where you have two inputs xone and xtwo and this neuron is\ngoing to fire \n if xone plus xtwo is greater than equal to one  that is clear  that is how the definition is  now \nif you look at this  xone plus xtwo greater than equal to one  now  let us ignore the greater than\npart first  so  we will just talk about xone plus xtwo equal to one  what is this equation of a \nline everyone gets that  ok  now  in this case since we are dealing with boolean inputs\nand we have two access xone and xtwo  how many input points can we have  four  right zero zero zero one\none zero one one  \n\f\n\nso  you could have these four points  so  just note that this is an xone and xtwo axis  but only\nfour inputs are valid here  this is not a real numbered access  this is only boolean inputs\npossible here \nnow  what is the line xone plus xtwo equal to one tell you which line is that  \nso  one which passes through one  zero here and zero  one here  this is that line  now  what do we\nwant that for all those inputs for which the output is actually one  they should lie on the line\nor on the positive side on the line  and all those inputs for which the output is zero  they\nshould lie on the other side of the line  is that happening  so  what is actually mp in unit\nactually learning linear decision boundary  it just what it is doing in effect is actually it is\ndividing the input points into two halves such that all the points lying on that line right\nare  sorry all the points for which the input should be zero lie below this line and all the\npoints for which the output should be one  sorry in both cases  it should have been output  \nso  let me just repeat it  all the points for which the output is zero lie below this line and all\nthe points for which the output is one either lie on this line or above the line  is that fine \nand   so    let   us   convince   ourselves   about   this   even   it   is   not   already   clear   from   the\nequation  for how many of you it is already cleared from the equation that this is exactly\n\fwhat it does for a large number of periods  but still we will just do a few examples and\nmove ahead  \n\n\nnow  for the and function what is the decision boundary  it is xone plus xtwo  no  that is\nthe decision boundary \n \nequal to two  so  again i have these four points  only these four points are possible  now \nwhere is my decision line \npassing through that one  one and intercepting this somewhere around two  zero and this around zero \ntwo  so  that is the line which i am interested in  now  again do you see that our condition\nis satisfied that all the inputs for which we want the output to be one are on or above the\nline and all the inputs for which we want the output to be zero or below the line  now  what\nabout this function  what is the threshold \nzero  so  what would the line be  xone plus xtwo equal to zero which passes through the origin \nright and again all the points are either on or above the line  so  this part we are going to\ncall as a positive half space and this we are going to call as the negative half space  \n\f\n\nnow  what if we have more than two inputs  in a two dimensional case  when we just\nhad xone and xtwo we are trying to find a separating line in the three dimension case  what\n \nwill we do \nplane  in the higher dimensions \nhyper plane  so  this is now your three dimensional case  again there are three axis here \nbut not all points are possible  how many points are possible  eight points and which is the\nfunction that we are trying to implement  \nor   so  for these eight out of these eight points  for how many is the output one \nseven and for one  it is zero  so  what is the kind of plane that we are looking at  we are looking\nfor a plane such that seven points lie on or above it and one point lies below it and which is that\npoint  \nzero  so now what is the equation of that hyper plane  x one plus x two plus x three is equal to one \nyou see this  so  you see that all the seven points are visible  but the points zero  zero is not\nvisible because it is on the other side of the plane  so  this is doable in three dimensions\nalso and again in higher dimensions also  right we could find in hyper plane  \n\f\n\nso    the   story   so   far   is   that   single   mcculloch   pitts   neuron   can   be   used   to   represent\nboolean functions which are linearly separable  so  a linearly separable function is such\nthat there exists a line such that for that function whichever points produce an output of one\nlie on one side of the line and whichever points produce an output zero lie on the other side\nof the line  \n\f"}
{"audio_filepath": "lec001_012.wav", "duration": 649.087, "text": "\nperceptron\nnow  let us go to the next module which is perceptron  \n\n\n \nso  far the story has been about boolean input  but are all problems that we deal with  we\nare only dealing with  do we always only deal with boolean inputs  so  yeah so  what\nwe spoke about is boolean functions  now  consider this example  this worked fine for a\nmovie example where we had these as actor so much and his director and so on  but now\nconsider the example where you are trying to decide  you are in oil mining company and\nyou are trying to decide whether you should mine or drill at a particular station or not  \nnow  this could depend on various factors like what is the pressure on the surface  on the\nocean surface at that point  what is the salinity of the water at that point  what is the\naquatic  marina  aquatic  life at that point and so on  so  these are not really boolean\nfunction  the salinity is a real number  density would be a real number  pressure would\nbe a real number and so on  right and this is a very valid decision problem  companies\n\fwould be interested in doing this  so  in such cases our inputs are going to be real  but so\nfar mcculloch pitts neuron only deals with boolean inputs  so  we still need to take care\nof that limitation  \nnow  how did we decide the threshold in all these cases  i just asked you  you computed\nit and you told me right   but  that is not going to work out  i mean it does not scale to\nlarger problems where you have many more dimensions and the inputs are not boolean\nand so on  so  we need a way of learning this threshold  \nnow  again returning to the movie example  maybe for me the actor is the only thing that\nmatters and all the other inputs are not so important   then  what do i need actually   i\nneed some way of weighing these inputs  i should be able to say that this input is more\nimportant than the others  now  i am treating all of them equal  i am just taking a simple\nsum  \nif that sum causes a threshold  i am fine otherwise i am not fine  but maybe i want to\nraise the weight for some of these inputs or lower the weight for some of these inputs \nso  whether it is raining outside or not maybe does not matter  i have a car  i could go or\ni could wear a jacket or an umbrella or something  so  that input is probably not so\nimportant \nand what about functions which are not linearly separable  we have just been dealing\nwith the goody stuff which is all linearly separable   but  we will see that even in the\nrestricted boolean case  there could be some functions which are not linearly separable\nand if that is the case  how do we deal with it  so  these are some questions that we need\nto answer \n\f\n\n \nso  first we will start with perceptron which tries to fix some of these things and then  we\nwill move forward from there  so  as we had discussed in the history lecture that this was\nproposed in one thousand  nine hundred and fifty eight by frank rosenblatt and this is what the perceptron looks like  do you\nsee   any   difference   with   the  mcculloch   pitts   neuron   weights    you   have   a   weight\nassociated with each of the input otherwise everything seems  \nso  this is a more general computational model than the mcculloch pitts neuron   the\nother interesting thing is that of course we have introduced these weights and you also\nhave a mechanism for learning these weights  so  remember in the earlier case  our only\nparameter   was   theta   which   we   are   kind   of   hand   setting   right   but  now   with   the\nperceptron  we will have a learning algorithm which will not just help us learn theta  but\nalso these weights for the inputs  \nhow do i know that actor is what matters or director is what matters  given a lot of past\nviewing experience  past given a lot of data about the movies which i have watched in\nthe   past    how   do   i   know   which   are   the   weights   to   assign   this    so    we   will   see   an\nalgorithm which will help us do that  and the inputs are no longer limited to be boolean\nvalues  they can be real values also  so  that is the classical perceptron  but what i am\ntalking about here and the rest of the lecture is the refined version which was proposed\nby   minsky   and   papert   which   is   known   as   the   perceptron   model    so    when   i   say\nperceptron  i am referring to this model  so  this diagram also corresponds to that  \n\f\n\nso  now let us see what the perceptron does   this is how it operates   it will give an\noutput of one if the weighted sum of the inputs is greater than a threshold  so  remember\nthat in the mp neuron we did not have these weights  but now we have these weighted\nsum of the inputs  and the  output is going to be zero  if this  weighted sum is less than\nthreshold  not very different from the mp neuron  \nnow  i am just going to do some trickery and try to get it to a better notation or a better\nform  so  is this  i have just taken the theta on this side  now is this  notice this here the\nindices were one to n  now  i have made it zero to n and the theta is suddenly disappeared  so \nwhat has happened  \nstudent  w zero is  \nminus theta  right and xzero is one  does anyone not get this right  if i just start it from one to n \nthen it would be summation i equal to one to n wi xi plus wzero xzero  but i am just saying wzero is\nequal to minus theta and xzero is equal to one which exactly gives me back this  right   so \nvery  simple xzero equal  to  one and wzero is  equal to  minus  theta   so   in  effect  what  i am\nassuming is that instead of having this threshold as a separate quantity  i just think that\nthat is one of my inputs which is always on and the weight of that input is minus theta \nso  now the job of all these other inputs and their weights is to make sure that their sum\nis greater than this input which we have  does not make sense  so  this is how this is the\n\fmore accepted  convention  for writing  the perceptron equation   so  it fires when this\nsummation is greater than equal to zero  otherwise it does not fire \n\n\n \nnow  let me ask a few questions  so  why are we trying to implement boolean functions \ni have already answered this  but i will keep repeating this question  so that it really gets\ndrill in   why do we need weights   again we briefly touched upon that and why is w\nnaught which is negative of theta often called the bias \n\n\n\fso  again let us return back to the task of predicting whether you would like to watch a\nmovie or not and suppose we base our decisions on three simple inputs  actor  genre and\ndirector   now  based on our past viewing experience  we may give a high weight to\nnolan as compared to the other inputs  so  what does that mean  it means that as long as\nthe director is christopher nolan  i am going to watch this movie irrespective of who the\nactor is or what the genre of the movie  so  that is exactly what we want and that is the\nreason why we want these weights  \n\n\nnow  wzero is often called the bias as it represents the prior   so  now  let me ask a very\nsimple question  suppose you are a movie buff  what would theta be  zero  i mean you\nwill watch any movie irrespective of who the actor  director and genre  now  suppose\nyou are a very niche movie watcher who only watches those movies which are which the\ngenre is thriller  the director was christopher nolan and the actor was damon  then what\nwould your threshold be  three \nhigh in this case   i  always ask this question do you know of any such movie always\ntakes a while  interstellar so  the weights and the bias will depend on the data which in\nthis case is the viewer history  so  that is the whole setup  that is why you want these\nweights and that is why you want these biases and that is why we want to learn them \n\f\n\nnow  before we see whether or how we can learn these weights and biases  one question\nthat we need to ask is what kind of functions can be implemented using the perceptron\nand are these function any different from the mcculloch pitts neuron  so  before i go to\nthe next slide  any guesses   i  am hearing some interesting answers which are at least\npartly correct  \n\n\nso  this is what a mcculloch pitts neuron looks like and this is what a perceptron looks\nlike  the only difference is this red part which is weights which has added  so  it is again\n \n\fclear that what the perceptron also does  is  it  divides the input space into two halves\nwhere all the points for which the output has to be one  would lie on one side of this\nplane and all the points where which the output should be zero would lie on the other side of\nthis plane  so  it is not doing anything different from what the perceptron was doing  so \nthen what is the difference \nyou have these weights and you have a mechanism for learning these weights as well as\na threshold  we are not going to hand code them  so  we will first revisit some boolean\nfunctions and then  see the perceptron learning algorithm \n\n\n \nso  now let us see what does the first condition  this condition if i actually expand it out \nthen this is what it turns out to be and what is that condition telling me actually w naught\nshould be less than zero  clear  so  now  based on these  what do you have here  actually\nwhat is this  a system of linear inequalities  right and you know you could solve this \nyou have algorithms for solving this not always  but you could find some solution and\none possible solution which i have given you here is wzero is equal to minus one  wone equal to\none one and wtwo equal to one one  \nso  just let us just draw that line  so  what is the line  it is one one xone plus one one xtwo is equal to\none  that is the line and this is the line and you see it satisfies the conditions that i have is\nthis the only solution possible  no  right i could have this also as a valid line  if i could\n\fdraw properly  right all of these are valid solutions  so  which result in different wone w\nnaught and w zeros  so  all of these are possible solutions  \nin   fact    i   have   been   telling   you   that   you   had   to   set   the   threshold   by   hand   for   the\nmcculloch pitts  neuron   but  that  is not true because you could have written similar\nequations there and then  decided what the value of theta should be   so   you could try\nthis out for the mcculloch pitts neuron also you will get a similar set of conditions or i\nmean similar set of inequalities and you can just say what is the value of theta  that you\ncould set to solve that  \n\f"}
{"audio_filepath": "lec001_013.wav", "duration": 250.563, "text": "\nerrors and error surfaces\nbefore  we go to the  next section  which is  on learning   i  just  want to introduce  the\nconcept   of   errors   and   error   surfaces   and   tell   you  what   it   relates   to   these   multiple\nsolutions that we were talking about  \n\n\nso  for simplicity what we will do is  we will just set the threshold to minus or minus w\nto one which is setting the threshold to minus one and now  i will try different values of wone\nand wtwo ok  so  i was saying that there are multiple values of wone and wtwo possible and\nthese are all real numbers  we are not constrained by having them as boolean values  so \nnow this is one solution which i tried  i tried setting wone to minus one and wtwo to minus one \nwhat is wrong with this line  does it lead to any errors  how many \njust one error  so  this makes an error of one out of the four inputs  now  let me just try some\nother values of wone and wtwo  this line  again one error  what about this line   not four  three\nbecause zero  zero is anyways on this side of a line \n\fso  now given this now tell me that i my quest is to find these w  so  i would want to\nfind wone wtwo and so on  given this discussion on errors  can you tell me a condition that i\nam looking for  i want to find wone wtwo or up to wn such that errors are minimized and in\nthe best case errors are zero  so  that is what i want  so  this just i want to make a case that\nthese search for w\u2019s is driven by certain objective and this objective is to minimize the\nerror \nso  now since we are doing this  let us plot the error surface corresponding to different\nvalues of w naught  wone and wtwo  \n\n\nonce again for simpler analysis we will just keep w naught to be fixed at minus one and\nnow what i have  so  just do not read this bullet as of now  even this one  so  i have this\nwtwo here  so  that is my one axis and i have wone here which is my another axis  now  what\ni am going to do is i am going to try different values of wone and wtwo  so  this axis can go\nfrom minus infinity to plus infinity  of course  for showing the sake of showing  here i\nhave just had it from minus four to four  \nso  now what i am going to do is i am searching for some values of w\u2019s wone and wtwo  so \nthat my errors is  zero  and let us do a brute force and i will just try every value between\nminus four to four  ok  in fact  one of the solutions which i proposed actually was this one one  one one\nright  that is the line which we saw on the previous slide and which led to zero errors and\nthat is the dark blue surface here   so   how did i compute this  error  actually  i just\n\fsubstituted minus  sorry one one  one one here and then  i put in all the four values combinations\nfor xone  x two  and i realized that i am able to satisfy all of them  so  i do not get any error \nnow  instead of that if i had put something different \nso  let me just go back to the previous slide which was see minus one  minus one which is i\nthink yeah somewhere around here right minus one  minus one i guess  so  for that i am in\nthis light blue region where the error was one  i make errors for one of the inputs  so  it\nis a very brute force way of finding this and this is not going to work  because we have\nlots of inputs to check   but  this is just to give you an intuition that we are looking at\nerrors and we are trying to find a value of wone  wtwo which minimize this error  so  that is\nthe idea behind errors and error surfaces \n\f"}
{"audio_filepath": "lec001_014.wav", "duration": 804.602, "text": "\nperceptron learning algorithm\nwe will now go to the next module which is the perceptron learning algorithm  \n\n\nwe now see a more principled approach of learning these weights and threshold   but\nbefore   that   we  will   just   again   revisit   our  movie   example   and  make   it   slightly   more\ncomplicated  \n\f\n\nnow   here   what   the   situation   is   that   we   are   given   a   list   of   m   movies   and   a   class\nassociated with each movie indicating whether we like the movie or not  so  now we\nhave given some data of the past m movies that we have seen and whether we like this\nmovie or not and now instead of these three variables  we have these n different variables\nbased on which we are making decisions  and notice that some of these variables are\nreal  they are not boolean anymore  the rating could be any real number between zero to one \nok and now based on this data what do we want is the perceptron to do actually  \nso  i have given you some data  these factors i have also given you the label one and zero \nso  if the perceptron if i tell you my perceptron has now learnt properly  what would you\nexpected it to do  perfect match   so  whenever i feed it  one of these movies it should\ngive me the same label as was there in my data and again there are some movies for\nwhich i have a label one which are positive and some movies which i have a label zero  \nso  i am once again looking to separate the positives from the negatives  so  it should\nadjust the weights in such a way that i should be able to separate  so  that is the learning\nproblem that we are interested in  \n\f\n\nso    now   with   that   i   will   give   you   the   algorithm   this   is   the   perceptron   learning\nalgorithm   we   have   certain   positive   inputs   which   had   the   label   one    we   have   certain\nnegative inputs which had the label zero and now i don\u2019t know what the weights are and i\nhave no prior knowledge of what the weights are going to be  i need to learn them from\nthe   data   so   what   i   am   going   to   do   is    i   am   just   going   to   initialize   these   weights\nrandomly as i am also going to pick up some random values for this  so  this should be\nsmall n  so  this should be small n and now  here is the algorithm while not convergence\ndo something  \nso  before i tell you what to do  can you tell me what is meant by convergence  when\nwill you say that it has converged  when it is not making any more errors on the training\ndata   right    or   its   predictions   are   not   changing   on   the   training   data   so   that   is   the\ndefinition of convergence  now  here is the algorithm   i  pick up a random from point\nfrom my data which could either be positive or negative  so  it comes from the union of\npositive negative  basically all the data that i have i pick up a random point from there  \nif the point is positive  right and this is the condition which happens what does this tell\nme  if the point was positive  what did i actually want greater than zero  but the condition is\nless than zero  that means  i have made an error  so  i have made an error  then i will just\nadd x to w i see a lot of thoughtful nodding and i hope you are understanding what is\nhappening  let us see  so  what is w actually  a dimensional  \n\fn dimension n plus one  right because w naught is also inside there   so   actually there\nshould be w naught also here  right and what is x again n dimensional  right and that is\nwhy this addition is valid   so   let us understand that w and x both are n dimensional \nnow  let us look at the other if condition  can you guess what the other if condition is if\nx belongs to n and  \nsummation   is   greater   than   equal   to   zero    then   so    that   means  you   have   completely\nunderstood how this algorithm works  well  that is  so  now consider two vectors w and\nx  so  remember what we are trying to prove is or get an intuition  not prove  actually\nget an intuition for why this works  ok\n\n\nso  we will consider two vectors w and x and this is what my vectors look like very\nsimilar to the case that we are considering wzero to wn and one to n  so  this again x naught is\njust one \nnow  this condition that i have been talking about is nothing  but the dot product  how\nmany of you have gone through the prerequisites for today s lecture  ok good  so  it is\njust a dot product  now  we can just read write the perceptron rule as this instead of the\ndot product  i mean instead of using that summation thing  we can just say that it is a dot\nproduct  \n\fnow  we are interested  in finding the line  w transpose x equal to zero  so  that is our\ndecision boundary  which divides the input into two halves  now  every point on this line\nsatisfies the equation w transpose x equal to zero  what does that mean actually \nso  just a simple example is that if i have the line xone plus xtwo equal to zero   then all the\npoints which lie on the line satisfy this equation  so  you could have one minus one  two minus\ntwo and so on  but two  two is cannot be a point on this line  at every point lying on this line\nsatisfies this equation  so  every point lying on this line actually satisfies the equation w\ntranspose x equal to zero  \n\n\nso  can you tell me what is the angle between w and any point on this line  how many\nsay how many of you say perpendicular  why \ndot product is zero  so  if the dot product is zero  they are orthogonal  so  that means if i take\nthis line  then my vector w is orthogonal to this  it is orthogonal to this point or this point\nto this point to every point on the line which is just the same as saying that the vector is\nperpendicular to the line itself  right as simple as that  so  the angle is ninety degrees because\nthe dot product gives you the cos alpha and that is zero  right and since it is perpendicular as\ni said to every point of the line it is just perpendicular to the line itself  \n\f\n\nso  this is what the geometric interpretation looks like  this is our decision boundary w\ntranspose x and the vector w is actually orthogonal to this line and that is exactly the\nintuition that we have built so far \nnow  let us consider some points which are supposed to lie in the positive half space of\nthis line  that means these are the points for which the output is actually one  now  can you\ntell me what is the angle between any of these points and w or you guys are actually\ntrying to tell me the angle we have got some measuring stuff   no  so  i will give you\nthree options i  e  equal to ninety  greater than ninety and less than ninety  \nless than ninety it is obvious from the figure  now  if i take any point which lies in the\nnegative half space   what is the angle going to be between them  it is greater than ninety \nagain  obvious   and it  also  follows   from  the  fact  that  cos   alpha  is   w  transpose x  by\nsomething and we know that for the positive points w transpose x is greater than equal to\nzero  that means  cos alpha would be greater than equal to zero  that means  the angle alpha\nwould be less than ninety degrees and for the negative points w transpose x is actually less\nthan zero  that means  cos alpha would be less than zero that means  alpha would be greater\nthan ninety degrees  \nso  it actually follows from the formula itself   but  it is also clear from the figure  so \nkeeping this picture in mind let us revisit the algorithm  so  this is the algorithm  \n\f\n\nnow  let us look at the first condition which was this  now  if x belongs to p and w\ntranspose x is less than zero   then means that the angle between x and the current w is\nactually greater than ninety degrees  but what do we want it to be  less than ninety degrees and\nour solution to do this is  but we still do not know why this works  now  anyone knows\nwhy this works  so  let us see why this works  \nso   what is the new cos alpha going to be  it is going to be proportional to this  it is\ngoing to be proportional to this  i will just substitute what w new is  fine  that means  if\ncos alpha new is going to be greater than cos alpha  what is alpha new going to be  it\nwill be less than and that is exactly what we wanted  this angle was actually greater than\nninety degrees  so  you want to slowly move it such that it becomes less than ninety degrees  it\nis not going to get solved in one iteration and that is why till convergence  \nso   we   will   keep   doing   this    i  will   keep   picking   xs   again   and   again   till   it   reaches\nconvergence  that means  till we are satisfied with that condition  \n\f\n\nlet us look at the other condition x belongs to n and w transpose x was greater than\nequal to zero  then it means that the angle alpha is actually less than ninety degrees and we want\nit to be the opposite  i  will  just quickly skim over this w minus this  x  ok i  forgot to\nmention that this is actually a positive quantity  i mean that is why that result holds  that\nmeans   cos   alpha   new   is   going   to   be   less   than   cos   alpha   and   this   slight   bit   of\nmathematical in correctness  i am doing here  but that does not affect the final result  \nso  i will just gloss over that and you can go home and figure it out  but still it does not\ntake away from the final intuition and interpretation  so  now the new cos alpha is going\nto be less than the original cos alpha  that means  the angle is going to be greater and that\nexactly what we wanted  \n\f\n\nso  we will now see this algorithm in action for a toy data set  \n\n\nso  this is the toy data set we have and we have initialized w to a random value and that\nturns out to be this  i just picked up some random value for w and ended up with this\nparticular configuration for w  \nnow  we observe that currently w transpose x is less than zero for all the positive points and\nit is actually greater than equal to zero for all the negative points  if you do not understand\nw transpose x  it is just that the all the positive angle points actually have a greater than\n\fninety degree angle and all the negative points actually have a less than ninety degree angle  so \nthis is exactly opposite of the situation that we want and now from here on  we want to\nactually run the perceptron algorithm  right and try to fix this w  how does it work \nremember we randomly pick a point  so  say we pick the point pone  do we need to apply\na correction \nyes  why  because it is a positive point and the condition is violated  so  now we add w\nequal to w plus x and we get this new w  so   notice that we have a new w  we again\nrepeat this  we again pick a new point and this time we have picked ptwo  do we need a\ncorrection \nyes  at least from the figure it looks like the angle is greater than ninety  so  we will again\ndo a correction  we will add w is equal to w plus p  this x is actually  sorry ptwo and this is\nwhere we end up  \n\n\nnow  again we pick a point randomly none  do we need a correction  so  this is what our\nw is  this line here and none  so  we need a correction  now  what is the correction going\nto be  it will be minus and then  the w changes  \n\f\n\nnow  we pick another point nthree   do we need a correction  no at least on the figure it\nseems like the angle is greater than ninety and we continue this  \n\n\nfor ntwo  we do not need a correction  now  for pthree again we do not need a correction  \n\f\n\nthe angle looks less than ninety  sorry actually it is we need a correction  the angle is\nslightly greater than ninety and this is our correction  and now we keep cycling  \n\n\nnow  as i keep cycling over the points  i realize that i no longer need any correction  \n\f\n\nit should be obvious from the figure that for this particular value of w  now all my\npositive points are making an angle less than ninety and all my negative points are actually\nmaking   an   angle   greater   than   ninety    that   means   by   definition   now   my   algorithm   has\nconverged  so  i can just stop it  so  i can just make one pass over the data  if nothing\nchanges  i will just say it has converged  now  does anyone see a problem with this \nit will never converge in some cases  so  can someone tell me why  we are considering\nonly cases where the data is linearly separable  that we already assumed  so  what you\nare trying to tell me is that you are going over these points cyclically  so   let me just\nrephrase and put words in your mouth that what you are trying to tell me actually is that i\ntake a point  i adjust w   but  now for the next point i maybe go back to the same w\nbecause that point asked me to move it again and i keep doing this again and again and\nbasically  end up nowhere  that is why this will never converge  that is exactly what you\nare trying to tell me  \nnow   that is exactly what i am  forcing you to tell me   so   that is not the case  this\nalgorithm will converge  \n\f"}
{"audio_filepath": "lec001_015.wav", "duration": 927.123, "text": "\nproof of convergence\nin  this  module  we  will  talk  about  the  proof  of  convergence  for  the  perceptron  or  the\nlearning algorithm that we saw in the previous module \n\n\nso  we have some faith and intuition that it actually works   we just need to formally\nprove it that it actually converges  so  that is what we are going to do in this module \n\f\n\nso  before that a very few very simple definitions  so  if you have two sets of points p\nand n in an n dimensional space and we call say that these points are absolutely linearly\nseparable  if there exists some n plus one real numbers which has wzero to wn  such that every\npoint which belongs to p right  p is the case where the output is one \nthen these set of weights satisfy this condition and every point which lies in the negative\nset the set of weights satisfy this condition  so  nothing very different from what has we\nhave been saying so far  it is just formally defining it \nnow  our proposition is that  if the set p and n are finite and there is a fixed number of\npoints in that which was the case in the toy example that we were doing and which will\nbe the case in most examples that  we do and linearly separable  the perceptron learning\nalgorithm updates the weight vector ok  before i go there ok  let me not give you the\ndefinition and let me ask you the definition \nso    now   i   have   given   this   definition    the   first   definition   and   given   this   part   of   the\nproposition  can you tell me what do i need to prove if i need to prove that the algorithm\nconverges   that   is   one   way   of   looking   at   it   but  what   was   happening   in   that   wrong\nargument which was i was making that it continuously kept toggling  that means  i am\nnot making a finite number of updates right i have to keep changing again and again and\nthis process continues in a loop \n\fso  that is how i am going to define convergence that the perceptron learning algorithm\nupdates a weight vector of finite number of times  it only needs to update it finite number\nof times and it will reach a configuration such that now  it is able to separate the p from\nthe n ok  that is what the proof of convergence means \nso  in other words if you are going to pick up these vectors randomly from the set p and\nn cyclically  as we were doing in the toy example  then a weight vector wt is found after\na finite number of steps which will separate these two steps  these two sets  so  that is\nwhat we are trying to prove  so  that is the definition of converge  does it make sense  \n\n\nso  proof is on the next slide and it is going to take me around five to ten minutes to prove\nit  so  just stay focused all right  so  here is a few set up right  so  i am going to  before i\ngo to the actual proof  i  am going to make a set up  so  that it becomes easier for us to\nprove it  so  the first thing that i am going to say is that  if there is a point which belongs\nin negative set then the negative of that point belongs in the positive set and that is very\nclear  because if the point belongs in the negative set then w transpose x is less than zero \nbut then w transpose minus x would be greater than equal to zero right   so  i  take the\nnegative of the point  i can just put it in the positive set  so  instead of considering these\ntwo different things p and n i am just going to consider one p prime  which is an union\nof p and all the n points negative ok  will the set up clear  if this is a setup then what is\nthe condition that i need to ensure for every point in p dash \n\fstudent  \n \nw transpose  p should  be greater  than equal  to  zero right   so  i do not  care about  the\nnegative case  i have just made everything positive now  and it is  i am not done anything\nwrong here  it is just a simple trick  ok and now  this is how the algorithm will look in\nthis setup   these are the inputs with label one inputs with label zero  n  minus  contains a\nnegation of all the points in n and p prime is a union of these   now   again i start by\ninitializing w randomly  while convergence i will do something  i will pick a random p\nfrom p prime  now  what is the  if condition \nless than zero \ndo i need the other if condition \nno right  because everything is now  positive ok and the other small thing that i am\ngoing to do is  i am going to normalize p ok  so  that again does not mean  because we\nare talking in terms of angles and i am not changing the direction of the vector  i am just\nshrinking it right  so  i am just or maybe scaling it  also i am just making it unit norm \nso  that does not change anything  so  it is still everything still holds \nand in particular you can see here  so  if this condition was true  this condition will also\nbe  true  ok  so  so  far just i am done some simple tricks to make things easier for me\nlater   on    so   now   p   has   been   normalized   now   remember   that   this   data   is   linearly\nseparable  that is what we started the proposition  if p and n are linearly separable then\nthe   perceptron   learning   algorithm   will   converge    so    now   if   p   and   n   are   linearly\nseparable  irrespective of whether we have the perceptron learning algorithm or not what\ndo we  know \nthere exists a w star which is the solution vector  right  there exists at least one w star\nwhich is the solution vector right  such that it will separate the p points from the n\npoints  so  this vector which we do not  know  but we just  know  that it exists  so you\ncan refer to it  so  we will call this w star fine  now we start the proof \n\f\n\nso  w star is some optimal solution which we  know  exists \nbut we do not  know  what it is right  now  suppose you had a time step t  so  remember\nthat this algorithm is going on while convergence  so  you have time step one two three you are\npicking up points  so  we are at a time step t  at which you pick up a random point pi and\nyou find that the condition is actually violated  so  this should actually be less than zero  if i\nknow  the condition is violated  so  now  what will you have to do \nw is equal to \nwone  so  i will just call it the new w wt plus one is equal to the old w plus pi  ok  now  what\ni am going to do is i am going to consider the angle beta between w star and wt plus one  i\ndo not know  what w star is  but we can still assume it exists and make some calculations\nbased on that  so  what is the angle between w star and wt plus one  its beta and what is the\ncost of that angle this \nand remember that we do not have w star here  because we had assumed that it is the\nnormalized vector  so  we do not need that but  this is actually equal to one ok so  now  if i\njust take the numerator  w star in dot product wt plus one now  i am going to expand wt as\nwt plus pi fair  that is exactly what i did on the previous step \nnow  now  what is pi actually  it is  so  what you had is you had these pone  ptwo  pthree  my\nhand writing is really horrible and up to pn right  so  i have just picked one of these pi\u2019s \n \n\fok  now  what i am going to define is  now  suppose this is my  these are my pi\u2019s  so \nthese are all the vectors that i have  now  suppose i have this w star  suppose this was the\nw star that i am interested \nnow  for each of these i could compute w star pone  w star ptwo and so on up to w star pn\nand i could sort them  now  what i am doing is that for whichever of these points w star\npi is the minimum  i  am going to call that value as delta  suppose w star p one is the\nsmallest quantity out of w star pone w star  ptwo w star  pn  and i am calling that quantity\ndelta \nso  i have this quantity here and my delta is the minimum of all the possible values that\nit can take  it can make w star pone ptwo up to pn  so delta is the minimum quantity  so  here\ni have an equality  \n\n\nnow  are you ok with this  this is the minimum quantity right  so  any pi that i put in\nhere it is always going to be greater than or worst case equal to delta \nnow  again this wtwo itself i could write it as wt minus one plus pj  because that also would\nhave come up from some update in the previous step  ok  again this is there which i\ncould call it as delta and still retain the greater than equal to here  ok fine  so  let us see\nwhere are we heading with this \n\fnow  notice that we do not make a correction at every time step  when i was running that\ntoy algorithm i was not making a correction at every time step  we were only making a\ncorrection at those time steps for which the condition was violated  so  now  if i am at\nt\u2019th time step  maybe i have made only k which is less than or equal to t corrections  at\nmax i would have made t corrections  but it could have been less than that also \nso  now  every time we make a correction we are adding a value delta to this  so  at the\ntime step t what would happen   i  had started off from w naught i have reached time\nsafety and i have made a case that  i have not made t updates i have made k less than\nequal to t updates   so  how many deltas would get added \nk delta  so  i could say that with respect to w naught where i had started from  this is\nwhat this quantity is  ok is that fine  anyone has a problem with this \n\n\nso  far what are we shown  we started with this  this condition was true again not less\nthan equal to and hence we made the correction and this was the point that we picked up\nat the t th step and thence we made that correction \nand we also showed that the numerator is actually greater than equal to this quantity  we\nshowed it by induction fine  now  let us look at the denominator and particularly let us\nlook at the denominator squared  ok is a step right \n\fthis is actually wt plus one dot product wt plus one  but  wt plus one can be written as wt plus p\ni   this   bracket   needs   to   disappear   right    is   that   ok   fine    now   what   is   what   is   this\nquantity \nthat is less than equal to zero  so  now  can you guess what is the next thing that i am going\nto write  \nthat is correct  yeah it is a negative quantity  so  that is going to be less than equal to\nthis  so  that is fine and what about pi square or this term  \nbecause this is less than right that is why \ncorrect is this fine  ok  now  what is pi square \none  now  can you guess what i am going to do  by induction \nso  what is wt square again  just this wt plus one square was wt square plus one  wt square is\ngoing to be wt minus one square plus one right and how many such ones will get added  k of\nthose right  starting from w naught ok\n\n\nso  what have we shown  the numerator is greater than equal to this  the denominator is\nless than this  ok now  if i put them together  i actually get that cos beta is going to be\ngreater than equal to the numerator over the denominator  ok now  what is this quantity\nproportional to k  k square  k cube square root of k  k by two \n\fstudent  square root of k \nsquare root of k right  you have  i mean roughly speaking you have a k here  you have a\nsquare root of k here  so  i could roughly speaking say that it is proportional to square\nroot of k  so  as k grows what will happen to cos beta  it will grow and that is fine right \nit can keep growing \nstudent  \n \nonly until one right  so  cos beta is going to be proportional to k what is k  the number\nof updates that you make  now  if i were to take that degenerate case which you guys\nwere hinting at  where that it will keep changing again and again  what will happen to k \nit will keep going to infinity can that happen \nno  because cos beta will blow up right and that is not allowed  so  k has to be finite  so\nthat cos beta stays within its limits right  hence are we done  how many if you think we\nare done  how many if you are satisfy that we are done \n\n\nso  yeah  so  this says that we can only have a finite number of such k updates that we\nmake and after that the algorithm will converge  so  we have a proof of convergence \nnow  coming back to our questions this is where we had started at one point  what about\nnon boolean inputs  so perceptron allows that  we took imdb rating and critics rating as\nan input  do we always need to hand code the threshold \n\fno  in our perceptron learning algorithm are all inputs equal  no we now  assign weights\nto input  what about functions which are not linearly separable  we still do not  know \nso  that is where we are headed now  not possible with a single perceptron  but we will\nsee how to handle this  \n\f"}
{"audio_filepath": "lec002_001.wav", "duration": 367.731, "text": "\nlinearly separable boolean functions\nso  in this module  we look at linearly separable boolean functions again and we will\ntry to make some more statements about them \n\n\nso  what do we have do  so  the guiding question that we have is what do we do about\nfunctions which are not linearly separable and let us see one such very simple function \ncan you guess what function i am going to talk about  all of you are paying attention in\nthe first lecture \n\f\n\nso  here is the xor function  now  these are the set of inequalities that result from xor\nfunction  i hope right  now  let us see the first condition implies that w naught should be\nless than zero  second condition implies this  third condition implies this  fourth condition\nimplies this  just looking at this can you tell me  can you find a configuration for w\nnaught wone  wtwo  such that these inequalities can be satisfied together  no  right because two\nand three want it to be greater than minus one minus w naught and when you take an addition\nof that  it has to be less than minus one \nso  that is not going to happen  so  you see a contradiction  so  this is a simple boolean\nfunction which the perceptron cannot handle because it is not linearly separable  it is not\nlinearly separable  there does not exist a line  if there does not exist a line  you cannot\nfind the line  in fact  you can look at it visually \nso  these are the red points for which the output should be zero or one and the blue points are\nthe points for which the output should be zero  if we need to change this  i think we were\nusing blue as positive and red as negative and you cannot just draw a line  there is no\nway you can draw a line such that the blue points lie on one side and the red points lie on\nthe other side  so  it is a simple two input function  so  it is not that i have taken a very\ncontrived example \n\f\n\nmost real world data is not linearly separable and it always contains some outliers  right \nso  here maybe you have some data where you are trying to say that people which live in\nthis part of the world belong to a certain or maybe people who live or work here have a\ncertain  qualification   people who work in this company may have a certain different\nqualification  and there might be some outliers  right  it is not that is always going be\nvery clean  so  now what do i mean   and it is not necessary that the points will only be\noutliers \nin fact  there could be a clear case where there are no outliers  but still you cannot find a\nline such that you separate the positive from the negative  can you think of such an\nexample  good right  this is clear data  there is no outliers here as well  i mean it is just\nsaying   that   everyone   who   lies   within   this   boundary   has   a   certain   characteristic   and\noutside that boundary people have a different characteristic  right and there is no outlier\nhere  but you cannot separate this data with a line  so  all functions that you deal with\nwill not go or are not going to be linearly separable \nso  we have to work around those  right and while a single perceptron cannot deal with\nthis  we will show that a network of perceptron\u2019s can indeed deal with such data  so  that\nis where we are headed \n\f\n\nso  before going there we will discuss some more boolean functions in more detail  and\ni will try to see what kind of non linearly separable boolean functions are there \n\n\nso  first of all how many boolean functions can you design from two inputs  how many\ncan you design  sixteen looks like a good number from three inputs two hundred and fifty six  how many if you\nunderstand this  let us see  so  let us begin with some easy ones that you already know \nright    so    these   are   two   inputs   xone    xtwo   what   is   this   function   always   off   the   other\nextreme is always on and i have already given you the answer f sixteen \n\fso  then you have the and function and or function then some other functions  right \nso  why did you reach sixteen  actually because with two inputs we will have these four\nvalues to take care of and each of these are again binary  so  you actually have two raise to\ntwo raise to n  right  so  for three inputs two raise to two raise to three would be two hundred and fifty six  now  that is\nthe easy part of these  how many are linearly separable  i will have to do any actually\nstare it in and seriously try to find the answer when you cannot really do that \nso  turns out all of them except xor and in  not of xor  ok  so for the two input cases \nthere   are   two   functions   which   are   not   linearly   separable    for   n   inputs   how   many\nfunctions would be not linearly separable  it is an arbitrary  n is not the answer  you are\nnot going to disappoint me  not n ok  but what is the answer  so  for n inputs  we will\nhave two raise two n functions of these we do not know how many are going to be not\nlinearly separable  that is not a solved problem  although i encourage you to go and find\nthe answer \ni am looking for a good will hunting kind of a moment  but all it suffices to know is that\nthere exists some which are not linearly separable  and that everyone agrees that there\nexists some  right and as n grows probably that number will increase and so on  but it is\nnot known exactly  you cannot write it as a function  so  what we have done so far is\nlooked at boolean functions  how many boolean functions can exist and of that we just\nhave concluded that there would be some which are not linearly separable \n\f"}
{"audio_filepath": "lec002_002.wav", "duration": 799.529, "text": "\nrepresentation power of a network of perceptrons\nwe will go to the next module  where we talk about a network of perceptrons and then \nwe talk about the representation  power  of a network  of perceptrons   so   this  module \nshould have been titled  as  network of perceptrons  so  now  in particular  what we are \ngoing  to   see   is  how   any  boolean   function   whether  linearly   separable   or   not   can  be \nrepresented using a network of perceptrons \n\n\nnow  what do i mean by represented during a network of perceptrons  what it means is\nthat i will give you a network of perceptrons  you take any boolean function feed any\nvalue of xone to xn and the network will give you the same y as it is expected from the\ntruth table  ok  that is what representation means just to put it out clearly \n\f\n\nand now i am going to again do a setup  i am not giving you the solution  i am just\nmaking some set up and then we will discuss the solution \nso  for this discussion we will assume that true equal to plus one and false equal to minus\none  so  instead of zero and one  we will assume minus one and plus one  and these are your inputs\nxone and xtwo we are taking the two input case  and i will have four perceptrons first  i will have\nfour perceptrons  and i will also have very specific weights connected to form the inputs to\nthese perceptrons  so  red means minus one and blue means plus one  right  so  the first two\ninputs are connected with a weight of minus one  the next two inputs with minus one plus one \nplus one minus one and the last would be \nplus one  now  once i have this  i will set the bias of all these perceptrons to minus two  so \nthat will  that means  they will fire only if they are weighted sum of the inputs is greater\nthan two  now after this i will have one more perceptron  so  i had two inputs  i converted\nthat to four values  these four values are now going to feed into one more perceptron \nand these weights i will not fix them  these are the weights that i am going to learn ok \nthese and the final output of this perceptron which is the green perceptron is the output of\nthe network  right  so now  coming back to what i said that it can represent any function \nwhat  i  mean  is  that   you take   any function    feed  in  any combination   of xone   xtwo  this\nnetwork will give you y and i am telling you that it will match the truth table of that\nfunction \n\f\n\nnow  let us define some terminology this will also stay with us for the rest of the course \nso  this network contains three layers   the layer containing the inputs it is called the input\nlayer  very creative  the middle layer containing the four perceptrons is called the hidden\nlayer  and the output layer which gives you the output is called the output layer  output\nperceptron which gives you the output is called the output layer right  so  you have a\ninput layer  a hidden layer and an output layer  \nand the outputs of the four perceptrons i am going to call them as hone  htwo  hthree and hfour  and\nthe red and blue edges are called the weights for the layer one  which we have not learned\nwe have actually set them by hand  and the weights for wone  wtwo  wthree  wfour are called the\nweights for the second layer  other the layer two weights and these are the weights that we\nwant to learn \n\f\n\nnow   i   make   this   claim   that   this   network    it   can   take   any   boolean   function    it   can\nimplement any boolean function  so  this same network can implement any boolean\nfunction  that means  if i take this network  and if i try to learn the values of wone  wtwo  wthree \nwfour  for any boolean function whether it was originally linearly separable or not  i will be\nable to implement it \nso    isn\u2019t   this   an   astonishing   claim   any   boolean   function    do   you   think   this   is   an\nastonishing claim  well not really if you really understand what is happening here right \nso  let us see what exactly is happening here  so  when will perceptron one fire  when the\ninput is false  false zero  zero  will it fire for any other input  when will perceptron two fire  any\nother input  same for the next perceptron  same for the next perceptron \nso  you start getting an intuition of what is happening here  you do   ok  let us see  so\nnow  for this particular network now that i have given you some intuition of what is\nhappening  basically every node or every neuron in the hidden layer is catering to one of\nthe inputs  and it will fire only for that input  it will not fire for anyone else \n\f\n\nso now let us try to implement the xor function  and see what are the  so now  let us\ntry to implement the xor function  and see what are the set of inequalities that result\nfrom this  earlier when we try to look at the set of inequalities  we ended up with a\ncontradiction  let us see if that happens now  so  this is xone  xtwo this is your xor function \nso  that is just like any truth table then i am noting down the intermediate values  and\nthen my final input to the green perceptron is going to be summation of these  and it will\nfire if this summation is greater than equal to zero  or else it will not fire \nnow  for the first case when the input is zero comma zero what is hone going to be  one and\neverything else is going to be zero  that is exactly what we saw in the previous slide  so \nwhat is the summation going to be  just wone right  so  it is wone hone plus wtwo htwo so on  but htwo\nto hfour are zero  so  only thing that remains is wone  for the second case  wtwo  for the third case \nwthree for the fourth case wfour \nso  is it clear now what is happening  let us go a bit more into detail right  so now  for\nthe xor condition  what are the conditions that we need  wone should be less than wzero \nbecause this should not fire  wtwo should be greater than equal to zero  wthree should be greater\nthan equal to sorry w naught not w is not zero  and wfour should not fire  so  wfour should be less\nthan wzero  are there any contradictions here  by design no right  so  we have made sure\nthat for the final layer only one of these guys feeds to it \n\fso  it does not matter what the remaining outputs are  they do not interfere with each\nother  unlike earlier where we had conditions like wone should be something  wtwo should be\nsomething  and then wone plus wtwo should be something  there are no such contradictions\nhere  because we have made sure that every neuron in the middle layer actually caters to\none specific input  and now the weights in the final layer can be adjusted so that we get\nthe desired output for that input \nso  i can set whatever value of wone i need to set  so  that i can fire the neuron  in fact  i\ncould just fix wzero as zero  and then i can adjust the weights of wone  wtwo  wthree  wfour  and i can\nimplement the xor function  so  are you convinced that this can be used to implement\nany boolean function  how many if you are not convinced  so  the negative question\nnever works  how many if you are convinced  sure \nnow  what if we had three inputs \n\n\nbefore that it should be clear that the same network can be used for any of the remaining\nfifteen functions  and for each of these functions we will end up with a different value of wone \nwtwo  wthree  wfour  but you will be able to satisfy the truth table  right and you can go home and\ntry it  which i am sure you will do \n\f\n\nah so  what if we have a function of three inputs  two hundred and fifty six  what is two hundred and fifty six  no \neight  fine  sure \n\n\nso  this is what it will look like  and anything specific about the weights of the initial\nlayer  can you tell me what the weights would be  just tell me red red red red blue blue\nwhatever colours you like this thing  first perceptron what would the weight colours be\nred  red  red  then \n\fenough so  this is how it will look right  right and now this same thing will work with\nthe same logic  for any boolean function of three inputs  you will get these eight inequalities \nand they will not interfere with each other  and you can set the values of wone to weight so that\nyou can satisfy it ok  fine \n\n\nso  what if we have n inputs \ntwo power n perceptrons in the middle layer  right  ok \n\n\n\fso now here is the theorem  any boolean function of n inputs can be represented exactly\nby a network of perceptrons containing one hidden layer with two raised to n perceptrons \nand one output layer containing one perceptron  we just saw an informal proof of this  we\njust constructed i just gave you the answer it this is how you will get it  now note that a\nnetwork   of   two   raised   to   n   plus   one   perceptron    is   it   sufficient   or   necessary    or   both \nsufficient yes that is what it says  is it necessary  \nno    we   already   saw   the  and   function   which   we   can   just   represent   using   a   single\nperceptron  right  so  it is not necessary  but it is sufficient  so  this is a very powerful\ntheorem if you think of it right  so now  this whole objection right remember this history \nand when we have the c i winter when people showed that perceptron cannot handle the\nxor function that is for a single perceptron \nif you have a network of perceptrons you can actually have any boolean function  but\nwhat   is   the   catch    as   the   value   of   n   increases   the   number   of   neurons   increases\nexponentially right  but still in theory you could have a solution \n\n\nnow again why do we care about boolean functions  i keep coming back to this  why do\nwe care about boolean functions  because you took this and so  the question that i the\nquestion that i want to answer is  how does this relate back to our original problem \nright we know any boolean function can be implemented  how do we go back to our\noriginal problem  which is whether we like a movie or not  right \n\fand you could see that there is a whole family of problems there  right  whether we like\na movie or not  whether we like a product or not  whether i want to go home today or\nnot  yes no any kind of a yes no problem  it is a whole family of problems there \n\n\nso  let us see so  we are given this data from our past experience  right  so  we are told\nthat this is what the movie looks like  these are the actor\u2019s  director\u2019s  joiners everything \nwe also know whether we like these or not \nso  we have a set of positive points and we have a set of negative points  right  and now\nwe want to have a computational model which can satisfy this data  that means  once the\nmodel is trained  once whatever algorithm i algorithm i use has converged  it should be\nable to give me the correct output for a given input  that is what we are interested in  and\nthat is a real classification problem that we are interested in  now for each movie we are\ngiven these factors as well as the decision \nand i said pi\u2019s and ni\u2019s are positive and negative points  the data may or may not be\nlinearly separable  it is not necessary that the data is linearly separable  those were the\ngoody cases it  but in general that may not happen  but do we worry about it now  no\nwhat the previous theorem told us is that irrespective of whether your data is linearly\nseparable or not  i can give you a network which will be able to solve this problem\nmodulo  that it might be very expensive in the number of neurons in the middle layer \nbut if you keep that aside  i have a solution for this \n\fand that is why we care about boolean functions  because many problems we could\nactually cast to it in a simplistic way  if we ignore the real inputs  and if you even think\nof the real inputs  suppose it could take all values between zero to one  you can always make\nit binary  you could say that is the value between zero and zero one is the value between zero one and\nzero two  and you could make it as small the scale as small as possible right  so  that is why\nwe care about this \n\n\nso  the story so far has been that the network of networks of the form that we just saw  it\nwhich contain one input layer  output layer and one or more hidden layers  these are\nknown as multilayer perceptrons  but a more appropriate terminology would be multi \nlayered network of perceptrons  \nbecause the perceptron is not multi layered  you have a network of perceptrons and that\nnetwork has many layers right  but generally there is abuse of notation  we always call it\nmlp which is multi layered perceptrons  and the theorem that we just saw gives us the\nrepresentation power of an mlp  and basically tells us that it can represent any boolean\nfunction that we want to deal with  so  that is where we will end this class  and in the\nnext class we will talk about sigmoid neurons \n\f"}
{"audio_filepath": "lec002_003.wav", "duration": 739.087, "text": "\nrepresentation power of feedforward neural networks\nwe are in lecture three of csseven thousand and fifteen  and today we are going to cover the following modules \nwe  are  going  to  talk  about  sigmoid  neurons   gradient  descent   feedforward  neural\nnetworks  representation power of feedforward neural networks \n\n\nso  let us start  so  here are some acknowledgments  so  for one of the modules i have\nborrowed ideas from the videos of ryan harris on \u201cvisualize back propagation\u201c they are\navailable on youtube  you can have a look if you want  for module three five  i have borrowed\nideas from this excellent book which is available  online it is the url as mentioned in\nthe footnote  \nand i am sure i would have been influenced in borrowed ideas from other places and i\napologize if i am not acknowledge them probably properly  if you think there are some\nother sources from which i have taken ideas and let me know i will put them in the\nacknowledgments \n\f\n\nso  with that we will start with module three one which is on sigmoid neurons  so  the story i\nhad is that it is enough about boolean functions \n\n\nnow  we have done a lot of boolean functions  but now we want to move on to arbitrary\nfunctions of the form y is equal to f of x  where x could belong to rn and y could belong\nto  r  so  what  do i mean by this   so  let  me just  explain  this  with the  help  of an\nexample  so  i will again go back to our oil mining example oil drilling example  where\nwe are given a particular location say in the ocean and we are interested in finding how\n\fmuch oil could i drill from this place  and that is what i would base my decision alright\nwhether i want to actually invest in this location or not \nand then what we are saying is that this could depend on several factors  so  we could\nhave xone  xtwo  xthree up to xn  right where this could be the salinity of the water at that\nlocation  so  this could be a real number  this could be the density of the water it is\naverage density  this could be the pressure on the surface of the ocean bed and so on and\nso forth \nso  each of these values independently belongs to the set of real numbers  so  each of\nthis is a real number and we have n of these  so  together they belong to rn  so  i can\nread that i have n such real numbers  and i could just put them in a vector and say that i\nhave a input x which belongs to r raised to n \nso  we have this x which we can say belongs to rn  and in this particular case  we want\nto predict y  we want to take this as an input and predictor y  and what is y in this case \nyou want to predict the quantity of oil that we could mine  so  what does ry belong to\nagain a set of real numbers  and it could be some gallons or litres or kill of water  so  this\nagain belongs to r  so  these are the kind of functions that we are interested in now \nwe want a function which takes us from i am having this x  which belongs to rn right it\nis a vector of dimension n  and takes us to a value belonging to r  so  you clearly see\nthat this is different from the case when we had n variables each of this was just boolean \nso  these were only zero one inputs now we have real inputs  and these are the kind of\nfunctions that we are interested in \n\f\n\nnow  can we have a network which can represent such functions  now  what do i mean\nby represent such functions  we already spoke about this when we were doing boolean\nfunctions  so  what do we mean by representing the function  we mean that if i am\ngiven a lot of training data  right so  i am given these xone  xtwo each of these belongs to rn \nand i am also given the corresponding labels  now i want a network which should be\nable to give me the same predictions as is are there in my training data \nso  it should be able to take any of these x\u2019s as input  and it should give me the same y i\ncorresponding to it  and i am saying approximately which means i am with some error\nrate  whether if it is within some to with as long as it is close to the actual value i am fine\nwith it  so  that is what i mean by a network which can represent such functions  is that\nworking definition of representing clear  right  so  that is a very similar to the definition\nthat we were used for boolean functions  we had said that we should be exactly be able\nto get the truth table the network should be able to represent the truth table exactly \nso  that is very similar to the definition that i am using here \n\f\n\nand then before we do this  right before we come up with a network which can do this\nfor arbitrary functions  we have to graduate from perceptron\u2019s to something known as\nsigmoid neurons  so  please remember this overall context that we dealt with a lot of\nboolean functions  we analyze them carefully and we saw that we could come up with\nthese networks which could represent arbitrary boolean functions \nand they could represent them exactly as long as we have one hidden layer  of course \nthe catch was that that hidden layer could grow exponentially  now we want to graduate\nfrom boolean to real functions  that means  you have a real input of n variables  and one\nor more outputs and you should be able to represent this exactly  so  that is where the\ntransition is where so  that is the story that we are looking for \n\f\n\nso  let us start so  recall that a perceptron will fire  if the weighted sum of it is inputs is\ngreater than the threshold  just recall that fine \n\n\nso now  i claim that the thresholding logic which is used by a perceptron is actually very\nharsh  now what do i mean by that  let us see  so  let us return to a problem of deciding\nwhether we like or dislike a movie  that is the same problem that we have been dealing\nwith  and now consider that we base our decisions only on one input  which is the critics\nrating which lies between zero to one  and this is what my model looks like  it takes the input\n\fas the critics rating  i have learned some weight for it  and my threshold is zero five  what\ndoes this mean  it means that if for a given movie the rating is zero fifty one will it predict like or\ndislike like  so  then i should go and watch the movie  what about a movie for which the\ncritics rating is zero forty nine  dislike  so now  you see what i mean by harsh \nso  both these values are very close to each other  but for one i say i like it  for the other\ni say that i would not like it  so  it is not how we make decisions  right you would have\nprobably said something equal for both the movies  you would have not given such a\ndrastic decision \n\n\nso  why is this happening  so  you might say oh this is a characteristic of a problem that\nyou   have   picked   up    maybe   that   is   the   critics   rating   which   is   between   zero   to   one   or\nsomething  but i want to convince you that this is not a characteristic of the problem that\ni have picked up  but this is something to do with the perceptron function itself  so  this\nis what the perceptron function looks like  so  this sum of all the inputs the weighted\nsum of all the inputs i am calling it by a quantity z  and this is what i am going to plot on\nthe this axis  so  this is my z axis \nnow  what does the perceptron say that  when this value of z becomes greater than w\nnaught or minus of w naught it will fire  and when it is less than minus of w naught  it\nwill not fire that is what it says  so  this is a characteristic of the perceptron function\nitself it is going to have this \n\f\n\nsharp decision boundary that whenever your sum crosses this threshold you will say one \nand whenever your sum does not cross this threshold you will say zero  so  in this toy\nexample over the movie critics it just happened that this was zero five  and so  it was saying\nyes for zero fifty one  and it was saying no for zero forty nine  so  this will happen for any problem that you\npick up \n\n\nso  to counter this we introduce something known as sigmoid neurons  and this is just a\nsmoother function or a smoother version of the step function  you see that \n\fhow many if you know what a sigmoid function  what is the formula for a sigmoid\nfunction  quite a few good  and here is one such sigmoid function which is called the\nlogistic function  so  remember that sigmoid is a family of functions  these are functions\nwhich have this s shaped  logistic function which i have shown here is one such function\nand the other function that we will see in this course is something known as the tanh\nfunction  so  let me just get into a bit more detail with this logistic function \ni  just  want you  to  understand  it  properly  so  this  quantity   here  remember   we were\nwriting it as w transpose x  which was summation i equal to zero to n  wi xi remember this \nso now  i am just going to consider this to be one over one plus e raise to minus w transpose\nx  now i am going to ask you some questions and try answering those \nwhat   happens   when   w   transpose   x   tends   to   infinity   what   happens   to   the   sigmoid\nfunction \none and that is exactly what is happening here as this tends to infinity as this keeps\ngrowing  so  remember this axis is z which is the same as w transpose x  right this is w\ntranspose   x    so    as   it   tends   to   infinity   your   sigmoid   goes   to   one    what   happens   if   w\ntranspose x is minus infinity \nzero and that is exactly what is happening here  and what happens when w transpose x is\nequal to zero  half  so  this is that value corresponding to half  is that clear   so  that is how\na sigmoid function behaves fine \n\f\n\nnow  we no longer see a sharp transition  it is a very smooth function  and the sigmoid\nfunction lies between the values produced by the sigmoid function rate  what is the range\nthat they lie between \nzero to one  what is another quantity of interest that you know which lies between zero to one \nprobability  so  that is one advantage of sigmoid functions  so now  you can interpret the\nvalue given by a sigmoid function as a probability  so  what does it mean in our movie\nexample again  so  it just tells me in those two cases  that with fifty one percent probability i\nlike the movie or with forty nine percent probability i like the movie  so now  this is not very\ndrastic or very harsh  right i am not saying yes or no i am not committing myself  i am\njust giving you a number which is proportional to how much i like the movie  so  it can\nbe interpreted as a probability \n\f\n\nnow   here\u2019s   the   overall   picture   it    so    this   is   the   difference   between   the   perceptron\nfunction and the sigmoid function  so  notice that here we had this if else condition  right\nwhich was leading to that sharp boundary \nnow  here we do not have that defence condition  we just have a function which is a\nsmooth function \n\n\nand here is another picture  so  this is not smooth  not continuous and not differentiable \neveryone   agrees   with   that    it   is   not   smooth   here    right   it   is   not   differentiable   here\n\fwhereas this is smooth continuous and differentiable  and the contents that we covered\ntoday it will be very important to deal with functions  which are smooth continuous and\ndifferentiable \nso  for lot of this course calculus is going to be the hero of the course lot of the things\nthat we do will be based on calculus  and in calculus always if you have smooth and\ncontinuous and differentiable functions they are always good  so  that is why we want to\ndeal with such functions  \n\f"}
{"audio_filepath": "lec002_004.wav", "duration": 951.48, "text": "\na typical supervised machine learning setup\nwe will start module two which brings us to a typical supervised machine learning setup\nthis is a very important module please pay attention \n\n\nso now we have a sigmoid neuron  we have taken care of the fact that the perceptron was\na very harsh function  so  we have a smooth function so  things are fine  now what next \nwhere do we go from here  what is my next topic going to be  yes  a lot of you are\ngiving the right answers  we need to learn these weights  it does not help just to define\nthe function  this function depends on certain weights  and now i need to give you an\nalgorithm which will help you to learn these weights \nnow remember when i talked about perceptrons before giving you an algorithm what did\ni revisit  what did i talk about the error surfaces  and then i had motivated from there\nthat our goal is to find a set of weights which give us close to give us zero error in that case \nor in general\u2019s speaking generally they should give us a minimum error  they should\n\fhelp us to minimize the error rate  so  i need to set up that similar story here  so  we will\nagain revisit the concept of error \n\n\nso now in the case of perceptron i had shown you this figure which they were this data is\nnot linearly separable which is obvious  and i told you that perceptron cannot handle this\ndata  but what do i mean by it cannot handle this data  it cannot give zero error  but what\nwould happen if i run the perceptron algorithm on this  take a guess  what does the\nperceptron algorithm do  \nfine and i could convergences my condition  i could make that condition a bit loose \nwhat is a valid convergence condition that you would lose here use here  till almost all\nmy points are separated properly  so  instead of aiming for one hundred percent separation  i\ncould have a threshold which says as long as ninety percent of the points are separated  i am\nfine with it  that looks like a reasonable thing to do \nso now  if i run the perceptron algorithm with that condition  what do you think will i get\nas a decision boundary  everyone has a picture in mind  ok  let us see  does this match\nwhat   you   had   in   mind   roughly    of   course    there   many   things   possible    but   it   will\nbasically pass through this  now  what is happening here  what is the problem  there\nare   three   blue   points   which   are   wrongly   classified   and  three   red   points   which   are   wrongly\nclassified \nbut in most real world applications we will find that this line is not too bad  you could\nlive with this error  this is probably three out of thirty on both sides which is roughly ten\npercent error  unless you are using it when some mission critical applications    or in\nhealth care where it is a life and death situation or something  in most cases you could\nlive   with   this   right    so   if   you  are   trying   to  predict   whether   people   will   vote   for  a\nparticular party  if you make this kind of error it would be largely ok unless it is a very\nclose election  but largely it would be ok \nso  we could live with this kind of errors in most cases  so  from now on we are not\ngoing to be too optimistic  and if you are going to say that there would be some error  but\n \n\fmy job is to find the weights  such that  my error is  minimized i  want the minimum\npossible error that i could get is that fine  so  again whatever weights we want to learn \nwe are going to be driven by some error function and we would want to minimize that\nerror function \n\n\nso    this   brings   us   to   a   typical   machine   learning   setup   which   has   the   following\ncomponents  so  this perhaps is the most important slide in the course  and i will say this\nat least for one hundred other slides in the course  but at least for now this is the most important \nso  you are given some data xi  yi and you are given n such elements right  so  let me\njust elaborate on this  and give me i will give you some instances of this  let me give\nyou some instances of this right  so  one thing we say i already told you was this  so  this\nis my x and this is my y  so  one example which i gave was about movies  so  this was\ngenre  actor and critics rating and so on  this is one instantiation of this problem  i could\nalso give you another instantiation which was i just told you oil right  so  this is how\nmuch oil can i get  and here my factors were salinity  density and so on  there were\nmany other factors \nso  this was my x  again x belongs to rn  where n is some number integer  and another\nexample could be say fraud detection  so  i have a customer  i am a bank  i have a\ncustomer who has bought some credit card  and i want to predict whether he or she\nwould commit a fraud  and i would look at factors like what is his occupation  maybe\n\fsalary  maybe family size and so on  there could be various factors which i could look at \nso  here again this becomes an x ok  and you could think of various such examples \nright  where you are given an x and you are given a y ok  so  this is the data that you\nhave \nnow  what is machine learning  where does machine learning fit into this  so  we know\nthat there is some relation which exists between y and x  in each of these cases all of us\nare convinced that there is some relation  so  whether a person would commit a fraud\nwould depend on these factors  it is reasonable to assume  that it is not a very wild\nassumption  whether you would find oil at a location would depend on some of these\nfactors and it is related  and similarly for the movie case \nso  there exists some true relation between x and y  such that if i plug this value of x into\nthe relation  it would give me the value of y  there exist a true relation  this true relation\ncould   be   governed   by   various   things   right    it   could   be   governed   by   physical   laws\nexample in the oil mining case  it could be even governed by biological laws  again the\nmarine life in that location and so on  it could be governed by economic law\u2019s  it could\nbe  covered  by  psychology   right  we do  not know   why  a  person  cheats  what  is  his\nfunction that he is using when he cheats and so on right  so  these could depend on\nvarious factors \nbut we all agree that some function exists  hence we get these values for this particular\ninput  for every input we get a certain value  so  there is some function which takes us\nfrom the input to the output  we do not know what this function is  we never know in\npractice it is a very  very complex function is all that we know  we do not know this exact\nfunction  if you knew this exact function then there is no problem to solve  we just use\nthat function and you can predict how much oil and all of us can become billionaires \nso  that is not the case  we do not know what this function is  so  then what do we do in\nmachine learn  we make an assumption  ok  we make an assumption that there is some\nfunction which takes x to y  and this function is governed by some parameters  and this\nis our approximation of how the real world works  and now under this assumption  we\nwant to predict the parameters of this model  given the data \nnow  let us take a very simple case where we could assume that y is equal to wx plus b  i\nam taking this in the scalar single dimensional case  now how would you estimate the\n\fvalues of w and b  oh come on  if i give you two data points  you can estimate the value  or\nshould i write it that would jog your memory  right  this is how we all learn right  so  m\nand c you can estimate if i give you two data points  so  that is the simplest case  now we\nwill   make   similar   assumptions    but   more   complex   functions   and   just   as   we   could\nestimate m and c from the data  we would expect to estimate w s also from the data  so \nthat is what the machine learning setup is so  let us see \n\n\nso  the model when we talk about a machine learning model  it is our approximation of\nthe relation between x and y  and we are free to make any such approximation  so  i\ncould say that this is what i think is the relation between y and x  and which is governed\nby some parameters w  do you know what is this function  have you seen this before \nno  not sigmoid \nwhich model is this  logistic regression  ok  but i could also have made a different\nassumption  i could have made this assumption  what do i get  linear regression  ok \nplease note that this error on the slide  ok  and i could make some other assumption  i\ncould   assume   that   y   is   actually   a   quadratic   function   of   x    i   am   free   to   make   any\nassumptions  the only thing i need to ensure is there is some parameter involved  what is\nwrong with making this assumption  if this is valid  is this also valid  if not  why  there\nare no parameters \n\fso  no not for any x we will get the we will it will depend still depend on the value of x \nif i plug in different values of x  i will still get a different output  there is nothing to\nlearn  what do i do with all the data that i have  there is absolutely nothing i can use it\nto learn  i have just said that y is equal to one over one plus e raised to minus x  i can ignore\nall the data that you had given me  whenever you give me a new x  i will just plug it into\nthis formula and tell you the answer  and that is bound to be wrong because i have not\nadjusted this formula \nnow  once i put in the w s  it gives me this degree of freedom where i can now adjust the\nformula  i can learn the w s in such a way that my predicted y\u2019s are very close to the\nactual y\u2019s  so  that is why we need always need a parametric form  of course  there is\nnonparametric   learning   also    but   i   am   just   saying   in   this   supervised   setup    we   are\nthinking of models whether you have parameters  so  you have the data  you have the\nmodel  the model always has some parameters \n\n\nin all of the above cases  w is a parameter right  either the small w which is a vector  or\nthe capital w which is a matrix right  so  notice that this is a matrix  this is one cross n  n\ncross n and n cross one \nnow  how do we learn these parameters  that is the question that we need to answer \nhow do we learn these parameters  we are convinced about two things that we never know\nthe true function  so  we come up with an approximate function  and we have to insert\n\fsome parameters in that function  so far good  now i have to be able to learn these\nparameters  \nnow for learning these parameters  we have something known as an learning algorithm \nso  did you see any learning algorithm so far  perceptron learning algorithm right  so \nyou already saw the perceptron learning algorithm  and it was able to learn the weights\nfor a perceptron \n\n\nthere are various such algorithms  today we are going to learn one such algorithm which\nis gradient descent \nnow  any kind of learning what is it driven by  learning is driven by errors  objective\nfunction  and so  the analogy which i like to give is  suppose you are trying to learn\ntrigonometry  you have  a chapter  that  is  your training  data   the chapter  has  a  lot of\nformulae  that is your training data  now what is your objective  there are two objectives \nactually i will tell you the easy objective  the training time objective is that once you read\nto the chapter a few times  at least whatever  formulae are given in the chapter  you\nshould be able to produce the correct output for that  so  if i ask you what is sine square\ntheta plus cos square theta  you should be able to answers them  and you should be able\nto give me the correct answer \n\fso  in other words you are trying to minimize the error on the training data  whatever\ntraining   data   you   have   which   is   the   chapter   content    you   want   to   make   zero   errors   in\nanything which is given in the chapter  that is your training error  of course  there is also\nsomething as known as a test error  because after you have learned the chapter  i will\ngive you an independent set of exercises  which might contain questions which are not\nseen in the textbook earlier  so  you would have seen sin square theta plus cos square\ntheta \nbut now i could ask you some other formula  which you should be able to give me\nanswers  if you have learned properly right  so now  right now we are just talking about\nthe training error  that means  getting all the formula in the chapter correctly and our\nchapter is actually the training data which is given to you  this is what we are reading \nso  this always going to be driven by an objective function and our goal is just as we\nwanted to minimize the errors that we make on the formula given in the chapter \nwe want to minimize the errors on the training data  is this set up absolutely clear to\neveryone  anyone who does not understand this has any doubts  so  this is something\nthis is the same framework that we will use again in lecture eighteen  nineteen  twenty and so on to\nexplain some more complex models  so  you have to absolutely make sure that you\nunderstand this  it is not very difficult  but just make sure you understand this fine  so \nlet us concrete at this a bit more \n\n\n\fand we will consider our movie example and try to fit that into this framework  so  what\nis our training data there  they are given movie comma like dislike  and when i say\nmovie  i am just using a shortcut it is actually all the details of the movie  genre  actor \ndirector  critics rating and so on  that is our input  and our output is the like  dislike value \nwhat is a model that i chose  what is a model that i chose  i do not know what is my\ntrue relation between when i like a movie or not  but i made this approximation  that this\nis how y depends on x  and i made sure i introduced some parameters there  i could have\nchosen some other functions also  but i chose this  now the parameter is w this should be\nbold w  the learning algorithm that we are going to use is gradient descent which we will\nbe seeing soon \nand what is an objective function here  can you tell me a formula  so  we have been\ntalking about it in terms of english  that i should be able to get predictions which are as\nclose to the true prediction  can you put it into a formula  y i minus \ny i hat where hat is the prediction  this is the prediction  so  that is y i minus y i hat that\nshould be minimized  is that fine  whole square of that  so  why do you square it  so \nthat is correct \nso  for all the training points  n training points  i want to minimize the square difference\nbetween y i the true prediction  and the prediction  sorry the true value and the predicted\nvalue  is that fine  and why do i use squares  differentiable is one  the other thing \nthe positive errors and the negative errors should not cancel  so  it would be happen that\non some movies  i make a positive error of zero five right  that means  the actual label should\nhave been zero five and i gave one  on some movies i make a negative error of zero five  and these\nwould cancel each other  and i will get the false impression that i am making zero errors \nbut once i square the values the negative values also become positive   so  this cannot\nhappen right  so  that is why we always use the squared error function  and also this is\ndifferentiable which is more important \nnow  the learning algorithm should try to minimize this particular quantity  ok  so  this\nis a typical machine learning setup  almost any supervised learning problem that you see\nyou could cast it in this framework  change the y hat function appropriately  change the\nparameters  appropriately  maybe use a different  learning algorithm  depending  on the\n\fproblem that you are trying to tackle  and you should be able to fit it into the same thing \nis that fine  ok  at least for this course everything that we do we will largely be able to\nfit it into this framework \n\n\n\f"}
{"audio_filepath": "lec002_005.wav", "duration": 703.004, "text": "\nlecture \u2013 three\nin  this  module  we  will  try  to  learn  these  parameters   and  initially  we  will  try  to  learn\nthem by guesswork  and i will show that that is actually infeasible  that is why we need a\nmore principled approach \n\n\nso  we will keep the supervised machine learning setup in mind and now we will focus\non this model  and discuss an algorithm for learning the parameters which are w and b \ngiven some data using a giving appropriate function objective function  so  that is what\nwe are going to focus on \n\f\n\nnow  sigma here stands for the sigmoid function  the logistic function in this case when\nthis sigma is actually the logistic function  and now i am going to simplify this further \nso  that it helps us to do a better analysis  i am just going to consider the case  where i\nam just in one input and the bias  ok  and also following the normal terminology in the\nliterature  this w naught from now on i am going to call it b  because that is the normal\nconvention b stands for bias \nso  i have two parameters w and b  which i need to estimate ok  and this is my model for\nthe   movie   example   and   the   other   change   which   i   am   going   to   make   is   instead   of\ndeciding whether i like or dislike which is one  zero  the setup that i am going to work with is\nthat i am giving the critics rating  and i want to predict the imdb rating  so  i am given\na real value  and i also want to predict a real value \nfor no particular reason this just makes life easier for me for explaining a few things  but\nthe same thing or the same algorithm would also hold if you add a binary output right \nand you will see that later on in the course  so  here is a setup  clear  we just have two\nparameters w and b  and we are going to assume that y belongs to real numbers it is a\nimdb rating and x also belongs to real number  it is a critics rating \n\f\n\nnow  let us see what we are given as training is a set of points  we are given some n\ntraining pairs  and now we understand what this means  that means  for a lot of movie i\nam giving the critics  rating and i am also given the true imdb rating for them  of\ncourse  in the two variable case this does not make much sense  but just bear with me \nand now the training objective is such that whatever my function predicts which is a\nfunction of w  x and b that should be very close to the true output that i know  this is the\nfunction that i want to optimize  now let me ask you this \n\n\n \n\fi am trying to tell you that i am going to give you an algorithm for training this network \nsuppose i have trained this  with two data points  zero five comma zero two and two five comma zero nine  right \nat the end of training  i will give you some values of w and b  let us call them w star\nand b star  these are the final values of w which i have given w and b  what do you\nexpect from these values  \nwhat do you expect at the end of training if i say  now  the network has learned  what do\nyou expect  you are still going to the test case i am just talking about the training still \nwe expect such that what happens if i plug in at the end of training  if i plug in the value\nzero five here what should happen  \nzero nine  so  this is what you expect at the end of training  if you plug in the value zero five it\nshould be very close to zero two the output and if you plug in the value two five  it should be very\nclose to zero nine \nthis is exactly what you expect and this is what training means ok  fine  in other words\nwe hope to find a sigmoid function such that these two points lie on that function  can\nyou imagine a geometric picture for this  what would happen actually  how many if we\ncan imagine it  ok  how many of you get it now  this is what will happen right  so  you\nwill get a sigmoid function such that these two points lie on that fair ok  and that exactly\nmeans that when i plug in this value  i will get this value and when i plug in this value i\nwill get this value right  so  that is what it means \n\n\n\fso  let us see this in more detail \n\n\nand now what we will do is our quest is for this w star and b star  i will try to find this\nmanually  i will do some random guesswork and try to find this because i do not have\nany clear principle algorithm for finding it as of now  so  i will just use some guesswork \nso  i will give my initial guesswork as w equal to zero five  b equal to zero for no reason  i just\npicked up some values right  and this is what the function that i got  what does this\nmean  this function an error  so the sigmoid formula should be here we should have this\nsigmoid formula here \nso  is this a good are you happy with the solution  if i give you   are you happy with this\nsolution  is this good  bad  ugly  has to be something  bad  we will not call it ugly  ok \nso  why is it bad  it is not passing through those points  i will ask you a question how\nbad is it  can you assign a number to it  we are always good at qualitative stuff  but\nquantitatively can you tell me a number  how bad is this  can you tell me a way of\nfinding how bad this is  i already told you in detail how to find that how bad it is \nthe loss function  right \n\f\n\nwe have the loss function  let us see that again and see if we can find out how bad this is \n\n\nso  this is what my loss function is  ok and i have two data points  i will just expand it out \nfine  now i will plug in the values i know this is zero nine  and i will compute the value of f\ntwo five  i will plug in this and i will plug in this ok and this is what i get  so  this is how bad\nit is  what did we actually expect it to be in the good case  zero  so  this is not zero  this is\nzero seventy three  so now we have a quantitative handle on how bad this is  ok  so  let us keep this\n\fin mind and let us try to continue guessing  so  we want the loss function to be asked\nclose to zero as possible  we are not there yet \n\n\nso  then i make a different guess  i say let me try minus zero ten  zero zero what happened now \nis it now good  bad  ugly  \nnow let us call it ugly  right  so  it is worse and how do i know it is worse  because i\nplugged it in to the loss function  and i got a value which is greater than the value at\nwhich i was  so  i clearly know this is bad \nso now this is how my mind is working  right oh i as far as w was positive things looked\nat least i was close to zero in the first decimal  now when i made it negative that does not\nlook good  so  let me just keep it positive and keep increasing it right  so  i saw zero ninety four and\ni also tweaked the b of it i have done complete random guesswork  right  now what\nhappened  good  bad  ugly  \nbetter ok  now what will you do  what would your next case would be  make w even\nmore positive  perhaps that would help  and be even more negative and so on \ni can continue in this manner and actually get close very close to the solution  so  i can\ndo this guesswork and find these values  but it is still an educated guess  right i am not\nguessing in the dark  this is what is helping me drive towards those guesses  and i am just\nlooking at these values and making an educated guess right  and that is the educated\n\fguess which i took that probably making w even more positive would help  but this is\nstill brute force in a sense  right this is not something that you would want to do when\nyou have one hundred  one thousand parameters and so on right  and one million data points and so on \n\n\nso let us look at something which is better than our guesswork algorithm ah  so  we are\nnot there yet actually  on the next slide i am still going to talk about the guesswork\nalgorithm \n\n\nand eventually we will get to something which is better than the guesswork that  ok \n\fso  since we have only two points and two parameters  what i can do is i can take all\npossible values of w and b  right that is what i was trying  i was picking up some values\nof w and b  why just pick some values of w and b  i will pick all possible values of w\ncomma b  right  and i will fix the range  i cannot fix pick it from minus infinity to\ninfinity  but i will pick a range i will say from minus six to six  let me try all values of w\ncomma b compute the loss and plot it  right  let me tell something about this error\nfunction  because this is going to stay with us for quite some time \nso  what you see here is something like a flying carpet  this is colour coded  red is bad \nred are the places where the error is high  blue is good  blue are the places where the\nerror is low  darker the shade of blue  lower the error  darker the shade of red  higher the\nerror  so  in particular if i look at this point  what has happened is  i have taken the\ncorresponding value of w comma b right  which is say minus four comma minus one  right\nsomething like that \ni have plugged that value into my loss function  and i got this as the loss function  this\nhas the loss value  and that is what i have plotted for all values between minus six to plus six\nand minus six to plus six  for w and b  so  everyone understands how i have constructed this\nerror surface \nnow  this of course  becomes and now what i can do is once i see this error surface  i\nknow how good this is the point where i need to be  this is the darkest ah shade  and this\nis where the error is the lowest  so  i can just pick a w comma b value which lies there \nthis is fine  for this toy example where you just have two parameters  but this becomes\nuntractable  once you have more data points and many more parameters \nand that is what happens in most real world applications  right  so  this is not a feasible\nway of going about things  right  and here again note that i have only taken the range\nfrom minus six to six  i do not even know what will happen if i have to look at all values of\nw comma b  right maybe there was something outside here  right which was even more\nlower error or something  right  so  i do not really know that \nso  i cannot  really  use this  so  i need  something  better  than this  plotting  the error\neverywhere and finding it order  that is pure brute force  or surrogate to this was the\nguesswork algorithm  but which is again something we cannot do for if you have large\nnumber of parameters \n\fso  everyone gets this  that this is a way of finding the solution  but this is not feasible \nright that is the only point i am trying to make \n\n\nand we look at the geometric interpretation of what was actually happening in the case\nof the guesswork algorithm with respect to the error surface \n\n\nso  i had chosen some values of w comma b  the first value that i chose actually gave me\nan error of if you remember it was some zero seventy three or something like that right  so  that is the\npoint then i decided to take a very random guess  and my error actually increased  so \n\fyou see that i am actually climbing up on this error surface  i have gone from a slightly\ndarker shade of blue to a lighter shade of blue right  and then i corrected myself and then\nkept moving in a direction where i was going towards the darker and darker shades of\nblue \nso  what i was actually doing is  i was trying to traverse the error surface and land up in\nthe good regions which were the dark blue regions  now  what i want to do is i want an\nalgorithm which will allow me to do this in a principled manner which is neither brute\nforce nor guesswork  so  that is what we learn in that module \n\f"}
{"audio_filepath": "lec002_006.wav", "duration": 1870.41, "text": "\nlearning parameter  gradient descent \nin this module  we will talk about gradient descent  \n\n \nso   what  we  want  to  do  is  find  a  more  efficient  and  principled  way  of  navigating  the \nerror surface   \n\f\n \nand  the goal is to find a better way of doing this  \n\n \nso  let us start by setting up things we will define some notations and some parameters \nand so on and from there on we will try to come to the algorithm  ok  so  my parameters \nin this case were w comma b  what i am going to do is i am going to put them into an \narray or a vector  right and call that vector as theta  so  theta is the vector of parameters \nand theta belongs to r  r what  rtwo  right  there are two parameters here  so  it is a two \ndimensional vector  \n \n \n\f\n \nnow  what i want is again what i will do is  i do not know what the value of w comma b \nis  so  i started with a random guess  so  that is always going to be my starting point i \nwill always start with a random guess and from there on move on to good values  now  \nonce i have started with a random guess i want  you to tell me some changes that i can \nmake  to  w  and  b   so   that  i  land  up  in  better  situations   right  that  means   i  land  up  in \nsituations where the error is less is that fine  \nso  that change in w and b  i am going to call it as delta w and delta b and that again is a \nvector which is storing these two values  so  this is the picture right i want to take theta \nand i want to add a small change to it  so  this is my theta  this vector is actually theta \nright this is the theta vector  i want to add a small change to it which is again a vector  \nthis is delta w comma delta b such that  i  will  get  a new value  for theta  new  so   theta \nnew would be what actually theta new is equal to w new comma b new  is that fine that \nis what theta new means  \n \n\f\n \nnow  what has happened is actually when i have added delta theta to theta i have moved \nin the direction of delta theta  i have come from here to here  now  i am going to be a bit \nconservative and i am going to say that while i am ok in moving in the direction of delta \ntheta i do not want to make a giant stride what i will do is i will just move by a small \nquantity in that direction  \nso  this delta theta is this large magnitude  so  all i am saying is that i will not i move in \nthat direction i am fine with that  but i do not want to make a giant stride i will just take \na small stride in that direction  so  eta is a scalar which actually scales down delta theta  \nso   now   if  i  am  going  to  take  only  a  small  step  in  that  direction  instead  of  this  large \nchange i will just get a smaller change theta new  so  red the red vector is actually going \nto be the movement which i make  that is the new value of theta  so  theta new is equal \nto the original theta plus a small step in the direction of delta theta  \nso   everything  is  clear   you  are  done  we  are  done  with  gradient  descent   what  is \nmissing  what is delta theta  right  i am telling you i want to move in a certain direction  \nbut  what is the right delta theta to use  how many of you know the answer to this  what \nis the answer  move in the direction  \nopposite  to  the  gradient   why   where  does  that  answer  come  from   not  the  ml  class \nfolks   how  many  of  you  know  why  we  need  to  move  in  the  direction  opposite  to  the \ngradient  why  ok  we will see  ok  so  that is the question that we need to answer  if i \n \n\fgive  you  an  answer  to  this  question  then  what  is  it  i  am  doing  i  am  giving  you  a \nprincipled way such that you start from a random value of theta move in certain direction \nand  you  will ensure that  your loss has decreased and then  you have to  keep doing this \nright  so  that is the set up and the answer to this comes from taylor series  \n\n \nso  now what i am going to do is i am going to give you the right direction delta theta  \nfine and for ease of notation i am going to call it as u  so  remember what this delta theta \nis  what is it  change in w comma change in b  so  it is a vector in rtwo  remember that  \nok i am just going to call it as u  now  this is what taylor series tells me what it tells me \nis that if i am at a certain value of theta and if i want to change that value a bit then what \nis going to be the new value of the loss function or any function for that point and this is \nthe formula for that  ok  now  what is let us see  what are some quantities here what is \nthis quantity scalar  vector matrix  \nscalar  this  \nvector  we just did that right  it is a vector  what about this  \nwhat is this quantity  actually  \ngradient  what is the gradient  what is the gradient  no  you are telling me how to use \nthe gradient  i am asking you what is the gradient you are giving me absolutely correct \nand absolutely useless definitions  \n \n\fthat is a very good answer  ok  so  now what i am going to do is i am going to digress a \nbit   and  i  am  going  to  tell  you  something  about  derivatives  partial   derivatives  and \ngradients and then we will come back to this  ok  so  now  suppose you have a function l \nthis  is  l  in  my  handwriting  this  function  of  w   and  say  this  function  is  w  square   ok  \nnow  what is what is this called  a derivative of the function with respect to w  this is \nthe  derivative  and  you  know  this  is  twow   ok   now   suppose  i  have  a  function  b  square  \nnow what is this quantity  \nis a partial derivative of the function with respect to w  why partial  \nbecause  it is considering b as a constant and taking the derivative with respect to only \none of the variables  right this happens to be and what is this quantity  oh sorry  so  is w \ncomma b  right this is the partial derivative with respect to b  ok  \nnow  can you tell me what is a gradient  the gradient is nothing but it is just these two \npartial  derivatives  taken  together  and  put  into  a  vector   right   now   suppose  i  had  a \nfunction which depended on hundred variables   what  would the  gradient be size of the \ngradient  \nrone hundred it would lie  it would be a hundred dimensional case  ok  so  now can you tell me \nwith this evidence in knowledge  but this primer can you tell me  what this is  this is a \ngradient vector  \nwhich is right there in front of you in a red ink  \nthis is what it is  right  fine  everyone ok with that  so  actually the right way to write \nthis and probably we need to correct in the slides would be theta  so  remember that theta \nis equal to w comma b  so  this is the derivative of l theta with respect to theta which is \nnothing   but  the  collection  of  the  partial  derivatives  with  respect  to  the  components  of \ntheta   is  it  fine   so   everybody  understands  what  is  a  derivative  partial  derivative  and \ngradient  ok  fine  so  now  the gradient is a vector in this case  fine ok  \n\f\n \nso  now what is this quantity  it is a  \nno  it is what is this  \nthe dot product between these two vectors  ok fine  now  one last thing and many more \nthings actually  so  what is this  square of the gradient  \nthis is  not  the square of the gradient  what  is  this  hessian  fine  everyone knows the \ntextbook what said can you tell me  what does it is a scalar  vector matrix  \nmatrix  what is the size of this matrix  \ntwo by two  what are the elements of this matrix  \nsecond order partial derivatives  right  so  it is the gradient of the gradient  right  is that \nfine  so  what does that mean you had this gradient  this is the gradient  now  you want \nto  take  the  gradient  of  this  again  with  respect  to  w  comma  b   right  that  is  what  this \nmeans   it  is  a  gradient  of  the  gradient  right   so   what  that  means   is  we  will  take  the \ngradient  of  the  first  quantity  again  with  respect  to  w   so   that  would  be  dou  square  by \ndou w what would this quantity be   \nwhat would this be  \n \n\fis that fine and you can fill in this quantity  right  so  now  it is clear  what the hessian \nis   it  is  the  derivative  of  the  derivative  and  it  would  be  a  matrix   ok   is  that  clear  to \neveryone   so   i  have  a  habit  of  doing  a  lot  of  these  basic  stuff  i  know  that  the  top  twenty \npercent of the class gets really pissed off when i do this  but as a philosophy i teach for \nthe bottom thirty percent of the class  \nso  i do not mind that and the other thing is i use slides  so  i do not write a lot of math \nso  i can cover a lot of material despite doing all this basic stuff  right  so  i am going to \nstick to that what i am trying to say is that write this in the feedback that you do not like \nthis basic stuff  but it is just that i am going to ignore that feedback   i mean just being \nhonest  right  so  i like doing this because it just takes me ten minutes to do this and for \nthe rest  of the  class  i  do not have to  look at  blank faces  afterwards   right  so   it really \nhelps me a lot fine  so  is that all clear all the quantities here are clear  \nso  now  so  this is the gradient this is the hessian and now eta  remember what did we \nsay about eta  \nit is a small quantity and what do we do with small quantities always in maths  \nwe ignore them  so  once we take their powers you are always ignore them  whether it \nis correct or not who cares i mean someone has told it  it is good to ignore  so  we will \nignore it  right  so  now  all these higher order terms we can ignore right  that means  i \nwill only consider this  fine  \nso  let us again look at what the setup is  the setup is that i have some value of theta i \nwant to move away from that value such that what do you say about this loss compared \nto  this  loss   i  will  call  this  the  new  loss  and  i  will  call  this  the  old  loss   what  is  the \nrelation between them  \nthe new loss should be  \nless than so if i or someone gives you a u  i am not getting ok  someone gives you this u \nthen what does what when would you say it is a good u  \n\f\n \nif this condition holds everyone agrees with that  right  so  i have found a good direction \nto move in if this condition holds  now  this condition actually implies that this condition \nshould hold  right  this is  l  theta plus  eta u   right  so   if  i just do minus here  i  get  this  \nright   so   this  quantity  which  should  be  less  than  equal  to  zero   implies  that  this  quantity \nshould be less than equal to zero and remember eta is a positive constant  ok  why cannot it \nbe negative  \nwhy  because you wanted to take a small step in that direction  if we make it negative \nwe will do what  \nwe will reverse the direction  we do not want that as of now  right  so  eta is that for a \npositive  quantity   so   that  means   this  quantity  should  be  less  than  zero   is  it  fine  with \neveryone  \n \n\f\n \nso  so far after all this story what we are left with is this condition should hold for the u \nthat i am trying to choose  so that i can be sure that i have chosen the correct u  right and \nthe  definition  of  correct  u  is  that  the  loss  at  the  previous  step  the  loss  of  the  new  step \nshould be less than the loss  at  the previous step  is  that fine  so  that is what  we have \narrived at  \nnow  what is the range of this quantity  that is why i asked you what is this  this is a  \ndot product  i will leave it at that  so  now  you tell me what is the range of this  people \nfrom the ml class cannot answer  did i cover this in the ml class  no  ok  fine  what \nis the range of this  not a very hard question  \nplus or minus  \nstudent  \n  \nvery good  how many of you understood that answer  he said plus or minus mod of u \ninto mod of gradient the gradient vector  right why is it so  easy  \nlet  beta  be  the  angle  between  u  t  and  this  between  sorry  it  should  not  be  u  transpose \nbetween u and the gradient then we know that this condition holds  cos beta is given by \nthis quantity and we know that cos beta lies between minus  one and one  ok  now  if  i just \nsay that this quantity is equal to k then i can just get this condition  \n \n\fnow  let us see what are we trying to do  we are trying to find a u such that this quantity \nis negative  we are trying to find the use  such that this quantity is negative  now i just \nstop at negative we would like to make it as negative as possible  right because the more \nthan negative it is  the more will be the decrease  in  my loss function right  because this \nquantity tell me tells me how much my loss decreases  so  the more the negative it is the \nmore the loss will decrease  so  let me make it as negative as possible  \nnow  what is that value  when will that happen  when alpha is  you know the answer \nyou started with the answer  \nstudent  \n  \nno  what is that one phrase which you have marked up move in the direction   \nstudent  \n  \nok now  think of that  \nstudent  \n  \nwhat would happen when this is the most negative it can be  what would the angle be  \nstudent  \n  \none hundred and eighty  degrees   how  many  of  you  get  that  because  when  this  is  the  most  negative   that \nmeans  the cos beta is actually minus one and when is cos beta minus one when the angle is \none hundred and eighty degrees  that means  u should be such that it is at one hundred and eighty degrees to the gradient  hence \nrepeat the phrase  \nstudent  \n  \nmove  in  a  direction  opposite  to  the  gradient   is  that  fine   everyone  gets  it  now   why \nyou need to move in the direction opposite to the gradient  \n\f\n \nso   this  is  what  the  gradient  descent  rule  is   you  are  at  a  particular  value  of  theta  you \nwant to move to a new value of theta such that your new loss is less than the current loss  \nwhat  gradient descent  tells  you is move in a direction opposite to  the  gradient  so   are \nyou  fine  with  this   now   with  gradients  i  have  come  to  scalars   but  i  will  just  explain \nwhat i have written here  \nso  this quantity is nothing  but theta t plus one  right  is equal to theta t  right and what is \nthis  right  so  the new theta is equal to the current theta minus  why because we want to \nmove in the direction opposite  so  it is basically theta t plus one is equal to theta t plus eta \ninto a negative direction  right the direction negative to the gradient hence  you get that \nminus one  \nnow  what are these quantities let me just take that carefully  so  this quantity is gradient \nof  the  loss  function  with  respect  to  w   sorry  the  partial  derivative  of  the  loss  function \nwith respect to w evaluated at w is equal to wt and b equal to bt  what does that mean  \nso   remember  when  you  are  dealing  with  derivatives  as  always  a  formula  and  then  a \nvalue add that at a particular value  so  what is the derivative of x square with respect to \nwhat does not matter twox  \nso  derivative of x square with respect to x is twox  what is the value of this derivative at x \nequal to one  two  right  so  you see the difference you have a formula which is twox  now you \nsubstitute in a particular value and you get the value at that particular value  ok  so  that \n \n\fis what this means because you are already at w t comma b t  now you cannot subtract a \nformula from here right you have to put subtract a value  so  you know what the formula \nis you plug in the values of wt comma bt get that value and subtract it from your current \nwt  is that fine  so  everyone completely understands what is the gradient descent rule is  \nfine  \n\n \nso  now we have a more principled way of moving in the w b plane  what do i mean by \nthat  remember this was our w b plane  this was our error  this is something what our \nerror surface looked like  it was this flying  carpet   i was randomly moving on the w b \nplane earlier  right and trying to guess what the errors or trying to compute the error and \nthen settle for a particular value  now  i have a more principled way of moving in the w \nb plane  i know what is the next step based on the current step i just need to move in the \ndirection opposite to the gradient  \nso  let us try to  so  this is what it tells me for one step  but i need to keep doing this till  \nwhat is that golden word  \nstudent  \n  \nconvergence  right  i have to keep doing this till convergence  ok  \n \n\f\n \nso  let us create an algorithm out of this rule i will start a time step zero  i will do this for \nsome max iterations  instead of saying till convergence i will do it for some iterations  at \nevery iteration i will this is how i will update my weights  i will take the current weights  \nsubtract the gradient from that and get the new weights i mean not subtract the gradient \nsubtract  this  quantity  and  get  the  new  weights   so   now   is  everything  clear   is  the \ngradient descent algorithm done  can you do it for the toy network which i had  is there \nsomething still missing  \nstudent  \n  \neta  is  fine  we  will  take  a  small  value  zero one  or  something   actually   not  told  you  what \nthese  are   right  i  means  to  write  it  you  know  these  are  derivatives   but  what  is  this \nactually  ok  so  let us see that now  so  that is what we are going to see next  \n \n\f\n \nso   now  we  want  to  find  out  we  are  in  the  car  quest  is  for  this  delta  sorry  the  partial \nderivative  with  respect  to  w  and  partial  derivative  with  respect  to  b   that  is  the  thing \nwhich  we  had  plugged  in  the  formula   but  we  do  not  know  what  that  is   right   so   we \nneed to find that out  so  now  for simplicity let us assume there is only one point of it \nwhich is x comma y  so  earlier we had this xone yone and xtwo ytwo  now  i am just assuming \nthere is only one point which is x comma y  \n\n \n \n \n\fso  now  what is a loss function earlier i had this summation over i equal to one to two  but i \nhave just one x comma y  so  i will just use that this is what my loss function and what \nare the quantities that i am interested in finding  one is this the partial derivative of this \nloss function with respect to w  \n\n \nso  let us do this lets actually derive this  so  this is what it looks like now you have to \nhelp me in deriving this what will i do first  \nstudent  \n  \ntell me the next step  \nstudent  \n  \ntwo into f of x minus y and push the gradient inside  of course the derivative is that fine  \nanyone who has a problem with this  next  y is a constant this is the true i remember  \nso  that is why this is a constant is not the predicted y  \nnow  this quantity  what is f of x actually  \nstudent  \n  \n \n\fsigmoid function  right   so i will just write it  now  this is the quantity that  i need the \nderivative for  so  i will just write it here  what is the next step  this is of the form one \nover x  so  what will it be  \nstudent  \n  \nminus one over x square and then you put the derivative and say it  is that fine  now  the \nquantity inside is of the form e raise to x  so  the derivative is e raise to x and you push it \ninside  is this fine  so  this on slide should come both these are coming together  so  is \nthat fine  now  what is this actually  \nstudent  f of x  \nf of x  what is this  \nstudent  \n  \nthis is actually one minus f of x  just take my word for it for now  you can go home and \nwork it out  right  so  this actually if you do one minus this and do some trickery you will \nget to this quantity  right  so  what you get is a very simple formula f of x into one minus f \nof x into x  i am going to substitute back here  so  now i exactly know what the partial \nderivative of w is  \n\n \n \n\fso   there is  only one point  then this  is what  the  partial  derivative with  respect  to  w is \ngoing to be of the loss function  right   if there were two points what would happen  if \nthere  were  two  points  my  loss  function  was  this  is  a  sum  of  two  elements   and  i  am \ntaking some derivative of a sum  i will get a sum of derivatives  right  \n\n \nso   how  many  of  you  will  not  cringe  if  i  say  this  is  the  answer   anyone  who  has  a \nproblem with this  you get this  how many if you do not get this  how many of you get \nthis   good   fine   now   can  you  do  a  similar  thing  for  b   can  you  tell  me  the  answer \nwithout actually deriving it  \nstudent  \n  \ni can perfectly understand what you are saying  \nstudent  \n  \nx would not be there  right because this last x that you see here came because w into x \nwas there  but b we are not multiplying x  so  what we will get is this  you can go home \nand check  \n \n\f\n \nso   now  we  have  everything  that  we  need   now   we  actually  have  everything  that  we \nneed   ok   no  more  trick  questions   so   now   we  will  write  code  to  do  this   ok   we  will \nactually implement the code and see what happens  so  these are the two data points that \ni had zero five comma zero two and two five comma zero nine  the first thing which i need is something which \ncan implement the sigmoid function  so  this is one over one plus e raise to minus w x plus \nb is that fine  \nnow  i need something which can compute the error  so  this is summation of half into f \nof x minus y the whole square  i go over all the data points summation of half into f of x \nminus y the whole square  is that fine  now  what i will do is i will take this try out a lot \nof values of w comma b and plot the error surface ok  but this is only for illustration  in \npractice i will not do this  we just know that this error surface exist i just want to verify \nthat  whatever  algorithm  i  come  up  with  does  not  efficient  navigation  of  this  error \nsurface  that is what i want to verify  that is why i am plotting this  \nnext  time  you  need  a  function  which  can  compute  grad  of  b   we  just  saw  this  on  the \nprevious  slide  this  is  f  of  x  minus  y  into  f  of  x  into  one  minus  f  of  x   right   simple  \neveryone  is  fine  with  this   then  i  need  a  function  which  can  compute  the  grad  with \nrespect to w same thing except that i have this x at the end  so  i have all the ingredients \nin place  now  what would i do  what is the next thing that i will write  the main loop \nright  i will write the main loop now  \n \n\fso  this is what the main loop look like looks like  i start with some random initialize for \nw comma b  remember that our initial theta which is composed of w comma b is going \nto be some random guess  so  i started with the random guess which is minus two comma \ntwo  i have chosen eta to be one  that means  i am not going to be conservative i am going to \nmove in  the direction  of the gradient   if  i  chosen at  zero one and zero one  i would  have been \nconservative  and  i  am  going  to  run  this  till  one thousand  epochs  which  is  my  notion  of \nconversions  \nnow   in  each  epoch  what  i  am  doing  is  for  every  data  point   so   remember  that  this \ngradient with respect to w was a summation of i equal to one to two and that formula  right  \nso   for  each  data  point  i  am  computing  the  grad  adding  it   right   so   that  is  the \nsummation part similar thing i am doing for b  once i have computed the gradient which \nis the summation quantity i am just moving in the direction of the gradient  is that fine  \neveryone understands the code  it is simple python code and it does exactly what i had \nshown in the pseudo code  \nnow   let  us  execute  this  code  and  see  what  happens   so   i  will  start  with  my  random \npoint  which  was  minus  two  comma  two  and  now   i  am  going  to  actually  run  this  code  and \nkeep  plotting  what  happens  on  the  figure   so   just  pay  attention  fine   so   now   here  is \nhow the code is running  see what is happening  what is happening actually  so  at every \npoint i am changing my w  so that i  am moving in the direction of the gradient i  keep \ndoing that as i keep doing that my error keeps decreasing  why  because that is exactly \nwhat we got from taylor series that if we do this the error is bound to decrease  right and \nthen we keep doing this and after a few iterations we will actually reach almost the value \nwhich  is  the  zero  error   right  and  this  same  thing  would  happen  if  you  start  from \nanywhere  else  it  will  keep  moving  in  a  principled  way  and  reach  the  low  error \nconfiguration  \nnow  some of you would say that maybe this was the shortest path  right  it could have \njust  rolled  over  from  there   but  that  is  not  a  principled  way  of  doing  that  right  we  the \nprincipled way of doing it is to move in the direction of the gradient  you might take a \nlonger route  but reach your destination  taking shortcuts is always risky  in life as well \nas here  so  so  do not please this is an advice for error assignments and so on  so  this is \nthe  more  principled  way  and  we  will  reach  the  solution   so   that  is  what  is  happening \n\fhere  so  we have actually derived everything that we needed and this is all you need to \nwrite for gradient descent for this toy example that you had  \nnow   answer  this  question   now   suppose  i  had  hundred  such  variables   instead  of  w \ncomma  b   i  had  hundred  such  variables   what  would  happen   you  do  not  have  to \nvisualize it  \nstudent  \n  \nin terms of the code  \nstudent  \n  \ni will just need to have these functions for all of those  i will have to calculate it by hand  \nbut  still  doable  it  is  just  a  lot  of  tedious  work  of  course   later  on  we  will  see  a  more \nrefined way of doing this where we can do a lot of these computations at one go  so  we \ncan directly start operating in vectors as opposed to scalars here   i am treating w and b \nseparately here i could have actually had a function which tells me grad of theta directly  \nright   and  later  on  we  will  see  something  like  this  ok   but  for  now  the  code  is  still \nrunning here  \n\n \nnow   it  suffices   so   later  on  we  will  see  gradient  descent  in  more  detail  in  the  course \nand we will also see a lot of variants of gradient descent  but for now it suffices that we \n \n\fhave  an  algorithm  which  can  learn  the  parameters  of  a  sigmoid  neuron   so   just  as  we \nhad the perceptron learning algorithm  we have the gradient descent learning algorithm \nwhich  can  help  us  learn  the  parameters  of  the  sigmoid  neurons  starting  from  random \nvalues  and it gives a principled approach for doing that  \n\f"}
{"audio_filepath": "lec002_007.wav", "duration": 2124.169, "text": "\nrepresentation power of a multilayer network of sigmoid neurons \nbefore we move on to the next modulate some small corrections from yesterday\u2019s class  \n\n \nso   one  was  this  partial  derivative  it  should  have  been  dou  w  square   so   we  already \ntaken one derivative with respect to w and now you are taking another derivative it is the \ngradient of the gradient  and similarly should this should have been  dou b square  and \nthis should have been dou w dou b  \n\f\n \nthe  other  small  thing  which  i  wanted  to  say  was  so   when  i  was  executing  this \nalgorithm  right  so   i  forgot  to  mention that just notice what  is  happening is  the black \ndot that you see the black dots that you see  right and which are very close to each other  \nactually  because you are just making small movements those are the changes in the w \ncomma b values and the red dots  are the corresponding loss to that w comma b values \nright  just to clarify  \nso  that is why you see a movement on the w b plane which is this movement and as you \nkeep changing that your loss function changes and it becomes better and better right  that \nmeans  it goes closer to zero  \n \n\f\n \nso   in  this  module  we  are  going  to  talk  about  the  representation  power  of  a  multilayer \nnetwork of sigmoid neurons  right  so  i am going to compare these two things which are \nwritten  in  the title  so   first  tell  me  what  was  the  representation  power of a multilayer \nnetwork of perceptrons  ok  i roughly hear what you are saying and basically what you \nare telling me is that a multilayer of network of perceptrons with  a single hidden layer \ncan be used to represent any boolean function precisely  right  no  errors that\u2019s what we \nsaw with that illustrative proof where we actually constructed once its network  \nnow  what is the representation power of a multilayer network of sigmoid neurons  so  \nmultilayer network of neurons with a single hidden layer can be used to approximate  ok  \nso  just see the difference in the language  so  this was a represent  that means  exactly \nthis  is  approximate   that  means  i  will  tolerate  some  error   any  continuous  function \ninstead of boolean function to any desired precision  so  this was not this was precisely \nwith no errors this is up to any arbitrary desired precision  \nso  what does this mean  actually what is the meaning of this  so  there is a guarantee \nthat for any function  ok which takes our original x from r n to r m what is the m that \nwe have been considering in  all our examples one right  we just care about  one output  \nbut  it  can  be  r  m  also   we  can  always  find  a  neural  network  with  one  hidden  layer \ncontaining  enough  neurons   so   that  is  the  operating  trace  here  enough  neurons  whose \noutput  g  of  x   so   that  means   you  would  have  a  network  it  would  take  as  input  n  x  it \n \n\fwould produce some y hat and that is what i am calling as g of x  right  that g of x would \nbe very close to the true function f of x  \nso  remember that we said that there is this true function f of x which gives us the true \ny\u2019s and we are trying to predict this y hat  so  the true why i am calling by f of x and the \ny hat i am calling by g of x and you can come up with a neural network which can give \nyou values which can predict values which are very close to your true values  does that \nmake sense  do you see the value of this theorem  what is it trying to tell me  tell me  \ncan you can you give me an interpretation of this  why is this so useful  do you know \nwhat this theorem is called universal approximations here and we did that in the history  \nright  \n\n \nso   this  was  one thousand  nine hundred and eighty nine   ok   what  is  the  significance  of  this   why  do  we  care  about  such \narbitrary functions and what does this theorem telling us actually  it is of course  telling \nus something about the representation power of a multilayer network of sigmoid neurons  \nbut why is this important  so  we will see that  \n \n\f\n \nso   this  the  remainder  of  the  lecture  i  have  borrowed  ideas  from  this  url  you  should \nactually  read  this  it  is  a  very  interesting  book  it  is  available  online  for  free  very \nillustrative  so  please take a look at it  ok  \n\n \nso   now  actually  what  we  are  interested  in  is  we  are  interested  in  knowing  whether  a \nnetwork of neurons can be used to represent any arbitrary function like the one shown in \nthe figure  ok  so  let me put some labels on this  so  they understand what i am trying to \nsay  suppose  this is salinity  again i go back to my oil mining example and i say that my \n \n \n\fdecisions are based only on a single variable which is salinity and this is actually how the \namount  of  oil  varies  right  as  the  salinity  increase   it  is  a  very  arbitrary  function   it  is \ndefinitely not a linear function  it is not even a quadratic function  it is not an exponential \nfunction  it is just some arbitrary function  but a mathematical function this is possible  it \nis quite likely that salinities has this influence or in oil production or maybe it does not  \nbut i am just taking that as an example  \nnow  what do we want the network to learn  if i take some data and train the network at \nthe end of training  what do i want  so  if i feed at this point after training  what should \nhappen  it should give me this value  right that is what training means and that means  i \nshould be able to approximate this curve  if i do that that means  i have learned from the \ntraining data  so  let us see  \n\n \nnow   we  make  an  observation  that  such  an  arbitrary  function  can  actually  be \napproximated by a lot of something that we call as tower functions  these are all single i \nmean  pulse  functions  which  you  have  many  of  these   and  you  could  have  an \napproximation right and you can see that this approximation is bad at many places  but \nstill it is an approximation it largely gives you the same shape as the original curve  what \nwould happen if i increase the number of such tower functions  \n \n\f\n \nstudent  \n  \nthe approximation would improve  right  if i keep increasing it the approximation would \ngo more and more better  right  so  now  just try to keep things in mind whether i write \nin the theorem  right you can make it arbitrarily close to the actual value  that means  you \ncan  keep  doing  something   so   that  your  approximation  becomes  better  and  better  and \nyou already see something of that sort  this is still in the sense of a figure  we need to \nrelate  this  back  to  a  neural  network   but  you  see  that  as  i  am  increasing  these  tower \nfunctions i become approx arbitrarily close to the actual function  \n \n\f\n \nnow  this is what is actually happening right i have multiple such tower functions i am \nadding them up all of them are shifted in time  so  this tower function is actually this one \nthis  tower  function  is  actually  this  one  and  so  on   right  and  i  have  not  drawn  the \nremaining ones i am taking all of these tower functions adding them up and getting my \noriginal  function   right  and  the  more  such  tower  functions  have  the  better  is  the \napproximation  \n\n \n \n \n\fnow  you make a few observations right all these tower functions are actually the same \nwhat is the only difference they just shifted and their magnitude changes right  but they \nare all tower function right  so  let us think of this that if i know how to make a rectangle \nthen i can make any rectangle  right  i just need to change the size of the rectangle and \nmaybe shift it or oriented differently or something  right  so  they are all similar i just \nneed to learn how to draw a tower right  that is what my quest is  \nnow  if i take the original input salinity pass it through multiple such blocks each block \nis  capable  of  making  a  tower  function  and  each  of  these  would  give  me  one  of  these \ntowers that i am looking for and i am looking for so  many of these  right  if i have as \nmany such tower makers then i could get these towers i could just add them up  and then \nget the original function back  and the more these i have the better is my approximation  \nright  so  i am taking as input the salinity and trying to predict the oil  does this make \nsense   still  we  have  not  figured  out  a  neural  network  way  of  doing  this  we  are  still \nbuilding intuitions of how to do this  \nnow  our job now is to figure out what goes in this black box that is the tower maker and \nhow does it connect to neural networks  if you figure that out then our story is complete  \nthen we know that a neural  network can  actually  do this and that precisely proves the \nstatement which i had made that it can it can represent arbitrary functions  so  we will \nfigure this out over the next few slides  \n\n \n \n\fnow  if you take the logistic function and set w to w to a very high value  what will we \nget  just try to think about it  the answer is already written  but i want you to imagine it  \nw covers what  \nstudent  \n  \nthe slope  right  as i make w very high what will happen is i will get the  so  let us try \nchanging the value of w  ok  i just increase the value of w and see what happens to the \nsigmoid curve  \n\n \nsome error here  actually there is some problem the w value should have increased and \nthat is how the sigmoid slope increases not the b value the b value comes later on  so  \nactually  sorry  about  this   the  w  value  as  i  keep  increasing   so   do  not  think  that  b  is \nincreasing think that the w is increasing   it will become sharper and sharper and it will \ncome very close to the step function  right  it will not become exactly the step function \nthat will only become in the limit  but if i keep increasing i will get very close to the step \nfunction everyone agrees with this  \nnow  what happens if i increase the value of b  it will shift everyone is confident about \nthat  can you tell me why  \nstudent  \n  \n \n\fwhat will shift actually  the point at which the transition happens  right  so  what is this \npoint actually  \nstudent  \n  \nthis is the point at which i get that half value  right and let us look at our function  this is \na function  when will i get that half value  when w x plus b is  \nstudent  zero  \nzero  right  so  that means  x equal to minus b by w  that is why it is proportional to b  so  \nas i keep increasing the value of b  this will keep shifting  ok   is that fine everyone ok \nwith this  \n\n \nnow  what  if  i take  two such modified sigmoid functions which are shifted differently \nand both are very close to the step function  right  so  here is where one threshold is  here \nis where the other threshold is and now i subtract this one function from the other  what \nwill i get  \nyou know the term  \nyou will get a tower  right  is that fine  everyone gets this  right  so  these places up to \nthis point both are zero  so  zero minus zero will be zero  at this for this small range this is one and this \n \n\fis zero  so  that one minus zero and then afterwards both are one  so  one minus one would be zero  so  you \nget that tower function  so  now i have my tower maker  \n\n \nnow   can  we  come  up  with  a  neural  network  to  represent  this  operation   i  want  a \nsigmoid neuron  i was working with a sigmoid neuron with some arbitrary weights  right  \nso  that  i  recover  that  step  function   can  you  imagine   now   given  x   i  want  this tower \nfunction and that is exactly what one of the blocks was  right  so  what i am asking you \nis oh god  so  i am asking you to give me a neural network for this  can you think of it  \ncan you try imagining it  \ntwo neurons in the hidden layer  how many of you agree with that  ok  can you can you \ntake some more time to imagine what it would be  \nand i have already  ok right  \n \n\f\n \nso  this w one  b one if i set it appropriately i will get this step function  if this w two b two  i set it \nappropriately i will get this step function  now  i needed to subtract one from the other  \nright  so  i will do plus one minus one  this is just a simple addition and i will get this  is that \nfine   everyone  agrees  with  this   this  is  just  a  adder   right   this  is  just  an  aggregator \neveryone  gets  this   so  now   i  have  given  you  the  tower  maker   if  you  put  enough  of \nthese tower makers and learn the w\u2019s appropriately what will you get  \nthat  function  that  we  were  needed   so   you  can  approximate  it  arbitrarily  to  any \nprecision that you want as long as you keep increasing the number of these units  right  \nso  these units actually give you one tower more of these units that if you have actually \nthis much this is the input ring  the more such tower makers that you have the more is \nthe  bars  that  you  will  get  and  then  you  can  approximate  everyone  gets  the  intuition \nbehind this  fine  ok  this all is always good in one dimension  \n \n\f\n \nnow  what will happen in two dimensions  what if we have more than one input  what \nis the tower there  do you do you guys all do all know what is the tower there  \nif you say no  i will give you a zero on the assignment  remember the last question of \nthe assignment  did  you all make a tower  did you all make a twod tower  did  you all \ncopy that  no  so  what if we have more than one inputs suppose you had again trying to \ntake a decision about whether we will find oil at a particular location of the ocean  right \nand suppose  now we base it on two two  right  so  say this is salinity  this should be x one  \nshould be x two should be y  and this is pressure  \nnow  just observe about the red and blue points  so  the red points are where  you will \nnot  for those configurations of salinity comma pressure you will not find oil and the blue \npoints are for which you will find oil  what is the one thing that you can tell me about \nthe red points and the blue points  not linearly separable  right  but we still want to be \nable to learn this  is that fine  a single perceptron cannot do it i will also make a case for \na single sigmoid neuron cannot  do it and then  i  will show  you that   in fact   first  i  will \nshow you that with a network of neurons we can do it and then i will show that  with a \nsignal single sigmoid neuron you cannot not actually do that  \nso  now this is again a valid problem you could have we could imagine that you will get \nthis kind of data where you have two factors and your function is some arbitrary function \nof  these  two  factors   it  is  not  a  need  linear  boundary  between  the  blue  and  red  points  \n \n\feveryone sees that the blue and red points are not linearly separable you cannot draw a \nplane  such  that  all  your  red  points  lie  on  one  side  and  the  blue  points  lie  on  the  other \nside   everyone  sees  that  ok   but  the  solution  which  i  have  plotted  here  that  is  a  good \nsolution   it  makes sure that  all the  red points are in  this  region  and the blue points are \noutside  \nso  it will predict a high value for these red points and a zero value everywhere for the \nblue points  is that obvious  how many of you understand that figure  ok  good  \n\n \nso  now i want to show that even in two dimensions i could come up with arbitrary  i could \ncome  up  with  a  neural  network  which  could  actually  approximate  this  and  again  what \nwill i look for  a tower maker  right  i just want something which can make towers and \napproximate it  \n \n\f\n \nso  this is what a two dimensional sigmoid looks like slightly incorrect because i have what \ni have done is i have actually said w two to zero  so  if you actually i would want you to do \nthis go back and plot this for w one equal to three and w two equal to three  \njust go back and plot this and see what  you get you will not get such a smooth such a \nnice  looking  s   but  you  will  still  get  something  which  looks  like  looks  like  a  snakes \nhood  right  so  in still get that s shaped function it just that it would be bent at some \npoints and it be thinner at some points and broader at the other points  so  just go back \nand see and then you will realize what is happening  right  \n \n\f\n \nso  here again what we want to figure out is from the single sigmoid i was able to take \nyou to a tower function  right  from a two dimensional sigmoid what does a tower mean \nhere and how do i take you to the tower  so  that is what i want to do  so  i have said w \ntwo equal to zero and i will it will become obvious why i have done that  so  just understand \nwhat the figure is doing  right  so  this if you just look at this is like the cross cut  right  \nso  you are looking at the front view of this figure and that is just the sigmoid function \nwithout the w two right  and now  since i have said w two equal to zero  no matter what i set x two \nto the same function will get repeated throughout  that axis  do you get that  \nso  that is why this entire function is just getting repeated throughout this axis and then \nyou just get a similar s shape function  everyone gets that  how many of you do not get \nthat  how many of you get that  so  this if you look at the front view this is the sigmoid \nof  one  variable   but  since  i  have  said  w  two  to  zero   no  matter  what  i  change  x  two  to   the \nfunction is going to remain the same  so  it will just get copied throughout the x two axis  is \nthat  fine with you  \nnow  what will happen if i increase w two  sorry w one  same thing right  it will just keep \nshifting  till  it  becomes  almost  like  a  twod  step  function   ok   now   what  will  happen  if  i \nincrease b  shift  i can do the same thing here also  same logic applies here also  ok  \n \n\f\n \nnow  what is the next step that i am going to do  take two of these which are shifted by \nsome point and  then subtract what will i get  everyone had this figure in mind  so  just \nsee right  so  this portion both are zero  so  zero minus zero would be zero this portion this is one  but \nthis is zero  so  that would be one minus zero and again in this portion both of them are one  so  one \nminus one would be zero  so  you will get this kind of function would you like to live in such \na tower  i am very serious  yes or no  no  why  it is open from two sides  right  you \ncannot live in this tower  so  you want something which is a closed tower  right  so  how \nwill you do that give me an intuition  \nwe will do the reverse thing  what will be set to zero  \nw one  ok  \n \n\f\n \nand  this is how it would look the orientation would change and again so  notice that this \nis your sigmoid function and since i have set x one to zero  no matter what i change along the \nx one axis the same function gets copied and you get a nice looking a sigmoid function  \nnow   again  i  will  do  the  same  thing  i  will  increase  the  w   i  will  get  a  close  to  a  step \nfunction i will increase the b  i will move along this axis  \n\n \n \n \n\fnext step  take two of these subtract get what  another tower function  this is also not a \ntower that you like to stay in  so  what do i do now  add them  sure  add this tower to \nthe other tower  \n\n \nso  now what will happen if  i add these two  will you get a tower function  what will \nyou get  \nyou  will  get  a  tower  function  with  a  parking  floor   right   is  that  what  you  will  get  \neveryone  understands  why  this  is   so   these  portions  both  are  zero   so   you  get  zero  same \nlogic applies for all the four corners  right  is that fine  now  for this portion or rather this \narea  right  so  this guy is zero  this guy is one  so  you will get a one  the same logic applies for \nall these four corners in the centre both are actually one  so  one plus one would give you two  so  \nthis is two  this is level two  this is level one  this is level zero  is that fine  \nso   what  am  i  done  so  far   i  have  taken  my  x  one   x  two  passed  it  through  some \ntransformations   right  this  what  are  these  transformations  we  will  see   but  transform  it \nthrough these multiple hoops  right where i adjusted a w\u2019s and b\u2019s and i have got some z \nright  and this is  how that  z behaves   for different  values of  x  one comma x  two   i  will  get \nthese  different  values  and  these  values  range  from  zero  to  one  to  two   is  this  pictured  clear   i \nhave taken my original x  one comma x two  passed it to some of these transformations and \nirrespective of what my x one to x two is this tells me the entire range of values that i will get  \nfor some combination of x one comma x two i will get zero  for some combinations i will get one  \n \n\ffor some combinations i will get two and some combinations also between one and two  right  \nso  these places where it transitions  is that clear  is that picture clear to everyone  \nso  now i can treat this as the output z  ok  now  from here how do i go to a tower  \nthreshold  it   how will  you threshold it   what  do  you want  you only  want  this  much \npart to exist  right this without the parking floor  how will you do it  any output which \nis greater than equal to two  you want to keep it any output which is less than two  you want \nto make it zero  if i do this will i get a tower  right  sorry  greater than equal to one any value \nwhich is greater than equal to one  you want to keep it  anything which is less than one you \nwant to make it zero  so  this entire thing will get demolished  how do you do this  this is \nan if  else  ok  \nif else if something is greater than equal to zero  do something else do something else  what \nis that  \nperceptron   right   but  we  do  not  want  to  use  perceptron   we  want  to  use  sigmoid \nneurons   have  you  learned  an  approximation  from  a  sigmoid  neuron  to  a  perceptron  \nvery high w  right  you get the intuition  let us see what we do on the next slide  \n\n \nso   i  take  this  any  z  which  comes  from  here  i  will  pass  it  through  a  sigmoid  neuron \nwhich are very high slope such that the threshold is at one  anything which is greater than one \nwill pass out as one  anything which is less than one will go to zero  so  everyone sees how we \n \n\ftook this structure and converted it to a tower  we have this tower now  now what do i \ndo with this  \n\n \ni  lead  multiple  such  towers  and  i  can  approximate  this  i  could  put  a  tower  here   here  \nhere  and  so  on   i  could  have  these  multiple  towers  and  here  of  course   all  my  towers \nwould be of zero height right in this region  right  so now  i can cover the entire twod space \nwith a lot of tower functions and approximate this exactly  that is a very weird statement  \napproximate  this  exactly  i  mean  approximate  this  to  arbitrary  precision   everyone  gets \nthis   do  you  see  why  we  constructed  these  tower  functions  and  now  we  can  put  them \ninside this cone and approximate it  \n \n\f\n \nnow   all  this  is  fine  i  was  making  some  towers  there   so   can  you  now  give  me  a \ncomplete neural network which does this  i want you to imagine that  remember you are \ntaking  what  i  am  asking  you  to  do  is  this  x  one   x  two  give  me  this  such  that  i  get  this  two \ndimensional tower  i do not know how to draw it  something like this maybe whatever  \nso   i  want  this  two dimensional  tower  what  is  this  network  of  perceptrons  going  to  look \nlike  just go back to all the operations that we did and try to imagine in your mind  \nno  we will not use perceptron because we can always use a sigmoid neuron instead of a \nperceptron with the high w  i do not expect you to answer this i just want you to imagine  \nright   we  just  try  with  a  there  is  something  known  as  a  pen   there  is  something  on  a \npaper  ok  so  here is the solution  \n \n\f\n \nso  what is happening here you have this salinity and oh actually this is slightly wrong i \ndo not know why you guys saying it is correct  actually  at both places i need both the \ninputs it is just that in one case i do not care about that input because i have said w two to \nzero  so  i learn these weights wone wtwo b  wone wtwo b of course  here the network should learn \nthat  w  two  is  equal  to  zero   right  and  then  you  get  this  one  tower  do  not  needs  this  to  be \nmodified  this figure is incorrect  \nso  we need x one x two both as inputs we need to label it with w one w two equal to zero and b and \nso on it  so  we will discuss this later  anyways  but you get the idea right that you take \nthese two inputs make one tower  take the inputs again make another tower  add them up \nto  get  this  function  pass it through this  step neuron function step sigmoid function  so \nthat  you  get  the tower   so  this is  one block   you will have many such blocks each of \nwhich will learn different w\u2019s and b\u2019s  so  that they get shifted and then you will place \nthem all together you have an aggregator on top of this which will combine them  just a \nminute  how many of you get this  ok good  \nyes   so   that  is  a  good  question   i  am  going  to  come  to  that   right   so   i  have  very \nconveniently given you a solution where i have what is the bad thing that i have done  i \nhave hand coded these things   right   i have hand coded w ones w twos and b\u2019s  is  that \nfine in practice  no  i mean that is where we started off and we do not want to hand code \nthese  right  \n \n\fso   now  you  know  a  learning  algorithm  for  a  single  sigmoid  neuron   now   what  you \nhave  is  a  network  of  neurons  right  for  this  network  of  neurons   i  need  to  give  you  a \nlearning algorithm driven by the objective function that whatever output it gives would \nbe very close to that arbitrary function that you are trying to model  \nif  i  give  you  a  learning  algorithm  then  you  would  be  convinced  that  if  this  has  to  be \nminimized and the weight configuration which need it needs to arrive at as w two is equal \nto zero  then the algorithm should be able to do that  right  because  we saw we have some \nfaith  in  these  algorithms  in  the  case  of  a  signal  sigmoid  neuron  that  with  the  right \nobjective function it will give me a principled way of reaching that objective function  in \nthis  big  network  my  objective  function  is  to  arbitrarily  to  approximate  this  of  this  true \nfunction  right  \nso  now if i give you that as the objective that whatever outputs the network generates  \nso  the network might generate something like this  so  that has to be very close to the \ntrue output that is the objective function that i am going to use in that learning algorithm \nand if that learning algorithm works which will prove then you should be able to arrive at \nthe necessary weights to make this approximation  right  is that clear and in fact  there \nmight you might not even have to do these multiple towers in practice  all i am trying to \nprove is that there is one solution which exists  \nif there is one solution which exists i can say that locate the network can learn  that is the \nonly claim i make  i am not saying this is the only solution  right  same as in the case of \nthe boolean functions  where i said that one solution exists where you have to raise to n \nneurons  of  the  hidden  layer  that  was  a  sufficient  solution   that  was  not  a  necessary \nsolution for the and function we were actually able to do it with a single sigma neuron  \nright  so  just keep that in mind i am just giving you a sufficient solution  \nand  the network could actually learn something better than this all right this is again a \nvery bulky solution  why  it scales with the number of neurones\u2019 proportional to number \nof input variables that you have  so  that is for a sufficient solution  but you would want \nsomething  better  than  that   all  i  am  trying  to  say  is  that  it  can  approximate  i  am  just \ntelling  you the representation power  and just as  we had the catch there that the hidden \nlayer is very large the same catch applies here also  is this story clear to everyone  \n\fso  i have given you a solution i have not told you how to learn the weights i have given \nyou a network  now  later on we will discuss a learning algorithm for this network  and \nwe  will  have  some  confidence  that  given  a  particular  objective  function  that  learning \nalgorithm can strive to  go to minimum error or minimize the quantity of that objective \nfunction that is going to come in two lectures from now  is that fine  \n\n \nand  that was for the tower function now  i could have actually directly done this right  \nso  i wanted to approximate these functions  so  i could have placed a lot of these kinds \nof things here and approximated it  right  so  that instead of that very high slope sigmoid \nfunction i could just use a normal sigmoid function also  ok  and again there is a error \nhere  but i hope you get the picture  it is just that you feed both the inputs to them  \n \n\f\n \nso  for one dimensional input we needed two neurons to construct a tower  for two dimensional \ninput how many neurons did we need  i am just counting these because these are simple \naggregators  right and this is one constant at the end  so  how many did we need actually \no of two n  i mean o of i mean  so  for n how many would we need  let us try to work that \nout  ok  so  i will ask you that in the quiz how many do we need for n dimensions  \n\n \nnow  why do we care about approximating any arbitrary function  we will again try to \nclose  the  loop  now   we  saw  that  we  can  arbitrarily  we  can  approximate  any  arbitrary \n \n \n\ffunction  but now again i want to come back to the point why do we want to do this and \ncan we tie this back to the classification problem that we were dealing with  \n\n \nand   this  is  the  data  which  i  had  given  you  which  was  there  were  some  points  some \nvalues of x and y sorry  this should be x one and x two it is  where this is pressure and salinity \nor salinity tendency and this is the output which is oil  \nnow   there  was  this  is  what  the  function  actually  looks  like  now  what  would  have \nhappened if i had used a single sigmoid neuron to try to approximate this function try to \nrepresent this function and sigmoid neuron in two dimensions  right  so  the two dimensional \nsigma  what  would  have  happened   can  you  give  me  one  solution  for  this   remember \nearlier i had said that perceptron cannot handle data which is not linearly separable  but \nthen i anyways used it for data which was not linearly separable  and we got some line \nsuch that we got some errors the red points and the blue points are not clearly separated  \nso  i am asking you for a similar thing here  i force you to use a sigmoid neuron  what \nwould you give me  \n \n\f\n \nis  this  fine   this  is  one  of  the  possibilities  of  course   it  could  have  been  oriented \ndifferently and several things  what is happening here is that for these blue points it is \nacting correctly  but for these red points it is not acting correctly  i am assuming red is \npositive and blue is negative  i think that should have been the other way round  but let \nus assume red is positive and blue is negative  again  now for these red points this part is \nworking fine  but it is misclassifying all these blue points  \nso  all these bad locations is actually saying that you can find oil and for all these good \nlocations here it is saying that you cannot find oil  that is what a sigmoid neuron would \ndo and you could have multiple solutions are possible here right  but all of them would \nhave this problem that will make errors on some red points and some blue points right  \nbut  the  true  solution  that  we  wanted  is  something  like  this   again  there  are  multiple \nsolutions  possible  right  you could have anything there are  you  could  have  even  finer \none  side  you  could  just  have  this  much  there  many  things  possible  this  is  one  such \nsolution  what the illustrative proof told  you is that  you can actually use a network of \nperceptrons  and  approximate  this  arbitrary  function  which  exists  between  the  input \nvariables and the output variable  \nso   if  this  is  the  function  which  exists  between  the  input  variables  and  the  output \nvariables  now   you  could  take  these  multiple  two  dimensional  tower  functions  and \napproximate it  with the catch that you might need many of these in the hidden layer  but \n \n\fyou  can  still  do  that   ok   so   that  is  why  this  in  theorem  important  because  now  any \nproblem that you take right any problem that  you will have in machine learning would \nalways want you to take an x learn a function of x which takes you to y this function will \nbe have some the function will have some  \nparameters   right  and  now  what  this  theorem  is  saying  is  that  you  could  adjust  these \nparameters such that you can arbitrarily come close to the true function  right  so  that is \nthe significance of this  any machine learning problem that you can think of in the sense \nof classification or regression  you would find that this  is  useful and  i  am  giving  you  a \nvery  powerful  tool  to  do  that   of  course   with  the  catch  that  i  am  not  giving  you  any \nbound on the number of neurons that you will need  i am just saying use as many as you \nwant  \n\f"}
{"audio_filepath": "lec003_001.wav", "duration": 1115.169, "text": "\nso  welcome to lecture  four of csseven thousand and fifteen  the course on deep learning  today   we will talk \nabout feed forward neural networks and back propagation  so  quick recap of the story  \nso far it  so  we started with mp neurons  we saw there were some problems with the mp \nneurons   they  could  handle  only  boolean  inputs  and  boolean  outputs  and  threshold \nneeded to be hard coded  so  from there  we moved on to perceptrons which allowed for \nreal inputs real outputs and sorry real inputs and binary outputs  and we also learned an \nalgorithm  for  learning  these  weights  and  parameters  right   so   we  need  there  was  no \nneed to hand code these parameters anymore  \nbut  then   we  found  that   for  a  single  perceptron   there  is  a  limitation   it  cannot   it  can \nonly deal  with  functions which  are linearly  separable  so  then we went  on to  a multi \nlayer  network  of  perceptrons  and  we  proved  by  illustration  that   it  can  handle  any \narbitrary boolean function  whether linearly separable or not  the catch is that you will \nneed a large number of  neurons in  the hidden layer  right  then we also observed that \nperceptrons  have  this  harsh  thresholding  logic   so   which  makes  the  decisions  very \nunnatural   it  is  zero forty nine   it  is  negative   zero fifty one  is  positive   so   you  wanted  something  more \nsmooth  \nso   the  smoothest  approximation  to  this  step  function  which  is  the  perceptron  function \nwas a sigmoid function  sigmoid is a family of functions and we saw one such function \nwhich  was  logistic  function   and  then   we  saw  that   it  is  very  smooth   now   it  is \ncontinuous and differentiable  \nnow   for  the  sigmoid  neuron  on  a  single  sigmoid  neuron  we  saw  a  learning  algorithm \nwhich  was  gradient  descent   and   we  proved  principally  that  it  will  always  go  in  the \ndirection where the loss decreases right  so  that is what is the basis for gradient descent  \nand then  we graduated from a single neuron to a network of neurons and made a case \nthat such a network of neurons with enough neurons in the hidden layer can approximate \n\fany arbitrary function right  ok  so  i have told you that  it can approximate any arbitrary \nfunction  what does that mean  and what is the thing in the network that does all this  \nall the tower functions and the tower functions depend on weights and biases  so  there  \nin that illustrative proof  again we were adjusting the weights and biases by hand right  \nwe knew that we wanted these very tiny tower functions and we were doing it  \nnow  from there  where should we go   \nstudent  \n  \nwe  need  an  algorithm  to  learn  these  weights  and  biases  right   so   that  is  what  back \npropagation  is   so   today  i  am  going  to  formalize  these  feed  forward  neural  networks  \nwe just did it by illustration the other day  i will introduce you to the terminology and \nsee  what  the  input  outputs  are  and  so  on   and  then   we  will  look  at  an  algorithm  for \nlearning the weights in this feed forward neural network  \n\n \nlet  us  begin   so  this   a  lot  of  this  material  is  inspired  by  the  video  lectures  by  hugo \nlarochelle on back propagation  he has a course on neural networks  \nit  is  available  on  youtube   you  can  check  it   ok   so   let  us  first  begin  by  introducing \nfeed forward neural network  right  \n \n\f\n \nso  what is a feed forward neural network  the input to the network is an n dimensional \nvector  so   ok  that  means   my  input  belongs  to  rn   that  fine   the  network  contains  l \nminus  one  hidden  layers   where  do  you  already  know  what  hidden  layers  are  right   we \nhave been defining that terminology since multi layered perceptron  so  you have these \nhidden layers and there are l minus one of these and then it has one output layer containing \nk  neurons   ok   those  are  the  feed  forward  neural  network  looks  like   what  is  missing \nhere   \nstudent  \n  \nthe weights  right  \nso  each neuron in the hidden layer  ok  before that each neuron in the hidden layer and \nthe  output  layer  can  be  split  into  two  parts  right   so   i  will  call  the  first  part  as  the  pre \nactivation and the second part  as the activation   have  you seen this plate before  right  \nwhat does the pre activation do   \nstudent  aggregation  \naggregation and what does the activation do   \nstudent  non linearity  \n \n\fnon linearity  right  so  we have this pre activation and activation at every layer and a i \nand h i are vectors  is that correct  because  this entire thing or rather this part is h one and \nthis  part  is  a  one   both  of  these  are  vectors   right   and  for  this  discussion   am  going  to \nassume that  everything till here belongs to r n  \nso   the  input  was  r  n  and  all  the  hidden  layers  also  have  n  neutrons   is  that  fine   so  \nplease pay a lot of attention to this couple of slides because  this is going to stay with us \nfor the rest of the lecture and perhaps two more lectures and even for the course alright  so  \nthis is very important that you understand this  the way we are defining a feed forward \nneuron network  \n\n \nthe input layer can be called as zeroth layer  what i mean by that is that  i could refer to \nthis as  h  zero  ok  there is  no a zero h zero here because  there is  no pre activation  activation  \nyou are just given the input  so  i just call it as h zero ok  and the last layer can be called as \nh of l  right  whatever you get from this green part  you will call it as h of l  ok  what \nis the dimension of h of l  r raised to k  it belongs to r k  because i have said here that  \nyou have k neurons  each corresponding to k classes  ok  \nnow  we have weights between the input layer and the first hidden layer  now  can you \ntell me this belongs to r n  this also belongs to r n  so  what is the dimension of w one  n \ncross n  right  because it contains weights for connecting each of these inputs to each of \nthese hidden layers  there are n here n there right  so  it is n cross n  \n \n\fand  what  are  the  dimensions  of  the  bias   n   one  corresponding  to  each  of  the  hidden \ninputs   fine   and  this  is  only  for  up  to  this  layer  because   till  here  i  have  assumed \neverything is n  \n\n \nnow  what about the output layer  n cross k and the biases k  k dimensional ok  so  this \nis  what  the network looks like  but  now   i have to  give  you some function  so   i have \njust  i  have  shown  you  a  diagram   but  what  does  it  mean  mathematically   because  \nremember that  we are always interested in writing something of the form y is equal to \nfunction of x  right  and that is not well defined yet  \n \n\f\n \nso  let us start defining that  ignore the red portion for now  ok  i will go over it  so  each \nof  these  activations  right  or  rather  the  pre  activations  is  given  by  b  i  plus  w  i  into  h  i \nminus one  so  what it means is that  these activations take inputs from the previous layer  \nmultiply by them by weights and also add the bias  is that clear  so  let us see it  right  \nfor example  if i look at a one which is this vector  so  that is three dimensional and assuming \nit is three dimensional for simplicity  \nso  it is a one one  a one one  a one two  a one three right  and that is equal to how do you get rid off this  b \none one b one two b one three plus this matrix multiplication is this clear to everyone  i know it is trivial  \nbut am still going over it right  so  let us not ok  and then  how do you do this matrix \nmultiplication  row was multiplied by the column  so  this is what you get right  and in \nthe end  i can write it as this  right  and this looks very similar to what  we have been \nseeing  throughout  it  from  a  mp  neuron  to  perceptron  to  sigmoid  neuron  and  now  this \ncase  right  \nso  it is just an aggregation of all your inputs or weighted aggregation of all your inputs  \nthat  is  the case which  i want  it to  know  and  that  is  obvious now  so   you  understand \nwhat these are right  \nso   this  is  r  n   in  our  case   we  have  assumed  n  equal  to  three   what  is  this   i  will  keep \nasking till this is completely fine with everyone  r n and this is  \n \n\fstudent  \n   \nn cross n and this is  \nstudent  \n   \nn cross one n cross n i mean r n sorry is it fine  so  everyone understands the operation \nhappening  here   it  is  a  weighted  aggregation  of  your  inputs   so   every  guy  here  is  a \nweighted aggregation of all the inputs  ok  \n\n \nnow  after that i do h i of x is some function of a i of x  ok  what does this mean  so  \nthis is again a vector  right  i have assumed that  it is three dimensional  so  these are the three \nelements of h i  so  these are the three guys  now  these are some function of these light blue \nguys  ok  now  how does that function operate on the vector  it operates element wise  \nnot all functions on vectors are element wise  but this particular function  we are going \nto do element wise  \nthat means  that h one one is equal to g of a one one  h one two is equal to g of a two and h one three is equal to \ng of a one three right  where if i take g of a one three  one of the functions that i could choose is the \nsigmoid function  so  it would just be one over one plus e raised to minus here  so  what is \nhappening is i am taking this value and passing it to the sigmoid function to get oh sorry \nam taking this value and passing it to the sigmoid function to get h one one taking this value \npassing it to the sigmoid function to get h one two right  \n \n\fso  the key thing to understand here that  this is a element wise operation  right  it is not \noperating on the vector  that does not make sense  it is operating on every element of the \nvector  right ok  and g is called the activation function  \n\n \nit could be logistic  tanh  linear anything right  so  we will see some of these functions \nlater on ok  \nnow  the activation at layer i sorry they are supposed to be activation at the output layer  \nthe activation at the output layer is given by the final function which is f of x is equal to \no of a of  so  let us see  so  this is a three  in our case  l was equal to three because  we had l \nminus one hidden layers and the lth layer was the output layer right  so  this is a l  so  this \nis what i have computed here  that light green part of the figure that you see right now  \nbased on that  i want to produce an output  \nso  that is  someone had asked me a question that why do we always  choose sigmoid  \nbecause  sigmoid will clamp the output to zero to one  what if i want to predict the amount of \noil which will not  be between zero to  one  right  that  is  why  for the output   we  will use a \nspecial  function  that  will  call  the  output  function  and  later  on   i  will  show  you  that  it \ndepends on the task at hand  so  it is going to change with the task that we are going to \ndo  right  so  we are just going to say that  the final output which is h of l is equal to \nsome function of the pre activation at that layer  is this terminology clear to everyone  \nhow is each function operating  is that clear to everyone  \n \n\f\n \nand  we  will  see  some  examples  of  the  output  activation  function  right   now  just  for \nsimplicity  am  going  to  remove  the  x\u2019s  from  the  brackets  right   so   instead  of  calling \neverything ai of x hi minus of x and so on  i will just call them ai  hi and so and so  that \njust simplifies things  but we know that everything is a function of x  because  x is the \ninput and that passes through some functions and we get the final output  right  so  this is \nthe notations that we are going to use  is the dimension of everything that you see every \nvariable that you see here completely  clear to everyone  \ndimension of ai  bi  w  hi  x  everything is clear ok  and the output layer has a slightly \ndifferent dimension than the other layers because  there we have k classes as opposed to \nn neurons everywhere else  ok  fine  now  i need to put this in the paradigm that  we saw \nfor supervised machine learning  what were the five components there  data  \nstudent  model  \nmodel  \nstudent  parameters  \nparameters  \nstudent  learning  \nlearning algorithm  \n \n\fstudent  \n   \nobjective function right  ok  everyone remembers that ok  \nso  i said that  we will do deep neural networks and we are trying to write this y hat as a \nfunction  of  x   but  then  what  i  gave  you  is  just a  diagram  from  which  this  is  not  clear \nwhether  y  hat  is  actually  a  function  of  x   how  many  of  you  think  y  hat  is  actually  a \nfunction of x  very few  ok  \n\n \nso  let us see what exactly is our model assumption here  right  so  the question let me \nrepeat  the question just to  be clear  so   i said  that  they are given some data we  do not \nknow  the  true  relation  between  y  and  x  we  make  an  assumption  that  y  is  related  to  x \nusing some function f right and it is has some parameters and then we like to try to learn \nthe parameters of that function  so  what is the function here  \n so  what  is  your model  what  have  you  assumed as the model  can  you write  y as  a \nfunction of x  if yes  what is that function  how many of you have the answer  i think \nyou  have  your  answer   ok   i  think  i  cannot  wait  more   so   i  will  give  you  the  answer  \nthen it will become very obvious ok  so  this is how y is a function of x  right  so  let us \nsee what is happening  i took the original x which was this  i transformed it  added b one \nthat was the dash at layer one   \nstudent  \n  \n \n\fno  this thing  \nstudent  \n  \npre activation at layer one  i passed it through the activation function right  ok  \nnow  again  let us be clear about the dimensions  what is the dimension of this   \nstudent  n  \nn  what is the dimension of this  n cross n  so  what is the dimension of this product   \nstudent  \n  \nn  what about this  so  what is the product the final dimension of this  \nr n  now  you are passing it through a function g that function is operating element wise  \nso  what is the output dimension  \nstudent  r n   \nr n  so  this is again r n  ok  now this  \nstudent  \n   \nso  now you see the whole story  right  so  now  this n cross n guy multiplies with this n \nguy  again   you  get  a  vector  again  pass  it  through  a  non linearity  was  it  so  hard   it  is \nobvious now  right  you just take an x  just note down all the transformations that you \nhave  done   that  is  what  a  function  does  right   it  passes  it  through  the  through  first  a \nlinear  transformation   this  is  a  linear  transformation   then  a  non linear  transformation  \nthen again linear non linear and so on  \nso  just see how far we have come from where we started off  right  we started off with \nsimple things like w transpose x  right  that was the perceptron model where we were \ntaking  decisions  based  on  w  transpose  x  and  we  were  saying  y  is  equal  to  one   if  this \nquantity  is  greater  than  something   y  is  equal  to  zero   if  this  quantity  is  greater  than \nsomething right  that is why  we started off with we made it slightly more complicated by \ndoing this  this was sigmoid neuron  \n\fnow   from  there   where  have  we  gone  to  this  right   so   we  have  increased  the \ncomplexity of the network with great complicity  complexity comes great  \nstudent  \n   \nno  power   right   we  have  already  seen  the  representation  power  of  deep  neural \nnetworks  right  so  it comes from this complexity that you have you have a lot of linear \nand non linear transformations  right  that adds to the complexity of the network  it has \nmore  parameters  at  each  linear  transformation  you  have  some  parameters  and  you  are \nalso using a lot of non linearity  so  that is the reason why deep neural networks are so  \npowerful right do you get that  ok  so just to impress again  right  \nso  any machine learning algorithm that you have you should be able to write it in this \nform right  that y is a function of x with some parameters and then your job boils down \nto learning these parameters  right  it just happens that here  y is a very complex function \nof the inputs  is that clear  ok  so  i am not deviated from the original story  i am still \nbeing  able  to  write  y  as  a  function  of  x  with  some  parameters   ok   what  are  the \nparameters   \nstudent  \n  \nall the w\u2019s  all the b\u2019s  right  so  w one to w l and b one to b l  \n\n \n \n\fand the algorithm that we are going to see today for learning these parameters is called \ngradient descent  but we will use it with back propagation  where back propagation will \nhelp us to compute gradients  it is ok  it does not  it does not make sense at  this point  \nthat  is  what  the  lecture  is  supposed  to  be  about   right   so   and  what  is  an  objective \nfunction   \nstudent  \n  \nloss function  so  i could just go with this loss function  right ok  there is an error here  \ni  thought  we  corrected  this   there  is  a  summation   so   actually  these  are  vectors   right  \nso  this does not make sense  so  you should have summation j equal to one to k yij minus \nyij  does that make sense  so  this is the vector y hat ok  for the i th example  it will be \ncalled as y hat high i which will have k elements  right  so  y hat i one y hat i two up to y hat i \nk  right  \nso  that is what my predictions are and i will have the corresponding true vector also  i \nam  trying  to  take  the  difference  between  them  which  is  going  to  be  an  element  wise \ndifference  everyone understands the error in the slide  how many of you do not get it  \nhow many of you get it  if you do not get it  please raise your hands  it is a minor thing  \ni can correct it  and how does deep neural networks fit into these this paradigm  \n\f"}
{"audio_filepath": "lec003_002.wav", "duration": 406.763, "text": "\nlearning parameters of feedforward neural networks \n \nnow   we  will  move  on  to  the  next  module   where  we  want  to  learn  the  parameters  of \nfeed  forward  neural  networks   and  we  first  start  with  some  intuition   and  then \nmathematical details  \n\n \nso   we  have  introduced  feed  forward  neural  networks   and  we  are  now  interested  in \nfinding an algorithm which can allow us to learn the weights of this network  \n\f\n \nso  recall our gradient descent algorithm  this is how it looked ok  i had initialized those \ntwo parameters w naught b naught  and then i was iteratively doing this in a loop  at every \nstep i was moving in a direction opposite to the gradient at that step  \nnow  can i write this a bit more compactly  we can write using vectors   \n\n \nso  are you ok if i write it this way  so  these two was actually nothing but vector at every \npoint  so  i can just write it this way  so  theta is the vector containing w and b  ok  or \ntheta  is  the  vector  of  all  the  parameters  my  network  had  it  just   so   happened  that \n \n \n\fnetwork had only two parameters  so  see where am going with this  how many of you see \nwhere am going with this  \ngood  so where delta theta t right just to remind you it was this the partial collection of \nall the partial derivatives with respect to all the parameters  in this toy example all was \nequal to two  right we just had two parameters  now you see where am going with this  ok  so \nnow  in this feed forward neural network  instead of theta equal to w comma b what do \nwe have  theta is equal to so many parameters  ok  so  what would grad of theta t now \nbe partial derivatives with respect to  \nstudent  \n  \nall the weights  but there is a problem here right  this is the matrix  how do you take the \npartial derivative with respect to the matrix  who asked you to use the matrix  how you \ntake the partial derivatives with respect to matrix  so  what  i am interested in this right \nthe question  i know there is some loss function which is a function of theta  one of the \nelements of theta has this matrix w one which belongs to r n cross n right  and now i want \nthe derivative with respect to w  so  see what i am trying to do  this is scalar and we take \nthe derivative of that with respect to a matrix  what is all that  the derivative with respect \nto  \nstudent  \n  \nevery element of the matrix  ok \n\f\n \nso  we can still use the same algorithm except that del this grad of hat of so now  i could \njust  say  that  theta  two  hat  i  mean  initialized  all  parameters  and  theta  naught   right  \ncompute the gradient with respect to all of them and then do this update  right  i could \njust instead of putting them in matrices  i could just think of them as a large vector just \nhad initially i had just had w comma b  now this vector is even more large  in fact  i will \nshow you actually how it is  \n\n \n \n \n\fso  this is the grad with respect to theta looks very nasty now  this is how nasty looks  \nright  so  you have this weight matrix w one  you have the derivatives with respect to first \nelement of w one  all the way up to the last element last element  so with respect to all the \nn cross n elements of w one  what is the next entry going to be w two hundred and eleven to \nstudent  \n  \nwtwonn next after wleleven ok and then after this  ok  \nstudent  \n  \nwhat  is  remaining  biases   right   so   you  have  beleven  to  bonen  this  slight  error  here   but \nintentionally this actual is k  because k is not equal to n  right the last layer has only k \nparameters   whereas  so  that  it  looks  ok   is  this  clear   so   is  this  are  all  the  partial \nderivative that we need  right  you do not need to worry about taking a partial derivative \nwith respect to our matrix  it just boils down to taking the partial derivative with respect \nto all elements of the matrix  \nso  earlier you just had two parameters  now you have these n cross n plus n cross n upto l \nright   so   l  into  n  cross  n  plus  l  into  n  that  many  number  of  parameters  is  what  you \nhave  you get the calculation  right or rather you have l minus one layers each of which \nhas n cross n parameters  right and l minus one layers which also have the biases  so  these \nare  the  w\u2019s  these  are  the  b\u2019s   then  the  output  layer  one  layer  which  has  n  cross  k \nparameters  and  k  cross  one  bias   so   these  are  all  the  number  of  parameters  that  you \nhave  and this is exactly what this size of this matrix is  right it has all these parameters  \nand you need to compute the partial derivative with respect to each of these parameters   \n\f\n \nso  this is what grad theta is composed of  it is composed of the partial derivatives with \nrespect to all the parameters of your network  ok so now  if someone gives you each of \nthese  quantities   same  oracle  give  you  each  of  these  quantities   then  can  you  apply \ngradient  descent  right   you  can  use  the  exactly  the  same  algorithm  that  you  are  using \nearlier  just the sizes of earlier vectors changes  \nhow many of you are convinced that now you can use that gradient descent  there is not \na  trick  question  how  many  of  you  convinced   how  many  of  you  not  convinced  \nassuming that someone  has  given  you these quantities  right   i know that  it is  hard to \ncompute we will see how to compute that  but let us assume someone has given you this  \nthen you can use gradient descent that is what the case i made in the previous slide right \nthat you could initialize with all the parameters compute the gradients with respect to all \nthe parameters and just do this update  fine  so now  we need to answer two questions  first \nis this is the key question  \n \n\f\n \nbecause we are taking derivative of what  loss functions  so  we need to know what the \nloss functions that is the crucial question  right  and then we are taking derivatives with \nrespect to all these elements  so  whatever i was told you that assume that oracle gives \nyou   now  you  have  to  do  the  hard  work  and  actually  find  it  out  right   so   if  you  can \nanswer  these  two  questions  then  we  are  done   we  have  an  algorithm  for  learning  the \nparameters  of  feed  forward  neural  networks   we  all  agree  that  if  you  have  these  two \nelements then we have done  \nso  here i will end this module  \n \n\f"}
{"audio_filepath": "lec003_003.wav", "duration": 1599.967, "text": "\noutput functions and loss functions \nwe go on to the next module where we will be talking about output functions and loss \nfunctions   \n\n \nthe question that we are going to focus on is how to choose the loss function  but i will \nshow you that it is tightly coupled with the choice of the output function also  remember \nthat we had said that we have a special o function as the output function  i have not told \nyou what that o is  and now that is what we are going to define  \n\f\n \nnow  the choice will be loss function actually depends on the problem at hand  and that \nis exactly the question which had come up  right that in some cases it is to have sigmoid \nas  the  output  function  because  your  values  are  between  zero  to  one   but  whatever  there  are \ncases  where  your  output  is  not  between  zero  to  one  right   so   it  definitely  depends  on  the \nchoice of the on the problem that you are trying to solve  so  we will illustrate this with \nthe help of two examples  and these two examples will cover a broad range of problems that \nyou will encounter or if you are working in machine learning right  \nso  the first problem is again  you are given the input as movie  you are using a neural \nnetwork with l minus one hidden layers and an output layer y hat right  so  this is  sorry  \nthis is a true one  so  you have an output layer and the output layer is going to predict the \nimdb rating the critics rating and the rotten tomatoes rating  \nis that fine  ok  so  what kind of problem is this  people have done machine learning  \nthis is a regression problem  and notice that the output values that you want to predict \nare not bounded it by zero and one  they are still bounded by one to ten  but in general you could \nimagine that there could be problems  so  there are no bounds at all right it could be a \nvery large number  is that clear  now here yi belongs to r three  \nso  remember in all these cases we were assuming that we just want to predict one value  \nbut nothing stops you from predicting multiple values at the same time  so  your output \nis  now  three  dimensional   you  are  taking  an  n  dimensional  input  and  trying  to  predict  three \n \n\fvalues from it  ok  fine the loss function should capture how much yi had deviates from \nyi  ok so  this is a valid or maybe we corrected on this way  ok  so  this is the formula \nwhich was supposed to be in there right  so  you take  you have predicted three values  and \nyou  know  the  true  three  values   you  just  take  the  difference  between  these  right   is  that \nclear  the first element of the predicted value  minus first value of the actual value and \nso on for all the three values that you want to predict  \n\n \nnow  you have a loss function  but what should be the output function in this case  can \nit be the logistic function  yes  no  it will be bounded between zero to one  and you know that \nyour output cannot be bounded between zero to one  ok  so  in such cases then what is a good \noutput function to use  one option is to scale it  so  i will keep that aside  why do that  \nit is unnatural and you are actually clamping it and then trying to scale it  right  so  can \nyou do something more natural in that  just use a sum which is linear function  right  so  \nwhat we could do is you could have o as a linear function  \nso   what  that  means  is   again  remember  that  this  is  a  of  l   ok  and  i  know  all  the \ncomputations that have happened so far  a linear transformation  non linear  linear  non \nlinear and then again linear  so  i have computed a of l  from that i want to compute the \nfinal output  right  so  i could just have it as a linear function of the input which is a of l \nin this case  \n \n\fdoes it make sense  how many of you feel it makes sense  ok why  because now it is \nno  longer  bounded   right  you  could  this  linear  transformation  your  weights  could  be \nadjusted in any way to get a value whatever you wanted  whether you wanted between one \nto ten or one to one hundred or one to one thousand  these weights could be adjusted to do that right  so  at \nleast  you  are  not  bounding  it  and  it  is  free  to  learn   what  is  the  range  from  the  data  it \nshould be able to run  but how should you adjust these w s  \nso  that you get the desired range  now tell me why would it not happen that  you learn \nw s you start predicting values like one thousand  ten thousand and so on in this particular case where \nyour input is  bounded by  one to  ten  sorry   your output is  bounded between one to  ten  why \nwould it happen  i this is my argument and you prove me wrong right  i would say that \nif you have chosen a linear transformation which is not bounded  i then network could \nlearn weights which start producing a rating of ten thousand  twenty thousand and so on because it is not \nbounded  \nbut you know that that is wrong because the ratings can only be between one to ten  so why \nwould that not happen  because  you are minimizing this loss function right  so  if you \nstart predicting values like ten thousand  when your actual rating was nine then you have a ten thousand \nminus ninety whole squared loss  that is a very high loss  so  it will start moving you away \nfrom that configuration right  so  the training is always guided by the objective function  \nso  if your training happens well it will try to prevent this  \nnow  suppose let us take a simple thing rate that  you are given a our same ball example \nfor probability  so  you are given an urn which has balls of three colors  say black  white and \nyellow  \n\f\n \nand you have to put the balls in that  so  you know that the true probability distribution \nis  actually  zero thirty five  zero twenty five and zero four  for red  black  and white  ok  this  is  the true probability \ndistribution   you have put  say thousands of balls  in  urn  now what  you do is   you just \nallow me to peep into the urn or you allow me to take some samples from there  you tell \nme take these one hundred samples  and  you ask me  tell  me what  this probability is  right  so  \nthis is the true probability that you know is true right because you know it because you \nhave estimated  \nnow  you just give me a small sample from there and ask me to estimate it  and based on \nthat  i  actually  estimate  this   ok   so   there  was  a  true  probably  distribution  and  an \nestimated  probably  distribution   now  i  want  to  find  out  how  wrong  i  went   right \nafterwards  you tell me the answer  you tell me that this is what the true was and this is \nwhat you predicted  \nnow   i want  a  way of computing how wrong  i  was  right  so  how do i  do that  you \nalready know this and these are two vectors  what can i do  you could just do the  this is \nvalid  anything  wrong  with  this   in  principle  no   you  could  just  treat  these  as  any  two \nvectors you have a true value you have a predicted value you just take the squared error \ndifference  between  them  right   but  you  know  this  is  a  probability  distribution   right  \nyou should be able to do something better than this  you know this is a special quantity \nthis  is  not  just  any  number  that  you  are  predicting   you  are  trying  to  predict  a \n \n\fdistribution   so   you  should  be  able  to  do  something  better  than  that  right   so   that  is \nwhat we want to see  how to do something better than this  that is what our quest is   \nnow   again  why  we  are  at  this   right   i  also  want  to  make  a  because  this  is  something \npeople do not immediately understand  so  i just want to make a case for something else  \nso  i will just do that ok  now suppose there is this ipl  ok and there are four teams in the \nsemifinal  let us call them a b c and d  ok now  i was not in town after the semifinal  \nso  i just know the results up to semifinal   and then the finals also happen  and one of \nthese teams wins  let us call it the b team  right the b team wins  can you express this in \nterms of probability  can you express this in terms of distribution  what do you mean \nmy zero and one b has won  \nso  it is a certain event because it has one now  so  what is going to be the distribution  zero one \nzero zero  right   so  this  event happens  with  one hundred percent  probability  ok   now   the same case \ncan you ok  so now  let us do the same thing that is as i said i was not in town  right and \nyou asked me tell me which team would win that is  i know these four teams have qualified \nin the semifinals  and i know who the players are and so on  \nand with my limited knowledge of cricket i will predict something right so  say i predict \nthis  ok can you again tell me how wrong i was  you know what the true label is and \nyou know what i predicted  you can tell me how wrong i was  ok  so  the case which i \nam trying to make is that even if the event is certain  you can still write it as a probability \ndistribution where all the mass is allocated to the correct output  can you relate this to a \nclassification  problem   when  you  see  training  data   you  have  already  observed  it  \nsuppose there were four classes possible  \napple  orange  mango and banana  if you have seen it is apple  and if you ask you what \nis  the  distribution   what  will  you  tell  me  zero   one   zero   zero   you  will  express  it  as  this  one  hot \nvector  where all the probability mass is concentrated on the guy which is correct  right  \nso  even certain events which happen with certainty you can write them as a distribution \nrate  where all the masses are located on the true label  so  that is how all classification \nproblems when you are dealing with multiple class classification problems it is often the \ncase that you will write it as this  \nthat  your  true  label  is  given  to  you  in  this  format   there  were  four  possible  events   four \npossible classes or k cost possible classes  out of which only one is correct and then you \n\fmake a prediction  and you want to now find out how different was your prediction from \nthe true label  you are trying to get the set of how this relates to a classification problem  \nand this is that is why this is of interest to us ok  \nso  this so  we will see this soon  now the next thing that we need is how many of you \nknow what is entropy  forget about cross just entropy  ok that is why i have left two slides \nintentionally blank ok  so  so now  let us see where i go with entropy  ok  how many of \nyou know what  is  expectation  please  fine  so   again  the same thing now  i knew that \nthis was the distribution which i think i am into  gambling  am not  i am into gambling  \nand i try to bet on these teams  \nand i bet some amount on each of these  can you tell me what is the expected  reward \nthat i will get  so  what am i saying wait  suppose this is the case that if team a wins i \nget  tenk  rupees  or  my  net  profit  is  tenk  rupees   if  team  b  wins  my  net  profit  is  twentyk \nrupees and c and d so on  right  you get the setup for every even there is an associated \nvalue with it  this is the value of event a winning  b winning  c winning  d winning  \nso  the net profit in each of these case  so  what is my expected net profit  no  give me a \nformula  sigma  overall  events  right   how  many  events  do  i  have  here   four  right   so  \nrather  i  should  say  i  equal  to  abcd   right  probability  of  i  multiplied  by  the  value \nassociated  with  that  event   so   this  is  how  you  compute  expectation   ok  everyone  gets \nthis  \nso now suppose say am doing this  right  there are suppose four symbols  i do not know \nwhat  i  am  teaching  ok   so   and  i  am  trying  to  communicate  this  from  a  source  to  a \ndestination  ok  and now  suppose these are the four symbols that i give  and if these one of \nthese symbols is say with probability one  and if i transmit it  what is the information that \nthis guy gets  so  this is assumed that a is that sun is going to rise today  if i tell you this \nwhen you are sleeping in the night  what will you tell me   so basically are not gaining \nany information  well it is a certain event you know this is going to happen right  \nnow   one  of  these  events   suppose  i  am  going  to  say  that  this  there  is  going  to  be  a \ncyclone tomorrow morning  what is the probability of a cyclone happening  in chennai \nalmost one  but still it is a very rare event  so  if i tell you something which is very rare \nthat  message  has  a  very  high  information  content  right   so   if  event  which  has  a  very \nhigh probability has a very low information content  and an event which has a very low \n\fprobability  has  a  very  high  information  content  right   so   you  can  measure  the \ninformation content of an event   \nso  so the point is that what you can have is that the information content of an event  you \ncan write it as  how many of you get this  how many of you have seen this before  all of \nyou have seen this right  so  this is the value associated with an event  ok  now can you \ntell me what is the expected information content  for every event now i have given you \nthe  value  associated  with  that  even   so   what  is  the  expected  information  content  \nsummation p of  i into information  content of  i   and this like and this  is  of course  log \nright  so  it would be so  what is this called  this is called the entropy  \nnow  what is cross entropy  how many distributions are you dealing with here  one which \nis the p distribution  which tells you how likely these messages were  and based on that \nyou  are  trying  to  calculate  the  entropy  of  this  situation  right   so  now   what  is  cross \nentropy   you  have  a  true  distribution  say  you  have  a  predicted  distribution   ok  this  is \nwhat  you  predicted   so   that  means   according  to  your  predictions   the  information \ncontent of every event is going to be log of qi  because that is what you predicted right  \nbut what are the actual properties which with these which these events are going to occur \npi\u2019s right  so  then the expectation has to be computed over pi\u2019s right  \nso  then what  you will have is summation pi log qi  so  this is what  you estimated the \ninformation content to be  but the actual events are going to happen with this probability  \nright  so  this is your value associated with the event  and this is the actual probability of \nthe event  right  so  this quantity is known as the cross entropy  is it clear  and this is a \nway of measuring when would this be in when would this be minimized  when both are \nsame that  means   if  your prediction  is  very  close to  your true distribution this quantity \nwill be low  minimized actually  \nso  that is  what  we wanted actually   you wanted to  predict some distributions  in  all of \nthese cases  and you wanted a measure which tells you that this prediction was good  and \nwhat is the definition of good  it is as close to the correct value  so  cross entropy gives \nyou a measure of telling how close a predicted distribution is to a true distribution  \nso now instead of using the squared error which was actually pi minus qi right  so  pi \nwas  my  true  distribution   and  qi  was  my  predicted  distribution  i  can  use  cross  entropy \n\fwhich is given by this model  and it does the same thing it gives me a principled way of \nmeasuring how close my predicted distribution is to my true distribution  do you get this  \n\n \nso now  so  this was for whatever we have done so far  right till this point this was for \nregression  right  now i wanted to enter into classification  for which i have built this set \nup  of  how  to  take  the  difference  between  two  distributions   so  now   let  us  consider  this \nproblem where we have this situation and which is a classification situation  that you are \ngiven four possible classes  out  of which one is  the correct  class  and this  is the true data \ngiven  to  you  this  is  the  true  distribution   all  the  probability  mass  is  focused  on  one  of \nthese classes  \nnow  we want to given an image classify this into one of k classes  if you could again \nuse a squared error loss  but since we are dealing with probability distributions here  we \nwant to use something special  so  before we get to what the special is going to be  what \ndo  i  first  need  to  tell  you   in  the  earlier  case  my  output  was  not  bounded  was  it  also \ndependent  was there any condition on if the imdb rating is something the critics rating \nshould be something else or the  rotten tomatoes rating should be something else  no  \nnow  in this case is there a tightly coupled behavior between the outputs  why  because \nthey should sum to one  we are trying to predict a probability distribution  so  the sum \nshould one right  so  i need an output function which ensures this  you get this setup  \n \n\f\n \nnow   we  should  ensure  that  y  hat  is  also  a  probability  distribution   whatever  we  are \npredicting is also a distribution  so now  can i use a sigmoid function   yes  it will give \nme values between zero to one and probabilities are between zero to one  but the sum would not be \ny so  sigmoid is ruled out  \n\n \nso  what  we use is  something known as the softmax function  how many if  you have \nseen  this  before   please  everyone  raise  your  hands   otherwise  you  will  get  zero  on  the \nassignment  fine  so  what does this  what does this function actually do let us look at \n \n \n\fthis function right  so  here you had a l which was say a l one  a l two  a l three  right suppose \nwe had three classes  ok  so  from here i actually want to go to hl or rather i going to want \nto go to y hat  right  which is going to consider y hat one  y hat two  y hat three  right it is going \nto give me probability of each of the three classes  \nlet us assume there are only three classes right  so now  what this function does is  how is it \ngoing  to  predict  y  one  hat   suppose  these  values  were  ten  minus  twenty  and  thirty   so   what  is \ngoing to be y one hat is going to be e raised to ten divided by e raised to ten plus e raised to \nminus twenty plus e raised to thirty  so now  you see how the output is comp computed from \neach of these values  right so  why did we do this e raised to stuff why could not i have \njust  taken  ten  plus  minus  twenty  plus  thirty  divided  by  the  sum   because  we  have  negative \nvalues  \nso  once we take the exponent even the negative values become positive  right  so  that \nis why we need the softmax function  i hope all of  you wrote this in  your assignment  \nthey  did  ok   so   you  get  this  we  have  a  different  output  function  now   and  this  output \nfunction  does  it  make  sense   it  gives  us  a  probability  distribution  now  the  summation \nwould be one  and each of these values would be  between zero to one  that is exactly what we \nwanted  \n\n \n \n\f\n \nand  now  that  we  have  ensured  that  y  and  y  hat  both  our  distributions  what  is  the \nobjective function that we are going to use  cross entropy  how many of you convinced \nit  is  cross  entropy   we  have  two  distributions  now   we  saw  that  a  principled  way  of \ncomputing the difference between two distributions is the cross entropy so  we will use the \ncross entropy  \nnow  can  you  tell  me  something  about  this  sum   there  is  something  special  about  this \nsum  what are these  three true values and these are the predicted values  what is so special \nabout this sum  how many terms are there in this summation  k as many as the number \nof classes  in this case four  how many of those terms will go to zero  all but one  right except \nfor the correct class everything else will go to zero  so  this just boils down to the following \nloss  function   that  if  l  is  the  true  class   right  for  that  class  yc  is  going  to  be  one  it  is \ngoing to be zero for everything else  that is exactly what this vector tells you only that term \nwill remain  so  were actually trying to minimize this quantity  \n \n\f\n \nlet  us  see  so   for  classification  problems  this  is  your  objective  function   you  either \nminimize the negative log of  y hat l or  you can say  you are maximizing this thing  ok  \nnow what is this quantity y hat l  no  it is a predicted probability of the correct event  \nright so  this is a probably no wait this is an important question  so   you have  y hat l \nhere  and this is a function of i mean this optimization problem is with respect to theta  is \nthis a well formed objective function  does y hat l actually depend on theta  yes  it does  \nso  theta because why i tell is a function of all these things  everything here  and then a \nlog on top of that  right  so  it is actually a function of all your parameters  so  this is a \nproperly set  objective function  we are trying to  minimize or  maximize with  respect  to \ntheta   ok  and  you  told  me  that  y  hat  l  is  actually  the  probability  of  the  predicted \nprobability  of  the  correct  class  ok   hence   this  quantity  is  also  known  as  the  ml  class  \npattern recognition class  log dash of the data  \nstudent  all \n  \nall good and fill in the blanks  \nso  it is a priority of the x belonging to the l th class and then hence y hat l because it is \nthe  probability   it  is  the  likelihood  of  it  is  called  as  the  log  likelihood  of  the  data  log \nlikelihood  \n \n\fso   what  have  we  done  so  far   we  started  with  a  feed  forward  neural  network   we \ndefined the hidden layers and the input layers and the weights and the biases  we kept a \nprovision for the output layer to be something special  right  then we went to two classic \nproblems   one  is  regression  and  the  other  is  classification   in  regression  we  wanted  to \npredict values of all sorts of ranges  \nso  we decided to use a linear layer there  so  that there is no bound on the values that \nyou can predict  and your objective function should take care of where the bound lies  it \nshould not allow values which are way off from the true values  right  and that is why \nwe  use  the  squared  error  function   there  the  other  problem  that  we  looked  at  was \nclassification  where we saw that the label actually can be treated as a distribution  where \nall the mass is focused on the true label and zero everywhere  \nand  our  job  is  then  again  to  predict  our  distribution   so   we  are  given  the  true \ndistribution  and we predict another distribution  so  the output again we want something \nspecial in this case which is a distribution  so  to ensure that use a spatial function which \nis  called  the  who  said  sigmoid  softmax  function   fine   and  then  we  got  a  prediction \nwhich  is  a  probability  distribution   and  then  how  did  we  find  what  was  the  objective \nfunction  what  is  the  difference  between  the  true  and  the  predicted  the  cross  entropy  \nright   so   we  use  cross  entropy  as  the  objective  function   and  then  with  some \nsimplification we realize that it boils down to maximize the log of the probability of the \ntrue class  or other log of the predicted probability of the true class  \n\f\n \nso  now  let  us  look  at  the  summary   so   if  your  outputs  are  real  values   what  is  your \noutput  activation  going  to  be   linear   what  is  the  loss  function  going  to  be   squared \nerror  if  your  output  is  a distribution  what  is  the output  function  going  to  be   softmax  \nwhat is this loss function  squared error cross entropy  right now this grid light actually \ntakes care of a wide range of problems that you will see right think of any examples that \nhave  been  giving  you  so  far   movie  prediction  or  sentiment  prediction  or  image \nclassification or anything  all of that you can fit into this frame of it  \nand so  if you know these two loss functions how to deal with them  then you can deal with \na large class of problems that you are going to deal  and for the rest of this lecture which \nwill happen tomorrow we are  going to focus on  this  at this particular output function \nand this particular loss function  how do we compute  i have a loss function  what i am \ngoing to compute now the gradient with respect to all the parameters  \nso   this  is  what  we  are  going  to  focus  on  right   so   we  have  seen  the  loss  function  in \ndetail   we  have  seen  that  the  loss  function  is  tightly  coupled  with  the  output  function  \nnow we are all set  but given this loss function how do we start computing gradients of \nthis loss function \n \n\f"}
{"audio_filepath": "lec003_004.wav", "duration": 821.971, "text": "\nbackpropagation \n \nthis lecture is on backpropagation and feed forward neural networks  so  we introduced \na  feed  forward  neural  networks   we  saw  the  input  layer  hidden  layer  and  the  output \nlayers  and we saw that the output layer actually the output function depends on the task \nat  hand   and  we  considered  two  main  tasks   one  was  classification  the  other  was  a \nregression  \nfor regression it made sense to use a linear layer at the output  because we did not want \nthe outputs to be bounded  they could be any range  and for the classification problem \nwe  realized  that  we  want  a  special  kind  of  output   because  we  are  looking  for  a \nprobability distribution over the output  and for that we use the softmax function  and in \nboth  cases  we  used  a  different  kind  of  a  loss   for  the  regression  problem  the  squared \nerror loss made sense  because we predict some values and we want to see how far we \nare from those values  \nbut for the other case the classification we realize that it is a distribution  so maybe we \ncould  use  something   which  allows  us  to  capture  the  difference  between  the  true \ndistribution and the predicted distribution  \n\f\n \nand therefore  we had this figure emerging which was depending on the output  whether \nit  is  real  values  or  probabilities   you  will  have  different  types  of  output  activation \nfunctions and different types of losses  \nand of these  combinations  today we are  going to  focus on softmax and  cross entropy  \nand  our  aim  is  to  actually  find  these  gradients   remember  there  are  many  of  those  we \nhave seen this large matrix which had many such partial derivatives  and we want to find \nthat entire matrix  i hopefully do it in a way that it is not a repetitive we could compute a \nlarge number of partial derivatives at one go  \nso   before  we  look  at  the  mathematical  details  we  just  get  an  intuition  for \nbackpropagation  \n \n\f\n \nand then we will get into the gory details of how to actually compute these gradients and \npartial derivatives  so  this is the portion that we are in we are intended to ask these two \nquestions  and this is where we are  \n\n \nso now this is what our network looks like  this is clearly much more complex than that \nsingle neuron that we had  and which had only two weights w and b that was very easy to \ncompute the gradients there  now imagine that i want to compute the gradient of the loss \n \n \n\ffunction  and let us assume it is a classification problem then what is the loss function \nminus log of y hat  \nso  this is the loss function  and we want to compute the derivative of this with respect to \none  of  these  weights  in  the  network   and  am  deliberately  taking  something  which  is \nmuch  farther  away  from  the  loss   but  why  do  you  say  why  do  i  say  it  is  much  farther \naway  it is right at the input layer  right  and the loss is somewhere at the output layer  \nso  we want to compute this gradient  \n\n \nnow  to learn sorry you want to learn this way  to learn this weight we know that we can \nuse gradient descent  we are all convinced that this gradient descent algorithm which i \nhave  shown  here   as  long  as  we  put  all  these  variables  or  all  these  parameters  that  we \nhave into theta  \nwe can just run the gradient descent algorithm and compute  them the only thing that we \nwill need is this partial derivative with respect to all the weights in the network  and in \nparticular with respect to this weight that i am interested at  \n \n\f\n \nnow  so   we  will  now  see  how  to  calculate  this   we  will  first  this  is  only  to  get  the \nintuition  so  we will first think of a very simple network  which is a very deep  but the \nthin network  it has many layers  but it is a very thin network  here you see what i mean \nby  a  thin  network   ok   now  this  is  what  i  am  interested  in   can  you  tell  me  how  to \ncompute this  this looks like a chain  so  it is justified the user chain rule of derivatives  \nso  what would the chain rule look like  \nyou want to compute the derivative of this with respect to this  and you have done this \nin high school  right  so  you have functions of the form of sine of cos of tan of e raised \nto  sine  of  x   and  this  is  exactly  how  this  chain  is  right   you  have  some  function  of  x \nfollowed by another function of x  another function of x function of x function of x and \nso on  you just keep making a composite function of the input  we actually wrote down \nthat function if you remember  it was just one function applied after the other function   \nor a very composite function  so  you just need to apply the same idea here  so  we take \nwe go step by step  so  i am almost accounting for every shade of color here  \nso  dl theta by d y hat  then dy hat by d a l eleven  there is only one neuron here  then this \nwith respect to the sorry h twenty one  then h twenty one with respect to a twenty one  a twenty one with respect to h eleven  h \neleven with respect to  a eleven  and then a eleven with respect to w eleven  \nso  i just traversed down the chain in the reverse order  this is how the chain rule works  \nright  anyone has  a problem with  this  it  is  straight  forward   right  and now what  i \n \n\fhave done is for convenience i have just compressed the chain  you see the red part and \nthe green part  i have just compressed this weight  so  that and this is again something \nthat you have done in high school if you have this you could just write the chain as the \nfirst and the last it  so  this is what you can do  and i also compress this other chain  ok \nand am going to use these kinds of compressions later on  \nso  what am trying to impress on you is that  if i want to go from here to here  right that \nis what my intention is  if somehow i have already travels from here to here  then i  can \njust reuse that computation  that is the idea which i am trying to impress on it  i do not \nneed  to  follow  the  entire  chain  every  time   i  can  do  these  partial  computations  up  to  a \npoint  have you seen this something similar idea somewhere else  dynamic program is \nsomething like that  so  you have just computed up to a certain point  and then it is reuse \nthe value for further down the chain  \nso  that is what we are going to do  and same for all the weights  right  for each weight \nthe chain size would be different depending on where it lies in the network  right for the \nweights which are very close to the output layer the chain would be very small  makes \nsense ok  so  this is the intuition and we will see the intuition a bit more  \n\n \nso   let  us  now  understand  this  in  the  terms  of  the  wide  complex  network  that  we  are \nusing  \n \n\f\n \nso   what  actually  is  happening  is  that   we  are  at  a  certain  stage   that  means   we  have \nsome  values  of  w s  and  b\u2019s  ok   at  the  initial  stage  we  just  have  these  w  knots  and  b \nknots  but let us assume that we have done some training  and we are at a certain level \nwe are at wt at time step t and bt at time step t  right for all the weights inverse  now we \nfeed it a training example  we do this entire compute computation  what do we get at the \nend  we get y hat which is a function of this x that we have fed it  but we also know this \ntrue y  we know the true value we know y hat  \nso  we can compute the loss function  so  we compute the loss and to our surprise we see \nthat  the  loss is  not  zero   we  are  getting  a  non  zero  loss   that  means   the  network  has  not  yet \nlearnt properly  right the weights and biases are still not in the right configuration that we \nwant them to be in right  so now  what do we do  we go on this path of investigation  \nwe want to find at who in this network is messing up things  there is someone who is \ncausing this problem  because of which i am not getting the desired output  and we are \non  our  quest  is  to  now  find  out  who  this  guy  is  who  is  responsible  for  this   so   what \nwould you do  where would you start  the output layer  \n \n\f\n \nbecause  the  output  layer  is  the  guy  who  give  you  the  output   right   so   go  and  talk  to \nhim  and we say that hey what is wrong with you why are you not producing the desired \noutput  right  now what is the output layer going to tell you  in very civil language  i \nwill  say  i  cannot  do  anything  boss   i  mean   i  was  just  given  some  weights  and  inputs \nfrom the previous layer  and those weights and inputs were messed up  \nso  there is nothing which i can do go and talk to them  so  who will it directors do  it \nwill say that i am just as good as wl hl minus one  and bl because these are the guys that i \ncompletely depend on  if these guys were ok  then i would have been fine  so  we then \ngo and talk to these guys  that what is wrong with you  \n \n\f\n \nso now they say ok fine wn and bl take the responsibility  they are the nice guys  they \nsay we are the weights  we are supposed to make a  we are the ones who are responsible \nfor the adjustments in the network  so  we have failed to do our job properly  and i think \nwe should get adjusted right  but then hl will resist  it will say it is not my fault  why \nwill it resist  because it against again depends on the previous activation layer  \nso  till then point as to what  the w s and b\u2019s in the previous layer  right  and you see \nhow the investigation is now proceeding where will we reach  well keep going down the \nnetwork  we are talking to everyone in the network  we are talking to every dark green \nguy every light green guy  every dark blue guy every light blue guy  we are also talking \nto all these weights and biases  and in the end what do we figure out  the responsibility \nlies with all the weights and all the biases  they are the ones who are responsible for this  \nnow  but now we find out that this is also one of those weights which is responsible  and \nthis is also one of these weights that is responsible  but it was have been very difficult \nfor us to  talk to  them directly  so  then what  are we going to  do   instead of talking to \nthem directly which is this  we will talk to them through the chain rule  so  we will talk \nto the output layer that is exactly how what we did maybe went to the first guy that we \nknew   that  guy  pointed  out  to  the  previous  hidden  layer   that  guy  pointed  us  to  the \nprevious hidden layer  and then finally  we get to the weights right  \n \n\fso  this talking to is fine  but where do derivatives figure out in this  why are why is the \nlanguage  derivatives   why  are  we  not  talking  in  english  or  hindi  or  something  else  \nwhat does the derivative tell us  so  talking about gradient descent like what we saw in \ngradient descent   but  in  general what  does  the derivative tell  us   if  i  change this  a bit  \nhow much does my loss change right  \nso  that is how much this guy is responsible for the loss  because if this is very sensitive  \neven adjusting a bit of this i could drastically reduce the loss right  so  that is what the \nderivative  tells  us   that  tells  us  how  sensitive  is  the  loss  function  to  the  weight  or  any \nquantity with this with respect to which am taking the derivative right  that is why the \nlanguage is of derivatives right  is that clear  is the intuition fine to everyone  \n\n \nso now will convert this intuition into actual math and try to figure out  how to compute \nevery  guy  along  the  way   right   and  we  will  use  this  idea  that  we  have  made  some \npartial computations and then well  use it for the rest of the chain  so  we have made this \nmuch  at some point  we will reach where we have made this much  and then you could \nuse it for the rest of the chain   in fact  we will start right from here well start with this \nguy and then keep expanding the chain  \nso   the  rest  of  the  story  is  going  to  be  about  computing  three  quantities   can  you  tell  me \nwhich are these three  quantities  gradients with  respect  to  the output units   gradients  with \nrespect to the hidden units  and then gradients with respect to the parameters  so  these \n \n\fare the three things that we need to do  if we do this we have everything in the chain  and we \nare done  and the other thing that we need to do is  we cannot sit down and compute this \nfor every element  right  we want to have it in a generic fashion  where instead of talking \nabout w one one one  w one one two and so on  \nwe should at least be able to talk about w one  w two and so on  so  that means  we have \nonly three matrices and three biases  right  at least  at that level  so  we have to do a collective \ncomputation instead of just  computing for  every guy  so  instead of looking at  scalars  \nwhich  is  what  we  are  doing  when  we  are  doing  gradient  descent  for  w  naught  and  v \nnaught  we were just computing the update rule for w and b  we want to now do it for \nvectors and matrices  \nso  that is that is the transition that is going to happen  and our focus is going to be on  \nwhat cross entropy and softmax  why is that important  because that is the loss function  \nso  that is the quantity that am going to take the derivative  if i change the loss function \nall the gradients are going to change  are all the gradients going to change  only the first \nguy will change in the change  all this should remains still same  right  modulus some \nconditions  but largely it should remain the same right  unless you change something in \nbetween   \n\f"}
{"audio_filepath": "lec003_005.wav", "duration": 1017.808, "text": "\nlecture   four \nnow   we  go  to  the  next  module  where  we  will  first  see  how  to  compute  the  gradient \nwith respect to the output units  well that was the first guy in our chain right that is the \nfirst person that we need to talk to  \n\n \nso  that is the part that we are going to focus on  \n\f\n \nso   this  is  the  output  and  when  i  say  i  want  to  compute  the  gradient  with  respect  to \noutput unit  what do you actually mean  what is the quantity that i am looking for  i will \nhelp you out  actually what i meant by output unit is this entire thing right  so  i actually \nmeant al\u2019s ok  but it is it is a fair answer and even y hat is a fair answer ok  in fact  am \ngoing to start with y hat and then go to al  so  i will have to start with this guy and then \ncome to this guy  \n\n \n \n \n\fso  this is the loss  this is y hat which is equal to y one hat  y two hat up to yk hat  so  these \nare the k values that we have here and we are looking at cross entropy  that means  we \nare looking at the classification problem  right  so  we have got a distribution over the k \nclasses   that  is  what  y  hat  looks  like   and  we  know  that  one  of  these  guys  is  the  right \nclass maybe say y two  so  the loss function is minus log of y hat two  because two is the correct \nclass  in  this  toy  example  that  i  am  considering  ok   so   the  loss  function  i  am  just \nrepeating the definition right that is how the loss function is  \n\n \nnow  oh god so again this is what our y hat looks like ok  now i want to compute the \ngradient with respect to any of the output units right  so  it could be y one  y two  y three  y four up \nto yk right  so  this i actually can take values from one to  k  in this case one to two right ok  \nnow can you tell me what is this loss  ok this much is fine  can you tell me what is this \nderivative  minus one by minus one by y hat l if y is equal to l  \nstudent  \n  \nand zero otherwise  how many of you get that  cool ok  so it is a very simple thing that you \ncan think of this as z and this is y  only if z is equal to y then the derivative would exist \notherwise it is going to be zero right ok  so  how do i write this fe part using  \nstudent  \n  \n \n\fhow  many  of  you  have  seen  indicator  variables  before  good   so   this  is what  you  are \ntelling me right  it is going to be minus one by y hat l  if i is equal to l ok  and if i is not \nequal to l  then these two things are not related  it this is a function of something else and \nyou are taking a derivative with respect to a different quantity   \nso  it is a constant with respect to that quantity and the answer would be zero ok  now i am \ngoing to write this as this right  so  this is the same as saying so this variable actually \nthis  is  known  as  the  indicator  variable   it  takes  on  the  value  one   if  the  condition  in  the \nbracket holds  otherwise it takes on the value zero  so  this is exactly i am writing exactly \nthis  but in a more compact manner ok  is that clear to everyone   \n\n \nso  this is what the quantity this is the quantity  that we have computed with respect to \none of the output units ok  so  this is  what  derivative  partial  derivative gradient   how \nmany of  you say derivative  no one likes derivative  partial derivative  that is always \nthe safest choice partially  fl  right and gradient oh  there is one brave soul who say is \ngradient do not worry well fix that ok  \nso  this is the partial derivative y because my y hat is actually a vector  and i am taking \nthe  derivative  with  respect  to  one  of  those  guys  ok   now  if  i  want  the  gradient  with \nrespect to y hat  what would that look like  a vector which is a collection of  \nstudent  \n  \npartial derivatives  so  let us see this is the quantity that i am interested in am interested \nin  the  gradient  of  the  loss  function  with  respect  to  the  vector  y  hat   so   remember  the \nvector  y  hat  is  y  one  hat  y  two  hat  up  to  yk  hat   right   so   this  gradient  is  going  to  be  a \ncollection of the partial derivatives with respect to y one hat y two hat and so on  \nnow   what  is  each  of  these  quantities   so   it  is  simple  right   so  this  quantity  the \nderivative is either going to be zero or is it going to it is going to be one by y one hat  right  if l \nis equal to one right  and that is exactly what  i have done  so now  how many elements \nhere are actually going to be nonzero  at a time how many of these going to be nonzero  \none  which one  \n  \n\fstudent  \n  \nthe one corresponding to l  right  everything else is going to be zero  so  this is a dash vector \ny not vector ok  so now  am going to write one hot vector like this  what have we done  ok \nwhere el is what one hot vector  such that  it is l th entry is one ok  that is what am going that \nis how am going to define e l  is that fine with everyone ok   \nand so  you see the story how did how we went about computing this  we started with a \npartial derivative with respect to one of t guys right we found a formula for y i  we saw that \nthis  formula  is  generic  enough   and  so  now   we  can  compute  the  gradient  which  is  a \ncollective of all these yis where i ranges from one to k  right  and then we just put that in a \ngradient vector  \nso   this  story  is  going  to  repeat  throughout  the  lecture  where  we  try  to  compute  the \ngradient  with  respect  to  one  guy  and  then  generalize  oh  sorry   we  compute  the  partial \nderivative with respect to one guy and then generalize and try to find the gradient fine ok  \n\n \n \n\f\n \n  \nso  what if i what do i have so far  i have this quantity  what does till which part of the \ndiagram am i currently  the dash green part dark green part i am till  here i need to go \ntill the light green party that is collectively the output unit ok  although i have divided \ninto  two  halves   but  when  i  say  output  unit  i  mean  that  output  neuron  right  complete \nneuron  so  what  i am  actually interested in is these quantities or more specifically ok  \nthis is what i am interested in  what is this  one of those guys right this al is actually \nalone up to alk right  so  this is one of those guys  so  this is going to be the gradient or \nthis is going to be the derivative  a partial derivative sorry ok  now what do how do we \nproceed from here  \n\f\n \nnow   i  will again  have to compute this   you already  know that  good  but  before that  i \nwant  you  to  answer  one  question  right   so   y  hat  l   what  is  y  hat  l   it  is  the  output \ncorresponding  to  the  correct  class   does  it  depend  on  an  arbitrary  al  i   so   in  the \nprevious thing we saw that only when i is equal to l there is a connection  in this case is \nthere a connection always or only when i is equal to l  \nstudent  \n  \nalways why  softmax so   \nstudent  \n   \ndenominator  has  all  the  ali\u2019s  right   so   this  is  there  it  is  y  hat  l  in  the  numerator  of \ncourse   it  only  has  this  unit  which  corresponds  to  the  l  th  probably  did  not  choose  my \nvariables very well  so  l th component of a capital l right  and but in the denominator \nyou  have  the  entire  sum  which  means   that  every  output  guy  here   each  of  these  dark \ngreen guys depends on each of the dash green guys light green guys good   \nso   that  is  at  least  settled  that  we  always  the  we  can  always  compute  this  partial \nderivative  we do not need an if else here there is nothing like l is equal to i then what \nwill happen it will always have this partial derivative  \n \n\f\n \nso  we will now derive the full expression for this  so  this is what we are interested in is \nthis fine  so this is a function of the form  so you are taking how do i say this  so  this is \nlog of a function  so  first you will take the derivative with respect to log and then push \nthe  partial  derivative  inside  right   so   that  would  be  minus  one  by  y  hat  l  and  then  the \nderivative with respect to y hat l  now what is y hat l  the softmax function right   \nso  it is  the l\u2019th entry of the softmax function applied to  that output  vector what  is  the \noutput vector  al right  so  it is the l\u2019th entry of the softmax or l\u2019th entry of the function \napplied to the output vector \nso  this was our al what is our output  right  so now  one of these guys here is the l th \nguy and one of these guys here is the l\u2019th guy right  so  what you do is  you take this you \napply  the  softmax  function  to  it  which  again  gives  you  a  vector  and  now  you  are \ninterested in the l\u2019th component of that vector that is what this quantity means  it should \nbe clear now  \n \n\f\n \nnow i will just do some simple math stuff here and we should be able to derive this  is it \nfine am just replaced by the actual softmax formula  this is a derivative of the form u by \nv  right   what  is  the  formula  for  that   yeah   it  perfectly  right  yeah   so   this  is  what  it \nwould be right  i mean it is you all know this i am not going to spend time on this  \nso now am just going to substitute the values here  yeah it is getting a bit nasty  but it is \nnot very difficult right  so  so this so this is our g of x so  am taking the derivative of \nthat then this is this one over h of x  you can just figure it out right anyway it everyone just \nread this for a few seconds and let me know if this is not clear  this is g this is h in this \nformula right have just substituted the gs and hs in this  now what is this quantity going \nto be  it is derivative of the form e raise to x right so it is e raise to x always  \nstudent  \n  \nif  i  is  equal  to  l  right   so  now  we  have  this  dependence  because  we  are  looking  at  a \nnumerator  but the numerator only depends on the l th entry right  so now  you are trying \nto take the derivative of the l th entry with respect to some arbitrary i th entry  so  only if \nl is equal to i you will get the derivative right  \n \n\f\n \nnow  what about this  how many terms in the summation would remain  \nstudent  \n  \none  which one  \nstudent  \n  \nwhere  i  dash  is  equal  to  i  right   so   the  i\u2019th  guy  would  remain  the  rest  of  it  is \nstraightforward right  this square i have just divided into two parts ok ah  now let us see  \ncan you simplify this  because i cannot ok  can you simplify this  what is this  \nstudent  softmax  \nsoftmax and which entry of the softmax  \nstudent  \n  \nl\u2019th entry  i\u2019th entry  l\u2019th entry with the saw with the indicator variable  but what is this  \nthis is our input hidden layer output  so  ok now let us see  what is the next step   this \nis should have been y hat i  but y hat is equal to f of x right  so  we can fix this unit  so  \nok fine so we have actually what  do we have now   we  have the derivative of the loss \nfunction  with  respect  to  the  i\u2019th  unit  of  the  output  layer   right   and  which  part  of  the \noutput layer  the pre activation pattern ok  now what am i going to do  i have a formula \n \n\fwhich tells me how to compute this  what was i actually interested in  so now  how am i \ngoing to go from here to there  i just put all the partial derivatives into a  \nstudent  vector  \nvector  and that vector is the  \nstudent  \n  \ngradient good  \n\n \nso  we have this one formula it is ok  if some of you did not get this derivation right  it is \nvery  very straightforward  if you go back and look at it i am pretty sure you will get it is \nnothing  in  this  is   very  simple  elementary  stuff  right   except  for  some  degree  here  and \nthere ok  so  now what would this look like   \nwe  should  add  actually  l  theta  here   this  would  look  like  a  collection  of  all  the  partial \nderivatives  we have a generic formula  what will we do now  what is the first entry  \nminus in indicator l equal to one minus y hat one  which is the variable that we are indexing \nover i right not l  oh god oh we are indexing or ok  have i goofed up oh that is wrong  is \nit oh yeah that is wrong fine  then this is fine we are indexing over i and then we can do \nthis  \nnow  can you simplify this  i am looking for ok this is the element wise difference of two  \n \n\fstudent  \n  \nof the indicator vector and  \nstudent  y hat  \n\n \ny hat  oh hey we should change all this  y hat is equal to f of x right  but i just want it to \nbe  consistent  as  y  hat   so   is  this  fine  this  is  a  simplification  fine   right   so   we  have \ncome  a  long  way  right  you  have  finish  this  part  ok   we  have  got  the  gradients  with \nrespect to the output units ok  this much part is a clear to everyone moduler bit of the \nmath which you can go back and look at it this entire derivation is fine  but you get the \nconcept  right  that we start with  one unit from  there  grow the  gradient then keep  going \napplying the chain rule  \nso  we started with the dark green guys and then went to the light green guys  now we \nhave the derivative with respect to the entire light green vector  and that is what we had \nstarted off with that we wanted the gradient with respect to the output units   \n \n\f"}
{"audio_filepath": "lec003_006.wav", "duration": 1251.25, "text": "\nback propagation  computing gradients w  r  t  hidden units \nnow  we will go to the gradient with respect to the hidden units  \n\n \nso  this portion  so  you already see there is a repetition  here and i do not need to treat \neach hidden unit separately i can just have a formula for the hidden unit and then i could \ncompute it for all the hidden units  so  that is what our aim is  so  let us do some simple \nstuff first and then you will come back to it  \n\f\n \nso  suppose you have a variable x you compute two functions from that one is x square  the \nother is x cube  i will call this as y one and i will call this as y two and i take y one and y two  and \ncompute a z  which is say a log of y one by y two  now what i am interested in is this  what is \nthe  answer  for  this   how  do  you  get  this   this  is  a  fair  question  to  ask  y  one  y  two  are \nfunctions of x  z is a function of y one y two hence z is a function of x  so  i can compute this \nderivative and i can ask for this derivative  how would you compute it  if i cannot really \ndo this right  \nso  if this path did not exist  then it is trivial it is just the chain rule along one path  but \nnow  you  have  two  paths   so   what  will  happen  add  them  right   so   can  you  tell  me  a \nformula  for  that   so   let  me  know  if  this  makes  sense  to  you  ok  does  this  make  sense \nnow let me complicate this a bit just let me just do it as y three now  \nstudent  \n  \nwhat will happen  \nstudent  \n  \nthat is all right  so  you see that if there are multiple paths you can just add up the chain \nrule across all these paths right  that is what chain will across multiple paths does  \n \n\f\n \nso  with this we will go back to this figure  so now  i am interested in i am interested in \ngoing to the hidden layers  again i will do this to bit calculation where i first asked for \nthis guy and then i will ask for the light blue guy right and am going to look at one unit at a \ntime  now what is  the   what  am  i interested in  the derivative of the loss function with \nrespect to say d h two two  right  the second unit of the second hidden layer  \n\n \nnow  what i am going to say here is exactly what i had written on the previous slide this \nwas our final  function  right  which was z  so  z was sorry again  i have not chosen my \n \n \n \n\fvariables well ok  but  if  so  we had exactly the  same situation  right  which is  which \nyou see here ok  so  we will just have to sum up the derivatives partial derivatives across \nall the paths  which lead from this guy to this guy and there could be as many paths as \nthere can be  but i do not care i will just sum across all those paths  in fact  actually here \nthere are not just two paths because we have always assumed there are k classes  so  there \nare actually k of these paths right  \nso  this form this is exactly the formula which i wrote on the next slide right this one  but \njust written in terms of the network that we are dealing with  so  you can just go back \nand look at this  but as long as you understand this figure you from my point of view we \ncan  go  ahead   so   everyone  understands  this  figure  that  we  just  need  to  compute  the \nderivatives across all the paths and add them up  \n\n \nso now let us start we again the same recipe we will compute it with respect to one guy \nand then go towards the gradient  so  what is this now  let me explain right  so  dl theta \nthere are k of these guys between right  so  there are k paths  so  this summation has to \nhappen over k paths just as you told me when there were two paths the summation was two three \npaths to three that is k paths of the summation over k guys  the derivative with respect to \neach of these guys and the k\u2019th the m\u2019th unit rate that is the index that i am iterating over \nand then the derivative of this guy with respect to whatever you are interested  \n \n\fthat  is  just  that  there  are  only  two  nodes  in  the  path  in  the  chain   but  there  are  k  such \nchains  how many of you exactly get this  ok how many of you have a problem want me \nto  repeat  this   you  have  problem  oh  many  of  you  ok   good  please  do  this   so   i  am \ninterested in this quantity  that means  i am interested in the partial derivative of this loss \nfunction with respect to this guy  \n\n \nand this guy is nothing but h ij that much is clear is the j\u2019th unit of the i\u2019th hidden layer  \nin fact  this is actually h two two  so  my i is equal to two and j is equal to two  now i just made a \ncase on the previous slide that  if  you have such  a function which first  computes some \nintermediate  values   and  then  your  final  function  is  computed  based  on  all  these \nintermediate  values  right   and  now  you  are  trying  to  find  the  gradient  the  partial \nderivative of this with respect to the original input that you had  \nso  then what you will do is you will sum across all the paths that lead from this guy to \nthe output  how many such paths are there  you already see two such paths here right  but i \nam saying there are k such paths  because there are some other nodes here which i have \nnot drawn we have already said that in the output layer we have k nodes right  so  there \nare k paths  so  that takes care of the first bit that the summation is going to be over the k \npaths  \nnow what is each of these paths composed of  this intermediate value and this quantity \nthat  we  are  interested  in   first  we  will  take  the  derivative  of  the  out  of  the  loss  with \n \n\frespect to this intermediate value  what is that  that is the unit in the  that is the unit in \nthe previous layer or the next layer rather  so  i am interested in i  so  i am looking at the \nunit in the next layer hence i plus one right  because that is what comes in my path the next \nlayer  is  what  comes  in  my  path   we  have  always  the  special  case   that  this  guy  feeds \ninto k guys  but all the other hidden units before that feed into n guys right  \nso  that is let us just keep that complication aside for the minute and we just look at this \ncase ok  is that fine  so  we have agreed there are k paths and each path is composed of \nthese two nodes  from  the  last loss function to  this  intermediate value  and  then from  this \nintermediate value to the quantity of interest  and why is this i plus one because the next \nnode in the path when i am at the i\u2019th layer  \nso  i will be feeding to the i plus one\u2019th layer right  and in fact  i will be feeding to all the \nnodes in the i plus one th layer  that is why i am taking or all the k paths right  and then that \nnode which is this node with respect to the quantity  that i am interested in  is this clear  \nnow  right  this  is  very  similar  to  the  toy  example  which  i  did  i  just  have  k  paths  now \ninstead of two paths there  \nso  let us move ahead  now what is ok which of these quantities do we already know  is \nthere any quantity that we know  this one  why  because in this special case i plus one is \nactually equal to l right  because we are feeding into the last layer and they have already \nseen how to compute the partial derivatives with respect to the last layer  \nso  this quantity is known we do not know this for the generic case yet  but we will get \nthat  but for this special case when we are feeding into the last layer we know this does \neveryone get this  ok now do we know this quantity  so  what you have told me is that \nwe know this quantity because that is what we have computed in the previous module  \ndo we know this quantity  we have to compute it  can you compute it  ok let us just do \nit right  so  let us assume that this hij that am dealing with is actually h two two ok fine  now \nwhat  is  a  i  plus  one  m   actually  which  are  the  elements  there  a  three  one  and  a  three  two   i  am \nassuming that i only have two units in the output layer ok  \nso   my  m  is  equal  to  two   now  is  this  fine   this  is  how  the  next  layer  is  related  to  the \ncurrent hidden layer plus biases ok  now what am i interested in one of these guys ok  \nlet me take one of these guys  so  can you tell me what is a three one  first row multiplied by \nthe first column there is only one right plus b two one   \n\fstudent  \n  \nsorry  \nstudent  \n  \nb three one   now let me just clarify something what is this in terms of variables i  j  k  m what \nis this  this is i this is j this is k this is m  this is i plus one right ok  this is one of the ms \nthat  i  am  dealing  with   now  i  want  the  derivative  of  this  with  respect  to  hij   in  fact   i \nwant  it  with  respect  to  h  two  two  where  this  is  i  and  this  is  j  is  this  clear   what  is  this \nderivative  w three one two everything everyone fine with this  now help  me find this  what  is \nthis ijkm and i  plus  one  what  is  this  this  is  coming from the m  how  many of  you see \nthis   because  that  is  the  unit  that  you  are  connecting  to  and  this  is  j   so   what  is  the \nformula  how many  as many as the number of neurons in the next layer a bias will be \nconnected to all the neurons in that layer right  everyone gets that right there are only two \nunits  \nso  there will be only two guys ok  so  what is the formula for this w i plus one mj  everyone \ncomfortable  with  that  ok  fine   you  can  just  go  back  and  look  at  this  and  it  should  be \ncleared  right   so   whenever  you  are  dealing  with  vectors  and  matrices  right  if  you  are \nreally good at it you can imagine the entries and figure out what is happening  if you are \nnot  good  at  it  do  not  be  lazy  just  work  it  out   right   you  just  need  to  write  down  this \nproduct and at the end remember everything is always  element wise and you are never \ndealing with a vector or matrix now just dealing with the individual components of them  \nso   you  should  always  be  able  to  compute  these  derivatives  or  partial  derivatives  with \nrespect to  the individual components   and that is exactly  what  i did here  right   if  you \njust  work  it  out  if  you  just  write  it  out  then  you  will  always  get  it  if  you  cannot   but \neventually try to get to a point where you can just visualize it  but if you cannot at least \ntry to work it out  \n\f\n \nso  this is what it will look like ok  now consider these two vectors one is this vector what \ndoes this vector look like  this is a collection of all the partial derivatives  so  this is just \na  collection  of  all  the  partial  derivatives  nothing  new  we  have  already  seen  this   now \nwhat is this vector actually  in fact  i have started with the matrix and i am saying look \nat this vector  what does this mean  this i plus one is just the layer in which the matrix is \nright  so  that index we do not really care about  for a matrix what we care about is the i \ncomma j index ok  now what does this dot comma j mean  all the i\u2019s belonging to j  that \nmeans  the dash column j\u2019th column everyone gets this  this is all the i\u2019s or all the entries \nbelonging to the j\u2019th column  \nso  it is effectively just the j\u2019th column  so  it is one comma j two comma j up to k comma j \nright  so  these are two valid vectors  now tell me what is this quantity going to be  this is \nthe dash between two vectors dot product dot product between two vectors is a  \nstudent  \n  \nis a summation over element wise thing ok  i have said enough now try to connect this is \na very simple maths the column that you will ever get in your life  try to connect this to \nsomething which is already there in the slide  how many of you think the answer is this  \nthis into this plus this into this plus this into this and just write it as a formula you will \nget  this  everyone  sees  that  ok   so  now   i  have  a  compact  way  of  writing  one  of  these \nentries  \n \n\f\n \none of these guys i have a compact way of writing this  it happens to be the dot product \nbetween two vectors one of them is the gradient  but do i know this already  do i know this \nquantity already  in this special case yes  because i plus one is equal to l and that i have \nalready  computed  this  of  course   i  know  right  because  these  are  the  weights  that  i  am \ndealing  with   where  do  i  go  from  here   this  dot  yeah  it  means  anything  from  that \ncolumn so  that means  the entire column  \nstudent  \n  \nah no these are weights right  so  this is a weight matrix it has columns and rows  i am \ntalking  about  the  j\u2019th  column   so   i  fixed  the  value  of  j   i  am  talking  about  the  j\u2019th \ncolumn  but i am not telling your given i\u2019th entry there am just telling you all the entries \nthere that just means the j\u2019th column you can take this offline ok  this is very simple i \nwill take it offline ah  now where do i go from here  \nstudent  \n  \ni plus one  \nstudent  \n \nok  no  in this specific case are we done  \nstudent  \n  \n \n\fwhere are we right  now with  respect  to  one unit  where do we want  to  go  the entire \nthing  so  what is the quantity that i am interested in gradient with respect to always say \nwith respect to h i right  \nstudent  \n \nwhere i is two in this case this special case ok  what is that going to be collection of all \nthese  guys  that  you  have  already  computed  ok   now  simplify  this   what  is  this  first \ncolumn of the matrix  multiplied by the same vector the second column of the matrix  \nmultiplied  by  this  vector   the  nth  column  of  the  matrix  multiplied  by  this  vector  this \nreminds  you  of  something  very   very  difficult   this  is  a  very   very  complicated  matrix \nmultiplication right  \nfirst row of the matrix multiplied by a column the second row of the matrix multiplied \nby column  how many if you get this  right  so  this is can you tell me what this is wi \nplus one transpose  \nstudent  \n  \nperfect  right   so  now   you  see  that  this  entire  quantity  we  can  compute  in  one  go  by \nusing a matrix vector multiplication right   so  that  is  what  i meant  when  i was saying \nthat we should not be doing these unusual computations  but we able to compute that at \none row right  so now  we can just do this matrix vector multiplication and get this entire \nquantity ok  now  what is still missing in this module  \nso   what  is  the  special  case  that  i  have  assumed   i  told  you  that  i  already  know  these \nquantities  but only if i plus one is equal to l  i need to tell you this in the generic case ok  \nso  we are almost there except that i do not know this when i is not equal to l or i is less \nthan equal to l minus one ok  that is the case that i am looking for  \n\f\n \nso  that is again very simple  again what will i do  i will compute it with respect to ok  \nwhat is this  this is the guy that i am interested in the generic i not the l\u2019th one right \nthe generic i  this is what the vector looks like the gradient vector looks like  i want each \nof these guys ok  now i will take one of those and i will write it as this ok  what am i \ndoing  am saying that  i already have the entries up to here ok  at  a very general level \neven here i could have said the same thing  remember that i had said that the output layer \nyou can always write as hl  right  \nso  even at the output layer i could say this chain rule always holds  how many of you \nagree with that  i want to go from the loss function to one of the lighter blue guys  so  \nam saying that i can go through the intermediary dark blue guys  that is all i am saying  i \nhave just compressed this entire path into up to the dark blue guy  remember i had said \nearlier that i will be compressing this chains  now how many of these quantities do you \nknow   the  first  one  is  what  we  computed  on  the  previous  \n   the \nsecond one looks very difficult sorry  \nso  h ij is nothing but sigmoid of a ij or any non linearity of the a ij  so  i can just write \nthis derivative as i will just write it as sigma prime ok  \n \n \n\f\n \nor g prime is this fine  now i have it with respect to one unit  what will i do  go to the \ngradient  fit  it  all  these  values   now  simplify  this   what  is  this   a  vector  right   what  is \nthis  another vector  there is a one to one correspondence between them  so  you have two \nvectors and you are doing a one to one multiplication  what is this  \nstudent  \n  \nhow many of you say dot product  dot product is always a  what is the output here  \nstudent  vector  \ncan it be a dot product  can it be a dot product  no please empathic no ok  so  what is it \ngoing to be  an element wise multiplication and this is how you denote that ok  so  what \nis  this  called   you  had  a  multi product  right   so   this  is  every  element  of  one  vector \nmultiplied by the corresponding element of the other vector ok  so now  again the entire \nvector we can compute at one row right  i am not i am when i am teaching this i am telling \nyou how to compute one element and then go to the gradient  but when you are going to \nimplement this we are just going to compute the gradient at one go  \n \n\f"}
{"audio_filepath": "lec003_007.wav", "duration": 754.691, "text": "\nback propagation  computing gradients w r t  parameters \n \n \nbefore we move on to the next module  a quick summary of what we have done so far  \nso  we introduced feed forward neural networks  and we wanted to learn the parameters \nright from the last layer to the first layer  and we figured out that what we can do is that \nwe  can  just  use  the  gradient  descent  algorithm  as  it  is   except  that   we  have  this  small \nproblem that we have so many parameters now  and located at differ different points in \nthe  network   right  some  at  the  initial  layer  some  at  the  final  year   and  you  want  to \ncompute the derivatives or the partial derivatives with respect to all of these  \nif you can do that  put them all in this large matrix  then we can just use gradient descent \nas  it  is   so  that  is  what  we  figured  out   and  then  we  wanted  to  find  out  the  gradients \nwith respect to or the partial derivatives with respect to all these parameters  so  then we \nrealize that this can be done using chain rule  because there is a path from  your output \nwhich is the loss function to any of these weights  so we just need to follow that path and \napply  this  smart  this  chain  rule  smartly  and  just  sum  up  the  derivatives  across  all  the \n\fpaths that lead to that weight  so  in that process we started from the output layer  we just \ntreated it a bit special  because the output function is special and this is the last layer  \nso  we just first computed the gradient with respect to the output layers  then we figured \nout how to compute the gradients with respect to any of the hidden layers  and now if \nyou are at a particular hidden layer  now the weights that feed into this layer we could or \nwe have not reached there  \nso now  the next thing that we need to do is that we have computed the gradients with \nrespect to any of these hidden layers  and now we want to find the gradients with respect \nto the parameters which is the weights and the biases  so  it is the do you all remember \nthis   or  it  is  all  long  history  or  the  story  is  back  right   fine   so  now   we  are  at  the  last \npoint which is computing gradients with respect to parameters  \n\n \n \n \n \n\fso  again this is the overall picture  we were in this chain rule  and we have come all the \nway to the last point where we are ready to now compute these quantities  so now start \nby  recalling  that  a  k  is  equal  to  b  k  plus  w  k  h  k  minus  one   right   this  is  our  activation \nformula   pre  activation  formula  right   so   i  am  talking  about  these  light  blue  guys   ok  \nwhich is clear in image  \nand now i what have i done so far  i have been able to come up with a formula to write \nthe gradient of the loss function with respect to any of these light green guys  right  that \nis what where we ended last time right  where we are able to compute the gradients with \nrespect  to  the   sorry  light  blue  guys  ok   and  now  i  want  to  compute  the  gradient  with \nrespect to any of these parameters or any of these parameters  \nso   any  parameter   it  does  not  matter  am  at  some  i\u2019th  activation  layer   pre  activation \nlayer   and  i  just  want  to  compute  the  gradients  with  respect  to  the  weights  which  feed \ninto this layer  and that is what we are interested in  so we are just taking any layer k  \nand you want to find the gradient with respect to the weights there  now can you tell me  \nso  can you tell me what is what is the thing that am going to do here  or what is the \nrecipe that we have been following  \ni need to  move  what  is  the recipe that we have  been following  apart from  yelling  at \npeople  who  come  late   we  find  the  element  wise  partial  derivatives  first  and  then  put \nthem  all  together  to  get  the  gradient  ok   what  is  the  element  here   what  is  what  am  i \nlooking for right now  i want to compute this fill this blank  what goes here  \n \n\fstudent  w  \nw any of these w is right  and in particular say w k that is what i am looking for  so \nwhat is the first thing that i am going to attack  \nstudent  wkij  \ngood  w k i j and once i have this for one of these guys i just know a generic formula \nwith respect to i j and k and i can just put it into a gradient vector ok  is that fine  ok so \nnow can you  ok  now from here to here if i want to reach from here to here  so  this is \nwhat  i  am  interested  in   right   now  how  is  the  chain  rule  going  to  look  on   look  like \nbased  on  whatever  you  have  already  seen   till  where  have  you  already  reached   you \nalready know this quantity  right  now if i want this how am i going to write it  \nstudent  \n  \ni will find up to the light blue guys  which is this i already know how to compute it  and \nthen from the light blue guys i will go to the this  is fine right  so  this is the quantity \nthat i am looking for  ok  now what is one element of this guy  dou a k by  is it fine  ok \nwhat is the dimension of this actually  is it a scalar  a vector  a matrix  matrix or a tensor  \nwhat is the tensor  what is it  is it a matrix  what are the dimensions  what does this \nderivative mean  or this gradient mean  i change one element of w k how much does \none element of a k change  how many elements are there in ak  n  how many elements \nare there in  w k   n cross n  so  how many partial  derivatives  which  i have   n cross n \ncross n  what is this  \nstudent  tensor  \na tensor  right  so  this is going to be a tensor  ok  so  when i say one element of this  i \nmean  this  ok   so   this  is  one  element  of  this  gradient   ok   now  can  you  tell  me  the \nformula for this  what is this quantity  hk minus  \nstudent  one \n  \nhk minus one or hk minus one j or  \nstudent  \n  \n\feveryone gets this hk minus onei  how many of you get this   \n\n \nso  let us do it  right  so  you have akone   aktwo  akthree that is  your ak vector  ok  you have \nbkone   bktwo   bkthree  plus  wkone  one   yeah   i  know  again  this  is  one  of  those  silly  things   but  if \neveryone does not raise their hands and compelled to do this  so  h k minus one one hk minus \none two hk minus one three ok  so  let us take one of these guys right  so  a k one can you tell me the \nformula for that  \nstudent  \n  \nplus  first row ok  one two this one three  now can you tell me this quantity  so  what is i here  one ok \nso  i want this by w k i j right so  i is one so  i can take any of the j  so  let me take j equal \nto two  so  what is it going to be  this will go off this is constant  this is constant only this \nterm  remains   and  the  derivative  is  hk  minus  one  two  which  is  j  right   so   that  is  what  the \nformula  says   so   i  have  a  formula  for  one  of  these  guys   ok  and  that  is  a  generic \nformula  so  always  remember if  you cannot  figure out  what  it is  just write it down in \nscalar terms  just add up all the terms and you will get the formula  right  so now this is \nwhat the chain rule is going to be   \n \n\f\n \nso   this  is  what  it  is  going  to  be   this  is  one  element  of  that  tensor   this  is  how  that \nentire thing is going to look  i have just flattened it out and put it here  \n\n \nnow let us take a simple example of wk belonging to r cross  three cross three everyone is fine \nso far right or anyone who everyone is fine please raise your hands  i mean fine i mean \nnot  in  life   but  with  the  lecture  fine   so   this  is  what  it  looks  like  right  for  a  three  cross  three \nmatrix  \n \n \n\fnow  let us see we already found out that this guy is equal to hk minus one comma j right  \nso  this is what this matrix looks like  nothing rocket science here right  so  each of these \nquantities is actually can be written in this form  where i appropriately substitute i k and \nj  and i know that this quantity can be further written as this quantity  right  that this is \nour clear right so  i have written it as this   \nnow   can  you  simplify  this   i  do  use  a  lot  of  this  ok   can  you  simplify  it   is  it  looks \nsimilar  to  something  that  you  did  on  the  assignment   does  this  look  like  matrix  which \nhas some very  regular patterns  yeah  i  can see  someone doing this  and this  everyone \ngets it  \n\n \nso  let  us  see  so  this  the  first  column   the  second  term  in  the  product  is  all  same \nthroughout  all  the  rows   right   what  i  mean  is  all  these  guys  are  similar   same  thing \nhappens in the second row the third row  right ah  that is sorry the second column and \nthe third column  what about the rows  these are all equal right  so what does this look \nlike actually  the outer product of two vectors  everyone gets this  raise your hands  ok \ngood  \nso  i do not need to do an example  so  it is fine right this is an outer product of these two \nvectors  one happens to the quantity to be the quantity that we already knew  right  and \nthe other happens to be a quantity that we can figure out  i mean we already know this  \n \n\fwhat  is  we  know  how  to  compute  the  hidden  representations   right   the  hk\u2019s  we  can \ncompute  \n\n \nso   fine  so   finally   we  come  to  the  biases   this  is  what  one  entry  looks  like   this  is \nexactly the sum which i had written out  now i take the derivative with respect to b k i of \nthe loss function  so  i  could  write it into as this chain  rule   where the first quantity is \nsomething i already know i have computed the gradient with respect to the pre activation \nlayers what about the second quantity  anonymous roar is what i was expecting  \nstudent  one  \none  ok   fine   we  can  now  write  the  gradient  with  respect  to  the  bias   what  would  it  be  \nwhat is this  what is this  it is just the gradient with respect to the pre activation layer  \nright simple  so now  we are done with all the gradients that we were interested in  \n \n\f"}
{"audio_filepath": "lec003_008.wav", "duration": 348.828, "text": "\nback propagation  pseudo code \nso   we  move  on  to  the  next  module  and  now  we  will  write  pseudo  code  to  for  back \npropagation   \n\n \nso  we have all the pieces of the puzzle  we have the gradients with respect to the output \nlayer  that was the special layer because  the output activation function is different  they \nare the  gradients  with  respect  to  all the hidden layers  that means   i  have the  gradients \nwith respect to the activations as well as the pre activations  \nso  in the h\u2019s as well as the a\u2019s and i also have the gradients with respect to the weights \nand  the  biases  and  this  is  all  index  agnostic  right   that means   i  am  just using  k  as  the \nindex everywhere  i have a generic formula  which applies at any layer for the weights as \nwell as the activations and the pre activations right ok  now  we can put all this together \ninto a full learning algorithm  so  let us see what the pseudo code looks like  \n\f\n \nso  we have this t equal to zero well run this for some max iterations we initialize all the \nparameters  to  some  quantity  will  randomly  initialize  them  ok   now   for  these  max \niterations   can  you  tell  me  what  is  the  first  thing  that  i  will  do   so   there  will  be  two \nfunctions here ok  tell me what those two functions would be  \nstudent  forward  \nforward  propagation  and  then  backward  propagation  right   so   you  do  a  forward \npropagation  and  you  compute  all  these  activations  pre  activations  output  layer  loss \neverything and then  you do this backward propagation  where  you  feed all these things \nwhich you have computed  these are the quantities which you have computed  you will \npass this to your backward propagation algorithm it would not look  so  nasty as this it \nwill not take  so  many parameters you could write it smartly and then you will just do \nthe parameter update  \nso   what  will  the  back  propagation  give  you  actually  all  the  gradients  all  the  partial \nderivatives  right  and  then  once  you  have  the  partial  derivatives   you  know  how  to \ncompute  the  update  law   so   now   let  us  look  at  these  two  functions  more  carefully  the \nforward propagation and the backward propagation  \n \n\f\n \nso  forward propagation is simple for all the hidden layers  that means  from layer one to \nlayer l minus one what will i do give me the code a k is equal to good then ok  and what it \nwhat is h of zero you are starting the loop from one right  so  you will need h of zero that is x \nand then you will have a special treatment for the output layer and your final output will \nbe whatever output function you use ok  this makes sense you can write this in python  \nyou will have to write this in python  \n\n \n \n \n \n\fnow we have computed all the h\u2019s and the a\u2019s what have we computed all the a\u2019s  all the \nh\u2019s and all and the y  right now you want to do back propagation  so  back propagation \nthe loop will be from i equal to one to n minus one good  so  the first thing i will compute is \nthe gradient with respect to the output layer  see  even here the output layer was outside \nthe loop  the same thing would happen here also  in the back propagation also first you \nwill compute the gradient with respect to the output layer and this is the formula  \nif  you  remember  from  last  class  right   that  is  the  formula  which  i  have  substitute  here \nand note that f of x is known to you because you computed that in the forward pass and e \nof y one hot vector which with a correct label said to one and you know what the correct label \nis because  we have given you the \n data right ok then what would the \nloop  be  l  to  one  or  l  minus  one   let  us  see  first  you  compute  the  gradients  with  respect  to \nparameters  it is l  \nso  because  we are using k minus one  then you compute the gradients with respect to the \nlayer below computes  gradients with  respect  to  the pre  activation right   this  is  exactly \nhow you will proceed this   is clear to everyone  the same three components that we have \nused you might be a bit confused about the ordering in which we have put them because \nwe computed the gradients with respect to pre activation first and then the weights  but \nonce you go back  you will realize because it is the way we have indexed it because this \nis already outside  \nso   this  has  already  been  computed   so   you  can  already  compute  the  gradients  with \nrespect to the weights of the outermost layer  is that fine  so  this is straightforward you \ncan go back and check this ok now anything remaining  or you have everything can you \njust take a minute and see if you can visualize the python code and we will just assume \nthat you are done the assignment you can read you will have multiple these vectors and \nmatrices and so on  and you are just doing a lot of matrix operations using  refer time  \nfour six  or \n or whatever you prefer right  \nnow  what is missing here  input is missing  ok  input we have given right  the ominous \ndata set has been given is there something that yours i have still not shown you how to \ncompute   oh  i  did  not  update  the  parameters  here  is  it   no  the  parameter  update  will \nhappen  in  the  outer  loop   right   so   those  forward  prop  back  prop  and  then  update  the \nparameters  right   so   the  main  algorithm  was  forward  prop  back  prop  update  the \n\fparameters   when  we  saw  forward  prob  an  obvious  seeing  backward  prop   so   what  is \nmissing one thousand iterations  something in  the last line before end of  course  do  you know \nhow to compute this  \n\f"}
{"audio_filepath": "lec003_009.wav", "duration": 102.906, "text": "\nderivative of the activation function \nwe have that activation function and we were taking the derivative of the activation with \nrespect to pre activation  and i just pushed it under the rug by saying  we will write it as \ng dash  so  i need to show you what g dash is  \n\n \nwhat  how to  compute  g dash  so  this  is  suppose  g is  the logistic function ok  so that \nmeans  what is z actually  it is one of those a\u2019s right  so  this is the activation that you \nare going to feed it right and then you are taking the element wise sorry z is actually the \npre activation that you feed it and then g is the activation function  so  i will do element \nwise activation function  now what is the derivative of this  so  i will just i will not do \nthis derivation  \nit is there and you end up with a very neat formula which is g of z into one minus g of z  \nso  now  that bit is also taken care of is there any more spoon feeding that i can do  you \nare ready for the assignment now  i will do one more bit  you will also have used a tanh \n\ffunction  so  this is the derivative of the tanh function  it again boils down to a very neat \nformula which is one minus g of zd whole square  so  we will end this lecture  \n\f"}
{"audio_filepath": "lec003_010.wav", "duration": 2657.009, "text": "\ninformation content  entropy   cross entropy\nso  for the next module we need something known as cross entropy  so  we will just try \nto make some develop some intuitions for cross entropy and get to the formula for that  \nand then i will tell you how it relates to the problems that we deal with  ok  so  first let \nus start with something simple that what is it that we are trying to do  ok  so  with that i \nwill give you an example and i will ask you a few questions and then from there we will \nslightly try  to go towards  cross  entropy  so  now  suppose you have an  urn which  has \nthousands of balls  and these balls are of three different colors which are red black and \nwhite \n\n\nso  you have an urn which has three different types of  and there are many such balls\nwhich you have put in it  and since you have put in it you know how many red balls are\nthere  how many blue balls are there  and how many white balls are there  so  for you it\nis very easy to compute the probability of each of these things \n\fso  that is say our probability is zero twenty five  zero thirty five and zero four  now  talking more formally what is\nhappening here is that  you have a random variable x which can take on the values red\nblue or white  right  and this is the probability of each of those or the random variable x\ntaking any of these values  ok  so  this is the setup  now i am your friend and you tell me\nthat you can peep into the zone  you cannot actually count take out all the balls and count\nthem and estimate the probability  we can just take a look into this  and try to give me an\nestimate of what these actually  what are these probability values  that means  what is the\nvalue \nso  what is the probability that x is equal to red or x equal to white or x equal to blue \nso  i just take a look at it turn it around a bit  and try to get some feel  ok  i see a lot of\nred balls  but a fewer blue balls or white balls and so on  and based on that i make my\nbest estimate  right  so  i will just say that maybe these probabilities are zero thirty five zero forty five and\nzero two  right  so  this is actually the true distribution  i will call it as p  right  because this is\nthe correct one  and what i have estimated  i will call it as q \nand remember now p has you can think of p as a vector which has these three values  p one  p\ntwo  p three because there are three possible events here  and similarly q has three values  q one  q two and\nq three  so  in this case i clearly know that  i am wrong or when i give you these values you\nknow that you are wrong  so  you tell me that whatever you have estimated is wrong \nthen i obviously  ask you  tell me how wrong was i  so  how would you give me that\nnumber  that is the thing that we are interested in \nso    the   general   problem   that   we  are   interested   in   is   that    there   is   a   true   probability\ndistribution  and there is an estimated probability distribution  and we want to find out\nhow bad was the estimation  now  can you tell me a simple way of computing this  it\nmay not be correct  but still it makes sense  \nstudent  squared error  \nyou can just take the squared error  so  what you are essentially telling me is that you\ncould just treat these two as any other vector  right  and you could take the squared error\ndifference between these quantities   so  what you are telling me is this  where i goes\nfrom one to three  ok  so  this is one valid way of doing this  but then we are ignoring the fact\nthat this is a distribution and hence  it has certain properties that the sum of the elements\nis one and so on all of them are positive and things like that  so  we are ignoring those kind\n\fof things we are completely ignoring the fact  that we are not dealing with a normal\nvector  but a spatial vector which happens to be a distribution  so now  we want to find\nout a more principled way of computing the difference between two distributions  and in\npractice why are we interested in this  because we will always have a true distribution\nand a predicted  distribution   so  that  is  what we want to do  we have some way of\ncomputing it  but you want a better way of computing it \nnow  let me make a case for why do we care about such differences right  so  let me take\na   simple   case   of   a   classification   problem   and   to   motivate   that   i   will   start   from   a\ndifferent example and then i will come to the classification problem  suppose there is a\ntournament going on  and there were four teams which leads the semifinals  let us call them\na b c d  ok  now  you were following the tournament up to the semifinals  and after\nthat you didn\u2019t watch the tournament  and you do not know who eventually won  well \nthe tournament is over and someone has won it \ni actually watched the tournament and i know that b has won it  now can i express this\nin terms of a probability distribution right  so  first let us look at what is the random\nvariable here  what is a random variable here  the team which won  right  so  that is my\nrandom variable and it can take one of these four values \nnow  i know that team b won  because i saw the tournament  and i have seen that they\nwon  so now  how can i write this as a distribution  what is the distribution comprised\nof  it comprised of these probabilities assigned to each of these events  and there are four\nsuch events here  so  how do i write this distribution  so  what you are telling me is i\ncould write it as zero one zero zero \nso  essentially they are telling me that all the probability mass is focused on one of these\noutcomes   because   that   is   the  certain   outcome    that   is   already   happened   no  one  can\nchange it so  that is the outcome for this tournament  so  i know that the probability of\nthat even is event is one and everything else is zero  so  in other words the probability that the\nrandom variable x takes on the value b is one and everything else is zero \nso  what i am trying to tell you is that  even for a certain event  you could still write it in\nterms of a distribution  where all the mass is focused on that event  now again i will\nbring the same setup that i did not watch the tournament after the semifinals  so now \nyou   ask   me   give   me   your   prediction    what   which   team   would   win    or   this   is   the\n\fprediction which i made before just after the semifinals or  just before the semifinals that\ni think one of these teams is going to win the tournament  and the chance of each of them\nwinning is something like this  so  i know the teams i follow this sport and i probably\nknow that  ok  b has a very strong team and they have a very good record in the past few\nmonths and so on  so  maybe they have a higher probability of winning  so  these are the\nnumbers which i assign \nnow  again i have made an estimate  was my estimate perfect  when would it have been\nperfect  if i had predicted with certainty won that b is going to win  but i was not willing\nto bet everything on b  so  i said there is a very high chance it will win  but there is still\na chance that there could be some surprises  now  how wrong was i  now again tell me\ncan you tell me what is p and what is q here  this is the true distribution and this is my\npredicted distribution  and what am i interested in again  the difference between them \nhow wrong did i go  and what again what is a simple way of doing this again  square\nerrors  so  again this is what my formula would look like \nso  this is fine in this toy case  but why do we care about in real life  examples  that we\nare going to deal with in machine learning  so  in watching learning will deal with a lot\nof   problems   which   are   classification   problems    and   in   classification   problems    you\nwould again have this setup where you have a label the good thing of the label as a\nrandom variable  and it could take off one of many values  so  i will again assume that it\ncould take  suppose you are trying to take  a picture  of fruits  and you are trying  to\nclassify them \nand i could again think that i have four fruits say apple  banana  cherry and dragon fruit \nok  and this random variable can take one of these four values depending on the image that\ni am seeing  ok  now i have been given some training data  so  for every training data i\nhave been given an image  and i have been given the correct label  so  for that training\ndata what is this distribution  suppose i have been given the image of a banana  what\ndoes  this  distribution look like   zero one zero zero  right  again i have seen it so  i know it is\ncertainly a banana \nso  i do not have any confusion all the probability mass is focused on that  now the same\nimage   we   are   going   to   show   to   one   of   our   models    ok   and   it   is   going   to   make   a\nprediction  and will again ask it to give us a distribution  the model will give us values\n\fperhaps like this  ok  so  this is the models prediction  again the model has given us a\ndistribution and we have a true distribution  and we are interested in knowing how wrong\nthe model was  so  that \nstudent  correct the \n \nwe can correct the parameters of the model  so  this is our dash function loss function \nright  so  a loss function is some notion of difference between p and q  right  and so  far\nwe have been dealing with a very simplistic notion of this difference which is just the\nsquared error loss  ok  and we want to do something better than this right  so  what i\nhave told you is that you could always have a true distribution always have a predicted\ndistribution  and you would be interested in finding the difference between them  that is\nthe   one   first   part    the   second   part   is   that   even   when   you   are   given   something   with\ncertainty  you could still write it as a distribution such that all the mass is focused on that\nevent which was which has happened right  which was the label was banana in this case \nand then you could still predict this from your model  and now you are interested in\nknowing how wrong you are model wind because  that is the loss function that you will\nuse  and then you will try to update your parameters with respect to this loss function \nmeans that is the setup that we are interested in  so  that is so  i made a case for why we\nneed to find differences between two distributions  how to do it  in a more principled\nmanner we have not seen that yet we will get to that  ok  so  before i get that  i also need\nto tell you something about expectations  so  let us written to as sports example where\nthere were four teams  and say based on pundits and that sport  they have said that these are\nthe probabilities of winning \nand now you are into betting  and you bet place your bets on these teams and you place\nour bets in a way that  suppose team events then you end up winning ten k rupees  if team\nb wins then probably will end up winning five k rupees  and if team c wins probably ten k \nand if one of the other team wins maybe will end up losing money or something like that \nnow  you  want to  know   what  is   your expected  reward   so now   let  us  see  what  is\nhappening here  this was a random variable which could take on one of these four values \nthese are the probabilities of the random variable taking that value  taking on the value\na or taking on the value b c and d  ok  this is your value or the gain or the profit\nassociated with the random variable taking one of these values \n\fso  you have a random variable  you have a probability associated with every value of\nthe random variable  and you have some gain or value associated with every value of the\nrandom variable  now how do you calculate the expected gain or expected profit which\nis this  there is a thirty percent chance that you will earn ten k  there is a forty percent chance\nthat you will earn five k  there is a twenty percent chance that you will earn ten k  and ten\npercent chance that you lose thirty k \nso  the way you will compute it is that and this is the simple expectation formula  which\nis the probability of now the event here belongs to abcd  right  this is one of the four\nteams that will win  probability of that event happening in to the value associated with\nthat event happen  this is a fair computation you get the intuition that this is how you\nwill compute the net reward that you have  so  this is how you compute the expected\nvalue with respect to a particular distribution  so  this is the background that we need \nnow  i will just go on to the next slide  and now we will talk about entropy first  perhaps\ninformation  content  be first then entropy and then cross sector  ok  so now  what is\ninformation content \n\n\nso now  again let us take the same case that we have a random variable which can take\non values a b c d  now let us what we are trying to say is that  if i know a certain thing \nwhat   is   the   information   that   i   have   gained    so    you   and   i   are   talking   you   tell   me\n\fsomething   and   i   want   to   see   whether   my   information   was   enhanced    whether   my\nknowledge was enhanced  that is how we will quantify information content \nso  if you are talking to me and you tell me that my name is mitesh  the zero information\ncontent for me  right  because i already know that there is no surprise in that ok  but if\nyou talk to me and you tell me that today there is going to be a lunar eclipse  then there is\na possibility there is some information content gain for me  right  because that is not a\nevent which happens every day if you just tell me you will see the moon today and you\nlive in a region where it is not typically cloudy and there is no information gain there \nright  so  what do you see here  when is the information gained high  when the event\nwhich happens is a very surprising event   and how  do you say supplies  in terms  of\nprobability \nstudent  \n \nit is a very low probability event  right  so  if there is again this tournament  and say d\nwas the weakest team in the tournament  and a was the strongest team in the tournament\nif   you   come   and   tell   me   that   d   won   then   i   would   be   really   surprised   that   some\ninformation which i had gained  but if you tell me that a won then probably i already\nknew   it   at   the   back   of   my   mind    because   a  is   clearly   the   strongest   team   in   the\ntournament and there is no information gained for me \nso  one thing that we are trying to establish that the information content i see  ok  the\ninformation content of an event is inversely proportional to the probability of the event \nthere is a that is a fair intuition  fine  now i want am still talking in terms of vague\nthings i am saying it is inversely proportional  but i still need an exact function so that i\ncan compute it  so  i want something i want a function where i plug in the probability of\nan event  and i get the information content of the event \nright now i do not have that function i am just building some intuition towards that\nfunction    ok    but   this   is   one   requirement   that   i   want   the   function   to   satisfy   this   is\nsomething that all of us agree with  ok  now think of two events which are independent a\nand b  ok  so  a is the event whether the ac is on here or not  and b is a event which\ntells  maybe  so  let us consider two different random variables \n\fso  x is the random variable which can take on values zero and one  sorry  so  x is again this\nrandom   variable   which   can   take   on   these   four   values  abcd   whether   who   won   in   the\ntournament  and y is this another random variable which can take on the value on and\noff depending on whether the a c\u2019s on in this room or not  what can you tell about these\ntwo random variables  they are independent random variables \nso  this is on or off and this is which team won the tournament  now i come and tell you\nsomething about the random variable y  and i come and tell you something about the\nrandom variable  x  ok  so now  i want you to tell me this  what is the information\ncontent of x and y  i tell you something about x  and i tell you something about y  and\nthese two events are in these two random variables are independent  then what can you tell me\nabout the information content  what is the condition that you would want  you gain\nsome   information   by   knowing   things   about   x    and   you   gain   some   information   by\nknowing things about y  so  what can you tell me  it should be the sum  right \nbecause these two are independent events  so  whatever information i am getting from\nthis random variable and this random variable which together enhancing my information \nright  it is not cancelling out anything or is there is no common intersection there  right \nif   the   two   events   were   not   independent   then   i   would   not   expect   this   to   hold   because\nknowing something about the first event only tells me something about the second event \nright  because they are dependent \nso  then that case the information gained would not be additive  ok  so now  let us see  i\nalready made a case that this function which tells me the information gain is actually\nproportional to the probability  ok  so that means  this is what the input is going to be \nright  and then what is the other condition that i want  this is a fair thing  right  i just\nreplaced information content by a function  and i know that the function should depend\non the probability  because that is what we have established here \nso  we know that the function depends on the probability  we still do not know what this\nf is exactly  but i am trying to impose some conditions on f  one condition of f  f is that\nthis condition should hold  ok  now let us look at this condition which i have underlined \nthis is f of is this fine because it two events are independent  you can write them as the joint\nprobability as p of x into p of y  this is clear to everyone  right  you seem to be a bit lost\narvind clear  ok \n\fnow  what is happening here  i have a function f of a into b and that is actually equal to f\nof a plus f of b  what family of functions do you know which has this characteristic  log \nright  that is why log is a good choice for this  that is why information content is going\nto be the log of the probability  but i wanted to be inversely proportional  right so  it will\nbe log of one by the probability  ok  so  that is why information content of this thing is  so \nyou see this how did we arrive at this log formula \nand this log can just be to any base  it does not matter  so  all of you get how we arrive\nat the formula for information contained  now just give me a minute i need to think of\nwhat is the next thing that i have to say  ok  and so  we have found out the information\ncontent of one of these events happening  which was the x taking on the value a \nnow  let us think of this random variable x  so  here actually i should have said x equal\nto a probability of x equal to a  ok  it makes sense because the random variable is x  and\nthe event is x taking on the value a  how much information content is in that  so  if i\nknow that x was a how much will i be surprised by it  ok  now let us take this event  this\nrandom variable x which can take on values a b c and d as i said with each value there\nis a probability associated with it  such that this sums to one  now  i did not need to draw\nthis diagram  ok  i should \n \nso  x is a random variable which can take these four values  which each of these values  i\nhave   a   probability   associated    ok    so    these   are   the   values   these   are   the   probability\nvalues  now what do i also have  i have the information content associated with each of\nthese  right  and the information content actually tells me the surprise of that evening \nnow if i ask you what is the entropy of this random variable x  so  remember i had this\ncase where i was betting i am with every poor outcome i had a value associated with it  i\nhad the same situation here  with every outcome have a probability  and i also have a\nvalue associated with this  and the value is the information content \nnow  if i ask you what is the entropy or the expected information content of this random\nvariable  then how will you compute that  i am asking you for an expectation \n\f\n\nso  i will compute summation i belonging to a b c d  p of x equal to i information can\ntake what is the formula for that \nstudent  minus \n \nminus i will just take the minus outside  ok  so  this quantity is called the entropy of the\nrandom variable  right  it is the expected information content in the random variable \nnow   if   you   see   what   would   be   the   expect   entropy   of   a   random   variable   if   it   is\ncorresponding to a certain event  that means  say the sun rises always in the east  right \nso  what is going to be the entropy of that zero  why  you will have one of the sums in that\nsummation as one log one  right  and every other sum would be zero into log of something \nso  zero into anything is going to be zero  even though that quantity is not defined zero into\nanything is going to be zero  so  the total entropy is going to be zero  ok  so  this is entropy \nnow what is it that we are actually interested in  cross entropy  so  we have not gone\nthere yet  ok  so  we need to perhaps add one more slide  so far everything is clear  ok \nso now  we are interested in something known as cross entropy \n\f\n\nso  there again the situation is that there is something which is the true distribution  and\nsomething which is the predicted distribution   now  actually before going there so  let\nme just erase this off  how  many  of you have thought that  entropy is related  to the\nnumber of bits that you need to transmit something  do you know why that connection\nexists  no  now again let us think of this that you are trying to transmit a message \n\n\nand that message is again a random variable  which can take on four values a b c d  so \nthink of these as four commands that we are trying to send to someone  right  and then\n\fbased on that command someone will take some action  now in the digital case how will\nyou transmit this  encode it to bits  so  what is the encoding that you will use  zero zero yeah\nwe will come to that  zero one one zero one one  so  how many bits are you actually using for every\nmessage \ntwo bits  ok  for every bit so  maybe this is a this is b this is c and this is d  so  for every\nmessage you are using two bits  let us see actually what when you are doing this  what are\nyou actually assuming  so  actually assuming that all of these are equally likely  if all of\nthese are equally likely can you tell me the information content of any of them  it is\ngoing to be minus log of one by four  ok  that is actually equal to minus log and this is to the\nbase two  ok one by four is two raise to minus two  that is equal to two \nso  the information content is actually equal to the number of bits that you are going to\nuse to transmit that message  now let us see if this is just in this special case or in a\ndifferent case also  suppose this could take eight values  how many bits would you use  three\nbits right  so  you will have zero zero zero  zero zero one  ok  and this would be a to h  now what are we\nactually assuming here  each of these is equally likely  what is the probability  one by eight \nwhat is the information content  two raise to minus three  that is equal to three \nso  the number of bits that you actually use to transmit something this  can you can talk\nof it in terms of the information content of that  now suppose i want to transmit this over\nthe long distance  so  i need to bit be a bit efficient in terms of number of bits that i use\nright  so now  in this one of these cases  suppose it is of the following form  right  that\nlet us look at the case where x can take one of four values  and say let me just put the right\nvalues  so  i will say one by two  one by four  one by eight  one by eight  ok  now what is the information\ncontent of each of these  one two three three  and this is the message that i am going to send \nso  what am  i doing here   i am  using  a different  number of bits  depending  on the\nprobability of that event  why does this make sense  why is this a smart thing to do  if\nyou want to transmit something which you are going to transmit a lot of times  you better\nuse less number of bits for that  and this is exactly what is happening here  a was having\nthe highest probability  and you are using the lowest number of bits for that  now what is\nthe expected number of bits that i will use up  if it is a i will use one if it is b i will use two\nif it is c three and d three \n\fso  what is the expected number of bits that i will use  again i have the same situation \nright  i want you to cast it into the same situation  i have the probability values  and with\neach of these guys  i have a cost or a value associated what is the cost  one bit  two bit  three bit \nthree bit  so  what is the expectation  now  can someone compute the expectation   one seventy five\nactually let me just write it down  it would be again i belonging to a b c d  p of x equal\nto i into the number of bits that you will use  so  that is just equal to log of log to the\nbase two of p of x equal to i  minus one  what does this quantity actually  this is the entropy\nwe just saw this  a  this is the entropy of the random variable and what is it telling us\nactually that the entropy is one seventy five \nso  what is the meaning of this actually  so  on average you will be needing one seventy five bits\nwhereas  if you are assuming everything is equally likely on average you are using two bits \nright  so  you see that on average you are making some savings here right  so  that is\nwhat the entropy tells you  if you know what the probability of these events is  then you\nbetter use that to decide the number of bits that you are going to use to send each of\nthese \nso   now   let   us   complicate   this   a   bit   more    now   we   have   the   entropy    now   let   us\ncomplicate it a bit more  so  there is some true distribution which exists  there is some\ntrue distribution from which these messages are coming  right  but you do not know\nwhat that true distributions  we never know the true distribution that is the entire problem\nthat we have been dealing with in machine learning \nso  what you will do is we will somehow try to predict this distribution  and this then the\nand the recipe that you will use is the same as that i used for the example where i had an\nurn right  so  there are these thousands of ten zero\u2019s of messages which has going to keep\ncoming   on    you   do   not   have   access   to   the   entire   stream    but   you   have   seen   some\nthousand of those messages just  as i had peeped into the urn  and i had seen some balls\nand i had made an estimate  that i think based on these messages that i have seen so far i\nthink these are the actual probabilities \nso  the true probabilities are say p one  p two  p three and p four corresponding to a  b  c and d  i\ndo not know what this two properties are  but i can estimate them looking at some samples\nor basically using my domain knowledge  right  maybe i would know that if one of these\nmessages  is  stopped   and   i  am  actually   trying   to  talk   to  a  computer   or  a  computer\n\fprogram  that maybe stop is something which are used very rarely only at the end of the\nprogram or something  so  you have some either some domain knowledge or based on\nsome samples i can estimate the value of this probability \nand i just try to relate it to the exact example of urns  where you had these ten zero\u2019s of\nballs  but you could not see all of them  you sampled some and estimated a probability \nhere again there is a continuous flow of message you cannot have access to all of these \nbecause they are going to continue  but i have seen some of those and based on that you\nestimate these probabilities  now based on this estimation how many bits \nso now  this  is the estimation  that  we have  now based on this  you will decide the\nnumber of bits that you will use for each of these messages  right  because you have\nsome estimate so  you want to be smart you do not want to keep two bits for all of them \nso  you will just say that i will use log qi bits for the i\u2019th message \n\n\nthis  is fair thing because i know  that  the information  content  is  proportional  to the\nprobability  in fact  it is exactly given by this formula  minus log of qi  so  based on my\nestimated probability i am going to do this  ok  and this is the number of bits that i have\nreserved  now do you see a problem with this  this is my estimation  but the data is\nactually going to come from the true distribution  it is not going to follow the distribution\nq  it is going to follow the distribution p \n\fso now  what is actually happened is this  right  this is the situation that we are dealing\nwith  we have p which was a true distribution  that is the rate at which the data will\ncome  but with each of these events the value that we have associated is now related to q \nbecause q is what i have access to  i do not have access to p  i just have access to q so  i\nhave associated a value based on q  does this make sense  i should have actually used\nlog p one bit\u2019s log p two bits and log p three bits  but i do not know what p one  p two  p three are \ni just estimated them based on some samples so  that is q one  q two  q three  and these are the\nnumber of bits that i am using  now if i have to compute the expectation how will i do\nit  i have to use p because that is the true distribution from which the data is coming \nso    what   would   the   expectation   now   look   like    everyone   gets   this    the   actual\nprobabilities are this  but because i am poor at estimating them i ended up associating\nthese values  which could be wrong  right  because i would have overestimated  the\nprobability of one of these messages and hence i have reserved lesser bits for that or\nunderestimated the probability of one of these events  and hence reserved more bits for\nthat or vice versa \ni could have assigned a wrong number of bits to them  right  p no  so  do we have access\nto p in the sense  so  someone knows that  right  i mean there is a again in the same case\nas in the label case  right  we have access to the true p there  and we are estimating a q \nwhen we are given these images for the training data  we know that the distribution is zero one\nzero zero if the image is b for banana \nstudent  \n \nthen it is validated  right  so now  this is what is this quantity called  this is called the\ncross entropy  ok  you get why it is called the cross entropy because now you have two\ndifferent distributions involved here  ok  you have the q distribution based on which you\nmade your decisions  you assign values to these events based on the q distribution  but\nthe true distribution is the p distribution \nso  the actual number of bits that you use up on average is going to be based on the true\nprobability  they try to understand that  now what will happen is for event a you have\nassigned a certain number of bits  now how many bits will get used up it depends on \n\fthe actual probability of p if that  message is repeated many times then that is how this\nsummation would be computed \nso    this   is   called   the   cross   entropy   but   now   why   is   this   the   difference   between   two\ndistributions  that is what we wanted  given two distributions we wanted to be able to find\nthe difference between them  now am telling you that cross entropy is a way of finding\nthat difference  why is it so  so what would you want this difference to what is the\nproperty that you would want this difference to have  if p is equal to q  then if p is equal\nto q then \nstudent  \n \nnot zero maybe it should take the lowest possible value  right  so  this function  right  this\nis actually telling you loss of p comma q  right  this is what this is and we are calling it\nas the cross entropy  this function you take it is minimum value when p is equal to q \nright  because now at that point you are not really making any loss that is the best you\ncould have done  does this function take it is minimum value when p is equal to q  yes \nwhy  how is that obvious  but why there could be something else which is lower than\nthe actual entropy  right  why  how you have to we are trying to minimize something \nso  you have to give me answer \nso  yeah so  let us do that  ok  so  how many of you it is obvious that q is the answer  i\nmean the answer is p is equal to q it is not  ok  now this is the part which i am a bit\nworried about  but i will just do it anyways  so  let me see how do i put this ok  so \nremember that we had a p and we had a q  and we want to find a q such that this quantity\nis minimized  that is what our objective is \nso  we want to minimize this with respect to qi  ok  now how do you find the minimize\nsuppose i have this problem  how do i find the minimum value  how do i find the value\nof x which minimizes this  take the derivative  and set it to zero  ok  and then in this case i\nwill get x equal to zero is that value  can i do the same thing here  and suppose it was this\nso now  this is a function of two variables  again i could do the same thing  i could take the\npartial derivatives and set them to zero \nand i will get the minimum value  now here this is actually a function of how many\nvariables  k in general right  so  q one  q two  q three up to qk  ok  now can you try doing the\n\fsame thing can you can you take the derivative and set it to zero  this is again a sum  right \nit is very similar to this situation  it is actually let me just write it down  it is p one log q one\nplus p two log q two up to p k log qk \nnow i want to take the derivative with respect to one of these guys  say  q two  what would\nit be  p two by q two is equal to what will i do  that is the derivative p two by q two  i will set it\nto zero  do i get anything  what is it that i am doing wrong here  there is something that i\nam deliberately doing wrong  is this an unconstrained optimization problem  there is a\nconstraint on the variables  what is the constraint  so  why my true optimization problem\nis minimize with respect to q i\u2019s  such that summation q i\u2019s equal to one \ndo you know an easy way of dealing with these problems  how many of you know the\nlagrange multiplier  how will i use it here  what will my objective function become \nthen summation of qi minus one lambda then minus ok  how many of you understand the\nintuition behind this  that is a good answer  now let us let me try to explain why this\nmakes sense  right  this is the constraint that we have to operate within this constraint \nwhat i have done is i have taken the so now if the constraint is not satisfied what will\nhappen to this quantity if the constraint is not satisfied  that means  my summation is\nnot equal to one that is what means whether the constraint is not satisfied  what will happen\nto this quantity  it will be a non zero quantity  right  fine then what will happen to my\noverall objective  and i think we have made a mistake this should be plus  i should add\nit  right  should be plus no it does not oh the lambda can be  ok sorry  so  let me assume\nthis is plus \nso  what i am trying to do is that  this is my objective function which i am trying to\nminimize  i have added another quantity to it  if this quantity is not equal to zero  then i will\nnot be the absolute minimum  i will be at the minimum plus something right  but if this\nquantity if the constraint is satisfied  then this quantity will go to zero  then am actually at\nthe minimum of the function  do you get this  right  so  this is the function that i want to\nminimize  i have added some quantity to it  now  that quantity is actually related to the\nconstraint that i do not want to violate  if i violate the constraint  this is going to be non \nnegative  right \nso  whatever minimum value i achieved  i will be slightly higher than that  because some\nnon negative value has got added to it  ok  is that fine  but if the constraint is satisfied \n\fthen i can achieve the minimum value  so  that is roughly the intuition behind using this\nlagrangian multiplier  it is a very crude intuition  but there is of course  a lot of math\nbehind that  but i am just giving you the intuition behind this which one \nstudent  \n \nyeah that is what you could adjust the lambda  and ensure that it is not negatively  ok  so\nnow  now can you do the same thing can you equate this to zero  can you take the derivative\nand equate to zero  what will you get now  this term will give you p i by q two as before  oh\nsorry p two by q two as before plus lambda times yeah plus lambda times one  ok  fine so \nequal to zero  so  then what will you end up getting  p two is equal to i think it is something\nwrong here  now this should be minus p two by k \nso  p two is equal to lambda times q two  ok  and then further actually you can show that\nlambda is going to be equal to one  how can you show that your constant is fine  so  do you\nsee how we will get lambda equal to one  so  what does it actually tell you  then p two is\nequal to q two  that means  all  in fact  you can show that all p i\u2019s are equal to q i\u2019s  that\nmeans  the distribution p is equal to distribution q \nso  this cross entropy term will be minimized when your true distribution  or when your\nplated  distribution   is   the   same   as   the   true   distribution    and   hence   it   captures   the\ndifference between the two  ok  and that is exactly what we were interested in  we were\ninterested in a quantity which can allow us to capture the difference between the true\ndifference between a true distribution and the predicted distribution \nso  we have arrived with that quantity and that quantity is cross entropy  so  therefore \nfor all our classification problems where we have this scenario that we are given the true\ndistribution where all the masses focused on one of the labels  and you are estimating a\ndistribution where you could give non zero quantities to many of those  and you want to\nfind out how wrong your estimates were with respect to the true distribution  you can use\ncross entropy as a measure for that right  so now  your loss function which you wanted\nto depend on the difference between p and q  it can just be the cross entropy between p\nand q \n\f"}
{"audio_filepath": "lec004_001.wav", "duration": 540.857, "text": "\nstochastic gd  adagrad  rmsprop  adam \nwelcome  to  lecture  five  of  the  course  on  deep  learning   and  so   today  we  look  at  some \nvariants of gradient descent  so  we will just quickly do a recap of gradient descent and \nthen  look  at  some  variants  of  it   or  some  ways  of  improving  it   which  is  momentum \nbased  gradient  descent   nesterov  of  accelerated  gradient  descent   stochastic  gradient \ndescent  adagrad rmsprop and adam  \nso  just to set the context  so  we started with this gradient descent algorithm for a single \nsigmoid  neuron   and  then  we  saw  how  to  extend  to  network  of  neurons  with  back \npropagation  so  we realized that all we need is the gradients or the partial derivatives  \nwith  respect  to  all  the  weights  and  biases   once  we  compute  that  we  can  just  use  the \ngradient descent update rule  \nnow  today what we are going to see is  are there better update rules which lead to faster \nconversion or better performance in various ways  so  that is why we are going to look at \nall these different variants or methods of improving on gradient descent  so  that is the \ncontext  \n\f\n \ni will just quickly rush through  so  for most of the lecture  i have borrowed ideas from \nthe videos by ryan harris on visualize back propagation and some content is based on \nthis course by andrej karpathy and others  when i talk about some tips for learning rate \nand so on  so  you can just look at those also  so  we will just quickly rush through the \nfirst two modules which we have already done  \n\n \n \n \n\fwhich  was   we  were  interested  in  learning  the  weights  and  biases  for  this  very  toy \nnetwork   with  just  one  input  and  one  output   and  we  started  by  doing  something  known  as \nguesswork where we were just trying to adjust these weights and biases by hand  \n\n \n\n \n\n \n \n \n\fand we realized that its clearly not  good and  but we still try to do a very  smart  guess \nwork   where  we  were  driven  by  this  loss  function   which  was  telling  us  whether  this \nguess   the  current  guess  is  better  than  the  previous  guess  or  not   and  we  just  kept \nfollowing our guess work and try to reach to some solution  and for this toy network it \nwas very easy to do that  \n\n \n\n \n \n \n\fand what we were actually doing is  there is this error surface which exists  which can \nbe plotted for all possible values of w comma b  and what we were trying to do with this \nguesswork is  trying to find path over the error surface  so that we enter into the better \nregions  so  red is bad  blue is good  the darker the shade of blue the better  and this of \ncourse  becomes intractable when you have many parameters and so on  \n\n \nso   we  wanted  to  have  a  better  way  of  navigating  the  error  surface   so   this  is  exactly \nwhat we were doing with the guesswork algorithm  \n \n \n\f\n \nso   then  this  better  way  actually  we  realized  that  we  could  arrive  at  it  from  a  very \nprincipled solution from  starting from taylor series  \n\n \nand we went to this derivative  where we finally came up with this rule that move in the \ndirection opposite to the gradient  \n \n \n\f\n \n\n \nso  that is the rule that we have been sticking to since then  and we also along the way \nrealize some of these things which we defined carefully which was  what is  what exactly \nthis  quantity  means   which  is  the  partial  derivative  with  respect  to  w  evaluated  at  a \nparticular weight comma bias configuration  and because this is an iterative process  you \nare at a certain value of weight and bias and you need to change it from there  \n \n \n\f\n \nand we then created an algorithm out of this and when we ran this  we actually derived \nthe full derivative and so on  \n\n \nand then when we finally  ran this algorithm  so  this is where  now i will slow down  \nso  when we ran this algorithm  so  let us see what was happening here right  so  i will \njust start the algorithm from the beginning  \nso  we are now going to run this code and you tell me something that you observe ok  \nso   i  am  just  clicking   so   there  is  no  change  in  the  pace  at  which  i  am  clicking  this \n \n \n\fright  so  every click of this is one time step and i am just continuously clicking this i \nwill start now  do you observe something  fl  ok  do you observe something  \nit  was  initially  slow  then  suddenly  picked  up  and  then  it  again  became  slow   why  did \nthis happen  the slope is small why ok  how many of you completely understand why \nthis slow and fast moment was there  please raise your hands good  so  that is what we \nwill focus on now right  so  we will try to see this  \n\n \nso  we will  i hope this has been fixed ok  so  let us take a simple function which is f of \nx equal to x square plus one right  this is how it will look like  now in these portions of the \ncurve  the curve is actually very steep right and in these portions the curve is a bit gentle \nand of course  it becomes very gentle over here right  all of you can see the pen marks \nproperly  \nso   now  let  us  see  what  this  means   this  steep  and  fast  and  small   so   let  us  look  at  a \nregion which is steep ok  now what i am going to do is  i am going to change my x by one  \ni  move  my  x  from  one  to  two   how  much  did  my  y  change   all  you  need  to  do  is  just \nsubstitute in this formula right for two it evaluates to five  for one it evaluates to two  so  when you \nmove from one to two  your function changed from two to five  ok so  there is a large change in \nthe function for one unit change in your value of x  everyone sees that  \n \n\fnow  let me do the same at a gentle portion of the curve  i will do it here  now when i \nchanged the x by one unit   again  one unit right  it  is the same change which  i  did  earlier   i \nchanged from zero to one  how much did my y change  \nstudent  one  \none ok now actually what is this quantity  delta y one by delta x one  \nstudent  slope  \nit is the slope  it is the derivative at that point  so  what are you inferring from this  what \nhappens to the derivative when you are at steep slopes  \nstudent  it is high  \nderivative  is  high   because  the  change  in  y  is  much  faster  than  the  change  in  x   what \nhappens to the derivative when you are at the gentle slopes  \nstudent  smaller  \nsmaller  because the change in y is small or relatively smaller as compared to the change \nin x or it could also be missing  but just these two are relatively different  is what i am \ntrying to impress upon right  and so  that means  the derivatives at the steep slopes are \nlarger in magnitude  whereas  for the gentle slopes they are smaller in magnitude  \nnow  can  you relate it to the observation that  you had on the previous slide  when we \nwere at the plateau it was a very dash slope  gentle slope what would the derivatives be \nstudent  small  \nsmall now what  are our updates   you have w is  equal to  w minus the derivative right  \nnow the derivative is small what will happen to the updates  \nstudent  small  \nthey will be small  what would happen if the derivative is large  \nstudent  the updates would be large  \n\fthe updates would be large  therefore  in the gentle areas you are moving slowly and in \nthe steep areas you are moving fast  you get this picture very clearly  now this is going \nto  be  the  basis  of  a  lot  of  things  that  we  do  today   so   it  is  very  essential  to  that  you \nunderstand this perfectly  all of you get this properly good  \n\n \nnow   now  you  might  say  that  this  was  only  that  special  point  again  and  i  always  get \nthose questions  so  let us see what happens  if you start from a different point  \n\n \n \n \n\fso   now  again  the  same  gradient  descent  algorithm  i  am  going  to  run   but  instead  of \nstarting  at  this  point  which  was  my  random  initialization   i  just  happened  to  choose  a \nvery different random initialization which is here  everyone sees that  \nnow  let us see what happens  what do you expect initially fast movement  because the \nsteep  the slope is a bit steep  now what would happen  it will become slow because you \nhave  entered  a  gentle  slope  region  and  then  again  fast  right   so   and  then  again  it  will \nbecome slow  \nso  see in this gentle region right  the changes in w are so small that all your black points \nare actually indistinguishable from each other  it is almost like a snakes body whereas  in \nthese steep slopes   you can see a large change in the w  you can see gaps between the \nvalues  of  w  right   so   this  is  irrespective  of  where  you  start  from   gentle  means  slow \nmovement  steep means fast movement that is the basis  \n\f"}
{"audio_filepath": "lec004_002.wav", "duration": 659.372, "text": "\ncontours maps \nso we look at something known as contours  \n\n \nso  now visualizing things in threed can sometimes become a bit difficult  especially for the \nperson  who  is  making  the  slides   so   we  can   can  we  do  a  twod  visualization  of  this \ntraversal  have i done this in the ml course  no good  can we do a twod visualization of \nthis traversal along the error surface  \nso  for that we need to understand something known as contours  how many of you have \nlooked at contour diagrams before  how many of you know how to read them  all of you \nknow how to read them  \n\f\n \nso  let us see  now suppose this is what my error surface looks like and i have a single \nscalar  variable   so   this  is  just  a  function  of  w  for  example   and  this  is  what  my  error \nsurface looks like  \nnow  what i am going to do is  i am going to take horizontal slices on this error surface \nfine   now  can  you  tell  me  how  this  is  going  to  look  from  the  top   sorry  let  me   you \nshould  start  answering  before  understand  the  question   this  is  this  error  surface  is \nactually  so  i was wrong in saying this is theta  assume this is w comma b and you are \njust seeing the front view of the error surface  what you are seeing here is just the front \nview  \nthis error surface is  actually like a dementors hat  so right  so  imagine that it is a hat \nplace like this and you are just seeing the front view of this  otherwise a top view does \nnot make sense right  so  now  i am going to slice this hat at two vertical positions  and \nnow you are looking at it from the top  what are you going to see  \nstudent  ellipsis  \nellipsis  everyone agrees with that  \n \n\f\n \nso  we will see something like this  do  you see something peculiar about this  is this a \ncontour map  is this  no ok  and all of you raise your hands when i asked do you know \ncontour  so  do you see something peculiar about this  what is it  how many if you get \nthat  so  what you are seeing here is  this portion right where the slope was very steep  \nthe difference between the two circles or the two ellipses is small and you can visualize it \nif you try to look at it from the top  this distance is actually going to be small right  and \nin  the  areas  where  the  slope  was  gentle   relatively  gentle  the  distance  is  more  and  you \ncan again visualize it right  from if you look at from the top this is the distance that you \nare going to see  and what do you say about these guys  what does that indicate  they \nare the same  \nstudent \n  \nvalue   across  that  entire  region  the  value  is  same   because  you  have  taken  a  verticals  \nyou  have  taken  a  horizontal  slice  at  a  particular  vertical  position  right   so   you  have \ntaken a horizontal slice at this position  that means  the error is going to remain the same \nthroughout that rim  is this clear to everyone ok  it is very important that you understand \nthis  \nso   there  are only  two things  that  you need  to  understand if  you want  to  read contour \nmaps  one is a small distance between the contours indicates that the steep slope exists \nalong  that  direction   and  a  large  distance  between  the  contours  indicates  that  a  gentle \n \n\fslope  exists  along  the  direction   so   everything  today  is  going  to  be  about  steep  and \ngentle slopes  and the other thing that you know need to know is that whenever you see \none circle  the error is the same along that circle or ellipse  whatever you boundary that \nyou  see   the  error  is  the  same   because  you  are  taking  these  vertical  slices   so   we  are \nready with this rule  everyone understands this perfectly  \n\n \nso   i  will  just  give  you  a  couple  of  exercises  and  you  have  to  tell  me  whether  you \nunderstand this or not  \n\n \n \n \n\fso  i have plotted a three d surface or two d  i have  what is this  \nstudent  \n  \nno  \nstudent  there is a contour  \nthere is a contour  everything is not going to look like clean circles always right ok  so  \nthis is a contour  every line that you see here represents one cut along the vertical axis \nright   that  means   the  error  is  the  same  there   now  what  you  are  seeing  is  a  contour   i \nwant  you  to  guess  the  three  d  surface  from  this   you  just  guess  it   i  mean  just  keep  it  to \nyourself fine  the color is the same right blue is good  red is bad  \nso  blue means the darker  the shade of blue the lesser the value of the error  the darker \nthe  shade  of  red  the  higher  the  value  of  the  error  ok   i  want  you  to  imagine  the  three  d \nsurface   if  you  can  do  that  then  i  will  be  sure  that  you  understand  what  how  to  read  a \ncontour  how many if we can imagine this  you can just say yes right i can never figure \nout whether you actually speak it  \nso   let me help  you with the first  one and then we will do a few more   so   let us start \nwith the extremes right  so  let me see how to do this  so this portion  i also need to do it \nfor the video ok  so  let me just do it here  so  this portion  what do you think about the \nslope there  very flattish why  because this is the line that you see and the other line is \nnot even in the figure right  so  it is basically very flat  the slope is very gentle  is it a \nlow region or a high region  high region fine ok  now what is actually happening here  \nwhat is the slope here \nstudent  high  \nvery high  that is why these two regions are very close to each other  so  from this high \nregion  what  is  happening   suddenly  there  is  a  slope  and  you  are  going  down  and  you \nknow you are going down  because you are reaching a blue region right ok  now what is  \nhappening here  \nstudent  very flat  \n\fvery flat  and this also flat  but slightly upper than the lower guy  is that fine  now can \nyou all imagine this ok  and is this what you thought it is  perfectly yes right is exactly \nwhat you thought ok  just a minute  so  the orientation here has been changed a bit right  \nso  this portion actually corresponds to this portion  are the two this is clear  this portion \ncorresponds to this portion right the just orientated fine  \nso   you  start  off  this  high  plateau  region  which  is  here   then  you  start  going  down  go \ndown and then you see a fold here right  that is this fold  so  you went to a darker shade \nand then you came up to a slightly lighter shade  the shades are ok  \nguess the threed surfaces  how many if  you want to play this forever now  start with the \nextremes the bad guys the good guys the plateaus and the valleys and then see how do \nyou go from the plateau to the valley ok  tell me the corners first  this plateau or valley  \nstudent  plateau  \n plateau  this plateau  higher than this or lower than this  \nstudent  lower than this  \nlower than this  this  \nstudent  valley  \nthis  towards the valley   it is still  between red  and blue right  it is not  like right  down \nthere and what happens to all these guys  all are very steep slows  all converging down \ninto the valley  so  can you perfectly imagine this  \nand you will tell  yes when i say when i show you the three d surface right  again you need \nto reorient yourself  so  this corner here is this corner  this corner here is this corner  so  \nwe  had  these  two  plateaus  at  the  top   we  had  this  slightly  higher  valley  slightly  lower \nvalley and then all of them going into a very deep valley  you see that   everyone  gets \nthis  how many   if you have a problem with this  if you have a problem with  this  you \nwill  just  sleep  off  in  the  rest  of  the  lecture   so   i  want  you  all  to  understand  this  very \ncarefully i do not mind repeating it  how many of you understand this  you understand \nthe regions with gentle slope \nstudent  yes  \n\fthe regions where you have a steep slope and you end up into that valley  which is the \nvalley here can you point it out  fine ok  so  we will move ahead  \n\n \nso  now  we know what contour maps are and how to visualize them and so on right  so  \nnow  we will try to see the gradient a descent algorithm  instead of running it on the threed \nerror surface  we will try to run on this twod contour map  \n\n \n \n \n\fso  this is what i already showed you right  i started from here and i showed you how it \ncomes here or something like this right  that was the gradient descent  let me just erase \nthis ok  that is something like what the gradient descent algorithm  \nnow  again  you just need to  reorient  yourself  so   let us  see   this  corner is  this  corner  \nthis  corner  is  this  corner  and  so  on  right   so   you  get  the  reorientation  right   it  just \nshifted  now i am going to start my gradient descent algorithm from here  from this point \nok  everyone see is that ok  i am going to start from there and you have to help me and i \nam not going to just keep clicking  you have to tell me what is going to happen  so  what \nwill happen initially  fast movement slow movement  \nstudent  slow movement  \nslow  movement  right   so   i  am  running  it  one  two  three  four  five  six  seven  eight   it  just  keeps  running  very \nslowly  now what will happen  \nstudent  fast  \nfast ok  now you see actually you can see the arrows these arrows are the quantity  the \nmagnitude of the movement right  so  earlier this movement was so small that you could \nnot even see the arrows  i have been drawing arrows right from the beginning  but  you \ncould  not  see  them  at  the  beginning   now  you  can  see  them  right   now  what  will \nhappen  \nstudent  slow  \nslow right  so  you see the exact same movement that i did on the three d surface  now you \ncan visualize it on the two d surface right and you can easily tell me where it will go fast  \nwhere it will go slow right and where it will just keep moving very drag its feet and so on \nok   so   this  is  where  it  starts  dragging  its  feet   and  the  same  thing  happened  when  we \nwere  in  this  region  right   so   just   you  just  make  the  connection  that  we  are  in  the \ncorresponding three d region there  ok fine  so  we are moving very slow and it just keeps \nrunning  \nso  that is where we lend this module  so  we just revised gradient descent  we saw that \nthings are proportional to the gradient that is why gradient descent  and the smaller the \n\fgradient the slower the  movement   the larger the  gradient higher the movement  gentle \nthe slope  \nstudent  smaller  \nsmaller the gradient  steeper the slope  larger the gradient  \n\f"}
{"audio_filepath": "lec004_003.wav", "duration": 1114.763, "text": "\nmomentum based gradient descent \nin this module we will look at momentum based gradient descent  \n\n \nso   what  were  the  observations  about  gradient  descent  that   it  takes  a  lot  of  time  to \nnavigate regions  having  a gentle  slope  so   what  is the practical implication  of this  in \npractice why it what does this need to  what does this mean right  it takes more time  so  \nremember we had said this max iteration equal to one thousand  \nnow   if  you  are  initialization  happens  to  be  such  that  you  are  stuck  in  this  large  flat \nregion   then  those  one thousand  iterations  just  keep  moving  around  that  flat  region  right   you \nwill  not  enter  into  one  of  the  valleys  and  valleys  is  what  you  are  interested  in  right  \nbecause values is where you will have some minima for your function right  \nso  if you have a very gentle slope  then for one thousand iterations you will keep moving around \nthat gentle slope right  that is why this has a practical implication  now this was because \n\fthe gradient in these regions were small  can we do something better that is the question \nright  so  yes we can and we will take a look at momentum based gradient descent  \n\n \nso  here is the analogy which i give  my ta\u2019s have heard this at least ten times  so  i will \njust repeat  it the  eleven time for them  so   i hope  that is  the one which  i  want  to  use here \nyeah ok  so  now  suppose you are standing at the velachery gate and you want to go to \nphoenix market city  something that all if you can relate to today  so  you want to go to \nphoenix market city and you ask the security guy at the gate that where do i go right  \nso  he will say take a left  no take a right  so  i am slightly dyslexic  actually i have a \nleft  right  dyslexia   so   take   take  a  right  ok   so   you  will  say  he  has  told  me  to  move \nright  but you would still be a bit  cautious right  we will just keep moving slowly in that \ndirection   that  is  how  we  find  ask  for  directions   you  keep  moving  slowly  in  that \ndirection  \nnow  one hundred  steps later or  one hundred meters later  you find another  guy and  you  ask him or her \nwhere is phoenix market city  he again points to in the same direction  keep moving left \nright  so  now   you will  what  will happen   you will increase  your  space and then  you \nask  again  someone   when  you  read  the  signal  where  it  is  and  he  again  points  in  that \ndirection what will happen  move even fast  \n \n\fso  what is happening here  if a lot of people are pointing you in the same direction  you \nbetter start taking larger and larger steps in that direction  does that make sense  that is \nhow we find directions and move around  so  just like a ball gains momentum as it goes \ndown a slope right  it is constantly moving in that direction  so it starts moving faster  so  \nnow   can  you  tell  me  a  way  of  incorporating  this   i  have  been  moving  in  a  certain \ndirection these directions are nothing  but the gradients  and now at this point someone \nasked me again to move in the same direction  what should i do  \nstudent  take a bigger step  \ntake  a  bigger  step   so   can  you  think  or  try  to  imagine   how  would  you  do  this \nmathematically  \nstudent  \n \nso  it is probably there are a few ways to do it  so  let us see  so  what i am doing here \nis  this  is  my current  gradient  right   so   i  asked  that guy  at  the signal  he asked me to \nmove in that direction  so  that is this direction and this is all my history  whatever i did \ntill step t minus one ok  so  now  what i will do is  i will  so  earlier i was moving like this  \nthis is what my update rule was wt plus one is equal to wt minus in the direction of the \ngradient right  i will moving in the direction opposite to the gradient  \nnow   what  i  have  is  in  addition  to  that  i  have  this  gamma  update  t  minus  one   so   that \nmeans  whatever i had done up till step t minus one i will also take that into account  so  i \nwill end up taking a larger step  is that clear  if it is not clear it will become clear on the \nnext slide  \n\f\n \nso  let us see what this means right  so  it basically means that in addition to the current \nstep also  look  at  the history  there  are three  guys  who earlier pointed  you in  the same \ndirection   so   maybe  this  direction  makes  sense  right   so   start  accumulating  that  and \nmove faster  \n\n \nso  let us just break this down and see right  so  this is what the update rule is  sorry this \nis  all  my  updates  and  this  is  the  update  rule   so   at  time  step  zero  my  update  is  zero  \nbecause  not  started  yet   at  time  step  one  this  is  what  it  will  look  like  right   and  this  is \n \n \n\fnothing  but just move in the direction of the opposite to the gradient  because this minus \nsign will come later on right in the next equation   \nnow  what  will happen update two  so   its  gamma times update one  plus the gradient at \nthe  current  step   so   remember  here  everything  is  positive   i  am  adding  the  gradients  \nbecause my final negative sign is going to come in the next equation ok  so  do not get \nconfused with that  eventually i am going to move in the direction opposite that opposite \nwill come from this negative sign  \nso   what  is  happening  i  am  moving  in  the  current  direction  plus  a  fraction  of  the \ndirection which was pointed earlier right ok  then does this make sense  so  can you tell \nme in general what is happening here at the t\u2019th time step what is happening  what kind \nof average am i taking  weighted average  but it is a dash weighted average  this is an \nexponentially weighted average ok  so  let us look at this right  \nso  when i am at step four  i have most faith in the current gradient right and this gamma is \nalways i will just set it to less something less than one right  so  i have a fractional trust in \nthe previous gradient  even smaller trust  in  the previous guy  and even smaller trusts in \nthe  previous  guys   so   i  am  taking  an  average  of  all  my  gradients   but  it  is  an \nexponentially  weighted  average   does  that  make  sense   my  maximum  faith  lies  in  the \ncurrent guy and then decaying faith in the previous guys  \nand as i move further and further away from the last guy that i checked right i will give \nlesser  and  lesser  weightage  to  that   so   everyone  understands  what  is  happening  here  \nanyone who has a problem is  just raise your hands if you understand this good  \nso  in general this is going to be the formula and you see that as  as i form problem here \nno  as t is larger this fraction is going to become smaller and smaller right  so  you are \nfirst  the  first  step  that  you  take   will  have  lesser  and  lesser  weightage  as  t  increases  \neveryone gets this fine  \n\f\n \nso   now  this  is  the  code  for  momentum  based  gradient  descent   i  will  just  give  you  a \nminute to stare at the code and see if it makes sense  so  this much part is ok  you are just \ncomputing the gradients with respect to all the points right  and now we are keeping this \nrunning sum ok  which is the previous gradients and the current gradient right and then \nyou are just subtracting that running sum   \nnow  this looking black curve that  you see here  that  is  gradient this  this  guy  ok  this \nblack curve that you see here  that is gradient descent when i have run it for around one hundred \niterations   now  i  am  going  to  run  momentum  base  gradient  descent  and  each  click  is \ngoing  to  be  one  step  ok  and  i  want  you  to  observe  what  happens  ok   so   slowly  a  red \ncurve will start appearing on the figure  \ninitially it will not be visible  so do not worry there is nothing wrong with your eyesight  \none  how many if you already see the red part  i see it  two  three  four  five  six  no now you can see it \nas is nothing great aboutseven eight nine  i want you to observe something here eleven  twelve  thirteen  fourteen came \nback right   so   gradient  descent  i ran it  four hundred iterations  it was  just stuck here  right  this \nwas a point and i ran this for less than like around fifteen or twenty is what we counted right and \nso  already entered into the valley  \nso  momentum base gradient descent is good  you see that wicked smile on my face and \nyou know it is a trick question  so  we are moving fast right  \n \n\f\n \neven  in  the  regions  where  the  slope  was  gentle  right  that  is  the  beginning  of  the  \nbeginning of our trajectory right  this was the gentle region  even that i was very quickly \nable to navigate right  within five to six steps i was away from that part right  so  even in the \nregions  where the slope  was  gentle  i was  able to move fast  but  is  moving fast  always \ngood  \nso   would  there  will  be  a  situation  where  momentum  would  cause  us  to  run  fast  ago  \nsame thing now instead of walking you are in a car  you ask the person at the security \nwhether i should go there  he says yes go in the right direction you keep moving there  \nsomeone  else  you  keep  accelerating   what  will  happen  eventually   you  will  go  fast \nphoenix market city then what will you do  \nstudent  take a   \n take a u turn come back  again while taking a u turn what will you do  \nstudent  \n  \novershoot and come to the signal and then go back again right  so  you see this you will \nend up taking a lot of u turns  so  let us change the input data a bit and see what happens \nto momentum based gradient descent  \n \n\f\n \nso  this is what my data looks like now  so  this is not what my data looks like  this is \nwhat my error surface looks like  so  earlier we had this error surface something like a \nflying  carpet   now  i  have  a  very  peculiar  error  surface   this  is  again  for  the  two \nparameter  problem  right  w  comma  b   that  means   i  want  to  learn  a  sigmoid  function  \nwhere i have these two plateaus at the top  the dark red regions that you see and then a \nvery sharp valley  can you tell me how i would have come up with this kind of an error \nsurface  what are the points that i would have chosen  just hold on to that part  \nso  i have this kind of an error surface fine  the error is high on either side of the valley  \nnow could momentum be detrimental in this case  yes  no maybe i do not care  i do not \ncare fine  \n \n\f\n \nso  let us see this is the  is this the two d equivalent of that three d surface  everyone gets it  i \ncan perfectly verify that you get it  everyone gets it i will assume right  so  these are the \nvery high plateaus where the error is very high  very sharp and narrow valley where the \nerror is low  \nso  now again this sorry looking black curve is what i have done with gradient descent \nafter  some  one hundred  iterations  or  something   now  i  am  going  to  run  momentum  based \ngradient descent and you have to help me understanding what is going to happen  again \nyou will soon start seeing that red curve appear one two three four five six  what will happen now  it is \nalready fast that is known  it was that black curve was after one hundred iterations or  so  it is fast \nnow tell me what will happen  \nstudent  \n  \nhe will go out  is actually almost come out of the valley right  it is almost at the top of \nthe  valley   now  what  will  do   take  a  u  turn   now  what  will  i  do   again  take  a  u  turn  \nnow  i  will  keep  doing  this   i  will  take  now  smaller  and  smaller  u  turns  and  it  will \nconverge right  so  what happens here is  because of this speedy movement and which is \nvery analogous to that car movement which i described  \nthis  overshoot  your  goal  you  will  have  to  take  the  u  turn  come  back  if  you  are  again \ncareless  you  will  have  to  keep  taking  these  u  turns   but  you  will  finally   end  up  at  the \n \n\flocation that you want right  it takes a lot of u turns before converging  despite these u \nturns  it  still  converges  faster  than  gradient  descent  right   because  gradient  descent  can \njust  not  move  at  those  gentle  slopes  right  it  just  cannot  move  from  there   because  the \ngradient is almost zero  because the slope is flat right and it just cannot move  but even with \nthis lot of u turn and lot of rework  after one hundred iterations momentum base gradient descent \nhas reached an error of almost zero whereas  gradient descent is still stuck at the plateau at \nan error of zero thirty six ye  so  see you have reached the minima now  \nstudent  ye \nnow  you will be navigating there right  but  you  know that now  your loss is  very slow \nlow   so   you  could  end  that  right   you  know  that  your  loss  is  very  close  to  zero   so   you \ncould have a condition that once you have reached something very close to zero you could \nend that   even if  you are making these very small  movements now  you could  just stop \nthere  \nstudent  but in the plateau regions is also zero  \nbut the loss is high right  so  if the loss is high and you are not moving you cannot stop  \nbut if the loss is low and you are not making movements you can just stop there right  \nso  you can just end  you can define that as your convergence condition  \n\n \n \n\fso  let us look at  we will come back to three d now  we look at a three d visualization and a \nvery different interpretation of what is happening  i really want you to understand what \nexactly is happening in this example which i had picked up right  \n\n \nso  this is what the three d surface looks like view from a different angle  you have these two \nplateaus  and  the  very  sharp  valley   now   this  is  the  corresponding  sigmoid  function \nwhere i started with  so  what  i am trying to tell  you is that  this is a sigmoid function \ncorresponding to w equal to six  oh no sorry w equal to two and b equal to six  \nthis is the sigmoid function that i got once i plug that value  so  sigmoid is one over one plus \ne raised to minus w x plus b and i have plugged in the values of w and b and plotted it \nfor all the values of x and this is the sigmoid that i got ok  so  that is my starting point  is \nthis good  how do you define good or bad \nstudent  \n  \nwhat do you expect at the end of training  it should pass through all your training points \nand these are my training points ok  is it passing through them  no its way off right  ok  \nso  now  let us  start  this  momentum based  gradient  descent  and what  just see how my \nsigmoid function changes  so  right now i am on the gentle slope  even that momentum \nbase  gradient  descent  it  is  going  to  be  fast   but  not  dramatically  fast   because  still \nbuilding up the momentum  \n \n\fso   it  is  you  see  that  these  sigmoid  that  i  am  drawing  here   they  are  almost \nindistinguishable from  each other  i  have already drawn three sigmoids here  so   i will \njust go back  so  there was this initial guy then i draw drew a red one then one more and \nthen one more  but they are all very close to each other \nnow   keep  viewing  both  these  sides  in  parallel   what  happens  here  on  this  figure  and \nwhat happens to this sigmoid ok and i will ask you questions  so  still i am moving a bit \nslowly   because  i  am  still  building  the  momentum  right   it  takes  time  to  build  that \nmoment   now  i have slowly started building the momentum my  sigmoids  have started \nmoving towards where they should be  everyone gets this what is happening here ok  \nnow   tell  me  what  will  happen   as  i  enter  the  valley   i  am  almost  entering  the  valley \nwhat  will  happen   i  have  gained  this  momentum  now   so   my  w  comma  b  values  are \ngoing to change much faster now  so  what will happen to these sigmoids  they no longer \nstick to each other  we will start seeing a difference they are already moving away from \neach other ok  so  that is what is happening to the function ok  now you see even faster \nchanges  ok   now  what  will  happen   i  have  entered  the  valley   this  is  how  my  sigmoid \nlooks at this point  now tell me what will happen  \nstudent  \n  \nit  will  go  fast   what  will  happen  to  your  sigmoid   how  many  of  you  know  what  will \nhappen to the sigmoid ok  i will tell you what happens and then it will be obvious right  \nso  now  i am entering the valley  all of us know that i am going to come out of the value \nof the other side right  so  let us see what happens when i come out of the valley from \nthe other side  the sigmoid changes that is why you have this situation that your error is \nhigh  on  both  sides  right   because  on  this  side  you  have  these  kind  of  sigmoids   on  the \nother  side  you  have  the  other  sigmoids  and  somewhere  in  between  lies  the  solution  \nwhere does the solution lie  at a very flat sigmoid right  \nso  now i start  this is where the oscillations will happen  so  notice what will happen to \nthe sigmoids  they will toggle between these two orientations ok  just see what happens \nto the sigmoids  you see it again moves keeps moving  keeps moving  it keeps oscillating \naround  the  solution  and  then  finally   you  reach  the  solution   so   you  see  that   should  i \nrepeat this  \n\fso  when i am on one side of this valley  i have one kind of sigmoids right  now when i \nmove to the other side of the valley i have this others kind of sigma and take a u turn  so  \nwhen  i  u  turn  take  a  u  turn  i  again  overshoot  and  go  to  the  other  side   and  this  keeps \nhappening and i keep toggling till i reach my final solution  \nso  these are all the oscillations that you are seeing  so  can you visualize this  what is \nhappening  do you understand all these  relates to the actual function that you are trying \nto learn  so  that is why we will end this module  this was on momentum base gradient \ndescent  now we will see a nesterov accelerated gradient descent   \n\f"}
{"audio_filepath": "lec004_004.wav", "duration": 708.89, "text": "\nnesterov accelerated gradient descent \nlet us look at nesterov accelerated gradient descent  \n\n \nso  now we know that momentum based gradient descent is good at these gentle regions  \nit  moves  really  fast   but  we  do  not   still  do  not  like  it   because  it  has  this  problem  of \noscillations  it has this problem that it overshoots its objective  its goal and then it has to \ntake  a  lot  of  u  turns   so   can  we  do  something  about  reducing  this  oscillation   so   the \nanswer is always yes  so  let us look at nesterov accelerated gradient descent  \n\f\n \nso  the idea here is very simple  look before you leap ok  now remember that this was \nthe update rule for momentum based gradient descent ok and i will write it down again \nwt plus one is equal to wt minus gamma into update t minus one minus eta into the gradient at \nthe current point  \nso  you see that actually i am taking two steps  one is this step and then one more step \nand i could just this is one way of visualizing right  that i move according to the history \nand then i move a bit more according to the current gradient  so  everyone sees that there \nis a two step movement happening here  \nnow  can  you think what could have been done  look before  you leap  so  we will see \nwhat we can do  \n \n\f\n \nso  we know that we are going to move at least by this one  and that is fixed we know \nthat  our  history  is  telling  us  to  move  at  least  by  this  one  and  then  we  will  move  a  bit \nmore by the gradient  \nso   now  can  you  think  about  it   i  am  at  least  going  to  move  this  much   what  if  i  had \nsome  way  of  looking  ahead  and  then  do  something  at  that  point   this  is  what  you  are \nsaying   of  course   i  can  verify  it   but  i  am  sure  it  will  become  clear  once  i  show  the \nequations  but i just want you to think about it a bit wait  it is very simple it will become \nabsolutely clear once i show you the answer  but just think about it a bit  \nso  here is the answer it  why not compute the gradients add this look ahead point right  \nso   you are  again  adding  it in  two steps  minus  the history  and then minus the current \ngradient  so  take this value  call it the look ahead point  i know that i am going to move \nby this much  so  let me not compute the gradients at the current point  let me move by \nthis much  then compute the gradients and see what happens at that point  \n \n\f\n \nso  this is the equation right that first i move by that one step  i had to make a two step \nmovement  so  i will move by that one step right  then i will compute the gradient at that \nposition  not at my current position right  this was earlier gradient at point t  now i have \nalready moved a bit  so  i can compute the gradient there and then move in the direction \nof that gradient  \n so  you understood this that there is a two step movement right wt minus history minus \nthe  current  gradient   gradient  computed  at  time  step  t  ok   now  you  know  that  you  are \nalready  going  to  move  by  the  history  right   so   why  not  just  move  there  and  then \ncompute  a  gradient  at  that  point  you  are  anyways  made  some  movement  you  compute \nthe gradient at that point and then decide which is the direction to move in right  \nso  that is what this look ahead value is  i know it is still not clear to many of you and i \nam very confident it will become clear in the next five minutes  we will show you one more \nvisualization for this  but this stay with  stay with me for a while  as long as you get the \nintuition i am fine i will move ahead and then i will explain it again in a different way  \nthis is fine ah  that should become clear good that you asked that question ok  so  ask me \nagain on the blank slide that i have and then i  it should be complete  \nso  for right now let me just show you what will happen with the code  and then i will \nagain explain it with a different way  \n \n\f\n \nso  this is what momentum based gradient descent it ok  now let us see what nested or \naccelerated gradient descent will do  again the code is simple you can just read it up and \ni have started executing   you see this blue curve coming over there fine  ok and now  i \nkeep running this  now what will happen  you see that all the u turns of the blue curve \nare inside the u turns of the red curve  \nso  the objective is being achieved at least empirically i have showed you that right  its \ntaking shorter u turns  what is probably not clear to all of you is  why is this happening  \nis it clear to everyone  why is this happening  can everyone visualize that ok  so  let us \nsee why this is happening  i will give you an alternate explanation for this  \n \n\f\n \nso  suppose this is my error surface right on a two by and i have a single variable with \nrespect to which i am trying to optimize  so  this is my w  i started off with some initial \nvalue w naught  \nnow  what is the gradient at this point  positive negative  negative right  because when i \nam going to increase w the function is actually going to decrease right  so  right  so  the \nslope  is  negative   so   where  will  i  move   this  is  the  number  line  right   so   this  line  is \nactually the number line  because it is a single variable  so  where will i move positive \nside of the number line or the negative side of the number  positive side  the derivative \nis negative  i am going to move in the direction opposite to the derivative  so  i am going \nto move in the positive direction right  so  i will end up somewhere here  is that clear  \nfine with everyone  \nso   now  i  am  somewhere  here   what  is  the  derivative  at  this  point   now  what  is  the \nderivative  here   positive  negative   negative  right  when  i  am  increasing  w  my  loss \nfunction is  decreasing  so   my  dl  by dw is  going  to  be negative  this  is  positive this is \nnegative  so  again i will move in this direction  \nso   what  is  happening  a  lot  of  negative  updates  are  getting   sorry  a  lot  of  positive \nupdates are getting accumulated  and now because of my momentum i am not going to \nmove only by this derivative i am also going to move by the history right  so  i will end \nup somewhere further  \n \n\fso  now at this point what is the derivative  again negative  when i am increasing w the \nfunction  is  decreasing   so   what  is  my  update  positive  or  negative   positive   so   now  \nyou  see  that  a  lot  of  positive  updates  are  getting  accumulated  right   my  momentum  is \nbuilding up  so  now  what will happen  now if i just move further  then again i will get \na  let  me  just  put  it  here  right   so   i  am  again  moving  largely  in  the  positive  direction  \nbecause this guy is also positive  all my history was also positive  so  i have moved in \nthe positive direction \nnow  what will happen at this point  what is the derivative here  no it is still its negative \nsorry  so  again i am going to move in the positive side of the number line ok  now at \nthis point i want you break down the movement into two points  one is what my history \nwas telling me  which was all these positive updates  but of course  i will not make such \na  large  update   because  i  am  waiting  them  exponentially  right  so   but  its  telling  me  to \nmove in the positive direction ok  and i know that the gradient at this point is negative  \nbut  i want  you to  ignore that for now   i just want  you to  focus  on the history   if  i just \nmove according to the history where will i end up  i will end up somewhere here right  \nbecause the history is very positive  so  i will keep moving in the positive direction and \nthis is my w look ahead  \nnow  what will happen if i compute the gradient here  \nstudent  positive  \nthe gradient is  \nstudent  positive  \npositive  so  where will i move  \nstudent  negative  \nnegative  so  you see now why momentum works  because you are able to look ahead to \nthis  point   instead  of  what  should  i  have  actually  done  is  i  should  have  looked  at  the \ngradient  at  this  point   the  history  is  positive   the  gradient  is  also  telling  me  to  move \npositive  so   i  would have moved a large positive and  i would have ended somewhere \nhere  instead i just moved by the history  i checked where i end up i end up here  \n\fnow  let me see whether what is the gradient at this point  have i already overshoot my \novershot  my  objective   when  would  i  overshot  my  objective   it  has  the  sign  of  the \ngradient  changes  right   it  became  from  negative  to  positive  and  now  since  its  positive \nbecause as  i am increasing w  the loss is  also increasing  so  now  where will i move  \nnegative   \nso  now  what is the second step actually its again bringing me close to here  so  instead \nof  taking  this  large  u  turn   i  end  up  taking  this  small  u  turn   is  this  clear  to  everyone \nnow  how many if you still do not get it  how many if you get it now  good sure ok  so  \nthis is what and now we can relate it to what was happening on the figure \nso   let us  go back right  so   you saw that  i was  making these  smaller u turns  because \nwhen  i  was  at  this  point right   i already moved  by the history  i knew  i  would land up \nsomewhere here  where i would need to go back right  so  i already accounted for that \nand made a very small movement  is this clear  everyone gets this how the nesterov of \naccelerated gradient descent works  sure raise your hands  \n\n  \nso   looking  ahead  helps  nag  in  correcting  its  course  quicker  than  momentum  based \ngradient  descent  right   so   it  is  already  looking  ahead  where  do  i  land  up  and  already \nmaking a correction if required  if not required it will again move in the right direction \nright  so  the update is this guy plus the gradient and my update happens on the original \nvalue not on the look ahead value  \n \n\fso   her confusion was perhaps  that  i  am  doing  w look  ahead minus update   where this \nupdate again has this quantity  you know that is what  your confusion was  but  i am not \ndoing w look ahead i am using wt there  everyone gets this  so  that is where ah  now it \nis clear that why the oscillations are smaller in the case of nag and it is able to correcting \nits course quicker   \n\f"}
{"audio_filepath": "lec004_005.wav", "duration": 841.162, "text": "\nstochastic and mini batch gradient descent \nnow  we look at stochastic and mini batch versions of these algorithms  \n\n \nso  we will digress a bit  actually we should have ended up somewhere else  but i was \njust going to digress a bit  \n\f\n \nso   this  is  the  original  gradient  descent  code  that  we  had   and  i  have  highlighted \nsomething in this red box  \nso   notice that the algorithm actually  goes over the entire data once  before making an \nupdate  it has going over this entire for loop  which is over all the data points  of course  \nin this toy example i had only two data points  but in i practice i will have many many \ndata points  i go over all the data points compute the derivatives and then make this one \nupdate  \nstudent  \n  \nbecause  that  is  the  right  thing  to  do  ok   this  was  the  exact  formula  that  we  painfully \nderived right that the gradient with respect to the loss function right  which we had the \nsummation  i  equal  to  one  to  n  remember   and  the  true  derivative  was  a  sum  of  the \nderivatives  with  respect  to  all  the  data  points   that  is  what  we  analytically  derived  and \nhence we are doing that  it was that is the right thing to do  not for any other purpose ok \nthat is what it should always be right  so  that is the right thing to do  because this is a \ntrue gradient and we actually derived it  \n \n \n \n\fand hence this was not an approximation  so all the theoretical guarantees hold  if i do \nthis i know that now this is the true gradient or the true derivative and if i move in the \ndirection  opposite  to  the  gradient  everything  falls  in  place   because  i  proved  it  using \ntaylor series  \nbut what is the flip side of this  this is the right thing to do  but what is the flip side  if \nyou have millions  of point   we will go over  all these million  points and make this one \nupdate  now imagine the consequence  when you are in a plateau region right  even that \nmomentum or whatever your movement in the plateau is  going to be relatively smaller \nright   you  are  going  over  these  million  points  and  making  that  tiny  delta  update  right  \nso  imagine how much time it will take your algorithm to converge  you get the problem  \nso  the algorithm will take a million calculations and then make one tiny update to your \nw ok  this is going to be very slow  can we do something better always right  so  let us \ntake a look at stochastic gradient descent fine  \n \n\f\n \nso  i have done a very subtle change to the code what is it  do not tell me indentation  but \nthat is what i have done  so you can tell me that  so  what is happening now  for every \ndata point i am making an update to my w values  \nnow   the  algorithm  updates  the  parameters  for  every  single  data  point   if  you  have  a \nmillion data points  how many updates will be make in one pass over the data  a million \nfor every data point will make an update right  so  that slowness factor in what is known \nas batch gradient descent right  batch gradient descent is when you look at the entire data \nand then make one update  \n \n\f\n \nwhat is the flip side  what does this module titled stochastic gradient descent  so  what \nis the flip side  these are not the true gradients  the true gradient is summation over all \nthe points  now this is no longer the true grading  this is just a point estimator  this is just \na  approximation  of  the  gradient  right   and  stochastic   because  we  are  calculating  the \ngradient based on a single data point right  it is a sampling one data point and computing \nthe gradient that this is what the entire population looks like right  \nthis is almost similar to tossing the coin once and saying that this is what the probability \nof heads is  if it lands at heads then the probability is one  otherwise its zero right  you see \nthe error you see the problem with that right  as opposed to tossing the coin a thousand \ntimes and then deciding the probability is just tossing it once  so  this is always going to \nbe a erroneous right this  this is going to be bad \nso   now  there  is  no  guarantee  that  each  step  will  decrease  the  loss  why   because  the \nguarantees  were  only  when  you  are  doing  the  right  thing  which  was  to  compute  the \ngradients over all the data points  now there is no theoretical guarantees right  because it \nis  all  stochastic  now   so   it  is  possible  that  in  a  particular  data  point  your  loss  might \nincrease also   the overall loss on the data  with  respect  to  that point it might  decrease  \nbut the overall loss right \nso  now let us see this algorithm in action and i want you to make certain observations \nabout this  so  this is the code that i am going to run now  so  let us see  \n \n\f\n \nso  i will start and you have to observe and let me know and this is really becoming an \neye test for all of you  but that is good  so  for nothing interesting to observe or already \nmaybe  \nremember  i  am  running  gradient  descent   this  is  not  momentum   not  nesterov  this  is \ngradient descent ok  i have already given you the answers  what do you observe  \nstudent   \n i can still pretend an answer a let us do that  we see many oscillations  why  why do we \nsee  the  oscillations   are  these  oscillations  the  same  as  the  oscillations  that  we  see  in \nmomentum  no these are different  everyone gets that right  why are there oscillations  \nwhat is each click here correspond to one data point right  so  what is happening here  \nbecause  we  are  making  greedy  decisions  right  we  are  looking  at  one  point   this  point \nsays to decrease the loss with respect to me move in this direction and we blindly move \nin that direction  \nnow  we look at the next point  it says oh no no wait you need to move in this direction  \nso  we again move in that direction  so  all these points are actually trying to just make \nthings  better  for  themselves   they  are  not  thinking  about  what  is  happening  to  all  the \nother points in my data right  so  all these points are actually competing with each other  \n \n\fso   some  decision  which  i  took  with  respect  to  where  to  move   which  was  locally \nfavourable  for  one  of  these  points  may  not  be  good  for  the  other  point  right   hence  i \nkeep  these  tiny  oscillations  which  i  make   these  are  the  stochastic  noise  that  you  are \nseeing now  \nnow  can we reduce the oscillations by improving the stochastic estimates  always yes \nfine  so  let us see what do i mean by that  \n\n \nso  we look at a mini batch version of this  so  what i am going to do is  instead of  so  \nthis  code  is  actually  for  mini  batch  stochastic  gradient  descent   it  is  a  very  minor \nalteration on the stochastic gradient descent  i will just let you stare at it for a minute or  \nso  what i am doing here is  i am  instead of doing it for every point  i am waiting for a \ncertain number of points and then making the update right  that is what i am doing here  \nnow  for this i have kept k equal to two what does that mean  i look at two points compute \nthe derivatives with  respect  to  them and then make an update for two points at  a time  \nwhat do you expect  no what do you expect with respect to this code  \n \n\f\n \nso  let us see we will try to run this now and you will start seeing a red curve here and \nmake some observations about this  so  this is the red curve  yeah its visible  oops i do \nnot read any of those  \nstudent  \n  \nif you need to fix this right  these bullet us should come only after the curve has finished \nthis journey ok  do not read any of that ah  so  what do you see about the red curve  it is \ncompletely contained inside the black curve  that means  its oscillations are smaller than \nthe black curve right  does that make sense why this is happening  because now you are \nnot listening to just one point   you are listening to two points and then at least  you are \ndoing something better right instead of just taking one  \nso   what  is  the  analogy  with  respect  to  our  coin  toss  experiment   you  are  tossing  the \ncoin  twice  and  then  deciding  what  is  the  probability  are  heads  or  tails  right   so   it  is \nalways going to be slightly better than tossing it only once right  and now what would \nhappen  in  the  limit  if  i  keep  increasing  this   you  will  end  up  with  a  batch  gradient \ndescent where you look at the entire data \nso  looking at only one data point is bad  because it is very noisy  looking at the entire \ndata is bad  because it is very time consuming  so  you need to do something in between \nwhich is mini batch gradient descent ok  and typically you look at values of sixteen  thirty two  sixty four \n \n\fbut it also depends on the amount of data you have and if you have a billion points you \nmight actually want to look  because if you have a billion points and you have a batch \nsize of sixty four you will take one billion by sixty four times to finish the data once  \nso  you might want to keep a larger batch size at that point right  but just ignore that  but \nyou will try different batch sizes and see which one works better  so  in the assignment i \nwill be asking you in to experiment with bad sizes  yes ok  no sorry wrong question  i \nwill be asking them to implement stochastic and mini batch also or only vanilla  \nstudent  mini batch  \nmini  batch  fine  that  is  fine  ok   so   you  will  see  this  in  your  assignment   so   everyone \nsees  what  was  the  difference  between  stochastic  and  mini  batch   you  have  better \nestimates now and therefore  this red curve is contained inside the black curve fine  \n\n \nso  you have some things to remember  one epoch get used to this terminology  one epoch \nis one pass over the entire data  one step is one update to the parameters  n is equal to the \nnumber of data points and b is equal to the mini batch size  now you have to fill in the \nsecond column  in vanilla or the batch gradient descent what is the number of steps that \nyou take in one epoch  \nstudent  one   \n \n\fone in stochastic gradient descent \nstudent  n  \nn n  \nstudent n  \nin mini batch gradient descent  \nstudent  n by b  \nn by b  everyone gets that  so  get used to this ok  so  this epoch step batch size all this \nis  something that  you will see regularly when  you are reading papers on deep learning \nfine  \n\n \nso  similarly we can have the stochastic versions of momentum based gradient descent \nand nesterov accelerated gradient descent  \n \n\f\n \nso  these are just the codes  it is very easy to see what is happening here  again basically \nthis is just an indentation right  so  if you look at the difference between the two codes  i \nhave just indented it inside  that means  i am making these updates for every data point \nright and same thing you could do for nesterov also  \n\n \nnow  let us see ah  this guess what is it  this is the gradient descent  stochastic gradient \ndescent  now let us see if you have really understood nag and momentum based gradient \n \n \n\fdescent   one  of  these  curves  here  corresponds  to  stochastic  nag   the  other  one \ncorresponds to stochastic momentum  tell me which one is which \nstudent  blue pill   \nblue pill red pill  blue is  \nstudent  \n  \nhow  many  of  you  say  that   ok  i  am  confused  ok   how  many  of  you  say  that  blue  is \nmomentum  how many if you say that red is momentum  oh there is so many   you do \nnot have an opinion \nstudent  sir not clear   \nnot  clear   i  will  buy  that   so   ok   so   look  at  this   who  is  taking  longer  u  turns  \nmomentum or nag  momentum roughly which guy is taking the larger u turns  \nstudent  red guy   \nred guy right   i mean  roughly speaking  there is only one point to judge by this here  \nbecause here they are almost same and that could happen in practice right  because this is \nnow noisy  so  the red curve corresponds to  \nstudent  momentum  \nmomentum   because it is taking a larger u turn   we saw that momentum takes larger u \nturns and the blue curve is corresponding to nag ok  so  no i remember this was an error \non the slide yeah  so  this has to be red and this has to be blue  so ok  so  the momentum \nis actually red and the nag is blue because it is taking a shorter u turn and the reason you \ndo not see it very clearly is because both of these are running in this stochastic mode  \nbut you still see the relative advantage of them that nag still takes shorter u turns  both of \nthem are faster  still faster than vanilla gradient descent  \n\f\n \nyou see that black curve at the top and both of these are faster than them  both of them  \nall three have run for the same number of iterations  after sixty steps you see what happens \nto stochastic gradient descent and what happens to nag and momentum basically gradient \ndescent   and  of  course   you  can  have  the  mini  batch  versions  of  momentum  and  nag \nalso   \n \n\f"}
{"audio_filepath": "lec004_006.wav", "duration": 790.193, "text": "\ntips for adjusting learning rate and momentum \ntips for adjusting the learning rate and the momentum  \n\n \nso  before moving on to these slightly advanced optimization algorithms  we will revisit \nthe problem of learning rate in gradient descent  \n\f\n \nso   one  could  have  argued  that  we  could  have  solved  this  problem  of  this  slow \nmovement  on the gentle slope by increasing the  learning rate  remember  that we have \nthis eta and we deliberately chose to be conservative  that we will take a small value for \nthe eta  but what if i just blow up the eta  i could just take a very large eta  what would \nhappen  it will overshoot right  \nso  what will happen is  i will see what happens when i take eta equal to ten ok  so  so i \nwill see what happens  when i take eta equal to ten  \n\n \n \n \n\fso   this is  stepone   step  two   step  three   its  moving  very  fast  on  the  regions  where  the  slope  is \ngentle   but  it  also  moves  very  fast   much  faster  on  the  regions  where  the  slope  was \nalready steep  \nso  when the gradient was actually high  you ended up blowing it further by multiplying \nit with  the eta which is ten  so   it is again  going to  have this effect  that you will move \nmuch faster in the steeper regions and again you will see these oscillations  because you \nwill  overshoot  your  objective   does  that  make  sense  right   so   it  is  not  that  you  can \nalways choose a high eta and get away with it \nso  what do you actually want  what is your wish list regulate theta  you want a adaptive \neta  right  that  it  somehow  figures  out  that  i  am  on  a  gentle  slope   so  i  should  move \nslowly  i should move fast and i am now on a very fast loop  so i should move slow  so  \nthis  having  this  one  eta  is  not  working  for  every  point  on  the  error  surface  right   for \neverywhere on the error surface  is that clear ok so  ok  so  we will see such algorithms \nsoon where we try to adjust this learning rate  \n\n \nnow  here are some tips for the learning rate  so  how do you  if  you are just going to \ndeal with this gradient descent or nag or momentum  how do  you adjust these learning \nrate  so  how do you fix a learning rate  so  a learning rate is typically something known \nas  a  hyper  parameter   so   why  is  it  called  a  hyper  parameter   so   what  are  your \nparameters  \n \n\fstudent  which i learned  \nwhich i learned using the objective function  eta is not a part of the objective function  \nyou are not computing radians with the respective to  it is a hyper parameter  so  you will \ntry to tune this hyper parameter  so  what you will do is  in practice you could try these \ndifferent  values  on  a  log  scale   next  what  will  you  do   run  this  all  these  for  a  few \nepochs  note down the dash  just note down the loss function  \nso  run all of these with different learning rates  for say five epochs  you will get some loss \nright  now which one will you pick  the one which led to the maximum decrease in the \nloss   i  will  keep  that  learning  rate  and  now  what  you  will  do   you  just  stick  to  that  i \nstarted off with a dash scale  \nstudent  log scale  \nlog scale now what will you do ok  so  now  run it for a few epochs  figure out which of \nthese learning rates on the log scale works well  now do a finer search around the best \nlearning  rate  that  you  discovered  right   so   say  zero one  was  the  best  on  the  log  scale   so  \nnow  look at zero two  zero three  zero four  zero five  look at values around it and see which one works better  \nso  this is how you will tune the hyper parameters  otherwise there is a very wide range \nright  if you put tune from zero one to zero one  there are just too many values to consider  so  \nwe will have to do this log scale and then a linear scale  will that make sense  \nthese are just heuristics  there is no guarantee that will always work or which of these \nare  clear  winner  strategy   but  you  have  to  try  this   so   tuning  a  learning  rate  is  an \nimportant  part   when  you  are  working  in  deep  learning   so   at  least  when  you  are \nworking with gradient descent or nag or momentum based gradient descent  \n\f\n \nnow   here  some  tips  for  annealing  the  learning  rate   so   there  is  something  known  as \nstep decay  so  what you can do is  halve the learning rate after every five epochs  can you \ntell me the intuition for this  what do you expect after five epochs  that you have moved \nenough and now you are closer somewhere to the solution  so  if i closer to the solution  \nif i closer to phoenix market city you want to move fast or slow  \nstudent  slow  \nwhat will you do  \nstudent  \n  \ndecrease the learning rate right  so  after every five  now this is again what is so sacrosanct \nabout  five  it is just a magic memory  so  this  is  again  hyper parameter  so   you could  fix \nsome  number  of  epoch  and  after  these  i  will  just  halve  the  learning  rate  ok   now  this \nsecond one is what my favourite is and i typically use this  what i do is  i compute the \nloss after epoch t  i run epoch t plus one  i compute the loss again  if the loss has increased \nwhat will i do  i will just throw away all the updates that i have made in this epoch  i \nwill decrease the learning rate and again learn  again start this epoch  what do i mean by \nthrow away all the updates  \nstudent  \n  \n \n\fso  after epoch t i will save my model  i will save all the w values that i have computed \nand  i  will  let  it  run  for  one  more  epoch   after  this  epoch  if  my  loss  function  actually \nincreases  i reload this model which i had saved  half the learning rate and then run this \nepoch again  does that make sense  so  i have run till epoch t  i have some values of w\u2019s \nand b\u2019s  i will save this values  i will just save it as a numpy array  \nnow  i will with the same learning rate that i have been using so far i will run the epoch t \nplus one ok and i  get some new values of w comma b right  i will plug this into the loss \nfunction  i will plug this into the loss function i will get two loss values  if this loss value \nis greater than what i was at the previous time step  that means  things did not work out \nwell in this particular epoch  \nso  i will throw away all these updates  i will just reload the model which i had saved  i \nwill just start from where i was at epoch t  i will decrease the learning rate i will make it \nhalf and run this epoch again right and hopefully now i should do better  because there is \nsomething   i  am  just  making  a  hypothesis  that  the  reason  i  did  not  get  to  a  better  loss \nfunction  was because my learning rate was not adapting to it  \nso  i will just halve the learning rate  because this solution was good  this was a low loss \nfunction  i just want to be something around it  i do not want to make any drastic steps  \nso  i will just half the learning rate from there  so  then you not see this drastic change \nthat   your  loss  function  should  not  improve   so   first  of  all  local  minima  is  known \nproblem in deep neural networks  \nso  what happens is that  in deep neural networks you do not have something which is \nlike a neat convex function as your loss function right  it is a non convex function which \nmeans  there  is  no  one  unique  minima   there  could  be  several  minima  and  there  are \nseveral  analysis  which  show  that  a  lot  of  these  minima  are  equivalent   so   in  practice \nthese are the things that you do  either once you reach a minima you just stay there  the \nsecond  thing  that  you  could  do  is   you  have  trained  your  algorithm  trained  your \nparameters for say one hundred epochs and you have stopped now \nnow   again  go  back  and  start  with  a  different  initialization   you  started  with  some  w \nnaught  b  naught  and  you  have  reached  to  some  solution  keep  this  solution   now  start \nwith a different initialization  that means  if you look at your wb plane  you have started \n\ffrom some other point  that means  you started from some other error location right and \nrun this algorithm again  and see if you reach a different minima  \nso  the only thing you  the way you counter this is  you just try different stochastic things \nright  should try to start with ten different initializations every time reach a minima and \nthen at the end select the lowest possible of these  did this make sense to most of you  \nhow many of you got this oh cool i thought i was just rambling  but yeah fine  does that \nmake sense  to you at least ok  does it fine \nyes a local minima is a severe problem in lot of deep learning optimization and typically \npeople get away by that  by just picking up one of these minimum fine  now the other \nthing  is  you  could  use  exponential  decay   where  with  each  time  step  you  just  keep \ndecreasing your learning rate  and if this case two  that means  at every time step you are \nhalving the learning rate  so you just get with something like this  \nbut the reason i do not like this is that you have one hyper parameter which is eta which \nyou  are  trying  to  tune  and  now  to  tackle  that  problem  you  have  introduced  one  more \nparameter  which  is  k   hyper  parameter  which  is  k   so   it  becomes  harder  to  tune  that \nnow  and there is a similar thing which is one by t d k  where you try to use this formula to \ndecay  or  learning  rate   so   both  of  these   i  typically  do  not  use  in  practice  i  use  the \nsecond one  i prefer the second one  \n\n \n \n\fnow  tips for the momentum can you make sense of this  you just stare at it it looking \njust come back ok  let us see what happens at t equal to zero  this becomes zero  \nstudent  log one  \nlog one is zero  this is two raise to minus one minus zero which is just two raise to minus one which is zero five  \nso  what is your mu t at t equal to zero five  does that make sense  is it fine with everyone or \nis it confusing  no ok mu max is typically this  let us assume mu max  \nnow  what happens at time step two hundred and fifty  this is two hundred and fifty by two hundred and fifty  so this becomes one  one plus one is two  \nthe best thing that you learn in this course log of two is one  so this become two raise to \nstudent  two raise to minus two  \nminus two which is zero twenty five  so  what is this  \nstudent  zero seventy five  \nzero seventy five  let us do one more i had t equal to seven hundred and fifty one minus one by eight  so that is what is going to be \nright ok  so  then what is happening as my time steps are increasing  what is happening \nto   what  is  happening   i  am  having  more  and  more  faith  in  the  history  or  the  current \ngradient  what am  i increasing  actually i  have made a mistake  actually this is mu is \ngamma there is not we did not use mu anyway what you guys just went along  so  this is \ngamma actually right that was a momentum term that we had  so  as a number of time \nsteps  is  increasing   my  gamma  is  increasing   that  means   i  am  having  more  and  more \nfaith in my  \nstudent  \n  \nno history  learning rate is eta  momentum is gamma  so  its gamma into update t minus \none and eta into gradient at the current time step right and here gamma is actually equal to \nmu  is there any more confusion that i can add  so  when i say gamma i mean mu and so \nthat is how it is  so  as i am increasing the number of time steps i have more and more \nfaith in  the history   that means   i do not want  to  now get  distracted by this  one update \nwhich i am making right  i want to go by the history  and i am not increasing this gamma \nor mu indefinitely  i am capping it by a max right  max i will have this much faith which \n\fis zero nine hundred and ninety nine in the history  does that make sense  this is again just a heuristic  do not worry \ntoo much about it  so  that is how it is   \n\f"}
{"audio_filepath": "lec004_007.wav", "duration": 550.47, "text": "\nline search \nso  we were looking at these different variants of gradient descent  we saw that gradient \ndescent  has  this  problem  that  it  finds  it  difficult  to  navigate  the  gentle  slopes   so   we \ncame up with tricks on momentum based gradient descent and also  nesterov accelerated \ngradient descent   \nthe  trick  in  momentum  was  that  if  lot  of  your  history  is  telling  you  to  move  in  a \ndirection   then  just  continue  to  gain  momentum  in  that  direction   so   instead  of  just \nupdating based on the current  gradient   you also update based on the history  right and \nthere we saw that this is always going to be a problem that you will end up taking u turns \nand  we  had  this  analogy  of  how  you  look  for  directions  and  you  just  overshoot  your \ndestination and have to come back and take a u turn and come back and so on  \nso  to prevent that we realize that the update done by momentum base gradient descent is \ntwo step update  you actually the first step is based on the history and then  another step \nbased on the gradient at the current time step  right  so  then instead of doing these two \nsteps at one go  why not just update based on the history  see what the gradient that tells \nyou and then  we saw this nice figure  i hope it was nice and where you saw that if you \nlook ahead point  then you will be immediately corrected with respect to your errors  so  \nthat was about nag and momentum  \nthen  we saw the stochastic versions of these algorithms  where we realize that if we do \nthe  batch  version   then  you  go  over  a  million  points  and  then   make  only  one  update \nwhich could be very slow in cases where you have large data  so  we then decided to the \nstochastic version where we just update for every point that again had these oscillations \nbecause  we  were  taking  greedy  decisions   we  were  just  relying  on  one  point  to  tell  us \nwhich was the right direction to go on and you saw that these esteem has become better \nas you increase the value of this k  \nso   k equal  to  one  is the  most stochastic version  and then  k  equal  to  two  you get  the mini \n\fbatch version and then  you could just have different values of k  so that you have more \nreliable estimates of the  gradients and in the limit if you have the entire data  then you \nare just doing the full batch gradient descent  right  this is the vanilla gradient descent  \nanything  else  did  we  cover   then  we  had  some  tips  on  the  learning  rate  and  the \nmomentum  these are again heuristic  i gave you some ideas and you could try these in \nyour  back  propagation  assignment  and  see  which  one  works  better  for  you   you  could \nsee  you  have  any  peculiar  observations  while  implement  the  back  propagation \nassignment  \nso   now  there  are  a  few  more  things  left  in  this  lecture   so   i  will  start  with  the  line \nsearch  first   so   this  is  one  more  thing  before  you  move  on  to  some  more  interesting \nalgorithms which are the current state of the art and lot of deep learning solutions  \n\n  \nso   most  people  that  you  read  would  look  at   would  have  algorithms  that  we  will  see \nafter ten minutes  \n  \n\f\n \nso  now this is where just to contest contextualize things  right  so  we are still trying to \nsee  what is the light right learning rate  to use a line search is one such method where \ninstead of just doing one learning  so  you can look at the code and just focus on this part \nand tell me actually what are we trying to do  how many of you get what the algorithm is \ntrying to do  so far what we were doing is  we were just having a single learning rate  ok \nand we saw that this learning rate can make a lot of difference  right because if you are \non  the  gentle  part   you  want  larger  learning  rate  and  if  you  want  steep  part   you  want \nsmaller learning rate  \nso  just fixing the learning rate to one value does not really help because then you will \nmake  you will suffer on one of the two cases are either on the gentle case or on the steep \ncase  now  what line search does is  instead of just using one learning rate at every step  \nnow  whether  it  is  vanilla  gradient  descent  which  is  the  batch  one  or  mini  batch  or \nstochastic  right just use a bunch of learning rate  so  i have used five different learning \nrates  here  and  i  have  computed  the  gradients   that  part  remains  the  same   the \ncomputation of gradients does not change  \nnow  you have the value  now   you want to be conservative   you want to multiply the \ngradients  with  this  eta  right   but  you  know  that  you  do  not  always  want  to  be \nconservative  in fact  in some cases when you are on the gentle slope  you do not want to \nbe  conservative  at  all   you  want  actually  blow  up  the  gradients   so   now  try  these \n \n\fdifferent learning rates and update w and b  ok  so  if you have five learning rates  you \nwill get five different updated values for w  b  \nnow  plug in all these w  b values into your loss function  right and see whichever is the \nminimum   retain  that  w   b  value  and  repeat  the  process   that  means  again  you  will \ncompute the gradients with respect to this new value of w  b and the new loss function  \nagain try out these five different learning rates and continue  everyone gets that  \nso  now are we using a fixed learning rate at every step  no and now do you see that if \nwe are at a gentle slope  it would pick probably this as the learning rate and if we are on \nsteep slope which should probably pick one of these as the learning rate and even lesser \nthan that  if  you have the disruption  it does not make sense  you see the advantage of \nthis  now   you  are  in  some  way  heuristically  trying  to  adapt  to  the  slope  of  the  error \nsurface  right by just giving a different learning rates  so  try all of these and whichever \nworks best  pick it up  ok  so  that is about it  we are trying different values  \nnow  what is the flip side of this  now  if you have k different learning rates that you are \ntrying  then at every step you have now increased your computation k times  so  earlier \nyou  just  add  one  learning  rate  u  just  going  by  that   but  now  i  have  k   so   now  this  is \nagain a trade off which is you have to see  now  i will give an example where this trade \noff clearly works  \nso  now if you are at the gentle slope  now making k more computations and moving out \nof  that  slope  is  definitely  worthwhile  as  compared  to  just  sticking  to  that  slope  where \neven  after  hundred  more  computations   you  will  not  really  move  out  of  that  slope   so  \nremember that gradient descent algorithm that we have seen where you just stick to the \ngentle slope after hundred iterations also right  but instead if i tried five different learning \nrates and there is a high chance that  i  could have moved out of the gentle slope  does \nthat make sense  you see the advantage of this  \n\f\n \nso  this is something that i have to talk about when i back to second order optimization \nso   i  will  see  when  to  teach  that   so   let  us  see  line  search  in  action   so   this  is  again \ngradient  descent   this  black  curve   which  is  visible  there   this  is  the  one  i  am  talking \nabout which is run for few iterations and is just stuck on the steep curve  you know this \nstory now and it is just get stuck there  \nnow  let us see what happens if i run  so  now i will start running the line search based \ngradients descent  so  what do you expect now  so  it will just move very fast  right  so  \non the first step itself  it is crossed wherever gradient descent was stuck after fifty iteration \nor so  i will keep moving fast  \nnow  here is an interesting question  would you see oscillations here  so  when you see \noscillations   it  is  when  your  loss  is  actually  increased  from  whatever  it  was  currently  \nwill that happen in line search  the answer is always no  \nit  could happen  when could this happen  so  it depends on the learning rates that  you \nhave chosen  right  so  if you have chosen the learning rates  so suppose at one point to \nreally be effective  you needed the learning rate to be zero one  ok and now if zero one learning \nrate was not in your set  right that means  everything that is there in your set is faster than \nzero one  so that it will again have the same problem as momentum because you will move \nfaster than what  you should actually move  so  it depends on this careful choice of the \nlearning rate set  \n \n\fso  that is all i have to say  so  there is a slight convergence would be faster than vanilla \ngradient descent  that  is obvious and we see some oscillations  ok and the statement is \nactually wrong  we need to remove that  ok  we see some oscillations and these could be \nthe  similar  wants  to  the  once  with  that  we  see  in  momentum  because  we  overshoot \nbecause we have not chosen the right set of learning rates  \none of the learning rates which was actually needed at a particular point right say at this \npoint  suppose i needed to move very slowly and that very slowly say zero one and that was \nnot  in  my set  then any  other learning  rate is  always  going to be much  faster then  so  \nyou could see oscillation  \n\f"}
{"audio_filepath": "lec004_008.wav", "duration": 2437.209, "text": "\ngradient descent with adaptive learning rate \nin this  module  we look  at  gradient  descent  with  adaptive  learning  rate  so   first  we \nwill see motivation or intuition for why we need this and once you get the motivation  i \nbelieve the rest should be straightforward  \n\n \nso  far  what  we  have  been  doing  is   please  pay  attention  on  this  slide   i  need  to  define \nsome notations and you should not get confused with that  so far we have been dealing \nwith  the  situation   where  we  had  just  one  feature  which  was  x  and  one  weight \ncorresponding  to  it  which  was  w  and  one  bias  which  corresponded  always  on  input  \nright  now  we are going to look at the situation where we have more than one inputs  \nthat means  earlier we were basing our predictions only based on the director and now  \nwe are the director  actor  genre  imdb ratings and so on  \nso  here x one x two x three x four  these are four different features or four different inputs that  i \nhave  and this is  not  x square  just  i know it is  obvious  but  i  am  just  making it  clear  \nright  so  this is x one x two x three x four  ok  it is not probably the best choice of notation  but i \n\fwill just stick to that  so  now each of these has a corresponding w one w two w three w four  ok and \nthis is how your decision looks like  it is the dot product between the weight vector and \nthe input vector  ok  this is how i am going to decide and that is a single sigmoid neuron \nagain  \nnow  given a single point xy  do i need to again go through this computation  sorry w  p \noh sorry  ok  i will just erase this  so  this w is actually the vector w  so  it includes w one \nw two w three w four and i am trying to take the derivative with one element of that vector  do i \nneed to  show  you how to  compute this  have  you seen this before  can  you tell  me  i \nwill show you the derivative with respect to w one  can you tell me it will be a product of \nsome terms  can you tell me what is the last term going to be  \n\n \neveryone  gets  this   you  remember  this  form   so   only  thing  which  is  changing  is  this \nguy  right  so  this part is exactly what we have derived and when we had one input  we \njust call it x and now  we have multiple inputs  so  it will depend on that particular input  \nright which ever w one corresponds to  \nnow  make an interesting observation there  so  sorry before that yeah this is obvious if \nthere are n points  we will just take the sum of the gradients with respect to the n points  \nok  now  what happens if the feature x two is sparse  what do i mean by that  it is mostly \nzero  ok  what does that mean  so  i am looking at lot of movie data  ok  amir khan acts in \na very few movies  so  if i have a feature which says actor amir khan  then that is going \n \n\fto be zero for most of the movies in my data scene  right  that is what i mean by sparse  \nso  if i have ten zero movies  then probably only fifty of them would have this feature as \none  ok  does that make sense  so  it is going to be very sparse  now  if the feature is \nsparse  why do we care about it  what will happen  what do we really care about when \nwe are talking about optimization in this course  the gradients  right  that decides how \nwell  we  move  in  the  plane  that  we  are  considering  w  b  plane  or  the  other  in  the  end \ndimensional region that we care about  \nso  now if x two is sparse  what would happen to this  it will be zero lot of times because x two \nis  zero  lot  of  times   right   so   now  just  take  a  minute  to  understand  this   right   so   now \nremember let us talk about stochastic gradient descent or mini batch gradient descent or \neven  batch  descent   you  are  going  over  all  the  ten zero  points  that  you  have   you  are \ncomputing the gradient with respect to all the parameters   \none of those parameters happens to  be w two   right  you have gone over  ten zero points  \nbut in how many of those you will actually get the gradient for this  only in the fifteen which \nx  two  was  present   right   everywhere  else  the  gradient  would  be  zero   so   that  means  your \nsum of the gradients  overall the endpoints is going to be small or big  \nstudent  small  \nsmall for this particular feature or for this particular weight  it is going to be small  right \nbecause you do not have enough samples where you are seeing this  so  now what would \nhappen  to  the  update   you  started  with  a  random  value  for  w  two   after  one  epoch  or \nmaking  one  entire  pass  of  the  data   what  would  happen  to  the  updates  for  w  two   very \nsmall   very  few  updates  compare  this  to  a  feature  which  is  dense   do  you  get  a  lot  of \nupdates  so  you see there is something unfair happening here  if a feature is sparse  it is \nnot getting updated enough   \nnow   that  was  ok   in  one  situation  if  this  feature  was  not  really  important   but  now \nconsider the exact example which i gave you which is this  an amir khan movie or not  \nbut  suppose  i  am  doing  a  classification  whether  this  movie  is  going  to  be  hit  or  not   i \nwould believe this feature is very important because almost always when he is the actor  \nthe movie is a hit  right  so  you really cannot ignore this feature  you want to learn the \nparameters  correctly  for  this  feature   do  you  get  the  setup  right   there  could  be  cases \n\fwhere your feature is very sparse  but at the day at the same time very predictive of the \noutput that you are trying to learn  right and in this case  the output is whether the movie \nwould be a hit or not  \nthe other example could be is christopher nolan  the director  so  yes probably directed \nless  than  ten  movies   but  all  of  them  have  been  at  some  point  in  the  imdb  top  two hundred and fifty  or \nsomething   right   so   that  is  a  very  important  feature   but  you  will  not  get  it  very \nfrequently in  your data   right  so   you cannot really ignore these features  that means  \nyou  still  want  to  learn  these  features  properly   so   you  have  sparse  features   you  have \ndense features  we understand that for the sparse features  the updates would be slower \nand  for  the  dense  features   the  update  would  be  faster   the  sparse  would  be  zero  in  most \ncases  no no  so  you will do this zero mean thing  \nno  but if it is a same value and you are going to zero mean the data  right  so  the value \neven if it is one  it is  going to be very close to  zero   right  so   you always assume zero means \notherwise all this does not make sense  right because if your features are not in the same \nrange  then anyways  you are in  trouble  right fine  so  this  is  what  i was trying to  say \nthat the gradient with respect to w t is going to be zero for most inputs and hence  w t will \nnot  get  enough  updates  and  as  i  said  if  this  is  an  important  feature   we  cannot  really \nignore it  we have to make sure that it learns better  \nso  what is the case that i am making for  what do we actually need  can you relate it to \nthe discussion on learning rate that we have been having  so  if the feature is sparse  you \nknow it is going to get very fewer updates  so  can we change its learning rate  so that \nfeature  gets  updates  a  bit  faster  as  compared  to  the  other  features   so   you  get  the \nmotivation  right  how to do this is a separate story  but at least we need to do this   \n\f\n \nso   the  intuition  is  decay  the  learning  rate  for  parameters  in  proportion  to  their  update \nhistory   so   you  have  been  recording  the  update  history   you  have  been  looking  at  the \nparameter   you  know  all  the  gradient  w  two  that  you  had  calculated  so  far   right   how \nmany times you had computed the gradients and what those values were actually now for \nthese sparse features  those are going to be zero  \nso  your cumulated history is going to be small  right  for a dense feature  it is going to \nbe high  so  why not make the learning rate inversely proportional to this history  that \nmeans  if the feature has been updated fewer times  give it a larger learning rate  if it is \nnot updated  if it is updated many times  give it to a smaller learning rate  can you give \nme a mathematical formula for doing this  this is the intuition  just think about it for a \nminute learning rate inversely proportional to update history  ok good  how many of you \nget that  but most of you will get it once i show you the answer  \nthis  is  my  gradient  which  i  had  computed  so  far   i  mean  at  this  time  step  i  will  keep \naccumulating it in a history vector  so  at time step zero  i will take the magnitude of this \nagain  i am taking the magnitude right because it does not matter whether you made an \nupdate in  the positive direction or the negative direction   you just matters  that whether \nhow much  by how much it move  so  i will just square this quantity  so that i can get rid \nof the sign  so   i am taking the magnitudes and  i am storing all that  so  at time step t \nwhat would vt contain  it is grad w zero square plus w one square grad w one square and so on \n \n\fup till time step t  \nnow  this was my if i ignore this quantity  this was my normal gradient descent update \nrule  now   do  you see  what  i  have done   i  have divided the learning  rate by whatever \nhistory i had accumulated  so  for the dense features what would happen is  the learning \nrate will increase or decrease with time  the learning rate will decrease  right and for the \nsparse  features   relatively  less   in  fact   if  you  have  written  gotten  zero  updates  so  far   so  \nwhen  you  have  to  update  the  first  few  times   you  will  have  a  very  high  learning  rate  \ndoes that make sense  right because this quantity would be zero  so  our eta would actually \nbe very large  so  you see how that intuition got converted into some reasonable formula  \n\n \nnow   can  you  tell  me  a  way  of  actually  realising  this   i  want  to  show  you  that  what \nhappens when you have sparse data and i want to do this with the toy example that we \nhad where we had only one feature and other feature was always on  right  so  how do i \ncreate  this  sparse  data   so   you  should  think  about  these  because  these  are  things  you \nwill have to  do when  you are practising machine learning  and if  you are  working with \nthe  problem  and  you  want  to  create  some  simulated  data   so  that  you  can  verify  some \nhypothesis that you have  so  how would you do this  \nsee i am going to create thousand data points  right which is x  y points and of course  i \nhave this x zero which is always on  right  so  x zero is always on  i cannot make that sparse  \nwhat about the other feature  if i am creating thousand data points  what should i ensure \n \n\fis that eighty percent of them or some ninety percent of them is always zero  right  just as the amir \nkhan case and most of the data it is going to be zero  so  what we will do is as i said we just \nhave two parameters w and b  b cannot make sparse is always going to be on  so  what \nwe will do is  we will make x sparse  we just create random x  y pairs and then  for eighty \npercent  of  those  we  will  set  x  to  zero   right   so   now  this  x  feature  is  going  to  be  very \nsparse   \nso  now i have created some data which is sparse  one of the features is sparse and now  \ni  want  to  see  what  happens  when  i  run  gradient  descent  momentum  and  nesterov \naccelerated gradient descent and how does the algorithm behave and now  if i apply this \nalgorithm  which  i  did  not  name   it  is  called  adagrad   ok   this  algorithm  is  called \nadagrad if i apply this algorithm  then what how does the situation change  \n\n \nso  this is what gradient descent momentum and nag do  now  at least  the difference \nbetween momentum and nag should be clear  nag blue curve is inside the red curve \nright  so  oscillations are slightly smaller  this is how they behave  \nnow   there  is  something  very  interesting  that  these  algorithms  are  doing  for  this \nparticular  data  set  that  i  have  created   can  you  spot  it   what  is  the  interesting  thing \nhappening  here  i  want  you  to  take  some  time  and  think  about  and  relate  it  to  the \ndiscussion that we just had how many of you see what is happening here  very few i will \ngive a hint   ok   it  is  almost  as  if these  algorithms went  to  a school where they did not \n \n\fteach  pythagoras  theorem   now   related  to  the  discussion  that  we  just  had  what  is \nhappening initially  so  initially what is happening is you started from here  ok and this is \nthe w  b planes  so  you have w on the horizontal axis and b on the vertical axis  \nwhat is happening to all your updates initially where are you moving  you are moving \nalong the b direction  are you making any movements along the w direction  no  why \nw was sparse  its gradients are mostly zero  it was not being able to make any updates in \nthe w direction or it was able to do make updates in the b direction  it did as much as it \ncould do after reaching here it realizes that there is no point in going to be further  right  \nit actually took u turn because it realise that there is nothing i cannot really go ahead  i \nhave to now start working in a direction of w  \nso  now in practice although in this toy example  it does not  it still converges fast  but in \npractice what will happen is you have just moved in one direction  reached a point and \nnow  from  there  again  you  are  going  to  take  right  turn  and  reach  to  your  destination  \nright  so  you are taking  you are doing something which is not fast  this is not how you \nwould  go  from  this  point  to  this  point   there  has  to  be  a  better  way   right  and  this  is \nhappening because w is not getting updated frequently  all the updates are initially done \nfor b  \nnow  when it is no longer possible to change b because you reached the optimum value \nfor b  then only you start changing w and that to very slowly because it will have to wait \nfor  many  updates  to  happen   for  that  to  happen  how  many  of  you  get  this   so   this  is \nexactly what is written on the slides because in our data the feature corresponding to w is \nsparse and hence  w undergoes very few updates and b is very dense and it undergoes a \nlot of a updates  \nnow   such  sparsity  is  very  common  in  large  neural  networks  which  have  thousands  of \nfeatures   right   so   you  can  imagine  this   now   if  i  have  thousands  of  features   now \nsuppose  i  am  doing  credit  card  fraud  detection   ok  now  say  one  of  my  features  is \ncorresponding to some education that the person had and suppose he has done some very \nless sort after degree or less sort after curriculum  \nso  that feature is going to be sparse where most of the cases  but i cannot ignore it  may \nbe  this  is  the  most  predictive  feature  that  i  might  have   right   so   you  could  think  of \nvarious cases where you have thousands of features out of which many are going to be \n\foff for a given example  right  everyone sees that this is the real world scenario where \nlot  of  your  features  are  going  to  be  sparse  and  in  many  cases   you  cannot  ignore  the \nsparse features  ok fine  now  let see what adagrad does  any guesses  \n\n \nso  i am running this  we should start seeing something a green curve starting from here  \ndo you see what is happening expected  now  try to guess if you are going to run into a \nproblem   i  have  deliberately  halted  the  algorithm   i  just  want  you  to  think  if  you  are \ngoing  to  run  into  a  problem   ok   all  of  you  think  you  have  something  which  makes \nsense  so  now i have run it for in this case again this is the toy example  hence  you do \nnot see a lot of difference between these algorithms in terms of number of steps taken to \nconverge   but  in  real  world  application   it  would  be  very  different   but  now  what  has \nhappened is i have run the algorithm for as much i can and i am then stuck here  i am not \nbeing able to move forward  why is this happening  \nwell  i am the histories accumulating it is growing  now  what am i doing to the learning \nrate  i am just killing it  right  it is eta by a very large constant  now  that is going to be \nvery small  so  no matter how big my gradient is  it is going to get multiplied by a very \nsmall learning rate and i cannot just move any forward anymore  right  so  see that will \nhappen  that is why in this case this is some point here which i do not want to go over \nnow and it is this  \nin fact  i do not have an explanation for that  but this one observation which people have \n \n\fmade  that  remember  we  have  the  square  root  in  the  denominator   if  you  remove  the \nsquare root in principle  you are still doing the same thing  right  you are still making it \ninversely  proportional  to  a  cumulated  history   but  it  does  not  work  well  when  you  do \nthat  that i do not know why it happens and i just read these comments at several places \nthat it does not work when you remove the square root from the denominator  but that is \nnot important for this discussion  that is just point for reference later on  \nso   right  now  what  i  am  trying  to  say  is  that  it  did  the  right  thing   it  started  making \nupdates  for  w  also  and  started  making  larger  updates   hence  we  see  this  simultaneous \nmoment in both w and b direction  but the flip side is over a period of time  the effective \nlearning  rate  for  b  will  decrease  so  much  that  we  no  longer  be  able  to  move  in  the \nvertical direction  right and if  i am not being able to move in the vertical direction  we \nwill  not  reach  the  minima   in  this  particular  example  not  always   but  in  this  particular \nexample  you  need  to  move  further  in  the  direction  of  b   but  a  learning  rate  is  not \nallowing you to do that  so  that is what is happening  \nso   now  can  you  avoid  this   yes   how   multiply  by   so  first  divide  it   so  that  the \ndecreases  then multiply it  so  that does not decrease  all of these are interesting ideas  i \nam not i mean it is very hard to say upfront whether this is wrong or right  but yeah these \nare  you  get  the  idea  basically  something  is  happening  which  is  you  are  aggressively \nkilling the learning rate  \n\n \n \n\fnow  i just want to make sure that you are not so aggressive  so  what happens because \nof the aggressive killing  is the frequent parameters  they start receiving fewer updates  \nnow  this is what rmsprop does  i want you to stare at this for a minute  assume that \nbeta is going to be something which is greater than zero ninety or zero ninety five or something and try to \nmake sense of what is happening  try to imagine what is vt is going to look like  in terms \nof grad w zero grad w one and so on  to start from v one and see what happens what was v one \nearlier and what it is going to be now ok  but it still grows my magnitude when i am still \nadding stuff  so  how does it help me in not blowing of the denominators  \nso  yeah i think you most of you get  so  again this is the trick is basically you are using \nthis  exponentially   exponential  moving  average   so   even  at  the  first  step  earlier  i  was \ndoing grad w t square  now  actually doing zero five into grad w t square  oh sorry grad w one \nsquare  right  so  that is what my v one is going to be  now  what is my v two going to be  it \nis  going  to  be  zero ninety five  into  zero five  grad  w  one  square  plus  grad  w  two  square   right   so   this \nquantity is  even shrinking further and  at  each step this is  going to  keep a zero five  ok and \nyou  see  now  at  each  step  this  is  going  to  get  multiplied  by  this  quantity  and  shrink \nfurther  \nso   now  i  am  not  aggressively  growing  the  denominator   i  am  not  considering  the  full \ngradient  but  only  a fraction of it and in fact   a very small multiple of it  so   i  am  still \naccumulating the history  but i am not being very aggressive while doing that  right  so  \nyou understand this  everyone gets this  \n\f\n \nso  now let us see if we run what would happen  any guesses  ok  so  initially now this \nis i think a brown curve  it is already there  but you can see it  so  i will keep running it \nand at  some point  it will  diverge  from the green curve  yeah do  you see that  now  i \nhave reached its destination  right  so  at the point where the b learning rate  the learning \nrate  for  b  was  getting  killed   in  this  case  that  does  not  happen  because  you  have \nprevented  the  denominator  from  growing  very  large  actually  multiplied  by  its  small \nvalues  so that it does not grow very fast   \nso  adagrad got stuck when it was  close to convergence because the learning rate was \nkilled  and  it  was  no  longer  able  to  move  in  a  direction  of  b   but  for  rmsprop   it \novercomes  this  problem  by  not  growing  the  denominator  very  aggressively   ok   now  \ncan you think of any further modifications  there is everything that you learned so far \nand my everything yeah   \nyeah i am not very sure why that  i agree that i am also bit surprised that it completely \noverlaps  with  it   i  checked  it  and  that  is  how  it  turns  out  to  be  and  guessing  it  is  an \nartifact of the artificial data that i have created  so  it is trying to say is actually making \nsense  that  it  should  not  overlap  so  much   right   initially  it  should  slightly  be  biased \ntowards b and then  probably that is what you are trying to say right  but i told it just an \nartifact of this data that i have  but what matters is from as going to say illusion  but from \nthe illustration is that it actually does not kill the learning rate  \n \n\f\n \nwhat  is  the  one  idea  that  now  think  of  everything  that  you  learned  in  starting  from \ngradient descent  then you tried to improve it using something  then you tried to further \nimprove  it  and  so  on  and  now   we  have  taken  a  slide  d  two   from  there  you  are  now \nfocusing on the learning rates  but there were other things  which  you are doing earlier  \ncan you bring those back  add momentum  how many of you say add momentum as if i \ncan just added  you are right actually  \nso  let us see what we can do  so  it does everything that rmsprop does  that means  it \ntries  to  make  the  learning  rate  inversely  proportional  to  a  sane  cumulated  history   by \nsane  mean  it  does  not  allow  the  history  to  blow up  and  it  also  will  use  the  cumulative \nhistory  of  the  gradients   so   let  us  see  the  update  tool  for  adam   so   what  is  this term \ndoing   actually  it  is  taking  a  moving  average  of   there  is  the  same  as  the  momentum \nbase role  right  just taking a moving average of your gradients  ok  the same analogy \nthat i am going to phoenix market city  i am just taking all my history into account  ok \nand  vt  is  again  a  cumulative  history   this  is  the  same  as  what  was  happening  in \nrmsprop  right where you get lost   \nnow  what would be the next step be  can you give me the final update rule  at least \nthink about it  mt into vt  no  ok just try to think about it and it is very hard to say it out \nthere are too many grads and suffixes and so on  so  just think about what you did in the \nmomentum case  ok  now  there is one more step which i am going to ignore  i will just \n \n\fsay what that step is and then  i will come back to that later on  \nso  this is something known as bias correction  ok  just ignore it for the time being  i will \ncome back to this discussion  just for the time being just assume that i am taking mt and \ndividing it by some quantity  right  so  for all practical purposes i am just using mt  just \ndividing it by a quantity  ok just for now  that should suffice and then  my final update \nrule is going to be this   \nso   let  me  go  over  this  what  did  you  expect  here  in  a  normal  gradient  descent   they \nshould  have  been  grad  w  t   that  means  the  derivative  with  respect  to  current  w   ok  \ninstead of that  i am using a cumulated history  instead of using just this quantity  i  am \nusing a cumulated history  does it make sense  this is same as momentum base gradient \ndescent  how many of you get that  ok and now  this quantity there is nothing new  this \nis the same as what rmsprop suggested that you divide the learning rate by a cumulated \nhistory of  gradients   right   so   just a combination of these two  one is  take care of the \nlearning rate and the other is use a cumulative history  does it make sense now  ok  fine  \nnow  this part is something that i need to tell you about  so  i will tell it to you after i \nrun the algorithm and then  i will come back to that  but is the update rule clear that it is \na combination of momentum plus killing the learning rate ok fine  \n\n \nit is a similar set of equations for bt  \n \n\f\n \nnow   let  us  see  what  happens  to  this  algorithm  is  actually  call  at  adam   it  stands  for \nadaptive moments  right  yeah what is can you tell me why that name  \nwhy moments  \nstudent  sir  mean is  \ngood  where is the mean  here this is a mean  this is a moving exponentially weighted \naverage   right   this  is  an  exponentially  weighted  mean   what  about  this   what  is  this \nquantity   if  you  take  the  average  of  this  is  the  second  moment   right  exponentially \nweighted second moment  right  so  using the first moment and the second moment we \ncome up with an adaptive learning rate  \nso  now i will run this algorithm  are you able to see this  see a coloured curve ok  so  \nit is here you see that now ok do you see what happen  do you see this curve  everyone \nsees  that   ok   so   what  is  happening   it  is  taking  u turns   right   so   again  whatever \nhappens because of momentum  it is happening in this case also and then  finally it will \nconverge again  let me be clear that in this case now it should be very clear  we need to \nchange   who  is  ta  for  the  slide   so   this  colour  needs  to  be  changed  or  it  should  be \nbright  right  from  the  first   so   what  is  happening  is  it  is  getting  overlaid  and  then   it \nbecomes bright when we need to have a brighter colour right from the beginning  ok  \nso  this again in this toy example  right  you do not really see the speed as such because \n \n\fall of them are converging you know almost the same number of steps  but this again i \nrepeat for the toy example  but at least  you see that the behaviour is very different and \nbehaviour  is  consistent  with  whatever  you  have  put  into  the  update  rule   right   in  one \ncase the learning rate gets killed  in the second case it does not decay and in third case \nwhen you using this moments  sorry this momentum term you again have this behaviour \nsimilar to the momentum gradient descent  where you actually overshoot and then   you \ncome back  ok  so  is that clear all these algorithms  ok  now  here is the million dollar \nquestion  \n\n \nwhich of these two you use in practice  so  what are the options that you have for your \nback propagation assignment  even if you have not read the assignment  you should just \ntell me based on whatever you have learned you have gradient descent  \nstudent  momentum  \nmomentum  \nnag rmsprop  \nstudent  adagrad  \nadagrad adam  ok  so  which of these would you choose and if there is one or which is \ncalled eve  but it did not really gain much momentum  but adam  so  in practice adam \n \n\fseems  to  be more or less the default choice   i  should  tell  you that recently  there was a \npaper or called couple of papers which actually show that there is a slight error i mean \nthere  is  you  could  showcase  where  adam  will  not  actually  converge  as  expected  with  \nbut still then after that as is the case in whole of deep learning resources that one person \nsays  this  work  and  immediately  the  next  is  someone  else   this  does  not  work  or  vice \nversa  right  \nso  someone show that this does not work  adam does not work in some cases  but then \nsomeone else did detailed study showing that in most practical applications  ok you have \ntaken a toy data set where  you can show something under some conditions  adam will \nnot converge  but if i look at real world data sets like mnist  image data or something  \nthose conditions do not hold there  so  adam really works well  so  in practice adam is \nmore  or  less  the  standard  choice   nowadays  at  least  all  the  image  classification  work \nwhich deals with convolutional neural networks and convolutional neural networks and \nso on  that uses adam as the optimization algorithm  \nwe  have  used  it  largely  for  a  lot  of  sequence  to  sequenced  learning  problems  and  it \nworks well  although it is supposed to be robust to the initial learning rate  right because \nyou are tampering with the learning rate as you go along  right  you are not sticking to \neta   but  you  are  conveniently  blowing  it  up  or  shrinking  it  based  on  your  requirement  \nso   it  should  not  be  sensitive  to  the  initial  learning  rate   but  we  have  observed  that  at \nleast  for  the  sequence  generation  problems  if  you  use  one  of  these  learning  rates  as  a \nstarting  point   they  work  best  of  course   of  course  these  are  heuristic   right   we  also \ndepends on how much data you have and so on  \nif you are going to train  but only thousand samples and first of all of course you should \nquestion why are you using deep learning  but you have gone pass that question already  \nhas everyone else has  then you are still be using a deep neural network and in that case \nmay be these learning rates are going to be very small  but in general for a large number \nof  data  sets  out  there  which  lot  of  academic  research  happens  which  are  of  reasonable \nsize  these learning rates happen to be well in practice  \nnow  having said that many papers report that sgd with momentum either the nesterov \nmomentum  or  the  vanilla  momentum  with  a  simple  annealing  learning  rate   we \nremember we did this learning rate decay either a constant decay or that heuristic decay \n\fthat after you look at the validation loss and then  decide whether to decay or not  that \nalso seems to work at par with adam  right  so  my advice would be that if you really \nknow what you are doing with sgd and momentum right  that means if you really know \nhow to look at the loss  how to track it  how to adjust the learning rates and so on  \nwith a little bit of manual tampering  it should work as well as adam  there are people \nwhich show that it works well as adam  but if you are just a practitioner who does not \nreally  want  to  bother  too  much  about  setting  the  learning  rate   setting  the  momentum  \nsetting the schedules on both of them  remember for momentum also we had a schedule \nand was just given by one of these papers and it might differ for your application  you \nmight  want  to  tweak  that a bit  so  if  you are not  really bothered about  doing all these \nthings   then  adam  would  just  be  over  all  the  best  choice   right  with  very  minimum \ntempering of the initial learning rate  \nas  i  said  some  recent  work  suggested  there  is  a  problem  with  adam  and  we  will  not \nconverge in some cases  but then it still i mean i would say that juries not out on that yet \nbecause  there  is  of  course  theoretical  angle  to  it  and  also   the  practical  angle   again \npractice has been used  widely  for the last  three to  four  years  at  least  and it  works well in  a \nlarge number of applications  right  so  that is why adam would typically be the overall \nbest choice  \nnow  there is this one thing which i need to do which is i need to tell you why do we use \nthis bias correction  so   now what  do  you actually  want to  you are taking a mean  ok  \nyou do not want to rely on the current estimate of the gradient  but you want to take an \nexponentially moving average of the gradients  \nnow  what would you actually would be doing all this  what is the intuition behind this \nsince  you are talking  about  moments  and so on  can  you think in  terms of probability \ndistributions  so let me just try to say this we write that your gradients  your values of \ngrad wt  right and  i will just i think alternately  use gt instead of grad wt  just needs to \ngradient in that form  it actually comes from some distribution depending on the point at \nwhich you are  right  the gradient would change  but it comes from a certain distribution \nand now  what you actually want at any time step when you are making this update  this \nparticular update ok  is it clear  yeah when you are making this update  what would you \nactually want  it should not move too much away from sorry  \n\fso   now  your  gradients  how  you  are  computing  say  if  you  are  doing  the  stochastic \nversion  you are computing it for every point that you have  right  with respect to that \npoint  you  would  have  some  loss  function  and  some  derivative  with  respect  to  your \nparameters  if you move on to different point  you will have some different parameters  \nso  there is some randomness in this  ok  so  i am saying that these gradients would be \ntreated as random variables which can take on values according to a certain distribution  \nand  now   what  do  i  mean   so   what  would  i  actually  want  when  i  am  making  an \nupdate   so   i  have  to  one  basic  choices   i  could  have  just  use  grad  wt  which  is  the \nderivative with respect to the current time step  add the current time step  ok  instead of \nthat i know why i am not happy with that because it has this problem that it could pull \nme to the extreme  so  at this point is actually saying change it  change your w value in a \nparticular way which is more suited to me some other point would say something else  \nso  what we want is that whatever update we make should be very close to the dash of \nthe distribution mean of the distribution  right and instead of computing the mean  we are \ncomputing a moving average and exponentially moving average  \nso   now  what  do  we  actually  want  to  say  i  said  that  gt  is  the  random  variable  for \ndenoting the gradient  what do i actually want  i want the expected value of mt should \nbe equal to what the true expected value of gt  this is what i want because i want to i do \nnot  want  my  updates  to  move  in  the  extreme   it  should  be  closer  to  the  average  to  the \nmean of the distribution  do you agree that this is my wish list  this is what something \nthat  i  should  desire  for   ok   now   let  see  what  is  mt  actually  if  i  want  to  write  it  as  a \nformula  \n\f\n \nso  i have mt is equal to one minus beta  i will call this as gt  right  so  remember the gt is \ngrad of w t  ok  so  now let us try to write formula for this  so  m zero  i will set it to zero  so  \nm one is going to be one minus beta in to gone  ok  mtwo is going to be beta into one minus beta into \ng one plus one minus beta into g two and m three is going to be beta into one minus beta square g one \nplus beta into one minus  \nstudent  beta square  \nsorry beta square  \nstudent  minus beta   \nminus beta  wait is the first term correct  \nstudent  yes  \nno beta  ok  so  wait what am i  oh beta is getting multiplied  to g two plus one minus beta \ninto g three  so  what is the general formula going to be  it is mt is equal to  so one minus beta \ncan come out  ok  summation i is equal to one to t one minus beta \nstudent  beta square  \nbeta raise to t minus i and gi  right ok  so  this is what my mt is  ok  now  let me take the \nexpectation of this  this fine now  ok this is b one minus beta  now  this is going to be is \n \n\fthat fine  so  what is this  this is an ap gp  what is the sum going to be  \nso  it is going to be one over one minus  oh it is actually sorry one minus beta raise to t over one \nminus beta  is that fine  so  what will happen is  this will get cancelled and what you are \nleft with this one minus beta raise to t into e of gt  ok  so  what is the relation that you have \ne of mt  ok  e of mt is equal to one minus beta raise to t into e of gt  what did you actually \nwant  \nstudent  e of gt  \nright so  now how will you ensure that divide by divide mt by one minus beta raise to t and \nthat exactly the bias correction that we have done  ok  sorry about this messy derivation  \nbut  i guess most of  you  get  it   if not  we will just  type it properly and upload  it in the \nslides  how many of you got this  most of you got  fine  so  that is the similar derivation \nfor vt also  fine  so  that is why we need the bias correction  \n\f"}
{"audio_filepath": "lec004_009.wav", "duration": 601.252, "text": "\n \n \nso  in this video we will try to look at an explanation for why we need bias correction in \nadam  or in other words i want to explain why do i do this particular step why did i take \nm  t  and  v  t  as  it  is   but  why  did  i  do  this  particular  step  which  i  called  as  the  bias \ncorrection step  \n\f\n \nso  note  that  in  the  case  of  adam  if  you  look  at  this  equation  for  m  t  we  are  actually \ntaking  a  running  average  of  the  gradients  and  storing  it  as  m  t  right   so  this  is  the \ngradient  and  we  are  taking  a  running  average  or  exponential  running  average  of  these \ngradients  exponentially decaying running average  \nso  the reason we are doing that is that we do not want to rely on a single estimate  so we \ndo not want to rely only on gradient of w t we want to look at the overall behaviour of \nthe  gradients  over  multiple  time  steps  and  then  take  a  decision   so  that  means   in  one \nparticular gradient at time t is actually pushing us in some direction we do not want to be \nvery hasty and start  moving there we want  to  accumulate the history and  appropriately \nweigh  everything  in  the  history   that  is  the  idea  behind  taking  this  running  average  of \nradiance  \nand  the  other  way  of  looking  at  is  that  we  are  interested  in  the  expected  value  of  the \ngradients and not the point estimate at time w t right  at time t rather so gradient of wt \nwhich is this quantity which is the point estimate at time t  we are not interested in that \nwere  interested  in  the  expected  value  and  our  behaviour  should  be  according  to  the \nexpected value that is what we desire  \nso however  instead of computing the expected value of this quantity which should have \nbeen  ideal   we  are  computing  mt  as  the  exponentially  moving  average   so  in  the  ideal \ncase we would want that these two quantities are the same that the expected value of mt \n \n\fthe way  i am computing it and the expected value of the gradient of w t  should be the \nsame   if that is the same then  i  am  fine because  then   that means   i  am  just taking the \nexpected value or the  of the gradient instead of relying on the point estimate ok  so  let \nus see if that is indeed the case  \n\n \nso   for  convenience  we  are  going  to  just  denote  this  gradient  w  t  as  g  t  because  it  is \ncumbersome  to  write  this  grad  symbol  and  we  will  just  not  make  it  so  readable  the \nderivation that we are going to do  so i am just going to replace that as g t so what i have \nwritten is g t here instead of grad w t right  so from now on i will just use g t for grad w t \nis that fine ok  so we have this expression for m t  \nso  now let us just try to expand it and see what happens right so m zero it is going to be zero \nbecause that is my starting points i have no history nothings  so i will just going to keep \nit  as  zero   m  one  is  my  first  time  step  at  which  it  is  going  to  be  beta  into  m  zero  so  i  am  just \nsubstituted  t  minus  one  and  t  here   and  in  the  original  expression  i  have  just  substituted \nappropriate quantities for m of t minus one and g of t  so m of t minus one is zero m zero and g of t \nis g one and of course  b zero m zero itself was zero  so what will be left it is one minus beta g one  \nnow  let us look at what happens is m two  m two is going to be beta m one plus one minus beta g \ntwo  but i already have an expression for m one  so i am just going to substitute that here and \nthis is what i get  now let us look at m three  m three is again going to be beta times m two plus one \n \n\fminus beta times g three and  i have an expression for m two so i am going to substitute that \nhere and see if that leads to something interesting  \nso  i have just substituted the value of m two here right and i already had the m three part here \nthe  this term here as it is ok  and now let us see so this already starts looking something \ninteresting  you  see  some  pattern  here   in  particular  we  could  take  these  one  minus  beta \nterms outside they can be taken common and then you will be left with beta square g one \nplus  beta square  g  one  plus beta  g  two plus  g  three  so  let us try to  write this more compactly \nright   so  i  have  taken  one  minus  beta  common  and  then  i  have  written  the  remaining \nterms as this particular summation and you can verify  \nso  when i is equal to one this is going to be beta three minus one which is beta square into g one  \nwhen i is equal to two this is going to be beta three minus two which is going to be beta into g two \nand when i is going to be three this is going to be beta raise to three minus three which is beta raise \nto zero which is just one into g three right  so we get back the same expression that we had here \nof course  there is a one minus beta outside  so this is a more compact way of writing it and \nthis was for the threeth entry right this was for m three  the third entry  \nnow  what if we want to write it for the t\u2019th entry in general  what if we want to write the \nexpression for m t  \n\n \n \n\fso  in general m t we can write it as one minus beta as i equal to one to t b beta t beta raised \nto t minus i into g i right  so this three is here i have just replaced them by t s right you can \njust verify that this is from you can just generalize from the third entry to the t\u2019th entry  \n\n \nso now  let us see we have the following expression we have simplified the expression \nfor  m  t  and  written  it  more  compactly   but  what  we  were  eventually  interested  in  the \nexpected value of m t right  we wanted to show that certain things holds for the expected \nvalue of m t  \n\n \n \n \n\fso  you just take expectation on both sides so this is what we will get ok  now one minus \nbeta  is  of  course   a  constant  so  i  can  move  it  outside  the  expectation   so  then  i  get  an \nexpectation of a sum  \nnow  the expectation of a sum is the same as the sum of expectations  so i can write it as \na sum of expectations ok  now again beta is a constant so i can take it outside the expect \nexpectation  so what i will be left with is beta raise to t minus i outside and expectation \nof g i right  so this is actually expectation of g one when i equal to one  then expectation of g \ntwo  expectation of g three and so on  \nnow   we  will  make  an  assumption  that  all  these  gi\u2019s   that  means   the  gradient  at  time \nstep one   the gradient at  time step  two  the  gradient  as time step  three and  so on they  all come \nfrom  the  same  distribution  ok   we  are  going  to  make  that  assumption  so  let  us  try  to \nunderstand the implication of that right  so let us say this was a distribution from which \ng  one  came  right  suppose  i  am  dealing  with  a  scalar  quantity  and  maybe  this  was  the \ndistribution  from  which  g  one  came   now  g  two  could  have  come  from  a  different \ndistribution  g  three could  have come from  a different  distribution and if that was  the case \nthen expectation of g one would be different from the expectation of g two and so on  \nso  what we have assumed to it will make things simple for us is that g one  g two  g three any g i \ncomes from the same distribution and hence you can say that the expectation of all these \ngi\u2019s is going to be just the expectation of g  that is this one single distribution from these \nwhich these entries come this of course   a very strong assumption  but we are going to \nlive with this assumption  \n\f\n \nso then this expectation of g i just becomes expectation of g  so i have gotten rid of the \nindex i  that means  i can move it outside the summation right so this is what i will get \nnow  these two have come out of the summation and inside i have this quantity  now let \nme just expand this quantity this is nothing but beta raise to t minus one plus beta raise to t \nminus two plus so on at last you will reach t minus t which is just going to be beta raise to \nzero  \nso  this is nothing but a sum of a g p with common ratio beta and i can replace that sum \nby this  formula   you know this is  the formula for the sum  of a  g  p with  common ratio \nbeta  so i have just replaced that and now what happens is this one minus beta and one minus \nbeta cancel out  so i get this particular expression that the expected value of m t is equal \nto the expected value of g into one minus beta t  \nso   i  will  just  take  one  minus  beta  t  on  the  other  side  and  i  can  move  it  inside  the \nexpectation because it is a constant it does not matter  so i will get as oh actually yeah i \ncan just move it inside so i will get it as expectation of m t over one minus beta is equal to \nexpectation of g t right and this  quantity the one which i have circled is nothing but m \nhat  t  right  this  was  exactly  the  bias  correction  that  i  was  applying   if  i  go  back  to  the \nprevious slide or the slide before that  so this was exactly the bias correction that i was \napplying  \n \n\fso  what i have inside is this  so what i have shown is that if i apply the bias correction \nthen  the  expected  value  of  the  bias  corrected  m  t  is  equal  to  the  expected  value  of  the \ngradient and that is actually what i wanted  i wanted that whatever m t  i am computing \nif i look at its expected value it should be the same as the expected value of my gradients \nand that is what i have arrived it  \nhence  this  bias  correction  makes  sense  and  hence  we  apply  this  bias  correction  for \nadam  so this we have shown for m t  we had a similar expression for v t right  so for m \nt  we  had  this  bias  correction  as  m  hat  t  and  similarly  for  v  t  also  we  had  this  bias \ncorrection as v hat t so you can derive the same kind of derivation for v t also and show \nthat that bias correction makes sense right  so this is an explanation for why you do bias \ncorrection in the case of adam   \nthank you  \n\f"}
{"audio_filepath": "lec005_001.wav", "duration": 1058.7219375, "text": "\nanalysis  singular value decomposition \nso  this lecture actually is a bit of a digression  and it is supposed to cover some of the \nbasics that we need for various sections of the course  so  it is very important that you \nunderstand some concepts for linear algebra specifically eigenvalues  eigenvectors and in \nparticular  today we will do principal component analysis  and the reason that i do it is \nthere is an very neat relation of pca and to autoencoders  an autoencoder is something \nthat well cover in the course  it is a part of any deep neural network course  \nand  singular  value  decomposition  is  something  that  we  using  when  we  learn  word \nvectors  the word vector is again something very important  i can just i can do the non \nsvd version of it where i just talk about what word to wick is  but that will not give you \nthe same probably not the same interpretation as if you start from svd and then reach \nword vectors  right  so  that is why i am covering these basics  \nso  how many of you know eigenvalues and eigenvectors  very embarrassing question \nhow many of you absolutely hate eigenvalues and eigenvectors  so  let us see if we can \nchange that today  i mean on the positive side   \n\f\n \nso  what happens when a matrix hits a vector  so  most of you a lot of people that i talk \nto right actually think that eigenvectors are the villains of linear algebra  it is very hard to \nunderstand  them  and  so  on   but  today  i  am  going  to  make  a  case  for  they  are  not  the \nvillains they are actually the superheroes of linear algebra  so  that is what the lecture is \nabout  so  what happens when a matrix hits a vector  \nstudent  transforms it  \ntransforms it right  so  actually what happens is that it strays from it is path  so  this is \nthe original \n this is the original vector x ok and now once i multiply \nit  by  a   that  means   if  i  do  the  transformation  a  x  then  i  get  a  new  vector   and  two \nthings happen right  one is the direction changes which is obvious  and in many cases the \nscale  also  changes   that  means   the  vector  might  get  elongated  it  is  magnitude  would \nincrease or it would decrease   \nso  if you really think about it actually right  so  matrices are the real villains of linear \nalgebra right  and we just look at this vector was minding it is own business going along \nit is own direction a metric comes and hits it and completely changes it is world right  i \nmean  it just throws it off path increases a dimension or slows it down or whatever it  so  \nthat is they are the bad guys now for every villain what do you have a super hero right  \nso  what is a super hero corresponding to orbit  what does a super hero do  know that \n \n\fis a very linear algebra  i am talking about comic books that this is very linear algebraic \nanswer he stands up to the villain right  \n\n \nand that is exactly what eigen vectors do it right  they refused to change their part they \ntell the matrix  you can hit me as many times as you want probably you can increase my \nyou  could  probably  slow  me  down  a  bit  or  push  me  ahead  or  something   but  i  am  not \ngoing to stray off from your path right  so  that is what eigenvalue eigenvectors do  \nso here is a matrix  which is a villain and here is an eigenvector which is our hero and \nnow when this matrix hits this eigenvector it refuses to stray from it is part right  it says \ni will move forward i will move back whatever  but i will not change my direction ok  i \nwill  just  stay  honest  to  what  i  am  and  these  vectors  are  called  the  eigenvectors   i  am \nmore  formally  you  can  write  it  as  ax  is  equal  to  lambda  x  right  so   that  means   the \ndirection  remains  the  same  only  the  scale  changes  it  will  either  get  slowed  down  or  it \nwill get boosted up right  so  the magnitude would change  but the direction remains the \nsame  \n \n\f\n \nnow  what is so special about eigenvectors  like why are why is it that  they are always \nin  the  lime  light   i  know  the  any  course  that  you  do  invariably  touch  eigenvectors  or \neigenvalues  at  some  point  in  that  course  right   where  be  it  machine  learning   image \nprocessing whatever you do you always speech everything that you do  you will always \nhave eigenvectors and eigenvalues  why is it so  well it is turns out that several properties \nof matrices can actually be explained away by looking at their eigenvalues  so  if i look \nat  a  matrix  i  would  probably  not  be  able  to  comment  much  on  it   but  if  you  tell  me \nsomething about the eigenvalues  \ni  can  see  a  lot  of  things  about  of  it  and  there  is  an  entire  field  on  this  way  this  entire \nspectral  graph  theory  which  looks  at  properties  of  laplacian  matrices  and  come  in \nsomething  on  the  properties  of  the  graph  and  so  on  right   and  that  is  just  an  example \nwhich we do not care about  but what we care about in this course there are a few things \nthat  we  care  about  with  respect  to  eigenvalues  and  eigenvector   and  that  is  what  i  am \ngoing to focus on right  so  that is what this lecture is going to be out  and i will take two \nspecific  cases  which  are  very  important  for  us  to  understand  certain  concepts  later  on  \nso  i will start with the first one  \n \n\f\n \nand  i  will  start  with  a  very  simple  example  to  motivate  this  problem   and  eventually \nwill  lead  to  a  result  which  will  help  us  understand  a  very  important  concept  in  deep \nneural  network training  which is  exploding and  vanishing  vanishing  gradient  we  will \nnot  touch  that  concept  today   but  we  will  use  these  ideas  when  we  are  looking  at  that \nlater on  \nso   let  us  take  this  example  of  two  restaurants   so   there  is  a  chinese  restaurant  and  a \nmexican restaurant   and on day one k one students eat in the chinese restaurant and k two \nstudents eat in the mexican restaurant  so  this is what my situation is on day zero  k one for \nchinese and k two for mexican  now what happens as is obvious people get bored or they \nhave different want to try out different things  so  on day two or other each subsequent \nday what happens is that  a fraction p of the students who ate chinese today will opt for \nmax  mexican   on  day  on  the  next  day  and  a  fraction  q  of  the  students  who  ate  ma \nmexican today are going to opt for chinese  \nso  you get this situation right  so  i started with k one  k two  so  what i am saying is on day \none that is the next day only a fraction p of the k one students will remain for chinese and a \nfraction one minus q would be transferred from mexican to chinese ok  and similarly only \na fraction q of the students would again stick to the mexican food and a fraction one minus \np into k one would shift from chinese to mexican is this setup clear ok  can you write this \n \n\fas a matrix operation it would be a matrix multiplied by a vector right can you tell me the \nvector  \nstudent  \n  \nk one k two k one k two and the matrix is in all this ok  this is what it is  and i am saying that this \nhappens on each subsequent day  it is every day now this keeps happening  so  on day one \ni started with say one hundred and eighty and now day two it change to something again day three it will change \nsomething by the same fraction  \nnow   let  me  call  this  as  matrix  m  and  this  is  of  course   v  zero  right  by  definition  as  we \ndecided now what would happen on day two what would v two be m applied to v one right and \nwhich would be m square applied to v zero  i am just substituting the value of v one which is \nm into v zero in general on the nth day what would happen m raised to n into v zero ok  so  \nyou see that the number of customers in the two restaurants is given by this series you had \nv zero then m into v zero then m square v zero and so on up to m raised to n vn ok  you see how \nthe number of customer is changing  \nnow  and this is how i represent it as a state transition diagram  right  so  i had certain \nnumbers on day one and it changed with the trans with the probability p they will stay back \nwith a probability one minus p they will move to the next or the different restaurant and so \non right  \n\n \n \n\fand now this though a very toyish example can you relate it to many things in real life or \nmany things that you will take in decision making right that you are  so  even if you are \nplaying a game for example  and even if you are playing atari games or something  you \nare in a certain state based on some action that will take will move to a different state and \nso  on  right   so   these  things  happen  in  various  real  world  applications  right  there  is  a \ncertain state for example  even in stock market prediction   you are at a certain value of \nfish stock it might change to a different value right and these values you could just say \nthem as high low or neutral that i am not going into the actual numbers  \ntoday the stock value is high it does it possibility that it will transition to something low \nand so on right  so  these kind of state transition diagrams occur in various real world \nexamples   now  this  is  a  problem  for  the  two  restaurant  owners  right   why  is  this  a \nproblem for the two restaurant owners  they do not know how much food to make  but \nevery  day  the  number  of  customers  is  changing  right   but  is  the  number  of  customers \nactually changing  will the system eventually reach a steady state  will it is it obvious \nthat it will reach a steady state or maybe it will not even reaches steady  but the way i \ndescribe  it  i  do  not  see  why  it  should  reach  a  steady  state  right  you  have  some  people \nhere they go there come back go there and so on  \nthe only thing which i have assumed is that the transition matrix which was the matrix \nm is constant across all the time steps right  so  every day it is at the same priorities by \nwhich  things  are  changed  right   so   what  is  your  guess  if  i  were  to  ask  you  to  take  a \nguess ok  let us see how many of you think and it is there is no correct answer here at \nthis point  so  just tell me how many of you think it will reach a steady state  how many \nof you think it will keep changing and why is the sum never equal to one ok  so  fine so it \nturns out that they will right and let us see how   \n\f\n \nso  we will define some things and some of these are just definitions some of them have \naccompanying  proofs   which  i  am  not  going  to  do  here  you  can  the  proofs  have  been \nlinked from the slides  so  you can take a look at them if you are interested  \nso  suppose there is a matrix a n cross n matrix which has eigenvalues are lambda one \nlambda two up to lambda n  now what this definition is saying is that  assume that there is \none eigenvalue which is greater there is no assumption actually the eigenvalue which is \ngreater than all the other eigenvalues is called the dominant eigenvalue  and when i am \nlooking at a dominant eigenvalue i am only concerned with the magnitude not the sign  \nso  it could be that an eigenvalue is minus ten and all the other eigenvalues are one two three four five  \nso  the dominant eigenvalue would be minus ten right and i will just take it as step is that \nclear the definition of a dominant eigenvalue  \nnow   how  many  of  you  know  what  is  the  stochastic  matrix   so   matrix  m  is  called  a \nstochastic  matrix   if  all  the  entries  are  positive  and  the  sum  of  the  elements  in  each \ncolumn is equal to one  so now  this definition is again slightly misstated  so  there is a row \nstochastic matrix the column stochastic matrix and also doubly stochastic matrix  right  \nso  what i am talking about here is a column stochastic matrix like our matrix have you \nseen  such  a  stochastic  matrix  any  time  in  your  life  in  the  last  five  minutes  the  m  matrix \nright   so   the  m  matrix  is  a  stochastic  matrix  because  the  sum  of  the  columns  was  one \n \n\fright  you had p one minus p q one minus q ok or was it some of the rows was one rows was one is \nit the columns  \nso   this  is  a  stochastic  matrix  just  a  definition   now  i  combine  these  two  definitions \nwhich is  dominant eigenvalue and stochastic matrix and give you a theorem  right  so  \nthe largest dominant or the dominant eigenvalue of a stochastic matrix is equal to one ok  \nso  to prove this  what do i have to prove  so  i need to prove two things one that one is an \neigenvalue of this matrix of any stochastic matrix and second all the other  eigenvalues \nare less than one  so  that is exactly what this proof does here you can take a look at it and \njust to give you a heads up  so  last year i use to do this that please see the proof go back \nand look at the proof people never look at the proofs  \nso  i used to ask them in the quiz where i should be sure that people not going to answer \nright  so  please when i say go back and look at the proof do that ok  so  and lastly if a \nis an n cross n square matrix and you have this series a v zero a square v zero up to an vn  \nthen this series will converge to the dominant eigenvector of a  what does a statement \nmean  let us not get into the proof right what does it actually mean ok  so  let us start \nwith very basic stuff right what is the series actually  what is each element in this series \nit is a vector  it is a vector everyone gets that every element in the series is a vector  \nnow  what  do  i  mean  that  a  series  of  vectors  converges  to  the  dominant  eigen  vector  \nwhat is convergence mean  if i keep finding the next element next element next element \nof this series and i keep doing this as long as i can  i will reach a value n right where n is \nthe nth element in the series which will just be a multiple of the dominant eigen vector is \nthat clear  you not seem to be clear everyone gets that  \nso   what  do  you  mean  by  if  you  take  a  series  of  numbers  and  if  i  say  that  the  series \nconverges to zero  what does that mean  if you keep finding the next element in the series  \nyou will hit a point n where you find the nth element of the series and it will be zero  refer \ntime  thirteen twenty  that ok  so  we will just i will leave it at that for now  now so stochastic \nmatrix dominant eigenvalues the connection between two and the convergence theorem for \na series of vectors which is a v zero a square v zero and so on  \n\f\n \nnow  let ed be the dominant eigen vector of m where m is a dash matrix in our case it is \na stochastic matrix  so  what with the corresponding dominant eigenvalue be  \nstudent  one  \none  ok  so  given  the  previous  definitions  and  theorems   what  can  you  say  about  the \nsequence  it converges to a dash of ed  \nstudent  \n  \na  multiple  of  ed  right   so   there  exists  an  n  such  that  the  a  length  nth  element  of  the \nseries  which  is  given  by  this  is  going  to  be  equal  to  some  multiple  of  the  dominant \neigenvector no  no  k is some multiple no this is not related to eigenvalues yet just wait \nfor the next statement  then you will see the difference that this is not the do eigenvalue \nyet  \nnow  my question is what happens from here onwards  what would be the next element \nin the series  how many of you say some k dash into ed  what is the other pause i do not \nhave the other option what is the other option  \nstudent  k into ed  \nk into ed how many of you say k into ed  a large number of ok so  you see that now just \nnotice the eigenvalue will come up right  so  at step n plus one you would have m into vn \n \n\fwhich  is  m  into  k  into  ed  and  this  quantity  is  actually  one   so   the  theorem  says  it  will \nconverge  to  some  multiple  of  k  and  now  if  it  is  a  stochastic  matrix   what  will  happen \nafter that time step  it will just remain the same vector  \nso  what would happen to the number of customers in the two restaurants it will remain \nthe  same  right  you  get  that  ok  fine   now   this  was  all  for   what  kind  of  matrices  \nstochastic matrices square stochastic matrices \n\n \nbut we generally care about any square matrix  in fact  we should care about any matrix \nnot discriminate  but any square matrix will do for now  so  for a square matrix let p be \nthe time step at which this series approaches a multiple of the dominant eigenvector  \nthe  theorem  was  for  any  square  matrix   remember  it  was  not  for  stochastic  square \nmatrices   we  just  use  this  value  that  for  a  stochastic  square  matrix  the  dominant \neigenvalue  is  one   which  it  need  which  leads  to  that  neat  result  that  the  num  then  the \nnumber of customers just becomes constant  but for any square matrix  i could write it as \nthis  that there  exist  some  step  p  at  which  the  element  of  the  p\u2019th  element  of  the  series \nwould just be a multiple of the dominant eigenvector  \nnow  what would happen at step p plus one  is this fine  what about step p plus two  and in \ngeneral at p plus k or p plus n everyone gets this  so now  can you tell me what does this \n \n\fknowing  this  dominant  eigen  value  tell  us  about  this  series   when  will  it  stabilize \nactually  \nstudent  \n  \nwhen lambda is equal to one that is the case we already saw if the dominant eigen value is \ngreater than one  what would happen  \nstudent  \n  \nseries will explode the series will explode and if it is less than one what would happen \nthe  series  will  vanish  ok   so   this  is  an  important  result  that  we  will  use  when  we  are \ndiscussing exploding and vanishing gradients  \nso  we will see that in the case of something one as a recurrent neural networks  you end \nup with something of this sort and then  i will make some comments on that  right  so  \nthat is why we will be using this will come probably six  seven or maybe more lectures down \nthe line ok  but we will be using it at this point  so  the main result from here is that if the \ndominant eigenvalue  this should be lambda d is greater than one  then it will explode less \nthan one it will vanish and equal to one it will stabilize  so  that is one result one important \nproperty  of  eigenvalues  and  eigenvectors  that  well  be  needing  at  a  later  point  in  the \ncourse   \n\f"}
{"audio_filepath": "lec005_002.wav", "duration": 667.489, "text": "\nlinear algebra   basic definitions \nnow  from here on we will go on to something even more basic  we will start defining \nsome basic definitions from linear algebra  and these are again important  for something \nthat i need in the next lecture  so  let us start with this   \n\n \ni mean  in the process we all just see  why the eigenvectors are important for us in this \ncourse   \n\f\n \nso  how many of you know what a basis is  so  a set of vectors belonging to r n is called \na basis  if they are linearly independent right  and every vector in r n can be expressed \nas  a  linear  combination  of  these  vectors   so   a  set  of  n  vectors  vone  to  vn  is  linearly \nindependent   if  no  vector  in  the  set  can  be  expressed  as  a  linear  combination  of  the \nremaining n minus one vector  so  a more weird we are stating it that  so  that everyone get \nconfuse is that if you take this linear combination  \nthe only solution to this is all the ci\u2019s is equal to zero and that make perfect sense right  that \nis  that  same  as  that  linear  combination   linear  independence  and  all  that  thus   is  make \nsense to everyone ok  so  what does linear independence mean that any vector from this \nset cannot be expressed as the linear combination of the other set  other vectors in the set \nand  a  more  formal  way  of  saying  that  is  this   everyone  gets  this  what  is  linear \nindependence   \n \n\f\n \nnow  let us consider some very stupid examples  again  the space r two and we consider \nthese  two  vectors  one  zero  and  zero  one   are  they  linearly  independent   yes   ok  they  cannot  be \nexpressed as a multiple of each other right  now  any vector ab belonging to r square \ncan be expressed as a linear combination of these two vectors ok  and x and y are linearly \nindependent the only solution is c one x plus oh sorry  c one x plus c two y is c one and y equal to \nzero  what about if i move to r three one zero zero zero one zero and zero zero one  so  x y and z axis right are the unit \nvector   \n\n \n \n \n\fso  in that x and y turns to be unit vectors in the direction of the coordinate axis  and we \nare used to representing every point in r two as a linear combination of these two vector is \nthat exactly what  i what we do  so  when we say that  i have a point two comma three  i am \nactually  telling  you  that  the  point  is  two  one  zero  plus  three  zero  one  right   i  am  expressing  at  are  the \nlinear combination of the coordinate axis   \n\n \nbut now this nothing sacrosanct about x and y right  i could have chosen just about any \nother axis  so  in particular we could have chosen this as our basis are these two vectors \nlinearly  independent  can  any  vector  and  r  two  be  expressed  as  a  linear  combination  of \nthese two vectors sure  so  i give you a vector a b  how do you going to express it as a \nlinear combination of these two vectors  so  you will do it this way right  how will you \nfind that values are the x one and x two  so  other linear system of linear equations right  \n \n\f\n \nso  this is what you will do  i know all are good in doing this and what do we actually do \nwhen we do this  what is the algorithm that we use  how do we solve this  what is the \nalgorithm that you use solving this  \nstudent  gaussian elimination \ngaussian elimination right  in two variables of course we do not call it an algorithm  that is \nwhat  we  did  in  eight  standard  or  something   but  when  we  come  to  engineering  we  call  it \ngaussian elimination right  so  the same algorithm  \n\n \n \n \n\fso  in general given a set of linearly independent vectors  we can express any vector that \nbelonging  to  rn  as  a  linear  combination  of  these  vectors  right   i  can  say  z  is  equal  to \nalpha one  u one plus alpha two u two and so on given alpha one to alpha n are linearly independent \nok  so  that means  any vector in rn can be expressed using these vectors which form the \nbasis of rn does that make sense  that is why call the basis vector because  anything else \nthese  are  the  fundamental  vectors  using  these  anything  else  can  be  expressed  in  that \nspace it is that clear  \nso  this is how it will be  how do i write this in matrix notation a  there are lot of these \nand these thing i do not really understand what you mean by that  yeah good  so  this is \nwhat you mean  so  that we writing same in matrix notation and now this is again a dash \na system of linear equation there was a lot of space to fill and one dash good  so  system \nof linear equation and again you can solve them using  \nstudent  gaussian elimination \ngaussian elimination  what is the complexity of gaussian elimination  let us see options \nright n n square n cube  fl   n cube right the gaussian elimination the complexity is o n \ncube  right   and  i  am  not  doing  all  this  just  to  the  sake  of  time  pass  i  have  a  point  of \nmake  which  i  will  make  on  the  next  slide  right   so  now   this  was  for  any  basis   that \nmeans  if you have any n linear independent vectors  \nnow  i will consider a special basis where instead of n linearly independent vectors  in \naddition   these  vectors  are  also  orthogonal  ok   orthogonal  vectors  are  linearly \nindependent ok  so  a set of orthogonal vectors are linearly independent  but the converse \nis not all this right  \n\f\n \nso now  let us see what if we have an orthonormal basis  that means  a basis consisting \nof orthonormal vectors   so  orthonormal is combination of two words  ortho means  the two \nvectors are orthogonal  and normal  means  all the vectors are unit vectors  that means   i \nam normalized them by their magnitude  \nso  what is the condition that holds ui transpose uj is equal to zero  if i is not equal to j and \nui transpose ui is equal to one  ok now  what happens in this special case  so we have this \nagain we can express any vectors z as a linear combination of these  now  let me try to \ndo  this  i  am  just  pre  multiplying  this  equation  by  u  one  transpose   what  happens  on  the \nright  hand  side  everything  disappears  all  of  the this  terms  will  disappear  because   they \nare of the form ui transpose uj where i\u2019s not equal to j and the first term is  \nstudent  one  \none  so what remains alpha one  so  you can directly find alpha one using a dot product of two \nvectors  what  is  the complexity of this operation n th is  just n products  ok  now how \nmany such alphas do we need to find  \nstudent  \n  \nn  of  those   so   what  is  the  complexity  n  square   so   that  is  now  you  see  why  an \northonormal  basis  is  a  very  convenient  basis   you  can  get  all  these  coefficients  just  by \ndoing a dot product between two vectors and later on i will show you that you  might not \n \n\fneed all of these  you might just need some subset k of these right so  that means  you \njust  do  k  of  these  dot  products  and  get  these  values   so   do  you  now  understand  the \nmeaning  of  what  is  why   why  do  i  say  it  is  orthonormal  basis  is  the  most  convenient \nbasis that you can hope for right  \nso  the another way of looking it right at it is again just to make few more comfortable \nwith  vectors  and  projections  and  so  on  right   so   this  was  your  original  point  z  one   z \nwhich is a comma b right  and how do you actually draw that vector that  this is a and \nthis is b ok  so  how do you find the coordinates actually you projects on to your basis \nvectors which were these x and y vectors  that is how you found the components along \nthose the coefficient along those  \nnow  instead of this x and y  i have any other set of vectors which is u one and u two and i \nwill do the same thing i will project this on to uone ok  i will project this on to u two and that \nprojection will give me alpha one and alpha two right  so now  what is alpha one and that sense \nthis is z  this is alpha one and this is theta right  so  alpha one is equal to z into cos theta ok  \nand what this cos theta  so  again you arrive at the same thing fine  \nso  essentially taking a  projection of a  vector on to  your basis is this  fine to  everyone  \nthere is just to difference arriving at the same formula  that alpha is are given by a dot \nproduct between the basis vector and your original vector  \n\n \n \n\fso  an orthogonal basis is the more convenient basis that  you can hope for  that is the \npoint  which i wanted to have you are convinced about that  \n\n \nnow   but  what  does  any  of  this  have  to  do  with  eigenvectors   i  started  off  with \neigenvectors   i  proved  one  property  there  and  then  i  came  to  this  linear  algebra  basic \ndefinitions and what a basis is set of linear independent vectors  and i eventually showed \nyou that  an orthonormal basis is the most convenient basis that you can hope  so  what \ndoes any of this have to do with eigenvector   \nstudent  \n  \nalways for us  square symmetric matrix right  why do you care about square symmetric \nmatrix  not sure yet  so  we get to that  so first of all  it is turns out the eigenvectors can \nform a basis and this is for any matrix  so  the eigenvectors of a matrix having distinct \neigenvalues are linearly independent  \nso  does every matrix  if i have an n cross n matrix will it have n  eigenvectors  no  it \ncan have less than or equal to eigenvector depending on the \n  so  what \nis this saying is that if these eigenvectors are having distinct eigenvalues ok  then these \neigenvectors  would  be  linearly  independent   fine  ok  and  turns  out  that  for  a  square \nsymmetric  matrix  that  is  the  even  more  special   the  eigenvector  of  a  square  symmetric \nmatrix are  \n \n\fstudent  orthogonal  \northogonal  right  and  we  already  know  that  orthogonal  is  good  right   so   remember  \nwhen we have orthogonal we do not really care about orthonormal  because  that is it is a \nsimple operation  if you have a set of vectors u one u two u three which are orthogonal  you can \njust divide them by the magnitudes and just get a set of orthonormal vectors  right  so  \northogonal and orthonormal  i will use it interchangeably ok  and  whatever i done thus \nthey  form  a  very  convenient  basis   so   the  eigenvectors  of  a  square  symmetric  matrix \nform a very convenient basis  \n\n \nso  that is how i connect the parts which was about the eigenvectors to the second part  \nwhich  was  about  basis   and  why  would  we  want  to  do  this  and  we  already  we  had  a \ncoordinate axis that is the very good basis one zero zero zero zero one zero zero one and n dimension similarly  \nso  why should i want to use the different basis  i have said that eigenvectors is a very \nconvenient basis  but why do i care about it  i already have a very  very convenient basis \nwhich  is  just  these  one  or  two  vectors  are  along  these  directions  right   so   why  do  i  care \nabout a different basis  i understand that i that is there somewhere  but something more \nthan that that is one advantage which i will talk about  what else more interesting  ok  \nin  what  sense  i  love  the  power  which  comes  with  my  job  right   that  you  give  a  right \nanswer and still i can embarrass you know  so  that is correct actually   \n \n\f"}
{"audio_filepath": "lec005_003.wav", "duration": 574.98, "text": "\nlecture \u2013 six \nin  this  module  we  will  study  eigenvalue  decomposition   so   the  answer  to  that  was \nactually  which  i  was  hoping  all  of  you  will  give  because  all  of  you  have  done  two \nprerequisite   which  is  linear  algebra  and  machine  learning   both  of  them  teach  you \nprincipal component analysis  so  i was hoping that you will give that answer  \nnow   can  you  give  that  answer  he  already  of  course   gave  that  answer   is  that  make \nsense  so  we relate it to that so  but before going to principle component analysis  we \nlook at eigen value decomposition  \n\n \nthis is very straightforward  so  let uone to un be the eigenvectors of a matrix a and let \nlambda one to lambda n be the corresponding eigenvalues  \nnow  i am going to construct a matrix u  such that the columns of u are these vectors uone \nto un  is that fine  what u looks like  and now i am going to do this product i am taking a \nthe product of the matrix a with the product of with the matrix u  where u is this right  \n\fit is the all the eigen vectors tagged one after the other is this fine  the next step i am \njust pushing the matrix inside  if you know the four different ways of multiplying a matrix \nyou will know that this is correct  or else for now just thing that you can just push the \nmatrix inside  \nnow   what  is  this  i  can  replace  them  by  the  lambda  one  u  one  lambda  two   because  a  u  one  is \nequal to lambda one u one by definition ok  now can you write this again as a product of two \nmatrices  one is of course  the matrix u and the other is  \nstudent  diagonal  \ndiagonal  so  the diagonal matrix will come first or the matrix u will come first  how \nmany if you say u will come first  how many if you say the diagonal matrix will come \nfirst  the sum is never one ok  \nso  it is going to be like this ok  and you can write this as u lambda  so  u is again the \nvector  the  matrix  containing  the  eigenvectors  of  a  and  lambda  is  a  diagonal  matrix \nwhere every diagonally element is a corresponding eigen value  \n\n \nnow  this is what we have so far a into u is equal to u into lambda  now suppose u \ninverse exists  i will assume that u inverse exists and later on i will tell you under what \nconditions it exists  then i could write it as this any of these two forms in  one case  i am \npost multiplying by u inverse in the other case i am pre multiplying with u inverse ok  \n \n\fso   this  is  known  as  the  eigenvalue  decomposition  of  a  matrix   and  the  other  way  of \nwriting it is known as diagonalization of the matrix right  you take a matrix apply some \noperations to it so that  the result is a diagonal matrix is this clear to all of you is very \nstraight  forward  ok   and  again  eigen  vectors  play  an  important  role  in  this   now  the \nimportant  question  is  under  what  conditions  would  u  inverse  exist   u  inverse  would \nexist if the columns of the matrix u are  \nstudent  linearly independent  \nlinearly  independent   ok   do  we  know  the  columns  of  the  matrix  are  linearly \nindependent  \nstudent  yes  \nyes  because it is a  \nstudent  \n  \nset  of  eigenvectors  and  we  already  saw  the  proof  that  the  eigen  vectors  are  linearly \nindependent  ok   this  just  follows  whatever  i  say  ok   now  do  we  need  proof  for  this  i \nslide nineteen we did this  i did not realize it fine  \n\n \n now if a is symmetric the situation is always more convenient  why is it  \n \n\fstudent  \n  \nwhat would u be  \nstudent  orthogonal matrix  \nwhat is an orthogonal matrix actually  \nstudent  \n  \nso the eigenvectors are orthogonal  so  we have this situation right  suppose i want to \ndo  u  transpose  u  ok   this  is  how  that  operation  would  look  like  ok   now  what  is  the \nij\u2019th entry of the resultant matrix   \nstudent  dot product  \nit is the dot product between the  \nstudent  ui and uj  \n ui and uj  everyone gets this right  the ijth entry of this product is  going to be the dot \nproduct between ui and uj  this dot product would be dash if i is not equal to zero or j  \nstudent  j  \nj and there is no point in this  so  each cell of the matrix q ij is given by the dot product \nand it is going to be zero if i not equal to j  and it is going to be one if i is equal to j ok  so  u \ntranspose u is equal to the identity matrix  that means  u transpose is the dash of u  \nstudent  \n  \ntranspose of u and of course  inverse also ok  so  u transpose is the inverse of u  and it \nis very convenient to calculate what is the complexity of inverse  so now  you appreciate \nthat  that  is  a  that  has  high  complexity  and  in  this  case  if  the  vector  if  the  matrix  is \northogonal  that means  it is a collection of orthogonal vectors and the inverse just comes \nfor free right  \n\f\n \nso now given this situation  and do not read the hint as if this is going to help  but yeah  \nwhat can you now say about the sequence  the same sequence that you saw earlier  so  i \nhave  given  you  that  the  evd  of  a  is  equal  to  u  sigma  u  transpose   where  u  is  the \ncollection  of  the  eigenvectors  and  sigma  is  the  eigen  values  the  diagonal  matrix \ncontaining the eigenvalues  \nnow  what given this and ignoring the knowledge of the first section of this lecture can \nyou tell me something about this series  what would be the nth element of the series  \nstudent  u sigma power n  \nu sigma  \nstudent  power n  \npower n  \nstudent  u transpose  \nu transpose and you arrive at the same conclusion right  where i was talking about this \noperation right  so  if we can say something about this matrix then we can say something \nabout this series what can you say about this matrix  if the largest eigenvalue is greater \nthan  one  as  you  keep  raising  it  is  power  that  value  is  going  to  explode   and  hence   the \n \n\fentire  product  is  going  to  explode  less  than  one   that  product  is  going  to  vanish  and \neverything else would be less than that right remember is the dominant eigen value  \nso   everything  would  be  less  than  that   so   that  product  will  vanish  ok   so   the  same \nconclusions you can arrive at right  so  that is why i want to do these sections again  so  \nyou  would  have  done  these  in  linear  algebra   but  you  would  have  not  arrived  at  these \nconclusions from a very different interpretation  but i want to focus on the interpretations \nthat i care about  i do not  how many of you have seen this series in the course on linear \nalgebra  you have ok  but i do not see why anyone else would teach this is not required \nis  only  required  for  some  things  that  i  want  to  do  in  the  course  right   that  is  why   i \nwanted to do this section  \nso  everyone is comfortable with eigenvalue decomposition it is a very simple stuff right \ni  mean  there  is  no  proof  or  anything  involved  there  we  just  use  some  properties  of \neigenvectors and eigenvalues and do it  \n\n \nnow  there is one more important property of eigenvectors  which well use today  so  let \nus see what this means right  you have a matrix a which is an n cross n matrix ok  and \nyour import interested in computing this value  x transpose a x where x belongs to rn x \nbelongs to rn  \n \n\fso   what  am  i  trying  to  do  here  of  all  these  vectors  possible  in  rn   i  want  that  vector \nwhich maximizes this quantity  what is this quantity scalar  vector  matrix  tensor  \nstudent  scalar  \nscalar ok  such that x is equal toone  this is the problem that i have been given to solve \nwhy it is not clear as of now  but suppose this is a problem i am trying to solve  or the \ninverse of this which is minimize the same thing  of all the vectors in rn find the vector \nwhich minimizes this quantity  subject to these constraints  then the solution for this is \ngiven by the smallest or largest the solution is the smallest eigen value of a  \nand x is the eigenvector corresponding to that  so  if you are trying to minimize and the \nsolution is a smallest eigenvalue  we need to clarify that if  you are trying to maximize \nand the solution  is  the largest  eigenvalue is  that clear and the value of x would be the \ncorresponding  eigen  value   so   largest  eigen  vector  is  the  same  as  something  that  we \nhave defined today dominant eigen vector right  \nso  let me just repeat  so  that there is no confusion  let us focus on this problem  the \nsolution  to  this  problem  that  is  the  x  which  will  give  me  the  maximum  which  will \nmaximize this is the dominant eigen vector of the matrix a right  is that clear  fine ok  \nand if you want to minimize it is going to be the smallest eigen vector  that means  the \ninverse of the dominant  \n\n \n \n\fso  there is a proof for that i will not go over the proof you can take a look at it at your \nown leisure  \n\n \nso   what  has  been  the  story  so  far   the  story  has  been  that  the  eigenvectors \ncorresponding to different eigen values are linearly independent  \nif  you  are  dealing  with  the  square  symmetric  matrix   which  is  something  that  we  will \ndeal  with  soon   then  things  are  even  more  convenient  because  the  eigen  vectors  are \nactually orthogonal ok  and they form a very convenient basis  and now we are going to \nput this to use when we talk about principal component  \n \n\f"}
{"audio_filepath": "lec005_004.wav", "duration": 1531.12, "text": "\nprincipal component analysis and it is interpretations \nso  in  this module we  will  talk about  principle  component analysis  and it  is  different \ninterpretations  in this model we will look at one interpretation and then in the rest of the \nmodule some other interpretations  \n\n \nso  the story i add is going to be this we will talk about pca and it is interpretations  ok  \n\f\n \nso  now  let  us  try  to  motivate  pca  first  consider  the  following  data  ok   in  what \ndimension is this data  \nstudent  two dimension  \ntwo dimensions  it is r two ok  and each point here is represented as  it is x coordinate and \nusing it is x coordinate and it is y coordinate ok  now it means that were using x and y as \nthe basis right  that is clear that is the standard way that  you would do any data point \nyou will just represent using that basis  \nnow  what if we choose a different basis  let me give you one basis and then let me ask \nyou some questions on this  \n \n\f\n \nsuppose we chose this basis  so  in the previous modules we made a case for the x and y \ncoordinate  axis   there is  nothing sacrosanct  about it  you could  use any  basis  the only \ncondition on the basis that the vector should be linearly independent and in fact  if they \nare orthogonal it is even better right  \nso now i have given you a different basis now what do you make any observation here  \nso  they have all the points here have a very small component along the u two axis right  \nso now  this so far this point right  if i consider at this point then this is the component \nalong the u one axis  so  that is it is u one coordinate as akin to the x coordinate and this is it \nis u two coordinate akin to the y coordinate  is a are the arrows clear here  \nso  that means  there u two coordinate is very small  and it is also very small for all the data \npoints right  so  it is almost as if there is some noise there it is all within some epsilon  \nnow  so  it  seems  that  the  data  which  were  actually  represented  in  r  two  can  actually  be \nrepresented in r one by getting rid of this noisy dimension right  so  if you had chosen a \ndifferent  basis   you  realize  that  with  just  one  dimension  you  could  have  captured \neverything that was there in  the data  and the other dimension was  just adding noise it \nwas redundant there is hardly any information there  \n \n\f\n \nso now can you state this more formally because this is this intuition  but can you stated \nmore  formally  in  terms  of  things  that  you  have  learned  and  say  probability  for  it  for \nexample  what is wrong with the direction u two  the spread of the data points along the u \ntwo direction is very small  what is the spread mean the variance right  so  we do not care \nabout u two because the variance in the data along this direction is very  very small ok  and \nin  particular  right  if  i  were  to  build  a  classifier   then  would  u  two  have  any  predictive \npower  because along this dimension the points are indistinguishable ok  so  think of it \nthat you are trying to find out whether  you have  so  you have say one hundred candidates and \nyou want to decide whether they would be good basketball players or not  \nand quite naturally all the people that have shown up are say six foot two and six foot three inch \nand so on  and there in a very small height difference between them and all of them are \nsix two is the average and very close the spread is not much  so  this feature is not going to \nhelp you decide whether this person is going to be a good basketball player or not  you \nwill have to  rely on other features  where the variance is  more  for  example  how many \nteams  has  he  participated  in  the  past   how  many  matches  as  he  won  as  a  team  as  a \nmember of some team and so on it  \nso   those  who  expect  some  spread  to  be  there  all  these  one hundred  candidates  might  have \ndifferent things right  but if the height is the same for all of them it is not going to be a \n \n\fgood predictor  and that is exactly what is happening along the u two direction  the points \nare almost indistinguishable there that is why  it does not matter  \n\n \nso  in general given any data  now this was a simple case where the data was r two i am \ntalking about the general case where the data is rn right  and you will find this situation \nin higher dimensions also  so  you would not want to use that entire n dimensional data \nwhere you know that there are some columns  along with the variance is very small  so  \nyou  want  to  represent  the  data  with  fewer  dimensions   such  that   the  data  has  high \nvariance along those dimensions  \nnow   let  me  just  clear  a  confusion  here  right   so   i  am  not  saying  that  take  your  n \ndimensional data ok  find the variance across each of these dimensions and then throw \naway the columns which have the lowest dimension  in this particular example if you had \ndone  this   what  would  happen   could  you  have  done  that  think  of  the  original \ndimensions  x  and  y   along   these  two  dimensions  there  is  enough  various  in  the  data  \nright  the x coordinates vary from  here to  here  and the  y coordinates  also  vary from \nthis  point  right  up  to  that  point  right   so   there  is  enough  spread  in  the  x  and  y \ncoordinates  \nso  in your original data i am not saying that pick look at each column  and see if there is \nno  variance  along  that  column  then  throw  it  away  that  would  not  work  because  you \nmight  end  up  with  the  situation  that  there  is  enough  variance  across  each  of  these \n \n\fdimensions  it is just that when you look at the data from a different angle  that means  \nyou projected onto a different basis this becomes clear right  \nso  you see the difference i that is not the same these two things are different operations  \nso  what i am looking at is projecting the data to a different basis  that is exactly what i \ndid  with  u  one  and  u  two   and  then  some  things  became  clear  about  the  data   now  this \nprojection along a different basis  i would be interested in doing that only if  i can get rid \nof  the  number  of  dimensions  right   if  now  i  had  already  had  one  basis  where  i  had  n \ndimensions  now if the new basis is also going to be that all these new n dimensions that \ni  have  come  up  with  are  important  then  you  are  not  gaining  much   i  do  still  have  this \nhigh dimensional data  but  you would like to project it in a way that  you get rid of  the \nlower variance dimensions  \nso   you  might  project  it  onto  n  dimensions   but  you  want  to  rank  these  dimensions \naccording to variance and then throw away some of these dimensions  is that clear  is the \nobjective clear  ok  fine   is that all that we care  about  n dimensions\u2019 project  to  a new \nbasis and throw away the key dimensions which have less variance  is that all  what else \nwould you want  people have done the mlpr course  no i would not  so  i am not going \nto classification or anything i just want a better representation of the data at this point  i \nam not really thinking about what i want to do with the data  maybe you are talking in \nterms  of  classification   and  we  have  already  seen  even  if  the  data  is  not  linearly \nseparable we have solutions for dealing with right  so  that is not a critical point ok  so  \nthere is something else that very interested in and let us look at that  \n\f\n \nnow  consider this data i have three dimensional data ok  do you find something odd about \nthis data  \nstudent  \n  \ny and z are  \nstudent  \n  \nare highly dash  \nstudent  correlated  \ncorrelated  right  do  you  want  these  dimensions   can  you  think  of  any  practice  such \ndimensions occurring  height in centimeter and height in inches   someone would have \njust  given  you  data  right   or  if  you  if  you  take  the  credit  card  a  credit  card  fraud \ndetection case right  someone would give you the salary and it would also give you the \nincome tax now these two are highly correlated right  \nso  then you do not really care if you have one you could probably almost with certainty \npredict the other right  modulus some rules right because you get some tax exemptions \nand all that  but still  so  you can have this in practice  but even in our oil mining case \nyour salinity pressure density those things could be related right  so  z is not adding any \nnew information  beyond what  y is  happening  so  the two columns  are highly  correlated  \n \n\fso   actually  yeah  this  is  the  formula  for  correlation   all  of  you  know  this  anyone  who \ndoes not know this formula good  so  not nothing is a stupid question right  so  you can \nalways ask  \nso  y hat is the mean of this column ok  sorry y bar  z bar is the mean of this column and \nthis is how you compute correlation this is just the formula ok  so  from every entry you \nsubtract  the  mean  ok   so   this  is  known  as  centering  the  data   so   if  you  do  this  what \nwould the mean of the new data be  \nstudent  zero  \nzero right  so  that is why it is called centering the data ok  so  i will have zero mean zero mean \nand  you   so   what  does  this   what  is  the  intuition  behind  this  formula   does  anyone \nknow  can anyone tell me  so  this is a summation ok  so  this quantity is going to be \nhigh if the summation is high  it is a summation of some n terms now these terms could \nbe positive or negative  if all the terms are positive what would we happen to the sum  \nstudent  \n  \nit would be high if there are some terms which are negative it would be low  now when \nwould all the terms be positive whenever y is above the mean  z is also above the mean \nright  therefore   this  quantity  is  positive  this  quantity  is  positive   whenever  both  are \nbelow the mean again  the product  would be positive  when one is above the mean the \nother is below the mean  then there is something wrong happening right and in that case \nyou will have a negative term right   \nso  for more details of course  you can refer your other textbooks and so on  but this is \njust the intuition an important step here is to zero mean the data  right  we are computing \nthe subtracting the mean of the data  ok another way of saying this is that the column z \nis actually linearly dependent on y ok  it is almost linearly dependent  i of course  have \nsome noise two one  zero seventy six and so on  but it is largely linearly dependent  i can get i can write z \nas some c times x fine ok  \n\f\n \nso  now  can  you  tell  me  the  refined  goals  that  we  have   we  are  interesting  the \nrepresenting  data  using  fewer  dimensions  such  that   remember  that  when  i  say  fewer \ndimensions  i  mean  a  new  set  of  dimensions  right   it  is  not  throwing  away  dimensions \nfrom  the  current  data   we  are  looking  for  a  new  set  of  dimensions   what  are  the \nconditions that we want from these new set of dimensions  \nstudent  \n  \none there should be high variance along these dimensions the new dimensions  and  \nstudent  \n  \nthe dependence are linearly independent or uncorrelated ok fine  \nand even better of course  if they are orthogonal  why  \nstudent  \n  \nbecause we are looking for a new dash  \nstudent  basis  \nbasis and the most convenient basis is  \nstudent  orthogonal basis  \n \n\forthogonal basis  ok  fine  \n\n \nso  now let us assume someone has given us this new basis ok  and let us call this p one p \ntwo pm  so  instead of this x y z and so on  someone has given us this new basis eventually \nwe will of course  figure out how to find the basis  but let us assume that someone has \ngiven  this  new  basis  right   and  they  are  both  linearly  independent  and  actually  it  is \nredundant  actually   so   yeah  this  example  of  a  redundant  feature  such  an   orthogonal \nvectors is sufficient  they are linearly independent  \nlet p be an n cross n matrix such that p one p two p n are the columns of p right  same thing \nas we had put the eigen vectors in a column and probably i have unknowingly given out \nthe solution  but ok  and let x one to xm be the m data points given to us ok  so  we are \ngiven this data as usual we have this x matrix each one of them belongs to rn  and we \nhave m  such data points  right  that is  the standard thing that we are operating   and  you \nalways write this as a matrix ok and we have already done the data is zero mean and unit \nvariance  \nactually unit variance is not required  but the data is zero mean fine that we will sorry i am \ngoing to deal with covariance  as a unit variance is not required  so  the data is zero mean is \nwhat i am going to assume  but what if the data is nonzero mean i can always make it zero  \nmean right  \n \n\fso  just to remember this is an important trick that you will always have to use whenever \nyou  are  doing  any  large  scale  machine  learning   so   whenever  you  are  participating  in \nkaggle competitions almost the first thing that you do is standardize the data  that means  \nmake it zero mean and unit variance  so  why is that important  \nstudent  scaling  \nright scaling issues would not be there right  so  if i have something in centimeters and \nsome  other  unit  in  kilometers  right   now  remember  that  always  you  are  doing \nsomewhere this linear operation w transpose x  you might add a non linearity on top of \nthat  but now if your xi dimensions some of them are in the range of zero to ten thousand some of \nthem are in the range zero to ten right  \nthen  there  is  some  abnormality  here  right   some  dimensions  are  winnings  in  terms  of \ntheir magnitude and some dimensions are losing out right  that is why you always make \nit unit variance and you also make it zero mean you center the data ok  so  we will assume \nthis and if we all understand if the data is already not zero mean and unit variance we can \nalways make it zero mean and unit variance  just scale it and make it centered  \n\n \nnow  we want to represent each xi right  so  xi is one of these data points that we had  \nthat  means   one  of  the  rows  of  our  matrix   ok   and  you  want  to  write  it  as  a  linear \ncombination of this new basis  \n \n\fso  if you have any basis any vector you can write it as a linear combination of that basis \n  is it fine  so far it is ok  ok  now for an orthogonal basis we know that we can compute \nthese alphas just by taking  a dot  product  of the  vector with  the dimension ok  and just \nrepeating some of the things right  \n\n \nso  now  let  us  see  what  this  means   for  one  of  the  dimensions  this  is  my  data  point  xi \nwhich i want to transform ok  for one of the dimensions i just had to take the dot product \nwith that dimension and this will give me how many values  one value  that means  the \ncoordinate  along  p  one   i  want  to  do  it  for  all  the  n  of  them  i  can  write  it  as  this  vector \nmatrix multiplication right  what is the dimension of this  \nn cross one  how many if you get that  ok so this oh not many why  \nstudent  \n \none cross n fine that is fine yeah how many of you get this  ok fine yeah  so  this will give \nme all the n alphas is that clear for this data point  \nso  it will give me alpha i one to alpha i is it  \n \n\f\n \nnow  i want to do this for the entire data right  so  i have done it for x one i also want it to \nbe  done  for  x  two  and  all  the  way  up  to  x  m   for  each  of  these  i  would  have  such  an \noperation where i have a vector multiplied by this matrix  if i just stack all these vectors \ni get back my matrix x  and the whole operation i can write as x into p ok is that clear to \neveryone ok  what is the dimension of x into p  \nstudent  m cross n  \nm cross \nstudent  n  \nn right so  for all the m data points i have alpha oneto alpha n is that clear anyone who does \nnot understand this  \nso   x  hat  is  the  matrix  of  the  transformed  points  is  that  clear   i  have  now  the  new \ncoordinates  instead  of  the  original  coordinates  according  to  the  coordinate  axis   i  have \nthe new coordinates in this matrix  \n \n\f\n \nnow i will just go through some very simple theorems or rather results  and i will not \nprove them you can prove them on your own  or other proof is there in the slides we can \nlook at it later on right  so  if x is a matrix such that it columns have zero mean and if x \nhat is equal to xp  then the columns of x hat will also have zero mean  is this obvious to \nmost of you  not really is it  how many of you think it is obvious  ok then let me just go \nover the proof  \nso  for any matrix a  one transpose a right so  that means  you have this vector this is a \nvector or a matrix   yeah this is  a vector  right   so   i have a vector of n one  so  one this is \nnothing but a vector of n ones  so  what is this product actually going to give me  \nit will give me a vector containing n elements  what is each element  \nstudent  sum of that column  \nsum of that column right  is this fine  ok this is very obvious to see from if i have this \nsuppose i have two three one and three six seven ok  and then of course  the corresponding  \n \n\f\n \nso  if i do this multiplication i will get a two dimensional output which would be just seven and \nsixteen right  so  that is just the sum of that column  \nstudent  \n \n\n \nso  now   we  have  this  x  hat  that  is  the  transform  matrix   now  let  us  see  if  i  do  this \noperation i x hat what happen  i can write it as this i can club it as this  what is this  it \nwill be all zeros because the original matrix was mean zero  that means  the of the elements of \nall the columns  each  column  independently was  zero   that what  this is  going to  be  a  zero \n \n \n\fvector   so   zero  vector  multiplied  by  any  matrix  is  going  to  be  zero   now  is  it  obvious  i \nhope this is obvious x transpose x is a symmetric matrix  i still have the proof for that  \n\n \nnow  if x is a matrix whose columns are zero mean  then a matrix sigma which i am going \nto call as a covariance matrix  which is given by this is actually the covariance matrix  \nhow  many  of  you  agree  with  this   how  many  of  you  have  seen  the  covariance  matrix \nbefore  so  all of you agree that this is the covariance matrix if you do not please raise \nyour hands  if you do not you will not understand the rest of the stuff now you have to be \ngiven the right incentives  \nso  let us see be the covariance matrix of x  now what is the covariance matrix actually \nfirst of all tell me that  if i say that i have an n cross n matrix x  \n \n\f\n \nlet me not make it any cross n  let me make it m cross n ok  what does the covariance \nmatrix actually capture  what is the dimension of the covariance matrix first of all  \nstudent  n cross n  \nn cross n ok  and what does each entry of the covariance matrix capture the covariance \nbetween the i\u2019th column and the j\u2019th column    \nstudent  \n  \nso  the entry ij of the covariance matrix captures  the covariance between the i\u2019th column \nand the j\u2019th column is that fine  now what is the formula for covariance suppose i give \nyou two columns right let us see i have give you x one one x one two x one three and x two one x two two x two three  can \nyou give me a formula and of course  i will go up to k or rather m  \nso  what is the formula summation  \nstudent  \n  \ni equal to one to  \nstudent  m  \nm  \n \n\fstudent  \n  \nmu one  \nstudent  \n  \nmu two anything missing  \nstudent  by m  \nby m anything else in the denominator  no  no is it fine ok  so  an what is mu one  mu one \nis  just  an  average  of  this  ok   so   this is  the  covariance  formula  now  if  the  mu\u2019s  are  zero  \nthen what does this boil down to  \nstudent  \n  \n x one i into x two i   what is this quantity actually  \nstudent  \n  \nthis is the dot product between the i\u2019th column and the j\u2019th column fine ok  now that is \npretty much the explanation right  so now  the c ij\u2019th entry is supposed to be given by \nthis formula  if the means are zero  you are just left with this formula  and this is nothing \nbut the dot product between the i\u2019th row and the j\u2019th  i mean the i\u2019th column and the j\u2019th \ncolumn is that fine  \nand now if you write it as a matrix then you can just say that it is the ij\u2019th entry of the x \ntranspose x matrix everyone gets this  no one has any confusion the people who raised \ntheir hands fine good  \n\f\n \nso  we now this is where the we are so far  that we have assumed that someone has given \nus these dimensions\u2019 p one to pn  which we have put in the matrix p  right  and we have \nalso  made  a  case  that  x  into  p  which  is  what  i  have  written  here  actually  is  just  a \nprojection of the original data onto this new basis right  everyone gets that ok  and i am \ncalling  that  new  projection  or  the  new  result  that  i  get  as  x  hat   so   that  is   what  my \ntransform data is  \nwhat is missing here  \nstudent  \n  \nwe do not know what p is that i am assuming someone has given me that p  now i need \nto figure out what is the p here  now using the previous definition  we get that this is the \ncovariance matrix of the transform data  so  let us just write that this is fine this is fine  \nwhat is this  \nstudent  \n  \ncovariance matrix of the original data ok  so  i will just write it as sigma fine ok  now \neach cell ij of the covariance matrix towards the covariance between columns i and j of \nx hat  where x hat is the transformed data  what is the property that you want to hold  i \ngive you two conditions or i will give you only one condition for now when i is not equal to \nj  \n \n\fstudent  \n  \nzero ok  so  what should the covariance matrix look like  \nstudent  \n  \nremember that this is  what is this  this is the covariance matrix of the transformed data \nright that is what i started with right  this is the covariance matrix of this transformed \ndata  what do i want this covariance matrix to look like  \nstudent  diagonal matrix  \na diagonal matrix ok because i want every non diagonal element to be zero right  and this \npoint i am not telling you what i want the diagonal elements to be i am just telling you i \ndo not want them to be zero  \nwell if it is zero what would that mean  \nstudent  \n  \nthat is the variance right if you take the along a diagonal  what you get is the variance it \nis if it is not clear right now well return back to that  right now we just know that the off \ndiagonal elements are the covariance between the i\u2019th and j\u2019th column and we want that \nto  be zero  so  we want  this condition  to  hold  this is  something very new  that  you have \nnever seen in  this  course before they have actually not  seen in  this  course before have \nyou seen this or not  \nstudent  \n  \nthank god fine  \nso  what is this  \nstudent  diagonalization  \nthe diagonalization of which matrix  this matrix right and what was this matrix it was \nx transpose x this is clear  so  what is the solution  all rows always lead to  \nstudent  eigenvectors  \n\f eigenvectors  right  \n\n \nso  we want p transpose sigma p to be a diagonal matrix and we know which are the set \nof vectors which i put in  p such that they will diagonalize sigma  \nstudent  eigenvectors  \neigenvectors of  \nstudent  \n \nx transpose x right ok  wait why did i put this  it is the matrix of the eigenvectors right  \nso  it is a matrix of the eigenvectors of x transpose x  \nso  now  have  we  finished  it   do  we  know  principal  component  analysis  now   so   we \nstarted with the intuition that we wanted to transform the data ok  i cannot stress enough \nthat we want to transform the data not chopped off dimensions from the existing data ok  \nthat means  we need to project the data to a new basis and we had a couple of conditions \nthe  variance  should  be  high   and  the  covariance  should  be  zero  we  have  satisfied  one \ncondition which is the covariance is zero  and we arrive at a solution which says that the \neigenvectors forms the basis that you should project on  so  that the covariance would be \nzero  \n \n\fso  we have a solution  we know exactly which basis to use to represent the data ok  so  \nthat  the  covariance  condition  is  satisfied   what  about  the  variance   did  we  do  anything \nabout the variance  \nstudent  \n  \nso we will come back to that  ok fine  \n\n  \nwhy is this a good basis  what does the what is a good basis  the best basis  \nstudent  orthogonal  \northogonal right because the eigenvalues of x transpose x are linearly independent that \nok and they are also orthogonal because x transpose x is a dash matrix  \nstudent  \n  \nok  \ngood  real symmetric  \nso  this method is called the principle component analysis for transforming a data to a \nnew  basis   and  that  where  the  dimensions  are  non redundant  because  they  have  low  \ncovariance and not noisy  because they have high variance  the second part i have not \n \n\fproved right and i will get to that at some point fine  no that is what we saw right no  \nwhat is i did not get that now in practice how many eigenvectors would you have  \nstudent  n eigenvector  \nn eigenvectors  do you want to keep all of them  which ones will you throw away  \nstudent  \n \nthe low variance ok  \nand now in the next interpretation actually we will try to see  what is the  what happens \nwhen  you throw away the least important dimensions right  what do you mean by the \nleast important dimensions  \n\f"}
{"audio_filepath": "lec005_005.wav", "duration": 1017.1349375, "text": "\npca  interpretation two \nso  that is what we look at in the second interpretation of pca right  \n\n \nso  again we have the same setup that given n are linearly independent for n orthogonal \nvectors  we can represent x i exactly as a linear combination of these vectors  what do i \nmean by exactly  perfect ok  if you actually describe the whole things in words ok  so  \nthat is exactly what i mean right  so  you are going to write x i as alpha one i into p one plus \nalpha two i into p two and so on  and when you do the summation on the lhs on the rhs  \nyou just get  back the  lhs  when  you do the summation on the right  hand side  you  get \nback the left hand side  \nso  that means  it can exactly be represented  when you use all the n eigenvectors now  if \ni start chopping of stuff what will happen  \nstudent  \n  \n\fit  will  just  be  an  approximation  ok   now  we  this  is  what  i  meant   and  this  is  this  the \nequation holds  that means  this is exact and we know how to find the alpha is  because p \njs are conveniently orthonormal  so  we know how to find that easily ok  now what if we \nconsider only the top k dimensions  what is going to happen  there is going to be some \nerror in the reconstruction i am not capturing all the information in my original data  but \nthere is some error which i am not being able to capture  and i made a conscious decision \nthat that error is not important i am willing to let it go   \nhence i want to  represent the data using fewer dimensions ok  so  this is exactly what \nyou do in pca when you take the top k dimensions is this fine ok  so now  we want to \nselect  pi\u2019s  such  that   we  minimize  the  reconstructed  error  ok   and  this  is  again  erratic \nactually we should try to write it as  x i minus x  since these are vectors and the square of \nvectors would just meet this right  \n\n \nso  but you get the point right were just trying to do the element wise squared error loss \nwere trying to minimize that  we want to do this  so now let us try to see that if you are \naiming to do this  what is the condition that  we arrive at ok  so  no i thought i would ask \nfor some changes on this   \nfor a minute all of you can you just bear with the fact that these are actually vectors and \nnot scalars  so  this square actually does not mean anything it actually means x i minus x \n \n\fi hat transpose  x i minus x i hat  so  when i use square with vectors this is what i mean \nis that  everyone can work with that notation fine  \n\n \nso now what is x i actually the real point right the correct point which can be obtained by \nthe  full  reconstruction   if  you  consider  all  the  n  dimensions   what  is  x  i  hat  just  an \napproximation where you are considering only the k dimensions  remember that each of \nthese quantities is a vector fine ok  now what is happening here  let me just try to say \nthis  so  let me just do this way  so  this is your original x and you are actually writing it \nas a linear combination of your p\u2019s somewhere you will have alpha k pk and then all the \nway up to p n   \nso  this is p k alpha n ok  now what is this full thing this is x and what is this x hat ok  \nyou see the picture what is the equation trying to tell you ok  now what is the difference \nbetween these two then  these guys right if i want to take difference between x and x hat \neveryone gets that it is the remaining term say  that means  alpha k plus one into p k plus one \nup to alpha n into p n is that clear  \n \n\f\n \nso  can i write it as yeah can i write it as this ok  so  you get this right  so  i am only \ntaking these guys because the rest will get subtracted  so  one is the full n dimensions the \nother is only k dimensions  so  if i take the difference between them what remains is k \nplus one to n dimensions  and that is exactly what i have written here ok  \nand now i am coming back to the proper notation where this is a vector right  so  i am \nwriting the square as the dot product between the same vector is this ok  these are the m \ndata point right  this sum this is overall the m data points  you need to minimize that is \nthat  clear  ok   so   this  is  fine   now  want  just  some  rearrangement   so   i  have  just \nexpanded out that summation  this is what it would look like right  i have just expanded \nout these two summations   \nnow let us try to  do this in  your head and see  what  are the kind  of terms  that  you get \nthere are two different types of terms that you will get  so  first of all let us understand that \nwhen  you  expand  this  you  will  end  up  with  a  lot  of  dot  products   you  will  get  a  dot \nproduct between this and this and this and so on right  so  can you split those terms into \ntwo different types  \nstudent  \n  \nsquare terms  so  one where i is equal to j and one where i is not equal to j is that clear \nfine   so   let  me  just  write  it  as  that   so   i  will  have  k  plus  one  to  n  right   that  means   n \n \n\fminus k terms  where i  would be equal to j  right so  that means  pk plus one was  getting \nmultiplied by k pk plus one  pk plus two was getting multiplied by pk plus two and so on and \nthen i will have these remaining terms where i is not equal to z right  so  these are the \ndot  product  between  the  other  vectors  is  it  fine   you  see  why  i  have  split  it  this  way  \nwhat will happen now  the second term will go to zero ok  and what about the first term  \nalpha ij square ok  now what is alpha ij actually how did you find alpha ij  \nstudent  \n  \n it  is  a  dot  product  between  we  did  this  right  finding  any  of  these  components  is  just \ntaking the dot product between x i and that dimension  so  x i transpose pj is that fine ok  \nis this fine and again this is slight abuse  so  this is actually  what no this is ok right  a \nthis is ok sorry i am just going to write it as this is this fine  i just written it twice and i \ncan change the order  since  it is a dot product   \nnow   what  i  am  going  to  do  is   so  this  is  actually  summation  over  an  index  i  and  a \nsummation over an index j  and i can change the two summations i can interchange them \nok  so  that is what i am going to do now is this fine  i will push the summation all the \nway inside what is this actually this entire thing actually m times covariance of  \nstudent  \n  \nso  is this i is this what you are telling me that this is m x transpose x is this fine  how \nmany if  you do not  get  this  i see  a lot of blank faces  how many if  you do not  get  this \nquite a few  so  this is so i is equal to one to m right  so  you are going over the data points \nok  so  this what is the dimension of this actually  \nstudent  n cross one   \nn cross one and this is one cross n  what does this product give you  \nstudent  \n  \nn cross n what are the entries in this matrix  so  this was say x one one up to x one n  and this is \nagain x one one up to x one n ok  so  that is going to be x one one square or rather let me just write it \nin the generic form right  so  it is going to be x one i into x one j right is that fine  and how \nmany such matrices are you adding  \n\fstudent  \n  \nm of these  so  what would you get then  what would the first let us  so  ok so  let us do \nthis   so   the  first  entry  of  this  matrix  is  going  to  be  x  one  one  square   what  about  the  first \nentry of the next matrix in this series  \nstudent  \n  \n x two one x two one square right  so  this is slightly tricky to demonstrate  let me just a give me a \nminute i will just collect my thoughts and do it properly ok  let us take a small example \nok  so  x one one x one two x one three suppose we have a three dimensional matrix three dimensional data  so  \ni am taking a sum of m such matrices ok  i equal to one to m  that means  this is going to \nvary this indexes the first index is going to vary from one to m  now  let us see the first \nmatrix and let us look at the first element of that matrix the first element of this matrix is \ngoing to be x one one square ok  \nnow  let us look at the next matrix  what is the next matrix going to be  it would be x two one \nx two two x two three right  and multiplied by x two one x two two x two three  what  is the first  element  of this \nmatrix going to be  \nstudent  \n  \nx two one square what about the third one  x three one square this is fine so far now you are adding \nall these matrices  so  what is the first element of the resultant matrix going to be x one one \nsquare plus x two one square plus x three one square  what is this actually  this is the dot product \nof x one with  itself right   and what  does that  give  you the  variance if the data is  zero mean \nright ok  now can you make a similar argument of the ij\u2019th entry is going to give you the \ncovariance between the i\u2019th and the j\u2019th entry is that clear right  you could do a similar \nanalysis  you  can  actually  work  it  out  after  going  back   how  many  of  you  have  found \ncomfortable with this  there is still many who are not ok  \nso  let us look at an ij\u2019th entry right  so  can someone help me with say that one comma two \nentry  ok or the first  matrix what  is  it going to  be x one one into x one two right  for the second \nmatrix  \nstudent  \n  \n\fx  no  this  is  some  yeah  correct  and  for  the  third  matrix  three  two  ok   now  what  is  this sorry \nwhat is the summation of these  when you take the full sum you will get these three as as  \nwhat is this in this summation tell you  \nstudent  \n  \ncovariance between  \nstudent  first and second \nthe  first  column  and  the  second  column  is  that  clear  now   is  it  with  everyone  now ok \nfine  so  what you have here is actually the covariance matrix you seems to be lost is it \nwith you sure  ok fine   \n\n \nso  what we have here is something of this form ok  \n \n\f\n \nso now what we want to do is we want to minimize this quantity subject to the following \ncondition  is  that ok  what  is  the solution  for this  if  i did  not  have the summation ok  \nsuppose i just wanted one dimension  so  i want to minimize say p sig p transpose sigma \np such that p transpose p is equal to one  what is the solution for this  \nstudent  \n  \nsmallest  eigenvalue  of  sigma  right   and  you  can  show  by  induction  that  if  you  want  k \nsuch things that here i am looking for n minus k such things right  then these would be \nthe  n  minus  k  smallest  eigenvalues  of  sigma   but  now  i  am  talking  about  the  smallest \neigenvalues  but in the first solution i said we need to pick the largest  eigenvalues  so  \nwhat is the difference  \nstudent  \n  \nthese  are  the  ones  we  are  throwing  away   these  are  the  ones  along  which  the  error  is \ngoing to be minimum if we throw these away the error is going to be minimum  so  we \nwill  throw  away  the  last  n  minus  k  dimensions  which  means  well  keep  the  first  k \ndimensions is that clear  so  you arrived at the same solution is that right so  that means  \nin  pca  you  are  actually  trying  to  pick  the  dimensions  in  a  way  such  that  your \nreconstruction  error  is  minimized   and  this  was  exactly  what  our  reconstruction  error \n \n\fwas  so  do not worry about this math bit  just see that we started with this quantity this \nis what we wanted to minimize ok   \nand  we  did  some  trickery  and  we  came  to  this  formula  that  minimizing  that  error  is \nequivalent  to  minimizing  this  quantity   and  for  this  we  know  the  solution  that  the \nsolution is the smallest eigenvalue and we want n minus k such things  that means  there \nwould be the n minus k smallest eigenvectors is that clear  that means  we are going to \nkeep only the k largest eigenvectors ok  that means  you are going to project your data on \nto k largest eigenvectors  \n\n \nnow   so  the  key  idea  here  is  this  right  minimize  the  error  in  reconstructing  x  i  after \nprojecting the data onto the new basis  \n \n\f\n \nso  let us take an example and we will work with our toy example again  \n\n \nso  this was the data that we had and suppose i give you a new basis which is one comma one \nand  minus  one  comma  one  ok   this  is  a  new  basis  this  is  an  orthonormal  basis  orthogonal \nbasis you can see that u one transpose u two is equal to zero ok  \nnow  i need convert it to an orthonormal basis  so  i have just divided by the magnitude \nis it ok fine  now consider the point three three comma three  this was our original point according \nto which coordinate axis x comma x  that means  this was three three and this was three ok  now i \n \n \n\fcan  find  the  alpha  is  right  because  this  is  an  orthonormal  basis  i  can  directly  find  the \nalpha is  now the perfect reconstruction would be this  so  actually if i do this i get back \nthe original point  \nnow   what  would  happen  if  i  throw  away  the  second  dimension   because  the  second \ndimension had corresponds to a smaller eigenvalue ok i will get this  so  you see that the \npoint  is  still  close  to  the  original  point  i  have  not  actually  lost  much  right   what  has \nhappened is i have actually projected the boy lie point on this line right  the line x equal \nto y that is  why i get x equal to y  and in doing that i am not losing much information \nfrom  the  original  data  is  this  clear  right   so   you  understand  what  happens  when  you \nreconstruct the data  \n\n \nthere  is  no  end  to  this  ok   so   just  to  recap  the  eigenvectors  of  a  matrix  with  distinct \neigenvalues  are  linearly  independent   and  we  use  this  fact  conveniently  at  least  in  the \ncase of square matrix where the also happen to be orthogonal  so  we know that they can \nform a very convenient basis and pca exploits this to find the top k eigenvectors which \nto be retained   \nand  while  doing  this  they  have  seen  that  two  things  are  at  least  ensured  one  the \ncovariance between the dimensions is zero because that is exactly how we formulated it and \nfound the solution  we saw that it turns out that we need to diagonalize a certain matrix \nand the solution is the eigenvectors  \n \n\fwe also saw a different interpretation where we saw that it is the same as throwing away \nthe  dimensions  along  which  the  error  would  be  minimum  right   and  both  these \ninterpretations led to the same solution which was project the data onto the eigenvectors \nof the covariance matrix of the original data  ok and this n minus k dimensions current \ncontribute very little to  the reconstruction  now what  is  the one thing which  i have not \nproved yet  what was our wish list  \nstudent  variance and covariance  \nvariance and covariance right high variance low covariance  i proved low covariance  i \nhave  also  proved  something  with  respect  to  reconstruction  error  because  that  is \nsomething  i  require  for  auto  encoders   so   just  remember  this  bit  about  reconstruction \nerror  \n\f"}
{"audio_filepath": "lec005_006.wav", "duration": 231.906, "text": "\npca  interpretation three \nnow   you  go  to  the  third  interpretation   where  we  will  try to  say  something  about  the \nvariance  \n\n \nso  we started off with the following the wish list  that we wanted low covariance and we \nwanted  high  variance   so   far  we  have  paid  attention  to  the  covariance  because \neverything was revolving around this  covariance matrix in both the solutions  but what \nabout variance have we achieved the goal with respect to high variance  \n\f\n \nso  let us see  so  what is the i\u2019th dimension of the transformed data it is this  you take \nyour data and project it onto the i\u2019th dimension right  so  x hat is equal to x into pi now \nwhat is the variance along this dimension  how do you compute the variance   so  this is \nmy projected data and let me just call it x hat i  \nso  this is the i\u2019th column after projection is that fine everyone is ok with this  now  for \nthis i\u2019th column i want to compute the variance  how will i do that  remember that the \ndata is zero mean what is the formula actually  it is going to be x hat i minus mu i into \nx  hat  i  minus  mu  i  right   but  mu  i  is  zero   so   it  just  turns  out  to  be  the  dot  product  dot \nproduct of x i hat with itself ok and of course  divided by m is this fine  \n \n\f\n \ni can write this as x p i and then when i take the transpose i will get this ok  now what is \nthis  quantity   this  is  exactly  the  moment  where  i  feel  like  saying   fl   what  is  this \nquantity  \nstudent  \n  \nno look at the circle  what is x transpose x times p i  \nstudent  \n  \nwhat is p i with respect to x transpose x  \nstudent  eigenvector eigenvector \neigenvector so what is this product going to be  \nstudent  lambda \n  \nlambda i p i is that fine  what is p i transpose p i  \nstudent  one  \none ok so what is actually the variance along the i\u2019th dimension  \nstudent  lambda \n  \n \n\fwhat is lambda i  \nstudent  eigenvalue  \nso  what will happen if i retain the highest eigenvalues  \nstudent  \n  \n i will get the highest variance dimensions right fine  so  all roads lead to  \nstudent  \n  \neigenvectors eigenvalues right   so  andrew ng in one of his lecture says that there are \nten different interpretations of pca  i only know three of these i do not know the remaining \nseven  maybe he was bluffing so that people like us can keep busy oh this is getting recorded  \nso  yeah   so   you  get  this   so   we  have  satisfied  everything  in  our  wish  list  variance  \ncovariance and also did this detour where we saw that it actually amounts to minimizing \nthe  error  in  reconstruction  where  we  are  throwing  away  the  dimensions   along  which \nreconstruction did not add much to our knowledge about the data  so  these are the three \ndifferent  interpretations that  i  have  right  so   hence we did  the right  thing by throwing \naway  those  dimensions   which  correspond  to  the  lowest  eigenvalues  because  lowest \neigenvalues is nothing the lowest variance also  \n\n \n \n\fso   this  is  the  quick  summary   the  covariance  between  the  new  dimensions   you  can \nleave actually those you can just read it later on  \n\f"}
{"audio_filepath": "lec005_007.wav", "duration": 120.7289375, "text": "\n\n \na  quick  summary   we  have  seen  three  different  interpretations  of  pca   and  eigenvectors \nplayed a very crucial role in that  and the other thing which played a crucial role was the \ncovariance matrix of the original data  and with these three different interpretations what \nwe realize is that  the solution that we get or the transform data that we get projecting the \noriginal data on to the on to a basis consisting of eigenvectors  ensures that there is high \nvariance across the new dimensions  \nand we can ignore of the bottom top n sorry bottom n minus k dimensions along with \nthese variance is not high  this also ensures that the error in reconstructing the data by \nignoring  this  dimensions  is  minimized  right  it  is  a  lowest  possible  error   and  it  also \nensures that the covariance between your retained dimensions is zero  because we are able \nto diagonalize the covariance matrix of the transformed data so that is what we had  \nso  now if you think of it right just to connect it two things that we need later on for auto \nencoder right  we are trying to learn a new representation for the data right and we are \n\ftrying to  also  compress  the data  and we want  this compression to  be such that it  is as \nlossless as possible right  we are going from n dimensions to k dimensions  and still we \nwant  to  retain  the  essence  of  the  data  and  do  not  want  to  lose  out  much  of  the \ninformation in the data ok  so  that is essentially what pca is doing  now let us see this \nin practice  \n\f"}
{"audio_filepath": "lec005_008.wav", "duration": 721.382, "text": "\nlecture \u2013 six \nso  we will in this module we will look at  practical example where  pca is used and  i \njust like to give you a flavor of why all this is important right  why do we need to throw \naway some dimensions and then how does it practically help  \n\n \nso   consider  that  we  are  given  a  large  number  of  human  images  right   so   this  is  like \nsome  faces  data  set   a  database  that  says  one  of  the  intelligent  agency  someone  is \nmaintaining  one of the government agency or may be aadhar data bases or something \nlike that ok  now each image here is one hundred cross  one hundred  that means  it is ten k dimensions \nright  it is a very high dimensional data ok  and your job is to actually store this on to do \nsome database for a large amount of the population right because you are collecting these \nimages from various people  \nso   now  we  would  like  to  represent  and  store  this  data  using  much  fewer  dimensions \nright  and you would be really ambitious that if you want to store that more than fifty to \ntwo hundred dimensions right  so you see the compression that i am looking at  you have ten k \n\fwhich is a big storage problem for me and i want to just bringing out to fifty to two hundred  but i \nhave know that this is crucial data right  i do not want to store information which is not \nable  to  distinguish  these  faces   i  was  still  be  able  to  reconstruct  the  faces  from  this \ninformation  right   do  well  i  mean  minimum  error  reconstruction  from  this  and  that  is \nexactly what pcas are allowing us to do right  so  now we construct a matrix of m cross \nten k  what is m  \nstudent  \n  \nthe numbers of samples you have  the numbers of data points that we have and each of \nthis is of dimensions ten k  ok  so  this is what matrix  what do we call this matrix  oh it \nis already given right  it is the x matrix the data matrix that we always have  now each \nrow of the matrix corresponds  to  one image and each image is  represented using ten k \ndimensions just to reiterate  \nnow   let  us  see   so  now  what  would  you  do   this  is  the  original  data   i  want  a \ndimensionally  reduced  data  right   you  want  store  this  ah   is  the  mike  working   you \nwant this data to  be represented by a fewer dimensions  so   what  is  your solution  do \npca  so  what will you do  x transpose x right and i did not get my slide  refer time  \ntwo forty three   \n\n \n \n\fso   we  retain  the  top  one hundred  dimensions  corresponding  to  the  top  one hundred  eigenvectors  of  x \ntranspose x right  so  basically we do a pca  find the one hundred find all the eigenvectors of x \ntranspose x and then just retain the top one hundred of those  now what is the dimension of each \nof these eigenvectors  should be straight forward take your time  it is early morning  \nstudent  \n  \nten k right  so  now can you think of a physical interpretation of this  so  what are you \ntrying  to  do   you  are  trying  to  store  faces  and  now  you  have  come  up  with  these \ndimensions  no sorry we have come up with these basis vector which is eigen vectors and \neach of them is also ten k  which is as same as dimensions of your faces  \ncan you think of a physical interpretation of what is happening here  none of went you \nthrough the slide except perhaps you or i do not know just think about it  so  what you \nare trying to  do is  you are trying to  represent  any  possible face in  your  database   right \nusing  a  linear  combination  of  some  vectors  ok   now  these  vector  should  have  some \ninterpretations right  it should be connected to faces and somewhere  otherwise how will \nyou construct a face from  taking a linear combination or some random vectors  do you \nget the point  \nso   can  you  think  of  each  of  these  ten  k  dimensional  vectors  which  is  the  same  as  the \ndimension of your original data  as a face and try to plot it  can you try to do that at least \nit make sense ten k dimensional ok  that is the same what you are image size was  i could \njust arrange these ten k dimension as one hundred cross one hundred and try to plot it ok  so  let us see if \nyou  do  that  what  happens  ok   we  convert  each  eigen  vector  into  one hundred  cross  one hundred  matrix \nand treat it as an image and let us see what we get  \nthis is what we get  so  this is the top sixteen eigen vectors that i have plotted  now can you \ntell  me  a  physical  interpretation  of  this   this  is  the  basis  for  constructing  any  face  in \nyour data base right  that what you are trying to say all the faces that you have in your \ndatabase or in the world you can combine them by looking at the these elementary face \nstructures right  which are your basis  \nand then you could scale them up by using these alphas  you will be multiply them with \nthe certain alpha right  and when you combine them you will get the base any face that \n\fyou had in your original database  does a physical interpretation make sense  how many \nof you get this   \n\n \nso  that is what is happening here  so  we have constructed this basis now i will come to \nthat  later   so   these  images  are  actually  called  eigen  faces  and  the  form  of  basis  for \nrepresenting any face in our database ok  in other words  we can now represent a given \nimage  as  a linear combination of these eigen  faces  so   this  is  my original  image ok   i \nwant to reconstruct it  now use sixteen or twenty five of these eigen faces  what do you think would \nhappen   we  will  get  some  face  which  has  some  error   there  is  some  error  in \nreconstructing this face  \nso  let us see what we get  so  i am using only one basis vector and i found out this alpha \none i how would i found it out  \nstudent  \n  \ndot product of the face vector with the basis vector ok  now i instead of one  i take two  you \nsee  i  have  took  this  two  basis  vectors   scaled  them  with  the  corresponding  alpha \ncoordinates and added it them up right  and i am trying to get some face value it does \nnot look it goes to the original face  that means  the dash is very high  the reconstruction \nerror is very high  \n \n\fthat  means   i  have  still  dropped  some  of  the  important  dimensions   i  have  still  drops \nsome of the important eigen vectors right  so  the value of k which is the top k eigen \nvectors is something that i need to take care of it and should be in a way  i can always \nconstruct the reconstruct  i can always compute the reconstruction error here right  how \nwill you compute the reconstruction error  \nstudent  \n  \ntake the square error between the original image and this second image that you see here \nright  so  you will take the square error between this and this and you will end up with \nthe number which is not acceptable right  now what i will do is i will go further i have \ntaken four ok still not quite there  but i can see a shape emerging right  \ni go to eight things becomes better  since you are already know what the original face at least \nyou can make sense of it  and by the time i reach sixteen i am almost there right  at least i \ncan recognize the face that is probably losing out some subtle things in the face  but i can \nrecognize it  \nnow  how many of you appreciate what is happening here  yeah of course  now what is \nhappening here  so  think in terms of a practical application right  what have you done  \nwhat have you able to be achieve  how may basis vectors where you able to store or did \nyou store  \nstudent  sixteen  \nsixteen   that means   sixteen  into  ten k values ok  and suppose  you had a million  images in  your \ndatabase  how many would you require to store  \nstudent  \n  \nwait let us we do it step wise forget about pca  if you had a million image in your data \nbase and each of them have ten k dimensions how much storage do we need  \nstudent  \n  \nmillion  into  ten  k  floating  point  values  ok   now  with  if  i  say  sixteen   sixteen  may  be  too \nambitious may be later on i will say fifty or one hundred is ok  but let us say sixty then how much \ndata we need to store  \n\fstudent  sixteen into ten k  \nsixteen into ten k and you can reconstruct any face  \nstudent  \n  \nyeah  alpha\u2019s  needs  to  be  stored  right   so   for  every  image  instead  of  storing  ten  k \ndimensions   you  will  just  store  the  sixteen  alphas  right   and  you  can  see  that  even  if  i  go \nthere to one hundred and its still manageable  instead of ten k i am going to just store one hundred alphas \nright  and as i go to one hundred what would happen  \nstudent  \n  \nthe reconstruction error would become even lesser ok  so  is  that is the intuition clear \nok  so  this eigen vector storage is a onetime storage  we are going to store this k eigen \nvectors  each of them are ten k dimensional and k is one hundred or two hundred  we do not really care \nbecause the original data was very large right  and for each image we just need to store \nthese alpha values k of them right  so  for each of them instead of ten k  we will store \none hundred to two hundred alpha values and of course  it is significantly reduces right  \nso  this is why we need to do all this right and what is the other advantage of doing this  \nanything of  something else  so what is pca actually allowing you to do  if you again \nthink  of  it  not   i  mean  subtract  the  math  out   just  think  it  in  terms  of  physically \ninterpretation  and  what  is that it is  allowing  you  to  do   if  you had to  say it in  english \nwhat is it allowing you to do  compression is a loaded word can you just spell it out for \nme what is this compression mean actually  \nstudent  \n  \nright   so   it  is  storing  all  the  relevant  information  in  the  image  and  discarding  all  the \nerror element information right  now this also ensures that if you have multiple images \nof  the  person   then  what  would  happen   you  should  take  the  image  under  lighting \nconditions  or  may  be  at  person  had  applied  some  makeup  or  something  like  that  right \nwhat would happen  \nstudent  \n  \n\fin the original space  the ten k dimensional space what would happen to these images  \nthey will be very far from each other right because the lighting conditions have changed  \nyou see a dark person  so have a light person something like that right  and now because \npca has helped you to throw away this dimensions right  may be the exact terminology \nwhich i am using may be the lighting condition  do not do it  because you can imagine \nthat  there  would  something  right  that  suppose  as  some  element  which  is  calling  the \nimage to look slightly different  but that is not the important information right  so  that \nwould get discarded off and only the relevant information would stay  \nso  then multiple images of the same person which were dash in the original space would \ncome dash in the new space  \nstudent  \n  \nfar  in  the  original  space  would  come  closer  in  the  new  space  right   so   this  is  what \ncompression  helps  you  to  do   so   this  is  what  you  want  to  learn  you  want  to  learn  the \nimportant  characteristics  of  your  original  data  and  that  is  what  pca  allows  you  to  do \nfine  \n\f"}
{"audio_filepath": "lec005_009.wav", "duration": 1549.382, "text": "\nsingular value decomposition \n\n \nso  with this we will move on to the last topic in the yeah so  that is something that you \nwill have to so  the way i would do it  right is that you keep aside some one hundred images from \nyour  data  as  validation  data   now   once  you  have  learned  these  eigenvectors   try  to \ncompute the reconstruction error for these one hundred images  and just vary it   do one hundred  one thousand  \nten zero written as many dimensions as you can  and see at what point is a reconstruction \nerror   ok  for  you   right  and  this  is  assuming  that  you  have  some  notion  of  what  is  a \nreasonable reconstruction error  so  we all know that the minimum is zero  \nbut  if  you  have  zero five   then  maybe  for  face  database  it  might  be   but  if  it  is  a  database \nwhere you are trying to look at mechanical parts  so  suppose you are looking at motors \nand rotors from a machine assembly  now there you want to be able to distinguish minor \ndetects defects on this and a detect could a defect could actually just be one single or two \npixels getting different  from the original image  right  so  there the reconstruction loss \nwould be much needs to be much more robust  you get the point  so  it depends on your \n\fapplication  so  you will have to take some validation data either have a domain expert to \ntell  you  what  is  reasonable  or  go  by  the  number  that  you  get   right   and  this  is  the \nvalidation error that i get  \nso  everyone understands the question and perhaps the answers  ok  so  we now go to the \nlast module  \nstudent  \n  \nyeah  if you can  \nstudent  \n  \nyes  you can now project any face into this database a  so  that is the  eigen basis that \nyou  have  got   you  have  got  the  basis  vectors   now  any  data  you  can  project  onto  this \nbasis  \nstudent  \n  \nnow   so   if  you  are  trying  to  learn  these  eigenvectors  by  say  using  one hundred  images  all  of \nwhich belonging to a particular demographic  say all caucasian images  right  and now \nat the runtime you have an asian image  then you will have  obviously  have some error \nright  but you have large even of data  say if you have if you are constructing this from \nmillion images  then it should generalize that is i mean just as for any machine learning \nalgorithm  \nthe training it from small data and you bring out some outlier at test time it is not going \nto work  right  but if you have reasonable data it should generalize  any other questions  \nto calculate the eigenvectors x is m cross ten m cross ten k yes  now  we move on to the \nlast topic for the basic portion  and the next class we will do auto encoders will be back \nto deep neural networks so  singular value decomposition  right  \n\f\n \nso   this  is  actually  the  stuff  that  i  need  an  important  theorem  from  here  at  multiple  two \nplaces in the course  so now  before doing the  right  let us get some more perspective \non what eigenvectors do and why are they actually important  \n\n \nso  let v one to v n be the eigenvectors of a and let lambda one be the corresponding eigen \nvalues  so  we know this a v one equal to lambda v one and so on  ok  now suppose all the \nvectors  in  r  should  be  r  raised  to  n   ok   so   if  a  vector  x  belonging  to  rn  can  be \nrepresented using this basis  ok  now  what if i am interested in the operation a into x  \n \n \n\fwhat is the advantage of representing it using this basis  so  this is what you are saying \nthe other day  \nstudent  \n  \nwhat  is  ax  it  is  a  matrix  vector  multiplication   right   and  it  is  going  to  be  a  heavy \ncomputation  now if all  my vectors in rn are  represented using the  eigenvector  as the \nbasis  what happens to this matrix operation  \nstudent  \n  \nit reduces to  \nstudent  \n  \nlet us see so  i was interested in ax  but i know x is this  so  you get this step  and what \nhappens finally  do you have the matrix anywhere here  so  what happens to the matrix \noperation  \nstudent  \n  \nit  reduces  to  a sum  of scalar operations  right   if  your vectors were representing using \nthe eigenvector as a basis  \nso  this is one reason why this is important  right  so  you can now get away of the get \nrid of the matrix operations and just do scalar operations  right  so now  there is a catch \nhere which  i am  going to ignore  just to  try it if  i bring in  the catch  you guys will  get \nconfused  so  i will ignore if anyone has a doubt maybe talk to me after the class  but for \nnow let us go with the fact that the matrix operation reduces to a scalar operation  \n\f\n \nnow  so far what we have done is discussed square matrices  i have said that they are the \nvillains  of  linear  algebra   but  who  are  the  super  villains  of  linear  algebra     rectangular \nmatrices  everyone  says  that   but  why   imagine  what  they  do  to  a  vector  yeah   so   can \nrectangular matrices have an eigenvector  \nstudent  \n  \nyes  obviously  yes that i mean any matrix can have an eigenvector  \nstudent  no  \nno  why   can  you  write  something  of  this  form   you  can\u2019t   right   because  when  the \nmatrix operates on an n dimensional vector what does it give you  \nstudent  \n  \nan  m  dimensional  vector   right   hence  they  are  super  villain   right   because  they  take \nthe  vector  from  one  space  and  transform   it  to  a  completely  different  space  that \ncompletely  lots  lost  it  is  identity   right   so   that  is  why  rectangular  matrices  are  even \nharder  \nso  now  we  just  saw  that  for  square  matrices  this  eigenvectors  form  a  very  convenient \nbasis where these operations reduce to a scalar operation  but now rectangular matrices \ndo not even have eigenvectors  so  then cannot we have the same advantage there  can \n \n\fwe  have  the  same  advantage  there   you  can\u2019t   right  because  you  do  not  have  an \neigenvector  but i would teach you about singular value decomposition  so  i better have \nsomething  so  get the connection  ok  there is a problem with square matrices with the \nrectangular  matrices   so  now   let  us  see   so   we  will  try  the  aim  is  to  see  if  we  have \nsomething equivalent to this scalar transformation that we had for square matrices  \nhow many of you have seen this in linear algebra before  so  you know whatever i am \ngoing  to  talk  about   fine   so   the  result  of  ax  is  a  vector  belonging  to  r  m  and  the \noriginal x belongs to r m  so  we do miss it miss out on this advantage that you could \nhave reduced the matrix operation to a scalar operation  and now we will try to see if we \ncan still get back that advantage  \n\n \nso   notice  this  is  matrix   you  can  think  of  it  as  a  function   which  provides  a \ntransformation from rn to rm  so  what is the set of inputs to the matrix  it is vectors \nbelonging to rn   that is the set of input  \nnow  suppose we had a pair of vectors v one u one v two u two vk u k  each belonging to these two \ndifferent universes one is rn the other is rm   and there was a specular relation between \nthem that a into vi is equal to sigma into ui  suppose i am just being ambitious let us see \nwhether  we  can  actually  have  this  pair   but  suppose  we  had  this  pair   then  can  you \nconnect this back to the discussion on scalar operations  so  let us just see that in detail  \nand  we  will  of  course   assume  that  these  are  orthogonal  and  form  a  basis   so   the  vi\u2019s \n \n\fform  a  basis  in  rn  and  the  ui\u2019s  form  a  basis  in  rm   is  that  clear  that  is  all \nstraightforward  we have these vectors  \nnow  every a vector belonging to rn which was the input space can be represented using \na linear combination of v straightforward  and any vector belonging to the output space \ncan be represented of  \nstudent  \n  \nof  u   right   so   that  means   any  x  in  the  input  space  i  can  write  it  as  this  linear \ncombination  and now if i do the matrix operation what happens  \nstudent  \n   \nyou get this a into vi  what is a into vi  sigma ui  i have still not shown you how to find \nthese sigma is ui by the way  right  ok  once again the matrix multiplication reduces to a \nscalar multiplication  \n\n \nso now let us try to look at a geometric interpretation of this  \n \n\f\n \nso   what  you  have  is  this  original  space  which  is  rn  you  are  using  a  as  a  matrix \noperation  right as a function and you are transforming vectors from n to rm right  so  \nthis  is  the  space  transfer  that  i  was  saying  it  vectors  are  being  picked  up  from  rn  and \nbeing put into r m  ok  and rn is a space of all vectors which can act as inputs to this \nfunction  and rm is a space of all vectors which are the outputs of this function ax  \nnow  we are interested in finding a basis u v such that v is the basis for the inputs  when \ni say basis all of you should immediately start thinking of dash vectors  \nstudent  orthonormal vectors    \northonormal vectors orthogonal or orthonormal  right  once we have orthogonal we do \nnot  care  about  the  rest   u  is  the  basis  for  the  outputs   such  that  if  the  inputs  are  and \noutputs are represented  using this basis  then all our matrix operations  reduce to  scalar \noperations   so   we  are  just  trying  to  find  the  rectangular  analogy  for  the  square  a \nphenomenon that we observed  ok  that is what we are trying to do  now can you tell me  \ni have told  you that if such a v and  u exists  then  you could  do this  can  you  give me \nsuch a u and v  \nso  what do we mean by so  here i said actually i said this  right  that the dimension of \nthe row space is actually k and the dimension of the column space is also k what do you \n \n\fmean by the dimension is i mean  right  here i am telling this is rn and this is rm  and \nnow i am telling you the dimension is k  what do i mean by that  \nstudent  \n  \nthe only k linearly independent vectors  \n\n \nand this is again something from linear algebra which i expect  you to know is that all \npossible vectors in rn only a subspace belonging to rk can actually act as input to a x to \nproduce a non zero output  so  i am talking about a null space column space and things \nlike that right  so  this should be clear if it is not it is  it is not very important at for us \nright now  \nand hence we have only k dimensions  \n \n\f\n \nso  let us look at a different way of writing this  so  you have this a v one is equal to sigma \none u one av two is equal to sigma two u two  so  i can again do the same trick that i put all the v\u2019s \ninto one matrix  where vi\u2019s are the columns of this matrix  and i will put all the us into \nanother matrix where ui\u2019s are the column of this matrix  is that fine everyone  ok  so far \nand  then  i  can  write  it  as  this  matrix  operation   same  thing  that  we  did  when  we  are \ndoing  eigenvalue  decomposition   right  so   we  had  written  it  as  a  into  u  is  equal  to  u \ninto sigma  right  because there we had the condition that ax is equal to lambda x  now \nwe have a u is equal to sigma v or rather the other way around  so  av is equal to did i \nmissed up  did i no  right  \nstudent  \n  \nsorry  \nstudent  \n  \nfine  yeah so  av is equal to sigma into u  \nso  is this fine no  but when you do the diagonal operation you will get it as u into sigma \ny  the same way as a x equal to lambda x  but when you write it is a into u is equal to \nlambda comes later on  \n \n\f\n \nand if we have k orthogonal vectors  so  remember i said that this basis consists only of \nk dimensions  right  because that is r the set of vectors which can act as input to a  so  \nwhat i  but i want a basis for the full rn  so  what do i do for the remaining n minus k  \nhave you heard this gram schmidt othogonalization  right  so  if i give you if there if \nyou are trying to construct a basis for n  ok  for rn rather and if i give you k orthogonal \nvectors  they can do k you can construct the remaining n minus k using  gram schmidt \northogonalization right  so  you can get the full basis ok fine  so  let me just see and this \nis  orthogonal  ok   so   you  can  write  so   you  see  these  two  forms  can  you  relate  it  to \nsomething that we have seen before in the course   \nthis is singular value decomposition  what else did we see before  \nstudent  \n  \neigenvalue  so  this exactly the same forms  right  and i have used the same set of tricks \nto arrive at it right  so  i first put the vectors into a column as columns into a matrix then \nwrote this in the matrix format  and then pre multiplied post multiplied by certain things \nand i got these two formats  and remember that v and u both are dash matrices  \nstudent  \n  \n \n\forthogonal  matrices   right   so   that  inverse  is  just  their  transpose   ok   ok   so  far \neverything is fine  now i still do not know what u and v are  all this analysis is assuming \nthat i know what u and v are  so now can you tell me how to get these u\u2019s and v\u2019s  \n\n \nsuppose  v  u  and  sigma  exist   then  we  can  write  this  right   so   a  is  u  sigma  v  so   a \ntranspose would be the transpose of that  now can you work with me  what is the next \nstep  \nstudent  \n  \nok next  \nstudent  \n  \nthis is u sigma v transpose  so  then this would be i think the next step is no the next \nstep is also wrong  that fine  ok fine  i just had some error with the transpose  ok  what \nwill happen now  what will disappear from here  \nstudent  \n  \nu transpose u that is i  right  u transpose the inverse of u  \n \n\f\n \nso  you get this  what does this look like  this looks like the eigenvalue decomposition \nof  \nstudent  a transpose a  \na transpose a  that means  v consists of the  \nstudent  eigenvectors  \neigenvectors of the  \nstudent  a transpose a  \na transpose a  so now  can you tell me what u would  \nstudent  \n  \n ok fine  \nso   this  looks  like  the  eigenvalue  of  eigenvalue  decomposition  of  a  transpose  a  \nsimilarly  we can show that a a transpose is equal to u transpose sigma square u  ok  so  \nthen u is the set of eigen vectors of a a transpose  right  and now here what was with \nwill the eigenvalue decomposition always exist for a matrix  \nstudent  no  \n \n\fno under what conditions would it exist  first of all it has to be a square matrix  \nstudent  \n  \n ok  right   but  now  for  a  rectangular  matrix  would  be  singular  value  decomposition \nalways exist  \nstudent  yes   \nyes   right   because  it  depends  on  the  eigenvalue  decomposition  of  square  symmetric \nmatrices  ok   is that  fine  ok  so  for any matrix shall always have the eigen value oh \nsorry the singular value decomposition  \n\n \nso   this  is  perhaps  yeah   ok   now  just  one  last  bit  and  let  us  see  if  all  of  you  can \nunderstand this  so now  i can write a in this form this is nothing but what i already said  \nright  this is u this is sigma this is v transpose  ok  now from here from this step do you \nsee how i got to this step  this is something that we were struggling with yesterday also  \nwhen we were trying to find out summation x i x i transpose something similar here  you \nknow the four ways of multiplying matrices  right  so  this is which way one of the ways  \nyeah so  does everyone get this  right  so  a simple thing would be first to just take these \nsigmas inside  right  because this is a diagonal matrix  right this is all zero\u2019s  so  these are \nactually you can write it as sigma one u one sigma two u two and sigma k u k  right  \n \n\fnow  this ends up being the product of two matrices  right  and you can write it as a sum \nof columns into rows right  so  what i am writing it as a sum of sigma one u one multiplied \nby v one  so  sigma one u one into v one transpose is a scalar matrix vector matrix right  so  each \nof these terms here is a  \nstudent  matrix  \nmatrix   and  you  are  adding  k  such  matrices   ok   now  try  to  relate  it  to  reconstruction \nerror  you are taking a matrix trying to write it as sum of many matrices  if i trim some \nterms from this some terms from this sum what would happen  if i have all the terms  \nthen  what  would  happen   i  will  get  a  back  exactly   if  i  drop  some  terms  what  would \nhappen  \nstudent  \n  \ni get an approximation of a  how good would that approximation be  \nstudent  \n  \nfirst is depending on the number of dimensions  but is there a natural ordering in these \ndimensions if i want to throw away some dimensions which one would i throw away  \n student  \n  \nsmallest  \nstudent  singular values  \nsingular values  sigmas are the singular values  so  you see that this is getting multiplied \nhere   every  matrix  is  getting  multiplied  by  the  singular  value  corresponding  singular \nvalue   so   if  i  drop  out  the  terms  which  have  the  smallest  singular  values   then  those \nmatrices  the  elements  would  have  been  very  small   so   i  will  not  lose  much  in  the \napproximation   so   again  the  same  idea  that  i  am  trying  to  approximate  the  original \nmatrix by a smaller rank   \nby of so now  the original matrix had m cross n entries  ok  how much if i use only k \neigenvectors or the sorry k singular vectors or k dimensions to approximate it  how much \n\fstorage would i need  how many values do i need  so  the original matrix was m cross \nn  how many entries are there here  \nstudent  \n  \neach of this is how much  \nstudent  \n  \nm for ui plus n for vi  ok  and plus one for the sigma  and how many of these are there  \nstudent  k  \nk  so   if  k  is  very  less  than  your  m  and  n   right   then  again  you  will  have  some \ncompression you get this ok  so  all of these ideas are related and i want you to be able to \nconnect  them   right   that  all  of  this  is  towards  doing  some  approximations \nreconstructing  some  reconstructing  a  matrix  from  it  is  components   and  doing  this \nreconstruction in a manner that you end up making minimum error in the reconstruction  \nis  this  idea  clear   even  if  some  part  of  the  math  is  not  clear   is  this  idea  clear   how \nmany forget this  ok so  some of you do not  you do not  \nstudent  \n  \nyeah  so   what  is  the  original  dimension  of  a   m  cross  n   right  now  i  am  trying  to \nreconstruct it using a sum of sum k terms  ok  so  hence this k comes  now each of these \nterms how many elements do i have  i have ui which is of dimension m  i have v i which \nis of dimension n and then i have the sigma i which is of dimension one  right  and i \nhave k of these  so  this is the total amount of storage that i need  i am saying that as k is \nmuch less than m and n which would typically be the case   \nthen you are getting a much lower space reconstruction of the original data  right  and \nyou are doing this reconstruction smartly  because you are not taking any k dimensions  \nyou are taking the k most important  dimensions  and this most important  is  defined by \nthe singular values  this is designed by the sigma is that fine  \n\f\n \nok and actually there is a formal theorem which says that sigma one u one v one transpose is \nthe best ranked one approximation of the matrix  is this a rank one matrix  is sigma one u n \ni hope you guys have done the assignment  right  sigma one u one v one transpose is the rank \none  matrix   and  if  i  take  this  idea  further   this  summation  is  the  best  ranked  two \napproximation and if i keep going  this summation is the best rank k approximation  so  \nwhat it says is that if you are trying to reconstruct the original matrix  right from these \ncomponents  and if  you go by the eigen or the singular values  and  you pick the ones \ncorresponding  the  top  k  singular  values  then  the  best  that  is  the  best  possible \nreconstruction that you could have done  \nnow   how  do  you  formally  define  reconstruction   how  would  you  make  it  as  an \noptimization problem  what are you trying to minimize  \nstudent  \n  \nthe  actual  matrix  has  some  values  which  is  the  matrix  a   ok   b  is  the  reconstructed \nmatrix  using  only  k  dimensions   how  many  of  you  understand   what  is  this  product \nsaying what is this  \nstudent  \n  \nfirst k columns of u  these are the k singular values  and these are the first k rows of  so  \nok i was just talking about this is the first k columns of u  these are the k singular values \n \n\fare put across the diagonal  and this is the first k rows of v transpose  and this is exactly \nthe product which i showed you here  is that fine  \n\n \nok  so   there  is  a  theorem  this  is  called  the  svd  theorem  it  says  that   if  you  want  to \nreconstruct a then this is the best rank k approximation that you can get  now if i want to \npose  it  as  an  optimization  problem  what  will  i  say  what  would  i  have  minimized \nactually  this is the reconstruction  right  so  let us call it a hat actually  and what does \nthis mean  this is the dash norm  \nstudent  \n \nfrobenium norm  what does the frobenium norm give you  squared difference between \nthe  elements   right   roughly  speaking  ok   so   it  will  tell  you  what  is  the  square \ndifference  between  the  ij\u2019th  element  of  a  and  the  ij\u2019th  element  of  b   so   whenever  we \nhave this situation if you are trying to if this is our objective function that we have trying \nto reconstruct a  or try to transform something and get a predicted a or a reconstructed \na then the best possible reconstruction would be given by this solution  \nso   this  optimization  problem  has  a  solution  that  you  just  use  the  eigenvectors  of  xx \ntranspose and sorry aa transpose and a transpose a  right  is that clear  ok so  this is \nthe theorem that we will be using when we are talking about autoencoders  and we will \ntry to  connect auto encoders to pca ok  so  just revise this is the prerequisite for next \n \n\fclass  whatever  we  have  done  in  the  last  three  sort  of  extra  lectures   you  have  to  revise  it \nbefore  you  come  for  class  tomorrow   right   ok   and  yeah  this  is  sigma  is  just  some \nterminology  sigma  is  actually  the  square  root  of  lambda  a  that  was  obvious   and  u  is \ncalled the left singular matrix of a and v is called the right singular matrix of a  \n\f"}
{"audio_filepath": "lec006_001.wav", "duration": 3142.202, "text": "\nautoencoders  sparse autoencoders  contractive autoencoders \nwelcome  to  lecture  seven  of  the  course  on  deep  learning  cs  seven thousand and fifteen   in  this  lecture  we  are \ngoing to talk about auto encoders and we will focus on their relation with pca  then talk \nabout regularization in auto encoders  wherein we will look at denoising auto encoders  \nsparse auto encoders and contractive auto encoders  so  let us begin with the introduction \nto auto encoders what they are  \n\n \nso  this is what a typical auto encoder looks like  and as you can see this is very much \nlike  a  feed  forward  neural  network  you  have  an  input  which  is  x  i   so   you  are  given \nsome  training  data  you  are  given  some  i  samples  x  i  to  x  n   so   this  is  your  training \nmatrix x  which  we  have  seen  in  the  previous  lectures   so   this  is  one  of those  training \ninputs x i  and then you have a hidden layer and then an output layer  so  let us look at \nwhat is the configuration of the hidden layer and what does the output layer actually try \nto do  \n\f\n \nso  it is a very special type of a feed forward neural network  what it does is it encodes \nwith input x i to a hidden representation h ok  and it uses an encoded function to do this  \nso  this is what the encoded function does  it first does a linear transformation  \nso  w is a matrix and x i is a vector and you again have the bias b as a vector right  so  \nlet us look at these dimensions right  so let us try to fix some dimensions  so  suppose x i \nbelongs to r n that is what we have been considering throughout the course  so  far and \nlet us say h belongs to r d  \nso  it is a d dimensional representation  so in that case what would w be  yeah  so  w \nwould be r n cross or the d cross n right  so  it will multiply with the n cross one vector \nwhich is x i and give you a d cross one output right  and similarly the b would also be d \ncross one  and then on top of that you have this non linearity g  which will be operating at \nelement wise just as we had seen earlier  so  it could be any of the sigmoid functions the \nlogistic or tanh and so on  \nso  the end result is you have taken an input x i and encoded into a hidden represent h by \nusing a linear transformation first  and then a non linear transformation right  so  i refer \nto w x plus b as a linear transformation  because it is a matrix multiplication  now once \nyou have constructed this hidden representation  \n \n\f\n \n the job of the decoder or the latter half of the feed forward neural network which is this \nhalf is to take this encoded representation  and then try to reconstruct x again from it  \nso  again let us first look at the equation  so this is the equation for the decoder  where \nagain  you  first  take  the  hidden  representation  do  a  linear  transformation  and  then  you \nagain have some function on non linearity on top of it right  so  we will see what this \nfunction can be so we will refer to it as f for  now we will not say whether this is sigmoid \nor linear or what kind of a function it is  we will come back to it later on  \nso  now let us again look at these dimensions  so what is x i  x i is again r n and your h \nwas r d  so  you have to go from a d dimensional input to an n dimensional output  so  \nagain your w star is going it to be r d cross sorry r n cross d  so  it will multiply with a \nd cross one vector and give you an n cross one output right  and that will pass through some \nfunction and it will give you x i hat which is a reconstruction of x i  \nso   why  are  we  trying  to  do  this  right  we  took  an  input  x  i   we  computed  it  is  hidden \nrepresentation by doing some non linear and linear transformation and then again we are \ntrying to reconstruct x i hat  so  why are we trying to do this  so reason we are doing \nthis  is  that  we  want  to  learn   what  are  the  most  important  aspects  or  most  important \ncharacteristics of the input data x i right  so  if you compute a hidden representation h  \nwhich is presumably smaller than your original input data  \n \n \n\fand  from  that  hidden  representation  if  you  are  able  to  reconstruct  x  i  right   then  that \nwould  mean  that  this  hidden  representation  captures  everything  that  is  required  or \neverything  that  is  yeah  everything  that  is  required  to  reconstruct  x  i  from  x  i  from  the \noriginal input right  \n\n \nso  the model will be trained to minimize the difference between x i and x i hat  so  you \nwant  to  make  sure  that  after  passing  through  this  bottleneck  which  is  the  hidden \nrepresentation you are able to reconstruct x i  and the reconstructed output is very close \nto the original input right  \nso   can  you  see  an  analogy  with  pca   where  you  are  trying  to  find  this  hidden \nrepresentation or this most important elements of the original input x i  so  there we had \nused this linear transformation where we are taking the original input x  and transformed \nit to a new basis and we had used that basis for representing the original input right  so  \nsomething  similar  is  happening  here   we  are  using  this  hidden  representation  h  to \nrepresent our original input  \n \n\f\n \nnow  let us consider a few cases the first cases when the dimension of h is less than the \ndimension  of  x  i   in  this  case  as  i  was  trying  to  say  earlier  if  we  are  still  able  to \nreconstruct x i hat perfectly from h  then what does it say about h it tells us that h is a \nloss  free  encoding  of  x  i   it  captures  all  the  important  characteristics  of  x  i  write  just \nrepeating what i had said on the previous slide   and now  you can see an analogy with \npca because h has all the important characteristics required from the original input data  \nso   it  has  probably  got  rid  of  all  the  noise  or  all  the  low  variance  dimensions  or  the \ncorrelated dimensions and so on  and this is just the compact representation  which is as \ngood  as  the  original  representation  and  from  there  you  can  reconstruct  the  original \nrepresentation   and  such  an  auto  encoder  where  the  dimension  of  the  hidden \nrepresentation  is  less  than  the  dimension  of  your  original  input  is  known  as  an  under \ncomplete auto encoder  \n \n\f\n \nnow  let us  look  at  the  other case where the dimension of the hidden representation is \ngreater than the dimensional of the original input  ok such an auto encoder is i will tell \nyou what it is called  so  we will we are looking at the case where the dimension of the \nhidden representation is greater than the dimension of the original input  \nso   now  in  such  a  case  the  auto  encoder  could  learn  a  very  trivial  encoding  by  simply \ncopying x i into h and then copying h into x i right  so think of this from a compression \npoint of view right  so  now  suppose you have ten bits initially right  and then you want \nto somehow compress it and store it only in four bits  \nand  now  this  four  bits  should  be  such  that  it  captures  everything  that  was  there  in  the \noriginal ten bits because you would want to reconstruct the original input again right  so  \nthis is what we do typically when we compress any of our files right we have a larger file \nwe compress into a smaller information while making sure that everything important is \nthere  so  that whenever i want to recover it  i can just recover it from there  \nso  this is definitely a hard task  but now what i am doing in this auto encoder is that i \nhad ten bits  i am actually giving it more bits now because the dimension of h is greater \nthan the dimension of the input  and then from these sixteen bits  i want to reconstruct the ten \nbits now this is a very trivial task right because all i could do is copy these ten bits into \nthe  first  ten  bits  here  leave  the  remaining  six  blank   and  then  from  those  ten  bits  just \n \n \n\freconstruct  the  input  that  is  very   very  trivial  if  you  give  me  more  storage  and  what  i \noriginally needed  then definitely i can easily reconstructed  \n\n \nso  this looks very trivial and this is what it could do right just copy the input to the first \nthe n bits  \n\n \nso  this was n and this was d and we are looking at the case where d is greater than n  so  \nit will just copy the input to the first n bits and then just take it back to the output just as i \n \n \n\fsaid in the case of you have ten bits sixteen bits and then again ten bits it is very trivial to do \nthis  \nso  such an identity encoding is useless because you are just not running any important \ncharacteristics  of  the  data   your  h  is  almost  the  same  as  x  i  it  also  has  all  the  useless \ninformation that x i had  in fact  it has slightly more because it has these blank units also  \nbut  this  is  not  really  useful  right  why  would  you  want  to  actually  learn  such  a  hidden \nrepresentation right  so  it is not clear why would you want to do that so we will take a \nlook at it we will come back to why this is important  \nso  such an auto encoder is known as an over complete auto encoder because it has the \nhidden  representation  has  more  number  of  neurons  as  compared  to  the  original  input  \nnow let us look at a case where this would actually be important right  so  this is a very \nrough intuition for why you would want an over complete auto encoder  \nso  let us consider the case where you have as input one of the features that you are looking \nas  bmi   so   suppose  you  are  trying  to  find  out  whether  the  person  is  likely  to  get  a \ncertain  disease  or  not  right   so   whether  he  would  have  a  heart  attack  or  whether  he \nwould have a diabetes  would have diabetes and so on  and you are looking at various \nparameters or various medical parameters of that person and one of them could be height \none of them could be weight and one of them could be bmi  \nnow  for whatever reason  you have not  computed  the height  and weight  and  you have \nonly looked at the bmi  so  now  what has happened in your input and all of you know \nthat bmi is actually body mass index which is a function of the height and the weight  \nso  now  what has happened is that in your original input there was already this compact \nthe your feature space is already compact  because you would actually look at you should \nhave  actually  looked  at  both  the  features  height  and  weight   but  for  some  reason  you \nhave only computed bmi and you could think of various some other correlated features  \nwhich are functions of many other features  but you do not look at all those features and \njust this final function of those features   \nso   now  in  that  case  if  suppose  your  prediction  is  that  this  person  has  or  has  a  high \nlikelihood of being of high likelihood of having diabetes at some point in his life  then \n\fyou would want to know whether it was the height  or whether it was the weight which \nwas responsible for this  \nso   in  your  original  input  your  features  are  actually  entangled  and  you  would  like  to \ndisentangle  them  right   so   you  would  want  to  go  from  this  smaller  feature  space  to  a \nlarger  feature  space  where  some  of  these  entangled  features  get  is  disentangled   so   in \nthose cases we reach an over complete auto encoder  however  the problem still remains \nthat  there  is  no  reason  why  the  machine  should  actually  learn  to  disentangle  these \nfeatures it could still just simply copy the bmi here and then copy it back here   \nso  that is why when you are dealing with over complete auto encoders you will have to \ndo something special to prevent  this kind of identity encoding  so  as  you just take the \ninput  and  copy  it  to  the  hidden  layer   and  then  copy  it  back  to  the  output   so   we  will \nlook  at  what  kind  of special treatment  you need to  do to  prevent  these kind of identity \nrepresentations  \n\n \nhere is the road ahead  so  first we will talk about the choice of f x i g x i right  so  we \ndid not say anything about what these functions f and g have to be  so  we will talk about \nthose and then we will talk about the loss function  so  i have just told you so far that we \nwill train this model in a way that x i is very close to x i hat right  and i have argued that \nif  we  are  able  to  actually  achieve  that  that  x  i  hat  is  the  same  as  x  i  in  which  case \n \n\fpresumably  presumably the loss would be zero  that means  our hidden representation has \ncaptured all the important characteristics of the original data  \nsame  as  in  the  analogy  of  ten  bits  to  four  bits  to  again  ten  bits  right   if  i  am  able  to \nreconstruct  this  without  any  error  that  means   loss  is  zero  then  these  four  bits  or  the  hidden \nrepresentation  of  my  original  x  was  actually  able  to  capture  everything  that  was \nimportant in x  so  that it can reconstruct x again as x hat without losing any information \nright   so   that  is  the  loss  function  that  we  would  want  now  what  is  the  actual \nmathematical formulation for this loss function that is what we will see next  \n\n \nso  first let us start with the choice of f  f and g  so  we will consider two case two cases one \ncase when your inputs are binary and the second case when your inputs are actually real \nnumbers right  so  the first we will look at the binary case  so  now  just some notation \nclarification   so   remember  our  original  data  was  this  matrix  x  which  was  m  cross  n  \nthat means  you had x one x two up to x n and each of these was r n   \nso  now when i am referring to the entire row or entire data instance i will use bold x i  \nas i have circled here  and i want to refer to one of the elements of this guy  then i will \nuse this notation x i j same as what i have written here  so  what i am saying is that each \nof these x i j\u2018s actually is a binary variable  \n \n\f\n \nnow  which of the following functions would be most appropriate for the decoder  so  \nremember was the input was binary  that means  your output also has to be binary you \ndo not want to produce numbers arbitrarily large belonging to any or want do not want to \nproduce any real number you want to produce numbers which lie between zero to one  so  in \nsuch  a  case  what  would  be  an  appropriate  loss  function  or  sorry  what  would  be  an \nappropriate function for the decoder  so  remember i am asking you what would f  be  \nso   i  am  giving  you  three  choices  it  should  be  tanh  or  just  a  linear  decoder  or  a  logistic \nfunction  which of these would be most appropriate logistic why would that be because \nit  will  make  sure  that  your  outputs  are  between  zero  to  one   tanh  would  give  you  outputs \nbetween minus one to one  but you do not want that because your inputs were between zero to one  \nso  when you are reconstructing  obviously  you want outputs between zero to one  you do not \none minus one to one  and linear of course  can give you any real number which is not what \nyou want right   \nso  if you produce any arbitrary real number like hundred and so on  your loss is going to \nbe very high because your inputs were just zero to one and you are producing these arbitrary \nreal numbers which are very different from what your input was ok  so  in this case the \nlogistic function makes  the most appropriate choice  and g is  typically that  means  the \nencoded function is typically again chosen as a sigmoid function  so  it could either be \n \n\fthe logistic function or the tan h function right  so  the there is you could choose any of \nthese as the encoder function fine  \n\n \nnow  let us consider the other case where your inputs are real valued  that means  when \nyou  reconstruct  something  you  should  again  produce  real  values   that  means   your \nfunction f should take whatever is the input given to it and map it to some real numbers \nright  so  that is what we want from this function f earlier in the binary case we wanted \nit to map it to binary numbers right  so  that is the difference that we have now  \nso   in  this  case  which  of  the  following  would  be  appropriate   the  second  one  right \nbecause  tan  h  does  not  make  sense  because  it  will  just  produce  minus  one  to  one   but  you \nwant to produce any possible real number because some of these are actually higher than \none greater than one  linear would be fine because it will produce any real number logistic is \nagain not fine because it will produce numbers between zero to one  \n \n\f\n \nso  the logistic and tan h as i said would clamp the output to certain ranges  so  that is \nnot appropriate hence you should choose the linear function and again in this case also g \nis typically chosen as the sigmoid function fine ok  so  the next thing that we look at is \nthe choice of the loss function  \n\n \nand again we will consider both the cases  where a case the first case is the inputs are \nreal valued and the second case is when the inputs are binary  \n \n \n\fso   let  us  look  at  the  real  case  first   now  here  the  objective  of  the  auto  encoder  is  to \nreconstruct  x  i  hat  to  be  as  close  to  x  i  as  possible   now  we  have  actually  seen  this \nbefore   so  something  similar  before  when  we  were  talking  about  regression   so   now  \nyou want to produce real valued outputs and they should match your real valued inputs  \nso   what  is  an  appropriate  loss  function  that  you  can  choose  the  squared  error  loss \nfunction right   \nso  what does this actually capture  it says that for all my input data x one to x m  for each \nof these dimensions x one to up to x one n right  i want to make sure that my original input  i \nwill have a similar x hat reconstructed where i will have x one one hat x one two hat and x one n \nhat  \nso  i want to make sure that each of these pairs of variables are actually similar  and i \ncan  capture  that  by  ensuring  that  the  squared  error  loss  between  the  ij\u2019th  entry  in  my \noutput is the same as this or sorry  rather i could capture the squared error loss between \nthe  ij\u2019th  entry  in  the  output  and  the  input  ok   that  is  what  this  function  is  trying  to \ncapture straightforward similar things we have seen while we were doing regression  \nexcept that there we had y hat and y  but here we are just trying to reconstruct the input  \nso  there is no y here we just have the x ok  and the parameters of the objective function \nare of course  all the variables or all the parameters that we have in a network  which is \nw w star c and b  \n\n \n \n\fand the matrix or the vector way of writing this is the following  so   we have x i  so  \nwhat i am looking at here is i have gotten rid of this summation and i am just written it \nin vector form  so  let me just explain what this means  so  this is what x i would look \nlike right  so  this would be x i one x i two up to x i n ok this is the vector  and then you have \nthe x i hat vector which is going to be x i one hat x i two hat up to x i n hat  \nso   taking  the  difference  between  these  two  vectors  that  is  what  this  term  is   so   what \nyou will get is essentially x i one hat minus x i one up to x i n hat minus x i n right  and then \nyou are taking the dot product of this vector with itself which will essentially give you \nthis summation right  so  the dot product of this vector with itself is actually going to be \nthis summation  it is going to be the sum of the squares of the elements of this vector and \nthat is exactly what we wanted  \n\n \nso  this is a more compact vectorial way of writing the same thing  and now we can just \ntrain the auto encoder by treating it as a regular feed forward neural network this is just a \nlike any other feed forward neural network you have find the loss function  \nand you can just use back propagation to treatment right  but and in this case all we will \nneed  is  a  formula  for  the  gradient  of  the  loss  function  with  respect  to  with  your \nparameters which are w and w star  i have again ignore the biases and the bias is here b \nand c  so  we will also need dou l theta by dou b and dou l theta by dou c right  so  these \n \n\ftwo gradients also you will need  but these are generally the easier ones to handle if you \nknow how to compute this b and c are very easy  \n\n \nso  let us look at this now what we need for back propagation as i said we will need this \ngradient right   all these four  gradients but let us  focus on one  of these  now we have \nalready  done  back  propagation  and  we  have  looked  at  arbitrary  neural  feed  forward \nneural networks here right  we did not have we just said that there are l hidden layers \nand in  this  case  l is  equal  to one  right   or other we had said  there was l  minus one hidden \nlayers and the l\u2019th layer was the output   \nso  in this case l minus one is equal to one  that means  there is just one hidden layer  so  it does \nnot  matter  we  had  actually  derived  it  for  the  general  case  when  l  is  equal  when  the \nnumber of hidden layers is l minus one and here we just have one eight n layer  so  it is much \nmore simpler than what we had learnt  and even for the number of neurons in the each \nof  these  layers  we  are  just  assumed  general  that  it  could  be  r  n   and  in  this  case  we \nwould have some r d  which is less than n or it could even be greater than n right   \nso but it does not matter because whatever algorithm we had or whatever equations we \nhad derived for back propagation  they did not care about what this n or d was we had \njust derive it in general terms right  and the same for the output layer  \n \n\fwe did not assume any number of inputs any number of neurons in the output layer we \nagain said that it has some k neurons  but there the catch is in the earlier case when we \nhad derived back propagation   we were dealing with  classification and we had these k \nclasses that we want to predict at the output  \nand in which case our loss function was actually the cross entropy or the negative log \nlikelihood  function  right   where  we  were  trying  to  maximize  the  probability  of  the \ncorrect class out of the k given classes  but here our loss function is slightly different it is \nactually this squared error loss between the input and the output  \nso   now given this  difference in  the loss function does it mean that everything that we \nlearn  in  the  previous  lecture  on  back  propagation  we  just  have  to  throw  it  all  away \nbecause now there is a new loss function  that means  my gradients are going to be very \ndifferent from what i had derived for the back propagation loss  where i was looking at \nthe cross entropy loss as compared to the squared error loss  so  does it mean that  i will \nhave to throw away all the hard work that we had done in that course in that lecture or \ncan we reuse something from them we can reuse  \nso  let us look at what we can reuse and i will just give you an intuitive explanation for \nthat   so   you  can  think  of  this  as  a  composite  function  right   and  you  are  taking  your \ninput passing it through a lot of functions and then arriving at the output  and then your \nloss function is actually a function of the output itself  \nso  what we have is something like this right we have a situation like this  that you had \nan input x you computed some function of it say x square right  so  i will call this as y one \nthen you computed some other function of it say y one say log of y one right  so  they this \nwas log of y one  so  in effect it is actually log of x square because y one is equal to x square \nand then some other function and then finally  you had the output  so  you had this other \nfunction which was sign of i am calling this y two  so  say this was sign of y two and finally  \nyou had this function which was e raise to y three  \nso  you have a very complex composite function of your original input right and this is \nyour final output function that you are considering which is e raise to y three  now the way \nyou would do this is if you want to take the gradient of d l with respect to your input d x \nright  in that case what would you do is you just apply the chain rule you will write it as \ndou l by dou y three dou y three by dou y two dou y two by dou y one and then dou y one by dou x right  \n\fand this is something very similar that we are done in the back propagation lecture we \nhad  constructed  this  chain  and  then  we  had  attacked  every  element  of  this  chain  and \nderived how to deal with that right derived an neat expression for that  \nnow  the question which i am asking you is that in that lecture we had assumed a certain \nl  and that l was actually cross entropy  but in this lecture i have actually changed the l  \nwhat i am saying is the l is actually equal to the squared error loss  now does that mean \nthat i have to throw away all this work that i had done no right  \nso  even in this example if you look at it suppose i change this function from e raise to y \nthree  to  say  square  root  of  y  three   so   i  have  just  changed  my  l   but  notice  all  of  these  other \nguys  are  going  to  remain  in  the  same   because  y  three  is  still  sign  of  y  two   so   that  the \nderivative of y three with respect to y two is not going to change  even though i have changed \nthe output function the loss function everything  else is  going to  be remain  in  the same \nright   \nso  that means  all these portions i could just reuse from the time when i had computed \nfor this chain  i just need to rework on this final expression and plug it in right  so  that \nis why all the work that we had done in the case of back propagation will not go to waste \nin particular everything that we had done  \n\n \n \n\fso  let me just go to the next slide  so  in particular everything that we had done for this \nportion of the network right which is actually dou a two all the way up to dou w right  so  \nif ok  so let me write it like this i want dou l by dou w  so  i can write it compactly as \ndou l by dou a two and then dou a two by dou w right   \nso  this portion is not going to change because i am not change any of the functions here  \ni  have  just  assumed  sigmoid  or  logistic  or  the  same  kind  of  network   the  only  thing  i \nhave changed is something at the output layer  so  i will just need to recomputed this and \nthe rest of it can be reused right  so  that is the intuition which i wanted to give you  \n\n \nand that is exactly what is written on this slide  so  i am written it as dou l theta by dou \nw star that is the first gradient i have interested in  and i could write it as dou l theta by \ndou h two dou h two and dou a two by w star right   \nnow  this portion as  i  was  trying to  say is  something that we have  already  seen in  the \nback propagation lecture  and nothing has changed in  the network in  that part  so   you \ncan just reuse it as it is and this portion is something that we need to recomputed right \nthat is the only thing that we need to recomputed and plug it into our back propagation \ncode or the algorithm  which we had in the previous lecture  and similarly if you want to \ndo dou l theta by dou w it is the same idea here that you could write it as the following \nchain   and  this  part  of  the  chain  you  already  know  how  to  compute  from  the  back \npropagation lecture  \n \n\fall you need to do is change the loss function and just try to find the derivative of the \nloss function with respect to your output layer which is h two  that is the final thing that \nyou have changed just as in my toy example i had changed e raise to y three to square root of \ny three right  that is the similar change that i am trying to do here fine  \nso   all  we  need  do  is  dou  l  theta  by  dou  h  two   but  dou  h  two  is  the  same  as  x  i  hat  right \nbecause  that  is  my  output  and  my  output  i  am  calling  it  as  x  i  hat   so   i  need  to  take \nactually the derivative of this  so  i am just using the vector form here i could have also \nwritten it as this summation over i equal to one to n x i j minus x hat i j the two whole square \nright  i could have also written it as am i just writing it as the vector here in the vector \nform  here  right   but  this  quantity  ultimately  is  going  to  be  a  scalar  because  it  is  a  dot \nproduct between two vectors which is the scalar  \nso  what i am doing here is taking the derivative of a scalar with respect to this vector  \nso  what is that derivative going to be  it is going to be a vector  \n\n \nand i am just  so we have similar stuff in the past  so  you can actually easily work this \nout   so   this  will  actually  turn  out  to  be  the  following  vector  which  is  to  times  x  i  hat \nminus x i right  so  this is very simple i have just computed this and all i need to do is \ngo back and  change my  back propagation  code   and change this derivative of the loss \nfunction with respect to the output clear and the rest of the code i can just reuse it as it is  \nso  now similarly  so we have both of these ready  \n \n\f\n \nnow  let us look at the other case when we have binary inputs ok  this is the most more \nthis  is  something  different  that  we  will  have  to  do  here   so   we  will  now  look  at  the \nsecond case where the inputs are binary  so  first we look at case when the inputs were \nreal numbers  and hence your outputs also needed to be real numbers  \nnow  we look at the case where inputs are binary and hence your outputs also need to be \nbinary ok  now  here  so each of these guys is actually a sigmoid functions  so  it is in or \nrather  if  you  look  at  the  output  you  could  divide  into  two  parts   so   this  is  the  pre \nactivation and this is the activation  so  your this is actually the pre activation and this is \nthe activation right   \nso  this activation is actually chosen as the sigmoid function or the actually the logistic \nfunction  not  the  sigmoid  function  of  course   logistic  is  the  sigmoid  function   but  the \nlogistic  function  which  was  one  over  one  plus  e  raise  to  minus  z  right   so   logistic  of  z  is \nequal to one over one plus e raise to minus z  and remember that this sigmoid function was \nelement wise  \nthat means  this is a is a vector it has elements a one a two up to a n and then you know apply \nthe  sigmoid  to  it  you  get  h   which  is  going  to  be  sigmoid  of  a  one  sigmoid  of  a  two  and \nsigmoid of a n right  so  it is just the sigmoid applied to every element of the activation \nlayer  that means  every element of this vector which have circled  \n \n\fso   now  in  this  case  your  outputs  are  going  to  be  between  zero  to  one  right   because  your \ninputs were also between zero to one and your sigmoid or the logistic function is going to give \nyou clamped outputs between zero to one  so  since this is between zero to one we could actually \ninterpret it as probabilities right  so  we could say that whatever you are reconstructing \nis actually telling you that with zero eight  \nsuppose  the reconstruction value is zero eight  then you could think of it that with probability \nzero eight it is telling you that the output should have been one right  and if it tells you that the \noutput is zero two if the sigmoid gives an output as zero two  then you could think of it that with \nprobably zero two the output was actually zero or rather the input was zero because an input is the \nsame as the output  \nso   that  is  one  way  of  interpreting  it  and  this  way  of  interpreting  it  why  does  it  make \nsense   so   we  will  just  look  at  that  right   so  before  at  if  i  do  not  give  you  this \ninterpretation  and remember that the sigmoid is going to produce values between zero to one  \nbut not necessarily zero and one right it will try to be as close to zero when the input is zero  but it \ncould  also  produce  zero five  and  so  on   and  when  the  input  is  point  nine  it  could  also \nproduce something like zero ninety five  \nso  at the output also you are going to get these vectors which are of which would look \nsomething  like  this  right   and  suppose  you  are  input  was  zero  one  zero  one  now  can  think  of  a \nsuitable loss function for this yeah  so  again these are two vectors these are x hat and x  \nso   once  again  you  could  have  just  gone  with  the  with  a  squared  error  loss  right   you \ncould  have  taken  the  squared  error  difference  between  these  two  and  you  could  have \nbeen fine  \nso   that  is  definitely  one  way  of  going  about  it   but  whenever  we  are  looking  at  these \nbinary  inputs   and  whenever  this  probabilistic  interpretation  is  possible  we  tend  to  do \nsomething better which is look at the cross entropy loss instead of looking at the squared \nerror loss   so   i am  not  saying that the square error loss is  wrong in  this case   but  you \ncould  also  use  this  cross  entropy  loss   and  in  practice  for  our  binary  inputs  the  cross \nentropy loss often works much better than squared error loss  \n\f\n \nso   let  us  see  what  i  mean  by  the  cross  entropy  loss   so   remember  that  you  have  n \noutputs  right   that  is  why  this  summation  let  us  not  worry  too  much  about  what  is \nwritten inside for the time being i will explain that  but that is the i just want to explain \nthe summation first  so  what you are saying is that for each of these green guys at the \noutput you are going to make some loss  and you just want to some over that loss that is \nwhat we are trying to see  \nnow  ideally  you could  have just written it as just done what  you had done before and \nwritten this entire replace this entire box by this squared error loss  and that would have \nbeen just fine right of course  there should have also have been this summation i equal to \none to m here  because you are going over all the m training instances and for each of the m \ntraining instances  you are trying to minimize this loss  so  this two summations followed \nby this squared error loss would just have been fine  \n \n\f\n \n   but  instead  of  that  i  have  this  something  special  here  ok   so   let  us  look  at  what  this \nspecial quantity is  ok  and now for that remember that i am trying to interpret each of \nthese inputs as a binary random variable  i am saying that they can take values zero or one  \nso   i  can  think  of  it  that  when  i  am  given  that  this  value  is  zero   i  can  write  it  as  this \ndeterministic probability distribution where i have p  and the probability mass is entirely \nconcentrated out on this zero value and my the probability mass on the value one is zero  this is \nsomething similar to what we had done earlier when we are given these labels suppose it \nwas apple  orange  mango and banana  and the class label was given to us that this is an \napple  then we could still write it as the probability distribution where all the mass was \nconcentrated on apple and everything else was here  \nso  i am saying something similar here right  so  you could think of it that two possible \nvalues can occur here one and zero and if i tell you this is zero  right  then i am telling you that \nwith  probability  one  into  it  is  zero  and  with  probability  zero  it  is  zero   so   i  still  write  it  as  a \nprobability  distribution  now  the  same  thing  i  can  have  at  the  output   so   for  this  unit \nwhen i am trying to reconstruct it and if i produce the output as zero two then i can or rather \nlet us say zero eight then i can say that with zero eight probability i am predicting zero and with zero two i am \npredicting a one right   \nso  now i can think of this again as two probability distributions  and once i some have \ntwo  probability  distributions  i  know  that  cross  entropy  is  the  right  or  a  better  loss \n \n\ffunction  to  look  at  right   and  what  is  cross  entropy  actually  in  this  case  it  would  be \ngiven by summation i equal to one to two right or rather i equal to zero to one because if the those \nare the values it can take p of i right into log of q i plus yeah  so  p of i into log of q i that \nis how i can write it  \nso  let me just since there are only two terms i can just expand this summation right  so  \ni can write it as p i or rather p zero log of q zero plus of course  it is a minus sign here  this is a \nminus sign at the out p one log of q one  i can just open up because there are only two terms  \nso  i can write it as this is that fine ok  \nnow  also i know that there is this relation between p zero and p one right  that p zero is actually \none minus p one yeah  similarly  you have this relation between q zero and q one that q one is equal \nto one minus q zero because the sum is going to be oneok now let us look at this sum right  so  \nin  the binary case this sum  becomes interesting  because  now suppose  your input x i j \nright which is the entity that i am looking at  suppose that was equal to zero  in which case \nall the probability mass would be concentrated on p zero and p one would actually be equal to \nzero  which means the second term would display  \non  the  other  hand  if  x  i  j  is  equal  to  one  then  the  reverse  situation  what  happen   that \neverything  would  be  concentrated  on  p  one   that  means   p  one  is  equal  to  one  and  this  guy \nwould become zero because p zero is going to be zero right  ok so  there is this another way of \nwriting it that you could day that instead of x instead of writing p zero and p one you could \njust write it as x i j right into log q zero plus one minus x i j into log of q one  \nso  now let us look at it again  so when x i j is zero first which is the same which happened \nhere just an \n in same thing right  because  whenever s x i j is zero p zero is \nequal to sorry it should have been q one and log q zero sorry i made a mistake here  so  it have \nbeen x i j into log q  so  or rather let me just rewrite it  \nso  this is going to be actually i can write it as  i look at this term first  so  i can write it \nas x i j into log q one and then the second term i am going to write it as one minus x i j into \nlog  of  q  zero  right   and  then  i  am  going  to  simplify  this  further   but  let  see  what  is  the \nconsequence of this  \nso   now  whenever  x  i  j  is  equal  to  one  this  term  will  remain  and  the  second  term  will \ndisappear and that is exactly what was happening in our original formula right  so  this \n\fis just an equivalent way of writing your x i j is equal to zero this term will disappear  but \nthis term will remain  that means  log q zero will remain this is exactly what was happening \nin our original formula right   \nso  that is so now  i have given you why a i can replace p zero and p one or rather p one and p zero \nby x i j and one minus x i j  and now i can make a similar argument for x hat i j also  so  i \ncan  think  of  q  zero  as  whatever  s  predicted  at  the  output  right  sorry  i  can  treat  q  one  as \nwhatever  is  predicted  out  one  output   so   whatever  my  sigmoid  function  predicts  i  can \nthink of it as it is predicting the probability of getting a one right  so  it is just predicting \nthe heads probability or the probability of getting one  so  i can instead q i q one i can write it \nas x i j hat  and similarly instead of q zero i can write as one minus x i hat i g right  so  did \nyou get that so these become very messy  \n\n \nso  let me just clean this up and i will just go over this again right   \nso  what i was trying to tell you is that in the ideal case you could have just replaced this \nby  the  squared  error  loss   but  since  you  are  dealing  with  binary  inputs  you  can  do \nsomething better because you can interpret the outputs as probabilities  so  when you get \na zero two here you can interpret it as it is telling you that the probability of this unit being one is \nzero two  it is very less  and that is the same as saying that the probability of this unit being zero \nis one right   \n \n\fso  you can interpret this as a probability  now if you think of it that way then you can \nsay that at the input you are actually given a probability distribution  so  which tells you \nthat  in  the  first  case  your  probability  distribution  looks  like  one  zero  right   because  all  the \nmass is focused on value zero because your input is zero at that case  and now suppose your \noutput was zero two right and zero two is what  you are treating as a probability of  so  this is the \nprobability of one this is the probably of sorry this is the probability of zero oops and this is \nthe probability of one  \nso  if your output is predicting zero two  that means  it is predicting zero eight for zero and zero two for one  \nnow if you think of it this way then you can capture the loss function between these two \nguys using the cross entropy formula  which is going to be summation i equal to zero to one p \ni log q i is that fine  ok and now i just said that the since there are only two terms i can \njust write it as p zero log q zero plus p one log q one  \nthen i focused on this relation between p zero p one and your input  so  whenever your input \nis zero ok  your p zero is  going to  be one  so   then  i can just replace p zero  by one minus  my input \nright   so   if  the  input  is  zero  then  this  guy  is  going  to  be  one  and  that  is  exactly  what  this \nexpression is also going to be  \nso  i can write it as one minus x j log q zero and similarly for this second guy  whenever input \nis one this guy is going to be one  whenever my input is whenever my input is one this p one is \ngoing to be one whenever my input is zero this p one is going to be zero  so  i can just replace p one \nby  log  by  x  i  j  and  now  you  can  see  that  this  expression  evaluates  to  the  same  as  this \nexpression right  you can substitute value of x i j zero or one you will get the corresponding p \nzero  p  one  which  would  be  one  or  zero  depending  on  what  your  input  was  and  these  two \nexpressions will evaluate to the same thing  \nso  just as i replaced the p\u2019s by x i\u2019s x i j\u2019s  i can similarly replace by a the q\u2019s by x i j \nhats  right  because  once  again  q  zero  is  nothing   but  one  minus  whatever  my  output  was \npredicted  because  whatever  is  predicted  i  am  treating  as  the  probability  of  getting  a  one  \nso  one minus that is going to be the probability of getting a zero  so  that is what q zero is and \nsimilarly q one  i can replace by x hat i j  and so that is exactly what  i have done in this \nexpression here  \nso  now this expression every term in these n terms captures the cross entropy for that \nparticular  random  variable  right   so   this  is  the  original  distribution  p  for  this  random \n\fvariable  this is the predicted distribution q for this random variable  and i have just told \nyou  that  this  the  cross  entropy  between  these  two  distribution  can  be  written  in  this \nsimple form as the function of x i j and x hat i j  \nso   this  is  the  standard  thing  to  do  when  you  are  dealing  with  bernoulli  random \nvariables  so  you can go back and read up a bit about it ah  but for now i guess with this \nexplanation it should suffice to know why this expression is used  and remember that i \nam  not  telling  you  that  this  squared  error  function  was  bad   i  am  just  telling  you  that \ninstead of the squared error function cross entropy loss function works better when you \nare dealing with binary inputs  \n\n \nso  with that let us pursuit and the another we have looking at it is the following you can \nnow look at this expression  and tell me when is this expression going to be minimized  \nso  we have x i j and x hat i j you can see that this expression will be minimized only \nwhen x i j or rather x hat i j is equal to x i j right  so  now  x i j could take value zero or one \nok and now x hat i j could take zero one or zero one  \nso  you can see that for these two combinations the value is going to be minimized only \nwhen x hat i j is actually equal to x i j  that means  if x hat if x i j was zero then x hat i j \nshould also be zero  and similarly in this case also if x i j was one  then the expression will \nbe minimized only when x hat i j is equal to one  so  let us see this so suppose x i j was zero  \n \n\fthat means  this term is going to go to zero  but this term is going to remain and now if you \nare x hat i j was not equal to zero  \nthen you will get some log of one minus x hat i j as the loss right  but if x hat i j was also zero \nthen you would get log of one which is zero  so  this whole expression would then evaluate to \nzero which is the minimum possible value for this expression right   \nso  that means  if x i j is zero then this expression will be minimized only when x hat i j is \nalso equal to zero  similarly  if x hat i j sorry if x i j is one then this one minus one will give you zero  \nso  this term is going to disappear  but this term will remain  so  this will just be log of x \nhat i j because x i j is equal to one  \nnow  if x hat i j is also equal to one then this is become log of one which is zero  that means  \nagain  this expression will  attain it is  minimum  value  when x hat  i j  is  equal  to  x i  j  is \nequal to one right  so  this expression now attain it is minimum value in two cases when x \ni j is equal to x hat i j is equal to zero or when x i j is equal to x hat i j is equal to one  so  \ncompactly i can say that this expression will attain it is minimum when x i j is equal to x \nhat i j  that is why this loss function makes sense  \n\n \nnow   again  we  have  this  problem  that  we  want  to  use  back  propagation  to  train  this \nnetwork  and once again for back propagation we will need the following gradients the \ngradients of the loss function with respect to w and w star ok  this is what we are going \n \n\fto need and i am going to make this same argument again that whatever hard work you \nhad done in the back propagation lecture you can just reuse all of it  \n\n \n because  the  only  thing  your  changing  is  this  final  loss  function   so   you  just  need  to \ncompute the gradients with respect to this loss function and everything else is going to \nremain the same right   \nso  that is exactly what i am going to do on this slide  so  whatever is in the boxes here \nthese two boxes that is something that you have already computed  and now what i am \ngoing to compute is the stuff which is outside the boxes  so let us look at that  so  i am \ninterested  in  computing  this  dou  l  theta  by  dou  h  two  this  is  the  derivative  of  a  scalar \nquantity with respect to a vector  say it is going to be a vector  and i am going to follow \nour usual recipe which is h two is actually equal to h two one h two two up to h two n  \nso  i am going to consider any of these guys which is h two j  i am going to compute the \nderivative of the loss function with respect to this one entry and since  i have that i am \ngoing to construct the entire gradient right  so  now  i will have this dou l theta by dou h \ntwo j right and once i have that expression i am just going to generalize it to all the other \nentries in this vector  \nso  let us look at that expression first ok so  now  if you look at this actually it does not \nhave an h two j right  but we know h two j is the same as x hat j or rather x hat i j right for the \n \n\fith input it is going to be x hat i j  because h two is equal to x hat i ok  you can just see that \nthe top left corner of the slides say x two is equal to x hat i  so  this is nothing dou l theta \nby dou x hat i j  \nso  now i want to take the derivative of this quantity with respect to one particular x i j  and \nremember that this quantity has the sum which is indexed over j  so  j goes from one to n i \nam looking at one particular j  so that means  if i expand this sum of all the js possible the \nderivative with respect to all  but one is going to be zero  because they do not depend on this \nparticular j  so  if i am looking at j equal to three then the term which has x hat i one is going \nto the derivative of that term is going to be zero  \nso  for all these terms in the expression only that term where a j is equal to the j which i \nam  considering is  going to  remain  ok   so that  means   only one term in the summation \nwould remain and for that one term  so let me just rid of the summation right  so that \nmeans  only one term in the summation would remain  i am trying to find the derivative \nof this quality x which has a lot x i j\u2019s with respect to x i j  \nso   now this  is  of the form a log x  so the derivative would a over x right  so   that is \nexactly what i have written here and similarly for the second guy this is one minus a into \nlog  of  one  minus  x   so   the  derivative  is  going  to  be  one  minus  a  over  one  minus  x  and  of \ncourse  there is this minus sign here which will then get adjusted appropriately right  so  \nthat is how this expression has been completely that is very straight forward and now as \nyou need the derivative of h two j with respect to a two j  \nso  remember that h two is equal to sigmoid of a two which means it is just an element wise \nsigmoid  right   so   i  just  need  to  compute  the  derivative  of  the  j\u2019th  entry  of  h  two  with \nrespect to the j\u2019th entry of a two all the other derivatives are going to be zero because they do \nnot depend on that particular entry of a two  so  now  that is just going to be sigmoid of a two \ninto one minus sigmoid of a two right   \nso   i  have  computed  these  two  quantities  i  can  just  plug  it  then  back  into  back \npropagation  code   the  rest  of  the  code  is  going  to  remain  the  same  and  i  have  the \ngradients ready with me  \n\f\n \n and as i said once i have this one guy i can just extend it i can just generalize it  so  i \njust had these j\u2019s here right for h two j  so i can just replace the j by one two up to n and i will \nget the same expression  \nso  that is the end of module one where we introduced auto encoders  what we showed is \nthat  they  are  actually  just  like  any  other  feed  forward  neural  network  accept  that  they \nhave this special objective  that they want to  reconstruct the input and the reason they \nwant  to  reconstruct  the  input  is  they  about  to  first  create  a  bottle  neck  which  is  this  h \nhidden representation  and then try to reconstruct from there and just as i gave you that \ncompression analogy that you have this ten bits you want to compress it to four bits and then \nreconstruct the entire input again  \nso  this will happen only if these four bits  capture  everything that is required or the most \nimportant  characteristics  of  your  original  input  right   and  then  we  could  have  a  loss \nfunction  which  tries  to  capture  the  difference  between  my  original  input  and  my \nreconstructed input  \nnow  we argued that this loss function will be dependent on the nature of your input  so  \nfor the real inputs it was straight forward we just said that we can use the squared error \nloss  function  for  the  binary  inputs  we  actually  did  something  special   we  said  that  we \ncan actually use the cross entropy  and then we had this funny way of writing the cross \nentropy which was this x i into log of x hat and one minus x i into log of one minus x hat  \n \n\fand just gave you some intuition that that is the same as writing p log a pi log or rather p \nzero log q zero plus p one log q one write and the i just gave you some explanation for doing that  \nyou can go back and check on how do you write the cross entropy for bernoulli random \nvariables  and  you  will  see  that  this  expression  makes  sense   and  once  we  had  this \nexpression computing the gradients was easy  so  the other thing that we relied on is that \nin the back propagation lecture we had taken  care of everything up to this point and in \nthis lecture we have actually changed the loss function  \nso   one  loss  function  was  the  sum  of  squared   squared  loss  errors  and  the  other  loss \nfunction was the sum of sum of cross entropies whereas  in the back propagation lecture \nwe had only dealt with cross entropy by the case that we made is that sense you have this \nchain  all you have done is change the last function in the chain right  you have changed \nthis l function all the other functions you have not changed  \nyou can just reuse the computations from these or you can just use the code that you had \nwritten for these in the back propagation assignment  and you just need to change this \nlast guy to adjust for the change in the output layer or the change in the loss layer  so  \nwith  that  we  will  end  the  introduction  to  auto  encoders   there  we  have  done  we  have \nactually covered how to train an auto encoder using back propagation  \n\f"}
{"audio_filepath": "lec006_002.wav", "duration": 1048.8529375, "text": "\nlink between pca and auto encoders \nso   we  will  move  to  the  next  module  where  i  would  like  to  show  you  a  link  between \npca and auto encoders  \n\n \nso  this is what i am trying to show you that under certain conditions  pca is or rather \nan  auto  encoder  is  equivalent  to  a  pca   and  the  conditions  are   if  you  use  a  linear \nencoder  if you use a linear decoder  if you use a squared error loss function and if you \nnormalize the inputs to this  so  for the time being  just ignore the last bullet  let us look \nat the other three bullets using squared error loss functions   \nso  remember i gave you different choices right  you could have used the cross entropy \nor  the  squared  error  loss   but  i  am  going  to  prove  this  equivalence  only  under  the \ncondition when we have the squared error loss  what do i mean the u encoder is a linear \nencoder  g is a linear function  we are not using a sigmoid or any logistic or anything \nlike that and linear decoder  again the same thing  we are not using the sigmoid or soft \n\fmax or anything at the output  it is a linear function  under these conditions  i will show \nthat or i will try to show you that pcas equal auto encoders equal to pca   \nwhat does this mean actually  now what do i mean by it is equivalent  what do i have \nto  show  you  actually   how  many  of  you  understand  what  i  am  trying  to  prove   how \nmany of you can mathematically define it  ok  so  we will try to make this clear over the \nnext fifteen minutes  \n\n \nfirst  let us look at the last condition right  which i ignored ok i always anticipate all this \nright  so  i have full faith in you guys ok  what is this mean  now  what i am doing  \ncentering  the  data  and  i  am  also  doing  one  by  square  root  of  m   why   mean   as  the \nstandard deviation  \nso  the operation in the bracket ensures that your data now has become zero centered right  \nit  is  a  zero  mean   and  now  let  x  dash  be  this  matrix  this  one  right  such  that   all  it  is \nelements are zero  mean is this still a flicker again alright  \nso   let  i  am  calling  x  dash  as  this  matrix  ok   so   this  matrix   where  i  also  have  one  by \nsquare root of m   i can write it as everyone gets this is simple  now do you see where \nthis is headed  what would x transpose x be  covariance matrix  so  i needed that one by \nm right at the out  \n \n\fso  now this is the co variance matrix  so  if i do this normalization to the original data \nand then if i take let x dash be that quantity and then if i take x transpose x then i will get \nthe co variance matrix everyone gets this that i did this to get the co variance matrix  so  \nthat i mean i did this  so  that when i take x transpose x i get the co variance matrix after \nthis normalization only it will be the covariance matrix  \n\n \nso  first we will show that  if we use the linear encoder  decoder and a squared error loss \nfunction   then  the  optimal  solution  to  the  following  objective  function   what  does  this \nobjective function  \nstudent  squared error  \nsquared  error  loss  is  obtained  when  we  use  a  linear  encoder   do  you  understand  the \nimplication  of  this   what  does  being  stated  here   ok   so   i  have  fixed  the  decoder   i \nhave said that the decoder is  going  to be a decoder  i have fixed the encoder or i have \nfixed the loss function  this is going to be a squared error loss function  this is given to \nme  now under these conditions  i am trying to minimize this loss function ok  \nthen i am telling you that the only solution to this is that the function dash should be a \nlinear  function  which  function   the  function  g  should  be  a  linear  function  you  cannot \nchoose sigmoid or logistic or anything else right  the optimal solution will occur when g \nis a linear function everyone gets what is being stated here  \n \n\f\n \nso   this  summation  that  i  have  written  right  or   in  fact   this  the  entire  objective  that  i \nhave  written  is  actually  equivalent  to  this  objective   is  this  fine  with  everyone   even \nthough i have not defined what h is just fine with everyone  so  we had   x which was x \none to x m ok  i had picked one of these xi\u2019s  what is the dimension of this  \nstudent  one cross  \none cross m and then i had multiplied it by a weight matrix w  not w star  remember that \nwhat do the dimension of w  \nstudent  n n  \nn cross k and what will i get as the output  \nstudent  \n  \ni got an h which was one cross k  what did i do this  \nstudent  multiply it by \nmultiply it by  \nstudent  w star  \nw star which was k cross m and what did i get as the output  \n \n\fstudent  x hat  \nx hat which was one cross n right  so  what i am telling you is that  i could do this together \nfor all these x i s  i could do this operation at one go and i can call this as x matrix and \nwhat will i get here  h one to h two to h m  and i can call it as the h matrix and i multiply it \nby w star and what do i get  x cap ok  is that fine  ok  but without defining these things \nalso it was fine  so  it does not matter  \n\n \nso  now how many of you get that this quantity is the same as this quantity  now how \ndo  i  explain  this  was  the  frobenius  norm  of  a  matrix   some  of  the  squares  of  the \nelements  \nnow  what is the matrix x  it is the x one one up to x one n and x m one up to x m n and all \nelements in between right  what is the matrix h w star  we just did that the same thing \nexpect that it is x hat  \nstudent  \n  \ni  take  the  difference  between  these  two  what  do  i  get     every  element  of  that  matrix  is \nequal to this quantity that i have underlined right  so  i get a new matrix such that every \nelement of that matrix is equal to this quantity  is that fine  now  i am taking the square \nof every element of that matrix and adding them up what is that equal to  \n \n\fstudent  \n  \na  frobenius  norm  how  many  of  you  get  that  now   almost  everyone   ok   so   this  is \nequivalent  to  the  frobenius  norm  ok   now   where  have  you  seen  the  frobenius  norm \nbefore what did we show in the svd theorem  \nlet us try to connect things right if you do not learn how to connect things it is going to \nbe very difficult  what is this x hat  it is a dash of x  \nstudent  reconstruction  \nreconstruction it is a dash of x approximation  what is the solution to this optimization \nproblem  what is the solution to this optimization problem  i shall started off with the \nanswer that  we saw this in  the svd theorem  and then  i asked  you  a question what  thirty \nhours  thirty two  hours   not  even  thirty two  hours  are  passed  since  we  did  this   come  on   what  is  the \nsolution to this  no  no that is fine  \nbut what is the solution x hat is equal to what  the best approximation to x is given by  \nwhat  is it fine yeah  so  some k yeah  but it is going to come from the svd theorem  \nright  is  that  fine   it  depends  on  what  rank  approximation  you  want   but  it  the  best \napproximation to this is going to be given by the svd of x  is it ok  everyone gets that \nyes forgot about it  but now do you remember it all those extra lectures eight\u2019o clock in the \nmorning  \n\n \n \n\fso   that  means   h  w  star  should  be  equivalent  to  this  that  we  know  from  the  svd \ntheorem  that   the  optimal  solution  is  going  to  be  given  by  svd   so   if  i  just  compare \nterms ok  then i could write that one solution is this that h  h is equal to u into sigma \nand w star is equal to v transpose  i could have chosen the other solution also where h \nis equal to v or sorry u and w star is  equal to sigma  v  ok  but  i will work with this \nparticular solution  you see  this i am just matching variables right  it is said that  a b is \nequal to c d e  so  i am saying that a is equal to c d and b is equal to e  \nnow  we will work with this  so  and we will try to show something  so  let us see what \nwe are trying to show  \n\n \nnow  first thing that we will show is that h is actually a linear encoding  so  what does \nthis mean  you first always understand what has been tried to prove right  i am saying \nthat  i am going to show that h is a linear encoding of x  then what is it that i am trying \nto show   \ni am trying to show that h is equal to a linear encoding of x when h is of the form w x \nand not  something of the form  w sigmoid of w x or something like that  or any other \nnon linearity  for  that  matter   is  the  statement  clear   that  is  what  i  am  trying  to  show  \nwhen i say h is a linear encoding  i mean that h is obtained by a linear transformation \nof x  \n \n\f\n \nnow  h as we defined on the previous slide is equal to this  now  if i already had an x \nhere  then i was done  but  i do not have any x there  yet  so  i want to a get to a form \nwhere i can show that h is equal to w in to x  so  i will just do some simple trickery and \narrive try to do arrive at that form  \n\n \nso  the first thing i am going to do is pre multiplying pre multiply by this quantity and \nthis is fair because this is just equal to i what next i will write these three x\u2019s as u sigma v \ntranspose and i will leave one x as it is that  \n \n \n\fnow   just  can  you  just  try  to  see  what  the  next  step  would  be  this  v  transpose  v  will \ndisappear because it is equal to i  now what happened here  i actually just expanded this \ninverse   so   i  will  think  of  this  as  a  b  c   so   a  b  c  inverse  is  equal  to  c  inverse  b \ninverse a inverse   \nso  i have just applied that it just that my inverse is a very straight forward matrices here \nthey are just the transform of the original matrices  everyone gets this step  well you can \nstare at for a for a few more seconds if you want  how many of you do not get this  how \nmany of you get this  ok  now what is next this u transpose  u disappears  \nstudent  \n  \nthis also disappears  \nstudent  \n  \nno  \nstudent  \n  \nit is this u is only  the first k columns of u right  this is not the entire u  this is just the \nfirst k columns of u  fine  now what next a into b inverse is  \nstudent  b inverse  \nb  inverse  a  inverse  what  will  happen   now  that  quantity  will  disappear   so   what  do \nyou have left now ok  so  this is something ok  so  now  let us look at this is let us say \nthis is n cross n and this is n cross k what is the output going to be  \nstudent  n cross k  \nn cross k and what is the output going to look like is the first k columns of  \nstudent  identity  \nthe  identity  matrix  everyone  gets  that  if  you  do  not  you  can  just  work  it  out  with  the \nsmall  matrix  after  going  home  and  you  will  get  it  right  if   so  if  i  done  the  full \nmultiplication   i  would  have  got  the  identity  matrix   but  i  am  just  talking  the  first  k \n\fcolumns  so  i will get the first k columns of the identity matrix  do not fed too much  if \nyou are not getting this  you can just work it out on paper and you will get it  \nso  i get the first k columns of the identity matrix and this inverse disappears this sigma \ntranspose into sigma transpose \n now what next what is this product \ngoing to be the first k elements of  \nstudent  sigma inverse  \nsigma inverse and that is going to get multiplied by sigma k cross k  so  that will give \nme the first k elements of  \nstudent  identity  \nidentity  matrix  there  is  some  very  simple  matrix  operations  where  you  are  just  taking \nsome columns right  so  if you do not understand this right  now do not worry  you can \nwork it out  everyone is confident  they can do this  please raise  your hands if you are \nconfident  and now  what do i finally  get this multiplication will give me  \nstudent  the first k columns  \nthe first k columns of v ok  so  have we come to the desired form what i have shown  \nnow h is a dash of x or linear transformation of x  that means  my optimal encoder was \na linear encoder and what was the optimal weight matrix w the first k columns of v yeah \ni  someone  pointed  it  last  time  also  i  could  not   i  ignored  it   i  will  just  pretend  i \nunderstood  \nbut i get it i know that there is a simpler solution  i do not know why do it this way  but \nthere  is  a  simpler  solution   i  just  like  making  life  miserable  for  you  guys   but   but  the \npoint is  you can figure it out  that it is a it is a linear transformation of x now  \n\f\n \nwe have that the encoder is equal to the first k columns of v ok  what is v eigenvectors \nof x transpose x ok  \nstudent  a  \nwhat is the other thing that you know about the eigenvectors of x transpose x they are \nthe solution for the  \nstudent  eigen  \nif you have given an matrix x then the pca is the eigenvectors of the co variance matrix \nwas the co variance matrix x transpose x what is are it is eigenvectors capital v right  \nso  what have we arrived at are we done with the proof  yes  how many of you think that \ndone with the proof  how many of you think that we are done now  \nso  it is done right  so  we have proved what we wanted to prove right  so  what did we \nwant to prove that you are doing auto encoders  you are trying to train an auto encoders \nand  you  are  loss  function  is  the  squared  error  loss  function   we  saw  a  neat  way  of \nwriting that squared error loss function as a matrix operation where x minus capital  h \ninto w  \nand  then   we  saw  that  these  squared  error  loss  function  is  nothing   but  the  frobenius \nnorm of this and we knew that the minima of this objective function the frobenius norm \n \n\fof x minus h w would occur when s w is equal to svd of x right  we started from \nthere and showed that h is actually a linear transformation of x and what was that linear \ntransformation which matrix was used for the linear transformation v capital v  what is \ncapital v  it is the eigenvectors of  \nstudent  x transpose  \nx  transpose  x   so   what  is  happened  in  effect  is  that  if  i  was  trying  to  train  my  auto \nencoder  with  this  objective  function   the  weights  in  my  initial  layer  w  would  actually \nconverge  to  v  which  are  the  eigenvectors  of  x  transpose  x   that  means   the \ntransformation that i have learnt this transformation which i have learnt is the same as a \ntransformation that i have had learned using pca  because pca would also have given \nme v into x where v was the eigenvectors of the co variance matrix and we just arrived \nat the same solution everyone gets it  now we are done with the proof  \n\n \nso  what we have proved is  under these specific conditions that the encoder of a linear \nauto encoder is linear auto encoder is equal to pca if we use a linear decoder  if we use \na squared error loss function and if we normalize the inputs to this and you understand \nwhy each of these steps was important why was the last step important  \nstudent  \n  \n \n\fonly  then   we  would  have  got  the  co  variance  matrix  why  was  a  step  before  that \nimportant because  only if it was the squared error loss we would have got that frobenius \nnorm objective function right  and why was the linear decoder important again the same \nthing  because x minus h w we wanted it to be linear right is it fine   \nso   you  see  why  all  these  assumptions  were  important  and  under  these  conditions   we \nhave proved that auto encoders e equivalent to pca   \n\f"}
{"audio_filepath": "lec006_003.wav", "duration": 736.289, "text": "\nregularization in autoencoders \n \nthen  we  will  go  to  the  next  module   where  we  will  talk  about  regularization  in  auto \nencoders  and we will talk about a motivation for doing that  \n\n \nso   poor  generalization  so   why  do  we  need  a  regularization   people  have  done  the \nmachine  learning  course  or  any  equivalent  course   why  do  we  need  regularization   to \navoid  \nstudent  or enable  \nor enable generalization  right  now in the case of an over complete auto encoder what \nis likely  overfitting is likely  why is it so  what does what do you mean when you see \ngeneralization actually  can you talk in terms of training time test time and so on  \nso  generalization is essentially that your are training  so  remember that at training time \nyou  are  trying  to  solve  an  optimization  problem   where  you  are  looking  only  at  the \ntraining data  so  it is quite likely that you will drive the error to zero for the training data  \n\fthat means  you have learnt perfectly everything for the training data  right  but now it is \nalso  possible  that  when  i  give  you  a  new  test  instance  which  you  had  not  seen  during \ntraining  that means  you had not seen instance while doing the optimization  that means  \nthis instance did not contribute to your loss function  \nthen it is very lightly that when i gave this instance  then you would get a non zero loss or a \nloss much higher then what you get for your training data  does that make sense  that is \nwhat over fitting is and it leads to less generalization  your model should have generalize \nto  unseen  data   but  it  cannot  do  this  one  typical  situation   where  over  or  where \ngeneralization happens is  if you have a dash number of parameters  now what did i ask \nactually  \nstudent  generalization  \nno  ok  if a case where a over fitting would happen is when you have a dash number of \nparameters  \nstudent  large number of  \nlarge number of parameters  right  now do you see why i am saying this  what is there \non the slide  an over complete auto encoder  what would it have  \nstudent  a large number  \na large number of parameters  so  what could it do  \nstudent  overfitting  \nover fitting  what do we do to avoid over fitting  \nstudent  regularization  \nregularization  so that is why we need regularization  i have still no told you why do we \nneed an over complete auto encoder  ok  still that is an random variable i still need to \ndecide  but can this happen in an under complete auto encoder also it can right  because \nunder  complete  auto  encoder  just  says  that  your  k  is  less  than  n   it  does  not  say  how \nmuch less it is right  so  it is it is still have and depending on a data that you are trying to \nmodel  it could still have a large number of parameters  \n\f\n \nso   for  example   let  us  take  an  example  for  the  under  complete  case   suppose  you  are \ndoing image classification where you have a digit three at the center of the image  ok  and a \nlot of these are white spaces  so  what is the dimension  and suppose this is a one hundred cross \none hundred image  what is the dimension of this image input  how many if you cannot multiply \none hundred into one hundred  \nstudent  ten  \nten k right of this a lot of data is not important  so  my n is ten k and at least by this thing \nthat i have drawn it looks like probably only twenty percent of that is what actually captures \nthe  digit   but  now  if  i  choose  k  to  be  equal  to  one thousand   it  might  still  be  large  for  this \napplication   so   i  am  using  an  under  compete  auto  encoder   but  it  could  still  be  a \nsituation that my under complete is still having a large number of parameters  all of get \nthis intuition  \nit is a very weird example  but still really do you get the intuition you could have a very \nhigh  dimensional  input  and  you  might  think  you  are  shrinking  it  a  lot  but  there  is  so \nmuch redundancy in  your input that even that shrinking still leads to a large number of \nparameters  and  you  could  still  over  fit   therefore   even  for  an  under  complete  auto \nencoder  you could still need over a regularization  \n \n\fso  fine so  that was the motivation  since the over complete case of course  the model \ncan  simply  learn  to  copy   we  have  seen  that   and  that  is  why  we  need  to  introduce \ngeneralization   fine   now  what  is  the  simplest   sorry   we  need  to  introduce  a \nregularization  what is the simplest regularization technique that you know  that is not \nthe simplest l two regularization  \nand you see why i say that is the simplest  we can take the derivative for those of you do \nnot get it do not worry we will get to it  or if you do not get to it do not worry  \n\n \nso ok the simplest solution is to add the l two regularization term to the objective function  \nso  this was my objective function  i wanted to minimize the squared error loss  i have \nadded a term  to  this  what  does  this  term  do  what  does it doing   first  of all  tell  me \nwhat is this quantity  theta is a  \nstudent  all  \nall the parameters that you have  right and i am assuming that they have just put it into \nlarge vector i am taking the ltwo norm of that vector  so  even you though you have those \nmatrices  and just flattening them all out and putting them into a large vector called theta  \nright   so   what  is  happening  here   i  am  not  allowing  my  weights  to  shrink  or  grow  \ngrow  because if my weights are very large what would happen  \nstudent  grow  grow  \n \n\fthis  quantity  would  grow   so   then  i  cannot  really  minimize  this  minimize  this  as \neffectively as  i want  right  why this makes sense  how many of  you why this makes \nsense  so  i am now why am i not preventing the weights to go to zero  ok  so  we will see \nthis in more detail in the next lecture  this is again a basic lecture on bias variance and \nregularization and so on  so  we will try to arrive at a more reasonable answer for this  \nfor now  just see that i am putting some constraints and the weights  \nso  effectively and  i  am doing  gradient  descent   i  am  not allowing the weights  to take \nvery large values  i am trying to restrict them to a certain area  so  i am not allowing to it \nto  explore  the  entire  w  comma  b  plane   but  trying  to  restrict  it  to  smaller  values  of  w \ncomma v how many of you get this intuitive explanation  \nso  in other words what i am trying to do is  that i am not giving it in a freedom so that it \ncan completely drive the error on the training data to zero  and my hope is that if i do not \ndo  this   if  i  do  not  allow  it  to  completely  memorize  a  training  data   then  it  should \ngeneralize well on the test  data   is that intuitive fine ok  now  i have changed the loss \nfunction again  i have the square  i have told you how to do it for squared error loss  for \nthe cross entropy loss and so on  but now i have changed the a loss function again  so  \nagain i need to teach you back propagation  no what will change now  again i need to \nderive with respect to the last layer  \nwhat is the minimalistic change that is going to happen now  just tell me  this theta is \nactually w one w two and so on  right  just assume all the parameters  just flattened out into a \nvector   fine   and  now  tell  me  what  is  dou  l  theta  by  dou  w  one  going  to  be  or  let  us \nsimplify things  let us call this l theta and let us call this omega theta  let us call this l \ndash theta  and then your l theta is the combination of these two terms  so  this derivative is \ngoing to be a sum of two derivatives  out of that one you already know  what is the second  \nstudent  two times lambda  \ntwo times lambda w one  so  it is  a very simply change to  your  gradient  descent  update \nrule  how many of you see that  whatever update you will had just add minus two lambda \nw one to that  ok  should have been two lambda w  but of course  you do a half here  so it is \nfine  is it   \n\f\n \n\n \nanother trick which is typically used at least in the context of auto encoders is to tie the \nweights  of  the  encoder  and  the  decoder   how  does  that  help   what  does  tying  the \nweights mean  now i appreciate what you are trying to say  so  one we have doing this \nis just say w star is equal to w transpose  you will enforce that  you actually have only \none  matrix  w   and  here  you  are  using  w  transpose   mathematically  does  that  make \nsense  all your operations go through  because this is going to be n cross k  and this is \ngoing to be k cross n  \n \n \n\fso  whatever effectively done  i have reduce the number of parameters in my network  \nright  i am enforcing  i am forcing this upon the network that i am not going to give you \ntwo sets of weights  you just learn the w\u2019s in a way such that when  you use w transpose \nyou should be able to reconstruct this  how many of you get this  not many  ok  please \nask me doubts if you do not  there is nothing very  \nstudent  why is it w transpose  \nwhy is it w transpose  because otherwise  \nstudent  \n claim that  \nhow can  you claim  that that would work  because  you have no linearity\u2019s in  between \nright  no w inverse would not  work  what  is  the  simplest thing to  do  why would  you \nwant to compute an inverse  that is an interesting question how would you implement \nthis  how would you  if there are multiple paths from a weight to the output  how do you \ncompute the gradient  sum it across all those paths  what is happening here  how many \npaths  to  there  exist from  the weight  to  the output  one is  this direct  path and then the \nother is another this path also  so  you just sum it across these two paths  do you get that  \nhow many of you do not get that  how many of you do not get that ok  \nso  if this was w star you did not have a problem  you could just have computed dou l \nby dou w star and dou l by dou w  now think of it as this  right that you have this  this is \none path  w  w to the output  ok  and now the gradient is  just going to  be sum  across \nthese two parts  one path is the single path and the other path is the double path  so  it is \njust going to be a sum across these two paths  oh  no  no  so  you just have one matrix w \nwhich are going to update  you do not have two matrices  you just have one matrix w  at one \nplace  you  are  using  w   the  other  place  you  are  using  w  transpose   but  just  look  at  it \nelement wise right  do not try to look at it in the terms of matrices  \nso  you have n cross k elements here w one one to w n k  right you have to computing the \npartial  derivative with  respect  to  each of these  and every time they are considering all \npossible paths to the output  and that value is getting updated  right  and at one place you \nare  using  a  particular  arrangement  of  these  w\u2019s  at  the  other  place  you  are  using  a \ndifferent arrangement of those w\u2019s  that so  it will just remain the same  right  is that ok  \n\fstudent  no  \nno   this  is  for  regularization   right   so   we  are  reducing  the  number  of  parameters  by \nhalf  \nstudent  \n  \nyes  \nstudent  \n  \nno   that  i  mean  that  also  has   but  that  is  not  be  the  objective   we  are  trying  to  do \nregularization   how  many  of  you  have  lost  at  this  point   please  ask  me  if  you  have \nquestions  really i do not mind answering  but if you just give me blank spaces  i cannot \nread them  so  this is used at quite a few places where you tie some weights right so that  \nso  effectively you are saying that learn it in such a way that it works at both the places  \nand you are reducing the number of parameters  so  weight tying is something which is \nvery commonly used for regularization in the context of neural networks  \nso  that is where we will end the motivation part  and it is too very simple ways of doing \nregularization  one is the standard known trick which is to use ltwo regularization  and the \nother one was something special that we saw which was tying the weights you all have a \nlot of doubts about tying the weights  \n\f"}
{"audio_filepath": "lec006_004.wav", "duration": 1567.366, "text": "\ndenoising autoencoders \n in this module we will learn about denoising autoencoders  \n\n \nso  the idea behind the denoising autoencoder is very simple  what  you do is you have \nyour original x i  now for the minute for a minute just consider the discussion when your \nx i s are binary inputs ok  so  each of these red guys can be between can be zero or one  now \nwhat i do is  before feeding it this input to my autoencoder  the box is the autoencoder \nwhat i do is i do a corruption  \nso  the corruption is as follows with probability q i will set x ij  that means  one of these \nguys to zero  right  and with probability one minus q  i will keep it as it is ok  so  with some \nprobability q  i am actually corrupting the data otherwise i am retaining the data as it is  \nand then feeding that data to the autoencoder  why would this work  binary input case \nas i said just assume that the inputs are binary   \n\fwe will also  see the other case  why would this work  what  was  our problem  earlier  \nthat  was  completely  able  to  reconstruct  the  training  data  right   but  at  test  time  i  had \nissues  now  what i have done to the training data  corrupted it just think for a minute \nwhat  will  happen   now  i  want  someone  to  ask  me  a  question   in  return  oh  that  is  the \ncorruption that i am choosing or you could flip it is what you are saying  yeah if it is zero \nchange it to one so that is also fine  that is the question i was expecting  what is the loss \nfunction now  what is the loss function  x hat my i minus x tilde i or x hat i minus x i \nwhich choice makes sense  \nstudent  first tilde  \nfirst let us the case take the case when i do x tilde i what happens in that case from this \nnetworks perspective it is still learning to memorize the training data right it just this is \nwhat it thinks as the training data  and just trying to learn that transformation right  so  it \nis not really helping my case  do  you understand that  i just corrupted the training data \nthat is  fine  but  from  the networks point of view   it still gets  away by memorizing this \ndata  and that is not what i want  so  what should i do  can anyone tell me the  i mean \ncan everyone tell me the answers  \nstudent  minimize  \nminimize the error between  \nstudent  an x i  \nan x i  how many if you understand  why that should help  all of you gave the answer  \nbut only few of your raised your hands why so  hard to deal with this inconsistency  \n\f\n \nbesides because i am still going to minimize my original objective function ok  now can \nthe network get away by copying the input to the output  so  input remember the input to \nthe network is this and what  i am trying to minimize this  if i just copy x tilde i to the \noutput  will my objective function be minimized  no right  so  it does not have incentive \nto copy now  so  what will it have to rely on  say a reasonable probably twenty percent is \nthe standard right  so  even if i reconstruct i will not get zero error i will at least get some \ntwenty percentage  \nso  let us let me give you an example and then let me know if you can figure out what \nhappens  this example will contradict something else that we have done before  but just \nplay along  suppose my input features were height weight and bmi and we all know that \nbmi depends on height and weight i hope all of us know  \nnow  can you think what is happening  i am corrupting one of these inputs  and i still \nwant  everything  to  be  reconstructed  back   so   what  will  the  network  now  have  to  rely \non  it will have a now rely on this relations between these inputs also  so  again if i take \nmy example of digit three  i have corrupted some of these pixels right  but i still want to be \nable to reconstruct three  so  it will have to be smart enough to learn that if i have seen this \nand i have seen this then it has to be something in between which gives me a three  do you \nget the intuition  \n \n\fso  now  i  am  making  it  is  job  harder   so   that  it  is  robust  to  changes  at  test  time   that \nmeans  a test time if my digit looked something like this  it should still be able to predict \nit  as  a  three  or  it  will  still  be  able  to  learn  the  same  representation  as  three  do  you  get  the \nintuition  right  so  that is what i am trying to do i am trying to somehow bring in the \ncorruptions that i would expect a test case and trying to make the model more robust  \nit can no longer get away by memorizing the training data because i am not feeding it the \ncorrect training data  it has to do something smarter than that  everyone gets this  i will \ncome back to your question everyone gets this please raise your hands yes  yes this is all \nunder regularization no this is regularization no so at that case i have already made that \noverfitting  can  happen  in  an  over  complete  as  well  as  under  complete  autoencoder  \neveryone gets that right  i show that example where it could happen in  both the cases  \nso  my figure maybe over compete  but it can just happen in any of these is that fine  \n\n \nit no longer makes sense for the network to just start copying the input data  \ndifferent  kinds  of  noises  means  yeah   so   let  me  try  to  answer  that  right   so   what \nprobably  you  are  trying  to  say  is  that  all  my  input  images  were  three  vertically  written   i \nadded some noise and managed it  but now at test time suddenly you show me a three of this \nkind  like that will not work also  that is what were your question was a different types \nmean different values of the noise twenty percent twenty five percent and so on  \n \n\f\n \nso   we  will  first  see  practical  application  in  which  autoencoders  are  used  and  then \ncompare it to denoising at autoencoders  so  this the next few slides for those of you may \ncare is also a small answer to the difference between machine learning and dp  \n\n \nso   suppose  you  are  given  this  task   which  is  handwritten  digit  recognition  i  see \neveryone paying attention  now  i should say this before every slide ok  so  this is  the \ntask handwritten digit recognition  you are given some data  where you want to classify \nthe digits into one of these ten glasses  the traditional machine learning approach to this \n \n \n\fis we just construct a feature vector this is a twenty eight cross twenty eight image  so  i guess twenty eight cross twenty eight \npixels  which  is  seven hundred and eighty four   i  treat  this  as  a  feature  vector  and  feed  it  to  any  of  my  machine \nlearning algorithms say svm or multi class svm or logistic regression or any of these \nright and do a classification based on them  this is what  you would have done in  your \nmachine learning course if i had given you this assignment right  \n\n \nnow  the autoencoder approach or in general the deep learning approach would be you \ntake  this  data  which  is  the  original  feature  representation  that  you  had   there  is  no \nengineering feature engineering happening here right  ideally i want to have features of \nthe form that if pixel twenty five comma thirty was black and if pixel thirty comma twenty was also black \nthen probably i am drawing a curve somewhere  so  it could be one of these curvy digits \nand  not  one  or  any  of  these  seven  or  any  of  these  things  right   so   you  want  to  do  some \nfeature engineering  \nso  typically in machine learning what  you do is  you start with these seven hundred and eighty four features  you \nobserve  a  few  things  and  you  have  these  handcrafted  features  added  on  top  of  these \nright  so  you will add some more features to the data  now the deep learning approach \nis  that  you  let  you  also  learn  the  features  on  their  own   so   how  did  we  learn  these \nfeatures   we  took  this  original  input  we  passed  it  through  the  an  autoencoder  which \ncaptured some of these relevant characteristics  \n \n\fthe  differences  we  do  not  really  know  what  these  relevant  characteristics  are   that \nmeans  you and i cannot read them and make sense of them  i cannot say that this pixel is \nactually  capturing  the  interaction   oh  sorry  this  neuron  is  actually  capturing  the \ninteraction  between  my  seven hundred  pixel  and  seven hundred and ten  pixel   i  cannot  do  that   i  could  have \nhandcrafted those features  if i believe that all my data is around the center  i could have \nhandcrafted some features which say that capture the interactions between those that is \nwhat you do in machine learning  \nhere you are trying to learn the features also on their own right  what would happen if i \nadd one more layer to this autoencoder  i would learn even more complex interactions \nbetween these features   so  this neuron is  actually learning interactions between all the \ninput  neurons ok  i  add  one  more  layer  here  again  this  neuron  will  learn  all  the \ninteractions  between  these  abstract  representations   right   so   i  could  learn  more  and \nmore abstract representations of the input  so  i am not doing feature engineering i am \njust throwing data at the network and i am assuming that it will learn better and better \nrepresentations  \nnow  i am doing this in the autoencoder setup where actually i am trying to optimize the \nobjective function of minimizing this loss and of course  the squared of this loss is just \nfine  so  first what i will do is  i am not happy with my original seven hundred and eighty four dimensions  so  i \ntrain a autoencoder to learn some k dimensions which are good   i know these are good \nbecause they are able to reconstruct the data perfectly to a certain extent right of course  \nbecause  you add  regularization it may not be perfect  but it captures the essence of the \ndata  you get that  \nso   i  have  better  dash  representations  now  feature  representations  right   my  original \nfeature  representation  was  seven hundred and eighty four  i  have  come  up  with  some  better  representations   now \nwhat will i do  was my task to learn feature representations  what was it  classification \nright  so what will i do now is i will i have learned this much from the autoencoder  \n\f\n \ni will throw away the last layer i do not care about the last layer  what i care at the last \nlayer is a classification problem  right  so  i will construct a new neural network  where \nthe first two layers of the network are the same as what i learned from the autoencoder and \non top  of that  i  will add an output layer  and now  i will try to  train  this  network  how \nmany  of  you  get  what  is  happening  here   those  of  you  do  not  get  it  can  you  ask  me \nsome questions let me just try to answer on my own it is like playing chess with yourself  \nso  this is my original input seven hundred and eighty four dimensions  what i have learned with autoencoders is a \nsmarter representation of this data ok  now one simple solution that i have is i have this \none hundred dimensional data suppose this is the representation  so  for all the training examples \ninstead of using that seven hundred and eighty four dimension data and feeding it to a multi class svm  what i can \ndo  is   i  can  first  compute  this  one hundred  dimensional  representation  and  feed  that  to  a  multi \nclass  svm   is  that  fine and  you  see  that  should work  better in  practice because  i  have \nreduced the dimensions  i have reduced the dimensions smartly  \nand now i can train this network is this fine  all i am saying is instead of a multi class \nsvm   i  could  also  have  a  neural  network  right   i  could  feed  that  representation  to  a \nneural  network   so   what  would  that  neural  network  look  like   one hundred  what  are  the \nparameters  here   w  belonging  to  one hundred  cross  ten  how  many  if  get  it  now  ok   so   this  is \nwhat i could have done  \n \n\fso  i have learned a better feature representation and now i am using that representation \nto  learn  my  classifier   if  i  do  this  in  an  end  to  end  manner   that  means   my  feature \nrepresentation  is  also  came  out  of  a  neural  network   and  my  classifier  is  also  a  neural \nnetwork then i have a complete end to end solution for this you get this  \n\n \nnow   we  will  see  a  way  of  visualizing  this  and  then  we  will  make  some  observations \nfrom the visualizations  so  first let me tell you what the visualizations is  \n\n \n \n \n\fso   i  am  returning  to  the  autoencoder  setup   so   i  had  this  input  and  i  had  this  h \ndimensional or k dimensional hidden layer  now i can think of each of these neurons as \nsomething which gets activated for a particular type of input  is that fine  what do i mean \nby activated it is output would be  \nstudent  one  \nremember this is a logistic neurons that we are talking about or even tanh neurons the \noutput would be one  so  it is  the maximum output that  you could  gain  fine   now so for \nexample  h one is equal to sigmoid of this when would this fire when where w one transpose \nx i is very high right when you are in that regime where the sigmoid flattens right  this \nregime ok  when it is very high   \nso  i want to be able to maximize w one transpose x i  do you get this  i want to be able to \nmaximize this i want to find my w one transpose is fixed now because i have trained the \nautoencoder   i  have  got  these weights this is  all  post mortem   right   i have trained the \nautoencoder  i have got these weights  now i want to find an input which will cause this \nparticular neuron to fire  \nso   what  is  my  max  what  is  my  optimization  problem  maximize  just  help  me  out  \nmaximize w one transpose x let me just call it x  and the optimization is with respect to x \nright because i want to find the x which maximizes this quantity my training is done  i do \nnot  no  longer  care  about  changing  ws  my  training  has  been  done  i  am  interested  in \nfinding x\u2019s which will maximally fire this  \n\f\n \nso   and  i  am  going  to  assume  that  all  my  inputs  are  normalized  this  just  makes  some \nanalysis  easier  and  remember  that  normalization  is  always  ok   you  always  do  that   so  \nthis is the optimization problem that i am interested in solving  what is the solution to \nthis  how many if you can solve this  no i want to find the x i  \nstudent  \n  \nnow   i  have  trained  the  autoencoder   now  i  have  known  all  these  the  one  i  am \nconsidering one column of the matrix w one  i want to see what is the input that i should \ngive  so  that  i am  sure  that this neuron will  get activated  and  i know that  this neuron \nwill get activated if i maximize this quantity right   \nso  i want to maximize that quantity and find an x such that it will get maximized  i was \njust hoping that no one brings in eigenvectors w one is a column it is not a matrix just try \nto work it out  what is this  this is a dash between w and transpose and x i dot product  \nwhen  would  the  dot  product  be  maximized   when  they  are  both  in  the  same  direction \nright   that  means   you  know  the  direction  is  going  to  be  x  i  is  equal  to  and  what  did  i \nwant the norm to be  now do you get it fine  \n \n\f\n \nso  the solution is going to be this is fine w one by the norm of w one  so  just remember \nthat this quantity is going to get maximized when the dot product is maximized the dot \nproduct is maximized when both x i and w one transpose are in the same direction right so  \nthat means  x i should be in the same direction as w one and i also wanted this constraint \nthat x i should be the norm of x i should be one  so  i am just dividing w one by the norm of \nw one  \nso   i  know  now  what  is  the  input  i  should  feed  to  the  network  so   that  one  of  these \nneurons fires  now what i am going to do is i am going to plot the xi\u2019s which maximize \neach of these neurons i am going to consider some one hundred neurons in the hidden layer  and \ni am trying to find out the input image which is going to maximize or which is going to \ncause each of these neurons to fire  do you get what i am trying to do  even though you \ndo not get why i am doing it  but do you get what i am trying to do  ok  \nso  what am i going to do is  this is a vector  right  so  i am just going to try to plot this \nas an image of the appropriate dimension  \n \n\f\n \nand this is what i get with a vanilla autoencoder there was no noise this is what i get and \nthis is for the mnist digit data set right  so  my data is two three one and so on digits  this is \nwhat happens when i get twenty five percent nice  and this is what happens when i get fifty percent  \nwhat  do  you  understand  from  these  figures   remember  that  each  of  this  is  the  figure \nwhich caused one particular neuron to fire is that clear each of these is a trigger which \ncaused one neuron to fire   \none  image  yeah  one  box  corresponds  to  one  column  yeah   so   it  is  just  that  the \ndimension  of  the  column  is  again  twenty eight  cross  twenty eight   so   i  am  just  plotting  it  as  a  twenty eight  by  twenty eight \nimage  so  i will just let me just clarify that is i think that is what i said yet  so  what is \nthe dimension of this  in fact  you just know this right this dimension of this is twenty eight cross \ntwenty eight  \nso  i can just take that vector and again plotted as a twenty eight cross twenty eight image  so  what i mean \nis this is seven hundred and eighty four  right  so  x i is a seven hundred and eighty four dimensional vector i am just taking it as a twenty eight cross \ntwenty eight image and plotting it because my inputs were actually images  so  i am just plotting \nthose images fine  so  at least you see  what i am doing here and what i am telling you is \nthat  each  of  these  boxes  that  you  see  corresponds  to  one  of  these  images   so   i  had \nimages x one x two up to x k such that  each of these caused the k\u2019th neuron to fire  ok  now \nwhat are  you seeing here i mean what how do  you make sense of what  you are seeing \n \n\fand remember in the mnist ok sorry  so  let us try to forget all this neural network and \neverything and let us just try to see yes the weights would be  \nstudent  more distinct  \nno why do you say the weights are more distinct yeah  but on average you would be still \nreducing it right ok  so  let me just explain what is happening then we can come back to \nthis   so  now   we  have  this  set  up  we  had  some  input   we  had  a  certain  number  of \nneurons here and then we had the output ok  this is what our neural network was trying \nto do  \nstudent  \n  \nnow  let us take this task of recognizing a digit  now how do i actually recognize a digit \nif i want to distinguish between a nine and a three  i would try to see if there is a curve in these \npositions and it is not there in this  hence this is a three this is a nine that is something roughly \nlike that right  so  in other words i am now i have given delegated so  that means  what i \ndo is i think of three  as a combination of you get the idea as a combination of these images \nwith these strokes right  so  this is actually this stroke this is actually this stroke  this is \nroughly this stroke and so on you get the idea  \nso  i think of three as a combination of many of these strokes  right  now what i would like \nis if this guy could detect one of these strokes  right  the other guy could detect one of \nthese other strokes right  now you see that some of these strokes are shared across digits \nfor example  all these strokes here look  at  the digit nine these strokes  gives common to three \nand nine both right  but some strokes would be missing for three some strokes would be missing \nfor nine and you would have extra strokes in both of these  so now  each of these neurons \ncould actually recognize these strokes  then a combination of the information that each of \nthese neurons is capturing could help me decide whether it is  a three or a nine  how many of \nyou get that intuition  \nstudent  \n  \nso  i would like each of these neurons to detect certain strokes ok  that means  i would \nlike this neuron the first neuron to fire for an input like this  where there is a stroke at the \nbottom   i  would  like  some  other  neuron  to  fire  for  a  different  input  whether  there  is \n\fstroke here  now can you relate this to what you are seeing in the picture  in the second \nand third picture  this neuron is firing for inputs which would have a stroke at the corner \nright  and  you  see  different  neurons  are  firing  four  different  strokes   so   each  neuron  is \ntrying to capture something relevant  and together now i could combine them to get the \nfinal output  how many of you see this  how many of you do not get this  \nso  to ask questions otherwise i cannot really help it  how many of  you want me to go \nover this again  which part yeah so let me just repeat what each of these boxes is right  \nso   each  of  these  boxes  is  the  image  which  causes  the  k\u2019th  neuron  to  fire  right   so  \nremember  i  decide  i  came  up  with  this  that  this  is  the  input  which  causes  the  second \nneuron to fire  what was the dimension of this input  twenty eight cross twenty eight   \nso  i am just plotting that twenty eight cross twenty eight input right  and i am realizing that this input seems \nto be something which has a dark spot here right  so now  just related to the analogy that \ni  am  trying  to  give  at  the  bottom  that  this  neuron  fires  for  inputs  which  have  a  stroke \nhere  that is that is capturing and there are other neurons which are trying to fire for other \nstrokes  \nand  i  would  want  these  neurons  to  capture  different  strokes   so   that  together  they \ncaptured  all  the  information  in  the  image  and  helped  me  decide  that  a  combination  of \nthese strokes gives me a nine a combination of these other strokes gives me a three is that clear \nnow you also  \nstudent  yes  sir  \nso   yeah   so  now   the  thing  is  this  right  the  again  the  same  thing  you  could  learn  to \nreconstruct the output  but you may not capture the important characteristics in the input  \nright   so  now   as  you  keep  making  it  is  job  harder  it  has  to  rely  on  capturing  these \nimportant  characteristics  in  the  input   right   and  actually  if  you  look  at  the  difference \nbetween the second figure and the third figure right let us look at the same guy here  \nso  you see that this is actually thicker and wider  the stroke that you see here is thicker \nand wider  so now  it is actually relying on more neighborhood information to fire it is \nnot firing just for this stroke  but it is fighting for a larger stroke it is also requires more \nneighborhood information because you are corrupting the pitch  \n\fso  it has to rely on information from the other guys  the same example that i gave for \nheight weight and body mass index right the same thing holds here  i have corrupted a lot \nof  inputs   so  now   it  will  fire  only  if  it  gets  a  lot  of  information  from  the  neighboring \ninputs also is that fine ok  and i now coming back to  your question   yeah i do realize \nnow what you are saying that the weights are actually becoming larger yeah  it makes it \nmore robust  but again  \nso  regularization just does not always mean that your weights have to be small right that \nis one way of constraining or regularizing  but this is another way of regularizing where \nyou  are  making  it  more  robust   but  it  does  not  necessarily  need  to  lead  to  the  same \nsolution where your smaller weights does that make sense  it is ok for most of you any \nplease raise your hands if this  \n\n \n and this is same thing that i have written here  \n \n\f\n \nnow  we saw one form of this function ok  which was just flip the input if the output is \njust corrupt the input right  you could also add a gaussian noise  so  you could take the \ninput add a gaussian noise to it with zero mean and then again try to reconstruct the original \ninput back is that fine  so  you could just use different noise functions to do this  so  we \nwill  now  see  such  a  denoising  autoencoder  where  we  have  actually  added  a  gaussian \nnoise  instead of the zero one noise or the corruption that we were doing  \n\n \n \n \n\fyeah  so  the  purpose  of  this  particular  example  that  i  am  giving  is  to  compare  an \nautoencoder  which  is  regularized  by  adding  this  gaussian  noise   with  an  autoencoder \nwhich  is  regularize  by  using  weight  decaying  right  the  ltwo  regularization   so   ltwo \nregularization is also known as weight decaying because you kind of decay the weights \nright you force the weights to be small   \nso  what they showed is that with denoising autoencoder using a gaussian noise   you \nactually  learn  something  known  as  edge  detectors  right   so   you  see  all  of  these  are \ntrying to detect edge  again the same thing is happening i am plotting the images which \nwill maximally cause a particular neuron to fire and it looks like all these neurons fire for \ndifferent edge patterns in your original data  \nso now they are capturing all the edges in the data  and the combination of these edges \nshould tell you what your final class is  ok  and this seems to work much better than the \nweight decay filter which is not really capturing any regular pattern ok  so  this is just an \nempirical  evidence  that  an  autoencoder  with  a  gaussian  noise  seems  to  do  better  than \nautoencoder with the ltwo regularization   \n\f"}
{"audio_filepath": "lec006_005.wav", "duration": 541.0439375, "text": "\nsparse autoencoders \nso  in this module we will talk about sparse autoencoders  \n\n \njust some concepts before we jump into the actual way of doing this  so  hidden neuron \nwith sigmoid activation will have values between zero to one and you say that the neuron is \nactivated when this output is close to one and it is not activated when its output is close to zero \nok  now  a spare encoder tries to ensure the neuron is inactive most of the times  what is \nthat mean  \nstudent  close  \nit is close to zero for  \nstudent  most of the  \nmost of the  \nstudent  \n  \n\finputs right  so  i am passing a lot of inputs to it  it will try to ensure that it is close to zero \nfor most of the inputs  so  in other words what does it trying you ensure  i am looking \nfor  the  word  average   the  average  activation  of  a  neuron  is  close  to  zero  does  that  make \nsense is that fine   \n\n \nso   this  is  on   what  you  see  on  the  left  hand  side   this  is  how  you  would  compute  the \naverage  activation  of  a  given  neuron   you  have  all  the  m  examples  you  see  what  the \nactivation was for each of these and take the average right   \nnow  if the neuron is sparse then the average activation would be close to zero is that fine  \nthis  is  all  just  different  ways  of  saying  the  same  thing   now   a  sparse  encoder  uses  a \nsparsity parameter say rho and it is very close to zero say zero five   \nand it tries to enforce the constraint that on average the activation of any neuron in the \nhidden layer should be equal to rho  which is again close to zero  now  can you think of a  \nthis is all fine in plain english right  you understand what we are trying to do  first of all \ntell me why does this makes sense  what is it that you are trying to ensure  over fitting \nhappens because there is lot of dash  \nstudent  parameters  \nparameters slightly abstract it out  \n \n\fstudent  memorization   \nlot of  \nstudent  memorization  \nmemorization ok  lot of freedom right  i mean the weights have a lot of freedom to move \nwhere ever they want to do  whatever they want to do such that they can just drive the \ntraining error to zero  what have we done to that freedom now  \nstudent  we are restrict  \n  \nwe  are  restricting  them   so   any  kind  of  regularization  always  tries  to  restrict  this \nfreedom that the parameters or the network have in general right  and there are different \nways  of  restricting  this  freedom   you  see  that  this  is  one  of  those  ways  right   you  are \ntrying to ensure that on average the neuron should not fire  so  it is clear that this some \nkind of regularization  any one has a doubt with that  no   \nnow  the second question is taking slightly more on this right  it is i can just move ahead \nand i have convince you that this is regularization  but can you think of bit more and see \nwhat is actually being tried to achieve here  what are we trying to do  how many of you \nget that or at least could here that first of all  only the second row ok  so  yeah how many \nof you can think about this  like what is it trying to achieve   \nstudent  \n  \nright  so  on average neuron is going to be inactive  that means  where ever it is active it \nis really  going to  capture some  relevant  information  right   so   it is going to be active \nwhenever  it  is  active  it  is  going  to  adhere  to  certain  patterns   so   we  are  ensuring  that \neach of these neurons are just a very few patterns and it has discriminative power in that \nsense  do you get that   \nso  now if  that means  if i show it a three  if i show it a two  if i show it a one every time if the \nneuron fires  when there is no discriminative power in that  but now  if i ensure that the \nneuron fires only a few times it will try to fire for meaning full patterns  so  it will try to \nfire for a curve or a curve in the between as you have it in the case of three right  you have \nthis cusp in the between  in the middle  so  it will fire for some kinds of pattern  \n\fso   that  is  what  the  hope  is   it  is  not  just  like  adding  some  math  and  adding  some \nregularization  but at least there is some intuition behind that  how many of you get that \nintuition   ok  good   and  now  can  tell  me  a  way  of  putting  this   everything  english  is \nfine  intuition is fine  but how do convert this to a mathematically equation   \nyou want to ensure that rho hat l is equal to rho  there will of course  be different ways \nof  doing  this   the  way  these  guys  do  it  by  adding  this  term  to  the  loss  function   so  \nremember your loss function is always going to be l dash theta plus omega theta right  \nwhere omega theta does the regularization and l dash theta is your regular loss  which \nwould be the squared error loss or the cross entropy loss or whatever loss you are dealing \nwith right   \nso   remember  this  term  is  always  there   but  the  reason  i  do  not  bring  it  up  so  often  is \nbecause we have already dealt with it  we know how to compute the gradients  we know \nhow to do the back propagation and all that  and now since your final loss is just a sum \nof these two terms  i know how to deal with this and i know that gradients are additive so \ni just need to deal with the second term  that is why i am only focusing on omega theta  \nl theta has been dealt with  is that fine   \nnow  this is what omega theta is  why does this make sense  when would this take its \nminimum value when rho is equal to  \nstudent  watt  \nwatt  everyone sees that how many of you sees that  please raise your hands ok  fine let \nus plot it and check actually  right  \n\f\n \n\n \nso   this  is  how  that  function  looks  like   so   i  have  plotted  the  function  which  i  have \nwritten here for a of course  a single k right and my rho that i have taken is zero two and if i \nplot that function for different values of rho hat l  it will reach the value zero only when rho \nhat l is equal to point  so  again go back and plot this and check and it is actually clear \nfrom  the  equations itself that it will be minimized only  when  rho hat  is  equal  to  rho l  \nright  \n \n \n\fso  that means  this is a genuine  i mean this is a reasonable thing to do  we would think \nof other ways of doing that and i am sure you can  but this is also a reasonable way of \ndoing this  \n\n \nso  now our last function is as i said it is going to be a combination of two values  l theta \nis  a  normal  squared  error  loss  that  we  have  been  dealing  with  and  omega  theta  is  this \nsparsity constraint that you have added   \nnow  you already how to calculate the first term  what are we interested in now  so  you \nsee that this pattern will keep repeating right so now  you can do whatever you want your \nloss function  you have this generic frame of called the back propagation algorithm and \nyou know that a last part of that back propagation algorithm is going to remain the same  \nright  only thing you are changing is the output layer or the loss function   \njust need to compute something there and the rest of it will remain the same how many \nof  you  get  this  general  idea   and  also  appreciate  it  right   that  is  why  this  is  a  very \npowerful frame right  you can just make minor tweaks at the top and you are rest of the \ncode has to remain the same   \nso   you  can  actually  go  back  and  try  out  these  regularization  terms  in  mnist \nassignments  right   if  you  really  want  to  see  what  happens  ok   so   now   this  is  what \n \n\fomega theta is and now what i am going to do  it can be rewritten as this  that is obvious \njust expanding out the law of function   \nand by chain rule this is what  i  get  now unfortunately the rest of the slide there is an \nerror the ta\u2019s please note this i can kind of overlooked this ah  but i will just convey the \nidea  right  so  you would want to do something of this sort  everyone agrees with that  \nremember what is rho hat  it depends on  sorry rho hat l it depends on  \nstudent  \n  \n h of l right it is the average activation of the l\u2019th neuron and this depends on some of the \nweights  so  that is why this chain rule makes sense and now how to compute this  there \nis an error on this slide but you have done enough gradients in the class for me to have \nconfidence that you can do it on your own  everyone is confident that they can work it \nout on your own   \n\n \nso   i will skip  this  we will fix these errors there are some summation and other terms \nmissing here and the second part is actually correct which has been derived on the next \nslide  \n \n\f\n \nbut i would not go over this  this is there are the slides again go back and look at it  how \nmany  of  you  are  confident  that  you  can  do  this  on  your  own   please  raise  your  hands \nyeah because we have done enough of this in class right   \nso  you can  you should be able to it no if you are not able to do it then i am not doing a \ngood job at teaching you right  so  you should be able to do it now fine and we will fix \nthese errors  so  ta\u2019s just remind me after the class  so  everyone gets the general idea  \nyou  find  a  loss   you  find  a  constraint   you  define  it  with  omega  theta   find  out  the \nderivative of that with respect to your parameters and just change your gradient descent \nupgrade tool accordingly   \n \n\f"}
{"audio_filepath": "lec006_006.wav", "duration": 508.5709375, "text": "\ncontractive autoencoders \nso  with that we will move on to something known as contractive autoencoder  so  this \nis yet another type of auto encoders again with the same aim that you want to do some \nkind of a regularization  \n\n \nso  it again tries to prevent and over complete auto encoder or even an under complete \nauto encoder for that point  from learning the identity function  \nso  it does not allow you to simply copy the inputs to the outputs  that is what it is trying \nto learn  and it does so by adding the following the regularization term to last function \nand the way it does this is by defining the following regularization term ok  what is this \nterm  ok  let us see some things which we already know  what is this  frobenius norm \nof some matrix  what is this matrix  \nstudent  jacobean  \njacobean  what is the jacobean  \n\fstudent  \n  \nwhat are the two variables here that you see  \nstudent  h  \nh and  \nstudent  x  \nh is a scalar matrix vector  \nstudent  vector  \nvector  x  \nstudent  vector  \nvector right  so  it is some function between two vectors ok and it is a matrix  so  take a \nguess how many entries would not you have  if x is r n and h is r k   \nstudent  n cross k  \nn cross k  even if you do not know what the entries are you are able to guess that it is \ngoing to be a n cross k matrix right   \n\n \n \n\fnow  let us see what this n cross k matrix looks like ok   \n\n \nso  it has  the input has n dimensions and the hidden layer has k dimensions  so  this is \nwhat the jacobean looks like  \nwhat is the first column  if the partial derivative of every neuron in the first hidden layer \nwith respect to the first input right and now you can see what the other columns would \nbe   this  is  what  the  jacobean  is   this  basically  the  derivative  of  h  with  respect  to  the \nvector  x   answer  is  just  you  are  taking  a  derivative  of  a  vector  with  respect  to  another \nvector you will get a matrix as the output ok  now  what does the jl\u2019th entry here capture \nactually   \nstudent  \n  \nwhat does a derivative capture  \nstudent  \n  \nhow much does h l change with a small change in  \nstudent  x k  \nx k right  that is what a derivative captures is that fine and then what does the frobenius \nnorm capture  it is just the square of sum of the square of all the elements of the matrix \n \n\fright  so  it is basically how much each of these elements vary with respect to the input \nand  we  are  just  taking  the  square  of  that   so   you  see  what  is  the  term  that  we  have \nadded  \n\n \nnow  tell me what is intuition behind this ok  so  when would this term  so  remember \nthis term is added to the loss function and you are trying to minimize the loss function  \nso  that means  you want this term to go to  \nstudent  \n  \nyou want the frobenius norm to be   \nstudent  zero \n  \nzero right ideally of course  that will not happen because there is always a tradeoff between \nl theta and omega theta  if you make it zero then l theta would be very high right   \n \n\f\n \nso  now what would happen  if one of these guys say dou h one by dou x one actually goes to \nzero  what does that mean  h one is not sensitive to variations in x one right fine  but was our \noriginal mandate  what did we want these neurons to capture  we wanted the neurons to \ncapture these important characteristics right   \nso  if x one changes we want h one to change  do you get that  how many of you get that  \nwe  wanted  the  neurons  to  capture  the  important  characteristics  of  the  data  right   but \nnow  we have added a contradictory condition which says that we do not want the neuron \nto capture a variations in the data  do you see this  so  what is happening here  l theta \nsays that i should be able to capture these variations right  otherwise i will not be able to \nreconstruct   \nif  all  my  h  i\u2019s  are  not  sensitive  to  variances  x  one   that  means   i  give  it  any  x  one  it  will \nproduces the same h i  is that clear is that with ok everyone right  that means  so see this \nis this  so  i have these training examples occurs all these training examples my bold x  \nwhich  is  vector  x  is  going  to  change   that  means   xi\u2019s  which  are  the  elements  of  this \nvectors are going to change   \nnow  what this condition is saying is that if i change xi  i do not want the h l\u2019s to change  \ni  do  not  want  the  values  of  the  hidden  representations  to  change   so    that  means   it  is \nchanging the respective of what is the input fed to it try to produce the same output  do \nyou get this argument  ok  that means  it is not capturing any important characteristics \n \n\fof the data  is that fine is that valid argument  but that is not what we wanted  we wanted \nit to capture the important characteristic of the data  so  what are we trying to do now   \n\n \nso  just i  it is hard for me to do evaluate what you have said  but just pay attention and \nsee if that is correct you can judge it on your own  right  so  that is the actually the idea \nright we have put these two contradictory conditions with each other right  l theta says \ncapture the important variances of the data  omega theta says do not capture variations in \nthe  data   watch  the  tradeoff  capture  only  very  important  variations  in  the  data  do  not \ncapture the variations which are not important  can you relate this to something that you \nhave seen before  \nstudent  bias variance  \nno  the other answer there are only two answers bias variance and pca when i say the \nother answer  \nstudent  pca  \nwhat  am i  trying to  force it to do capture only the important  variation   it is if it is  not \nclear right now  we will come back to this ok   \n \n\f\n \nso  let us try to understand with this with the help of an illustration right  how many of \nyou get the argument which i made on this slide ok  most of all   \n\n \nnow   this  is  the  situation   i  have  u  one  and  u  two  as  my  dimensions  fine   which  of  this  is \nimportant  u one  the variations in the data across u one is something that i should care about  \nbecause i can see that brings in some difference what about the variations in u two  \nstudent  not important  \n \n \n\fnot important  they seem like noises because these variations are there they are not all \nlying on the central line  they are slightly away from the line  here are some variations  \nbut should i go out of my way to capture these variations  does it make sense to do that  \nno right  so  it makes sense to maximize a neuron to be sensitive to variations along u one   \nbut  it  does  not  make  sense  to  make  the  neuron  sensitive  to  variations  along  this  other \ndimension  which  is  u  two  ok   by  doing  so  we  can  balance  the  two  conditions   so   one \ncondition was trying to capture all the important variations ok do this  but do it only for \nthe  dimensions  which  really  matter   the  other  conditions  says  that  do  not  capture \nimportant variations ok do this  but do it only for those dimensions which do not matter  \nwhat is this remind you of  at least the diagram should have it away right  \nstudent  \n  \nit is same as principle component analysis right  so that is exactly what you try to do in \npca   you  try  to  capture  the  variations  across  the  important  dimensions  but  not  across \nthe  non  important  dimensions   how  many  of  you  get  the  concept  of  contractive  auto \nencoders  ok good  so  i think that is a where we will end lecture seven   \n\n \nand  just  a  quick  summary   so   we  showed  that  under  certain  conditions  auto  encoders \nare equivalent to pca  \nand we use this result very crucially there  that svd theorem i will not state it   \n \n\f\n \nand  then  we  looked  at  different  types  of  regularizations  for  auto  encoders  where  we \nlooked at weight decaying  that means  the standard ltwo norm  we looked at the sparse \nauto  encoder   the  contractive  auto  encoder  and  we  also  looked  at  these  denoising  auto \nencoders right  so  that is the summary of this lecture   \n \n\f"}
{"audio_filepath": "lec007_001.wav", "duration": 642.504, "text": "\naugmentation  parameter sharing and tying  injecting noise at input  \nensemble methods  dropout \nso   in  this  lecture  we  are  going  to  talk  about  a  bunch  of  regularization  techniques  for \ndeep neural networks  you might find some very familiar terms here  for example  ltwo \nregularization   perhaps  something  else  also   but  i  promise  you  that  we  will  see  a  very \ndifferent interpretation of this from what you have done in your earlier courses  right  \nso  again as is the trend in this course  i will start with some basic concepts  i will take \ntoday\u2019s lecture to finish off the basic part which is the bias variance tradeoff and i will \ntry to make it more informative  then  what you have done in your earlier courses and in \nthe rest of the lecture which will happen on friday  we will build upon these basics and \nthen  try to look at these as the regularization forms  \n\n \nso  let us start  so  these are the sources which i have looked at  so  one of them is the \nchapter  seven  from  deep  learning  book   other  is  this  very  good  lecture  by  ali  ghodsis  on \nregularization  and  of  course   this  paper  on  dropout   so   let  us  start  with  bias  and \n\fvariance   again  some  five ten  minutes  would  be  similar  to  what  you  have  seen  in  the \nmiddle class  but then i will go on to something different  \n\n \nso  we will begin with a quick overview of bias variance and the tradeoff between them  \n\n \nso  let us consider the problem of fitting a curve through a given set of points  ok  now  \nremember i have always been telling you that there is always this true relation between x \nand y which is f of x  right and which we never know  \n \n \n\fso  we do not know what this is  in the movie example  we do not know what this is in \nthe credit card fraud detection or in the oil mining  example  in this particular example i \nknow it right  so  what i have done is  i know that the true relation between x and y is \nthe sinusoidal curve  i know this  but instead of giving you every point on this sinusoidal \ncurve what i have done is  i have such sampled some points from it  i have taken some \npoints and given to you  \n\n \nso  from now on we will behave as if we do not know that this is how it came  it is a big \nsecret  and we now want to fit a curve to this  that means  i want to learn the function f \nhat of x ok  which of course will have some parameters and what will be my goal is that \nnow let us look at this  again my goal would be if i feed at this point after the model is \ntrain  the output should be as close to this point as possible  that is our training criteria  \neveryone gets this  \n \n\f\n \nso  we consider two models  the first model is a simple model  how many parameters \ndoes it have  \nstudent  two \ntwo parameters  right  the other model and this is what happens when i train the simple \nmodel  of course  i will get a line  but do you see something special about this line  why \ndid  i  get  this  as  a  line  or  this  as  a  line   so   on  average  it  is  trying  to  minimize  the \ndistance from all the points  if i have this as the line  then i will have a very high error \nfor these points  right  so  just something which goes along the average  and hence the \nsum of the squared errors would be minimized right  \nso  it is important that when you see these figures  you should make these connections to \nthe math behind it  so  this is the geometry  you have to make connections to the math \nbehind it  right and i hope all of you make that connection  now  i take a complex model \nwhich is a degree twenty five polynomial ok  so  this is w one x w two x square w three x cube and so on  \nit  is  a  degree  twenty five  polynomial  that  i  have  used  and  i  again  learn  the  parameters  of  this \nusing how will you learn the parameters  you have a quiz two days from now on gradient \ndescent  \nwhat  else  do  you  know   if  you  know  any  other  algorithm   of  course  you  know   but \ngetting this end right what else will you use  you can use gradient descent for learning  \n \n\fthese  parameters   the  same  idea   right   you  will  define  a  loss   you  will  compute  the \ngradients with respect to all these parameters  how many of them are there  here twenty six and \njust update those parameters till a fixed number of iterations or any convergence criteria  \nok and this is the curve which i get for the complex model  note this in both these cases  \nwe are making an assumption about how y is related to x  right  in this case  i made a \nsimple assumption   in this  case   i made a slightly  complex assumption  but  in  both the \ncases  we do not know what is true relation is  \nthe true relation is actually the sine curve  but we do not know that we are just making \nan assumption  so  you remember the five things in machine learning  you have a data  \nyou make an assumption about how the input is related to the output  so  these are my \ntwo assumptions  then  i have some parameters  you know the number of parameters in \nthese cases  i use a learning algorithm which happens to be gradient descent and then  i \nminimize an objective function which would be squared error loss in this case  fine  \nnow  the training data actually consists of one hundred points ok  but you do not see one hundred points \nhere  \n\n \nso   what  i  have done is   i  have sampled some  twenty five  points  from  here and  use that  as the \ntraining  data   so   i  have  learned  my  parameters  w  one  and  w  naught  or  w  twenty five  up  to  w \nnaught   using these  twenty five  points   now  i  will repeat  this experiment  k times  what  i do is \nevery time i will get a different sample of twenty five points and i will try to learn the parameters \n \n\fof the model  will i get the same curve every time  will i get the same function every \ntime   no   my  parameters  would  change  slightly   right  because  my  training  data  is \ndifferent  \nso   i am  trying to  learn  it differently to  adjust to  that training data  so   my  function is \ngoing to be different  it is the same form  it is either the linear function or the polynomial \nfunction  but the parameters  the coefficients are going to be different  \n\n \ni will actually draw these different functions and we will make some observations from \nthat  so  this is the black curve that you see is the true sinusoidal curve from which the \ndata  has  come   the  blue  line  is  one  of  these  functions  which  i  have  trained  from  one \nrandom sample of the data  right  \nnow  i train different functions from different random samples of the data and see what \nhappens   i  get  different  lines   this  obvious  can  you  relate  to  this  every  time   i  am \nbasically learning a different value of w one and w naught  is that ok  and i have done this \ntwenty five  times  and  plotted  these  lines   what  do  you  observe  with  respect  to  each  of  these  if \nyou compare any line to any other line  \nso   if  you compare one  of these lines  to  the remaining  twenty four lines  what  do  you observe  \nthey are very close to each other  they are not very different from each other  however \nthere is  a problem  they are very  far from  dash  the actual function   that means  we are \n \n\funder  fitting   we  have  very  few  parameters  in  fact  only  two   that  is  why  we  are  under \nfitting  let us look at the other case  fine  this is the function  the polynomial  the blue \ncurve that you see is the polynomial that i learned from one random sample of the data  \nnow  i am going to learn this from a different sample of the data  you see what happens  \nyou see that the green curve is actually very different from the blue curve  you see that \nhere actually this was peaking whereas  this is going down  similarly this was peaking  \nbut this is going down and so on  so  you see that there are clear differences between the \ntwo  curves  and  if  i  draw  the  next  curve   you  see  it  is  even  more  different   the  same \nfunction learnt from different data point is turning out to be very different  why  because \nit is over fitting on those twenty five points that i have given  the simple model did not even have \nthe capacity to do or fit because it is just two parameters  \nhow much can i over fit  i will just end up drawing the average line right  but here it is \nreally  able  to  overt  fit  and  you  see  that  these  twenty five  curves  or  i  do  not  know  how  many \ncurves that i will draw  all of these are going to be very different from each other  you \nsee that  and everyone agrees that this would happen if you actually try to do this ok  so  \ncomplex models train on different samples of the data are very different from each other  \nwhat is happening there is over fitting ok  \n\n \nnow  let me define two concepts  from statistics one is bias  bias is very simple  it tells us \nthat this is the true function  if you are trying to learn the approximate function and you \n \n\fdo it many times  then you will get an expected value of the function  so  it tells you how \nmuch does this expected value differ from the true function  ok  you get the definition  \nthe definition is straight forward  ok  now  for the simple line or the simple model  the \ngreen line that you see is actually the average of all those twenty five lines that you had seen  ok  \nwhat can you say about the bias  very high right because this difference is very high  \nthis  green  line  is  very  different  from  the  red  curve  which  is  my  true  function   right \npredicted and true function  now  what about complex model  the blue curve that you \nsee is actually the average of all those twenty five different curves that i had drawn  so  what is \nthe bias  it is very low  does that make sense  this means that the simple model has a \nhigh bias and the complex model has a low bias  is it clear to everyone  \n\n \nnow  let us define another quantity which is variance  everyone knows what variance is  \nso  this is one of the functions that  i have learned  this is the average of that function \nand the variance tells me the spread  now  based on the figures that you have seen  can \nyou tell me what would happen for the simple model  low variance or high variance  \nstudent  low variance  \nlow  variance   because  all  these  models  were  very  close  to  each  other   there  was  not \nmuch spread in the models  what about the complex model  \n \n\fhigh variance  all these models were very far from each other  the spread was very high  \nok   so   roughly  speaking  it  tells  us  how  much  the  different  f  f  act  f  x  is  that  you  are \nlearning  how different are they from the average f of x  \n\n \nso   informally  i  can  say  the  following  simple  model  has  a  high  bias  low  variance  \ncomplex  model  has  a  low  bias  high  variance  and  as  always  going  to  be  a  tradeoff \nbetween the bias and variance  \nso  why is there always a tradeoff between the bias and variance  people have done ml \ncourse   why  is  there  a  tradeoff   how  many  of  you  know  the  mathematical  answer  to \nthat   you  have  not  done  this  in  the  ml  course   no   so   it  turns  out  that  both  bias  and \nvariance contribute to the mean square error and let us see how  \n \n\f"}
{"audio_filepath": "lec007_002.wav", "duration": 673.967, "text": "\ntrain error vs test error\nso  we would start the next module where we will talk about training error versus test error\nand before that we will see this bias variance tradeoff \n\n\nso  now what have we done so far in these complex models and the simple models  we have\ntrained them using the dash data training data and what are we interested in always a test\ndata  right  i already know  what was the oil amount of oil mined from the training data\nlocations that i was given and i am not interested in predicting those  i am less interested in\nlearning those  so that if you give me a new location  i should be able to do the right\nprediction \nso  i am always interested in the test data  so  now consider a new point which is not seen\nduring the test data and there are several such points that you could see  now  if you use the\nmodel f hat x to predict the value of y  then the mean square error is given by you get this  it\n\fis just the expected value of this squared error that i will get  so  what is the randomness\nhere  y expected value because the x that i am going to feed at test time is going to vary for\neach of these different xs  i will get a different error \nso  hence that is a random variable  do you get that  so  please focus on these things  right  i\nmean just do not take a formula for granted  just see what is it trying to see  so  whenever\nyou see an expectation over something  always question  what is a random variable here  so \nwhat is the random variable here  it is the squared error loss  why is it random  it is because\nit changed the input x  you are going to try it over a multitude of test examples  you will take\none thousand text examples  ten zero text examples and so on right  for each of this  you will get a\ndifferent squared error  that is the randomness  so  you want to see what is the expected\nvalue of this or very loosely speaking the average value of this  now  it turns out that this \nnow  just try to remember that this is also some expectation and you had the terms f x and f x\nhat here  this also had some expectation and term f x and f x hat and so on  right  if you do\nnot remember the exact formula it is ok  but you do remember there were some expectations\ninside and the terms f x and f x hat whether they are  so  this is just simple  you are dealing\nwith a minus b the whole square on the left hand side \nif you if you open it up  rearrange some terms  you will get this  right  so  you can show that\nthe mean average or the expected square error on the test data is actually the bias square plus\nthe variance  that is a small amount of irreducible error  you can go back and work this out\nand actually the proof is given here on the link  ok but i hope you get the intuition  you have\nthis a minus b the whole square \nif you open it up and rearrange the terms  you should be able to get this  now  what does this\ntell you  what happens if the bias is high  the squared error is going to be high  what\nhappens is if the variance is high  it is going to be high  so  that is why you do not want a\nvery high bias  you do not want a very high variance also  you want this sweet spot in\nbetween where the bias and variance are just about optimal  you get that  that is why there is\na tradeoff between bias and variance \n\fyou cannot rely on simple models which have high bias  you cannot rely on complex models\nwhich have high variance  you want something in between \n\n\nnow  the parameters of f hat x remember that they are trained using the training data which\nconsists of these end points that you have  at test time we are interested in evaluating the\nmodel on a validation set which was different from the training data  this gives rise to the\nfollowing two quantities  one is the training error which you deal with at dash time training\ntime  that is the error that you are trying to minimize  right  but a test time you have a\ndifferent error which is the test error and that is the error that you care about \ntypically these two errors exhibit a certain trend  do you know what the trend is  now  on\nthe x axis  i have model complexity and on the y axis  i have error  as a model complexity\nincreases  what would happen to the training error  it will go to almost zero  that is exactly\nwhat happened from the linear function to the polynomial function  this is how it will\nbehave  as the model complexity increases  as the model complexity increases what would\nhappen to the validation error  it will decrease up to a certain point  right because you are\nstill not over fitting on the training data  your answers are still generalized \nso  you had this degree one polynomial degree twenty five polynomial  if i take in something in\nbetween  then probably this is where i would have ended up with the training error and that\n\fwould not have been too bad for the test error  you see this  ok  now  you see i will mark two\npoints two regions rather one of this corresponds to high bias  the other one corresponds to\nhigh variance  tell me which one is  which do this  i cannot understand  so  let me ask this\nis  this is ok \ngood  so  you see that there are  these two extreme and we want somewhere to be in\nbetween  ok  at least you get the intuition behind this fine ok  and you are looking for this\nsweets spot which is the perfect tradeoff between the bias and the variance  right  so  now\neveryone gets why there is a tradeoff and how this relates to model complexity and therefore \nwe are looking for the ideal model complexity  how do we achieve the ideal model\ncomplexity  well  we cannot really  ideal is ideal  but we try to do this using dash  what is\nthe title of this lecture \nstudent  regularization \nregularization  i will try to use regularization to achieve this  ok  so  let us formalize this a\nbit more  and remember that this curve is actually because of this equation that you see  right\nhigh bias  you will be in this region  i am actually inserting it  ok fine ok \n\n\n\fso  the intuitions that we have developed so far is that if there are n training points and m test\npoints  then we have a train error which goes over the training points and we have a test\nerror which goes over the m test points  ok \nso  i am just taking a total of n plus m points  the first n is training the next  last m is test \nnow  as the model complexity increases  what happens to the training error  it becomes very\noptimistic and gives you a very wrong picture of how close the predicted function is to the\ntrue function whether it makes you feel that you have done a perfect job  this you have\nactually discovered the true function  but that is not correct  it is giving you a false picture of\nthat  therefore  we should always look at the dash error \nstudent  validation error \nvalidation error  so now you see that why you always do this train validation and test split \ntest is unseen  you try to optimize on the training error ok but you should always tune for the\nvalidation error  your optimization algorithm is going to take in the training error  it is going\nto be very optimistic  it is going to try to drive to zero  but you should look at the validation error\nand try to see that you are not over fitting on the training data everyone gets this intuition \n\n\nso  now this is all intuition  we will have to concretize this mathematically  so  that is what\nwe will do now  so  that d be these training test points that we have  we know that this\n\frelationship holds  we do not know what f is  but we know that this relationship holds  so \nwhat am i trying to say here that we know that there is a true relation between y and x which\nis given by the function f  but i am also willing to admit some noise that may not be a very\nneat function  but a small noise might exist \nthat is the epsilon i  ok and i am going to assume that epsilon comes from a normal\ndistribution with zero means  so  on average the noise is going to be zero  but there is a small\nvariance  everyone gets this  this is a true relation  but i am willing to admit some noise in\nthe relation  ok fine and of course  we do not know f we never know f  right  now  going by\nour paradigm where we have these five components  we use f hat to approximate f f hat will\nhave some dash which will i try to learn from the training data  what is this dash parameters \nright which will try to learn from the training data  the training data t is a subset of your total\ndata which is thus those endpoints  right and we are interested in knowing this quantity  this\nis what we are actually interested in  can we compute this quantity  how many of you say\nyes  how many of you say no  we cannot  why cannot we compute it \nwe do not know f  so  why cannot you raise your hands if you all can answer in chorus  so \nwe do not know what f is then how do we compute this quantity right  but what do we\nactually know  so  now we are going to see something which is true expectation and\nsomething which is empirical estimate expectation  how many of you know this  what is the\ndifference between the two  most of you should  but it is not confident about it ok  so  we do\nnot know what fxi is the true thing  but what do we know  we are given some training data \nright \nwe know these yi\u2019s for was training data and we know these yi hats for those training data \n\f\n\nso  this is something that we can estimate  yes or no  this is given to us  so  this expectation\nis going to be an empirical estimate  right because we are going to look at some one thousand ten thousand\ntwenty thousand training points and estimate this  right  it is an empirical estimate  how many of you\nget that  now  i am just going to rewrite some of this  so  what i have done is  i just defined\nthat yi is equal to fxi plus epsilon i  so  i have just replaced yi by that  ok  is that fine  now \nthis is of the form a minus b the whole square  so  i am going to treat it as that and just open\nup the bracket  so  i will have a square minus two a b plus b square and now  this is a sum or\ndifference of expectation  so  i can push the expectation inside  so  this is what i get this  is\nthis fine ok \nnow  i am just going to rearrange the term  so  remember this was the quantity that we were\nactually interested in  but this is the quantity that we had a handle over because these were the\ndata points given to us  so  i will just rearrange the terms and i can write this which was my\nquantity of interest as this  can you estimate everything on lhs  on rhs  this  this  what\nis this variance  sigma square  we assumed it came from zero  sigma square distribution and\nthis  can estimate the answer is no for the same reason  we do not know what f of x is ok \n\f"}
{"audio_filepath": "lec007_003.wav", "duration": 1070.378, "text": "\n\n\nso  we spoke about bias and variance and we saw that simple models have a high bias  but\nlow variance and complex models have a low bias high variance and so on  and we saw it\nsome illustrative examples that what that is what that means \n\f\n\nand  then  the important thing to note was these two formal definitions of bias \n\n\nand formal definition of variance which you all know anyways  and then the important\nconcept that we spoke about was the train error versus test error  right \n\f\n\nso  this was the curve that we were interested in and one corner of this curve was related to\nhigh bias low variance  and the other corner was related to low bias high variance  right \n\n\nso  i am looking for something in the middle  that is what our quest is in this lecture  right\nand we want to find ways of falling somewhere in middle \n\f\n\nand this led to the definition of two quantities of interest or training error and test errors  so \ntraining error is computed from the training points  these are the points that you actually look\nat while you are solving this optimization problem  so  the training always involves solving\nan optimization problem which is the objective that you want to optimize or maximize and\nthe test error is something that you want to use it for at eventually \nso  you all have these two quantities of interest that we design and we realize that the training\nerror is more optimistic whether the test errors actually gives us the real picture of what we\ndo and we tied those back to things that you have done previously in the machine learning  or\nother courses that we always split the data into training  valid and test  training it on the\ntraining data  do some validations on the validation data  but never look at the test data  that\nis for the final evaluation \nso  that\u2019s the this is this intuition which i have been trying to build with these two curves is\nthe explanation for why we do things that way \n\f\n\nnow  we are interested in doing a more mathematically rigorous analysis of this intuition \nright  so  that is where we left off  so  what we are interested in  so  now i will just start\nfrom this point is that we are given some data which is m  n  m training points and n testing\npoints and we know that there is a true function between the outputs and the inputs  and we\nare also expecting or accepting some noise in this relation just as in any other relation so\nwhich means that y is related to xi  but by some true function  but there is also this noise and\nfor simplicity we assumed as this noise comes from a normal distribution with zero mean and\nsome small variance and as usual we never know f  right but we are trying to approximate\nthis f hat  and we come up with some parametric form for f hat and then  try to learn the\nparameters of f hat from the training subset of the data that is given to us \nso  this is what we always do and we have already seen different variations of f hat  one of\nthem being the deep neural network and what we are actually interested in is this quantity  the\nexpected difference or square difference between the predictions made by our model and the\ntrue value of the output with respect to the true function right  then  we asked i asked you\nwhether we can actually estimate this quantity and all of you said no  why  it is because you\ndo not know what f of xi is  right  so  we will see how to estimate this empirically \n\f\n\nso  then we started off with this information that we have we know what y i hat is because\nthat is the prediction that we make and we know yi  what yi is  we do not know the function \nbut we see the output of the function in the form of the training data points given to us or any\ndata points given to us \nso  we wrote this by making this particular substitution where we notice that yi that we see is\nactually the true function plus some noise and then  we did some trickery and try to simplify\nthis  and then  we just realize that this is the term that we are interested in  so  we moved it\nto the other side of the equation and came up with this neat left hand side or neat right hand\nside that we need to analyze now  so far everything is clear \nthis is where we ended the last class  right  you just went to it very quickly  but i assume\neverything is clear at this point  ok fine  so  we are left with a bunch of expectations  right\nand we have i am assuming we have no clue how to estimate this  right i mean  so and\nremember that when you are dealing with expectations as always this true expectation and\nthen  there is this empirical estimation right  so  what we are going to move towards  so \nthese all equations when i write e here  capital e here  i am talking about\nthe true\nexpectation \n\fnow  we will see how to approximate the true expectation with an empirical expectation and\nthen  based on that we will make some observations \n\n\nso  that is what we will do now \n\n\nso we will just take a small d two and i will just tell you what expectations are or what\nempirically expectation is  how to compute them  so  suppose we have observed the goals\n\fscored in k matches  there is some k football matches that we have seen and we have seen\nthat the goals scored were the following \nnow  if i asked q what is the expected value of the goal  now  the number of goals for what\nwill you do  take the average of this  this is what you will do  so  what is it that you are\ndoing here  you are taking a dash estimate of the expectation  empirical estimate  you are\nmaking some observations  these are the observations given to you  these are the k matches \nwatch as much  as many football matches as you want after the semester ends and then \nnotice the number of goals that were scored in them and then  you can compute this\nexpectation  right and this is how you do empirically  so  there is something that we do on a\nregular basis  but i just want you to realize that what you are doing is actually an implicit\nestimate of the true expectation \nnow  can you relate this to the quantity that we are interested in  we are interested in\ncomputing a certain expectation which is this  can you take an analogy and tell me how you\nwould do this  the hint is we have done this a million times in the course already  fine  so \nthis is how we will do it and have actually done this a million times in the course  so  when\nyou compute this  we are actually doing an empirical estimate of the data \nso  let us just take a minute to understand this  we are given some data  we are interested in\nthis to expectation which we cannot compute  so  we will take this data  we will assume there\nis enough of this  we are given m samples which are enough and from that we will make an\nempirical estimate and just as in the case of these goal scored  right  as you see more and\nmore matches  you will have a better understanding of how many goals can be scored when\ntwo particular teams are playing  in the same analogy goes here  as you see more and more\ndata  your estimate would become better  but that is how you will do the estimation \nso  now we will come back to  so now do not get surprised when i am going to replace all\nthese e\u2019s by this  all the e\u2019s that we had in our original equation  i am going to replace them\nby these summations  ok fine \n\f\n\nso  this was our original equation that we had derived and we were interested in this left hand\nside quantity which is a sum of some terms on the right hand side  so  now this expectation i\ntold you that we can estimate it from data  but which data  training data or test data  both \nso  we will try to estimate it from both and see if there is any difference which arises when\nyou estimate it from one data and the other data ok \nso  the first thing that i am going to do is  i am going to use test observations to estimate this \nso  can you tell me what are my summations going to look like  it is summation over n plus\none to n plus m right  we assume that the first endpoints are training points and the remaining\npoints are test points \nso  the quantity on the left hand side is true error  remember that because that has f x which\nwe do not know quantity on the right side  the first thing is empirical estimation of the error \nok  the second thing is a small constant  however  the epsilon i square and we assume that\ncomes from a normal distribution with a small variance  what is the third quantity  actually i\nhave given you the answer already  but i want you to think about it  i am saying it is the\nco variance between two things \n\fwhen i say it is the co variance between two things  what is the first thing that i need to\nprove is that the two things are dash random variables  i mean first thing we need to see is\nthat the two things are random variables epsilon i clear  it is a random variable \nwhat about this other thing or rather epsilon is a random variable what about the other thing\nand depending on the training instance that you have sampled  this ongoing difference is\ngoing to differ  right  you are having your training or test instance whatever is this x i this is\ngoing to differ because these x\u2019s are different  they are all random variables  so  there is\ndifference between these two quantities also going to be a random variable  is that fine ok \nbut still is this the \nso  then i have told you this is x and this is y and what i am saying is that the co variance\nbetween x and y is just e of x x into y  is that correct \n\n\nthat is how you define co variance  what is the definition of co variance  if you have\nbothered to look at the prerequisites  no expectation in the form of e  so  co variance is e of x\nminus mu of x into y minus mu of phi  what is our x epsilon and what is our y  what is mu of\nx zero \n\fso  i will just simplify this a bit ok  i will open up the product  what is mu of y into e of x \nwhat is e of x  what is the expected value of the noise zero \n\n\nso  then this turns out to be as that  is that fine  that is why we are writing the co variance is\njust the product of the two things \nso \nlet us just\ntake a minute to again understand this  the true error is the empirical\nestimation of the error plus i mean plus or minus a small constant ok  and then  this nasty\nquantity that we do not know what to do with it  so  let us look at this quantity and see what\nwe can say about it \n\f\n\nnow  what is the co variance between these two  i am trying to compute this expectation\nfrom the test data  just remember that  so  each i here is a test instance are these two random\nvariables dependent or independent is the question that i am trying to ask  it is independent \nso  let us look at it piece wise  so  remember that we had said that y is equal to f of x i plus\nepsilon i  right  this epsilon i had no relation to f of x i  i mean i could choose any x i  but\nthis noise is going to be random  so  there is no relation between these two \nnow  is there a relation between f hat of x i and epsilon i  we are doing tests  so  how did we\ncome up with f hat of x i  how did when i say how did we come up with f hat is i mean \nhow did we learn the parameters of a f hat using the training data and what are we computing\nexpectation with respect\nto now test data  these these epsilon improve \ninfluence the\nparameters that we had learned further from the training data  no  since there is no\ndependence between these two guys \nso  that is why epsilon i is independent of the other random variable that you see in this\nexpectation  is that clear  do you get the intuition f hat x i further  no  but this is the mean \nthis noise is what is present in the test data and you have not seen this add training time \nwhen you are training the parameters  you did not look at this noise  you are looking at the\nnoise in the training data \n\fso  this is not participated in the estimation of the parameters of f hat  but that was for the\ntraining data right  but this now i am doing the expectation from a test data \n\n\nso  these two random variables are independent  that means  i can write this as is this fine \nwhat will happen to this zero ok so  what did we eventually conclude that the true error is equal\nto empirical test error plus a small constant  right \nso  what does this tell you  now  tell me forget the math  tell me in english  right  what\ndoes this take  what does this mean  can you relate it to  now why you do this training error \nvalidation error  test error  so  what does this tell me  this tells me that if i have trained a\nmodel and now if i take an estimate of the error on some data which i had not used for the\ntraining  then that error which i see is actually very close to the true error  it only differs by\nthis small constant \nhow many of you get that  that is why when i look at the validation error  it is not being\noverly optimistic  it is giving me a true picture of what the actual error is  right  so  there are\ntwo things that you need to understand here  one  this is the quantity that we are interested in\nwhich we cannot estimate  we are trying to estimate it by using this  we are trying to make an\napproximation  so  we are trying to see how good this approximation is  what this derivation\nis telling us is that if you are approximated it using the test error or the test data  then this\n\fapproximation is actually very close to the true error and how close it is actually  it just\ndiffers by this small constant \nso  you get the importance of what we are seeing here right  ok  now  to truly appreciate this\ni need to tell you what would have happened if you had used the training data for this\nestimation  right  it is largely dependent  but that is again a normal assumption that you\nmake  so  this is ok  good that you asked at this point  i will be doing a couple of things\ntoday where we will be deriving some things  we will\ntry to prove some things\nmathematically  but all of these would have underlying some assumptions \nso  if you remember the adam derivation with this we did there  also we had made this funny\nassumption that the gradients are actually coming from a stationary distribution which will\nnot happen in practice  so  this reminds me of this joke from big bang theory  which says\nthat i have a solution  but it only works for squared eggs in a vacuum  right  so  it is basically\nall these things always have some assumptions underlying them  but the idea is to kind of\nignore those assumptions and see what happens in a neat setting and at least see whether in a\nneat setting everything works fine or not \nso  that is what is happening here  so  is a valid point that you are assuming that the noise\ncomes from a zero mean distribution  now  if the noise did not come from a zero mean\ndistribution  then this would have not gone down to zero and the mean would have been\nhigher than this is no longer a small constant and so on  so  those things are there  so  this is\ngoing to happen in some of the other derivations that i do today  it is not that i am teaching\nyou something wrong  it is just that you have to take it with a pinch of salt in the sense that\nthese assumptions are there and the original derivations these are not my assumptions  and\nthey work only under those assumptions \nso  you have to be careful about that  but the idea is that still with these assumptions  can we\nat least make something meaningful out of it  right is that fine with everyone  can we all\nwork with that basic premise  so  what i have done so far is told you that if you are\nestimating the errors from the validation data  you are doing a good job \n\f\n\nnow  let us see if i would estimate the error from the training data  take a guess what would\nhappen  what would my argument for this be  now  this will not disappear  right because\nthese two are not independent now  i cannot write it as a product of two expectations  that\nmeans  it will not go down to zero  so  that is the argument which i am going to make \nso  hence actually the true error if you see  right it is equal to the empirical estimation plus\nsome quantity  that means the true error is dash as compared to the empirical error  that\nmeans the empirical error that we see is pessimistic or optimistic  optimistic  that is what i\nstarted with that  you gave a very optimistic estimation of your error if you are looking at this\nempirical estimation from the training data because you have ignored this quantity  is it fine \nso  what is missing in the story \nlet us see now what was this quantity  so far all our discussions l theta right  but now\nsuddenly i have realized that my true error is actually l theta plus something else right  you\nsee where i am headed with this ok  so  that is what we need to see now ok  now think it\nwould be we should  but i am pretty sure it is positive  i cannot work it out right now but i am\npretty sure it is positive  and you can see and if you find it is not then let me know \n\f\n\nso  how is all this related to model complexity  we started off with this idea that model\ncomplexity tells you how much is the bias  how much is the variance and because of that you\nget these two curves that you are not happy with  one curve being very optimistic and the\nother curve being a bit pessimistic  now  how does this discussion tie up to model\ncomplexity \n\f"}
{"audio_filepath": "lec007_004.wav", "duration": 517.7429375, "text": "\ntrue error and model complexity \nso  now we will try to see that how does this true error that we see depend on the model \ncomplexity  \n\n \n  \nso  using steins lemma and some trickery  we can show the following  what is steins \nlemma  so  i had this deal with my students last year  you do not ask me what steins \nlemma is  i will not ask you what steins lemma is  ok  so  it is some lemma which tells \nus  that   this  quantity   what  was  this  quantity  the  last  term  which  was  troublesome  rate \nthat  covariance  term  which  was  troublesome   that  is  this  quantity   this  quantity  is \nactually equal to this quantity   \nso  let us buy that  let us all of us agree that steins lemma is correct and it tells us that \nthis is the case ok  and you saw the quiz one paper ok fine  from last year i mean  ok  so  \nnow we will work with this premise and we will see what it actually tells us  now  when \nwill this quantity be high  so  what this is telling us  i mean jokes apart  let us try to \nfocus again that this quantity is actually equal to the summation of this quantity  \n\fnow   let  us  take  one  term  in  this  summation  when  would  dou  f  hat  x  i  by  dou  y  i  be \nlarge   what  does  it  actually  tell  you   if  i  change  one  of  these  yi\u2019s  a  bit  when  the \nprediction for it is going to change by a lot  do you get that  how many of you get this  \nsome of you do not get this  just think about it  when would this be high  what does the \nderivative  capture   if  the  derivative  is  high   that  means  a  small  change  in  the \ndenominator is going to lead to a large change in the numerator   \nwhat  is  the  denominator   actually  the  true  y  that   we  have  observed   what  is  the \nnumerator   that  is  the  predicted  y   so   what  you  are  saying  is  that  if  there  is  a  small \nchange in y i  then there is going to be a large change in the prediction  ok  when would \nthis  happen   would  this  happen  for  simple  models  or  complex  models   complex \nmodels  how many of you say complex models  so  this is the link to model complexity \nrate and i will make a more intuitive case for this  but at least some of you get this that if \nyour model is very complex  that means it is even one of your data points changes and \nthe prediction of the model is going to change largely  \nso  now  relate this back to that sinusoidal model that we had and we had this complex \nmodel   every  model  that  i  was  training  which  was  strained  on  a  different  set  of  twenty five \nexamples  the model was vastly different and that is exactly what was happening  when \nyou  were  changing  even  one  data  point   your  predictions  were  changing  largely   that \nmeans   your  model  was  changing  largely   do  you  get  that  intuition   so   indeed  a \ncomplex  model  will  be  more  sensitive  to  the  changes  in  the  observation  whereas   a \nsimple  model  will  be  less  sensitive  to  it   and  hence   we  can  say  that  the  true  error  is \nactually  equal  to  the  empirical  train  error  plus  something  which  relates  to  the  model \ncomplexity  \n\f\n \n  \nnow  let us first verify that indeed a complex model is more sensitive to minor changes \nin  the  data   so   this  is  some  data  that  i  had  sampled  from  the  same  distribution  and  i \ntrained one simple model which is the green line which you see that was a linear model \nand i trained one complex model which was a twenty five degree polynomial which you see  ok  \nnow  what i am going to do is  i am going to take one of these points and change it a bit \nand i retrain the model   \nwhat  happens  to  the  simple  model   it  does  not  change  much   but  what  happens  to  the \ncomplex model  it is more sensitive to these observations that i have and that is exactly \nthe  quantity  that  we  were  interested  in   that  means   a  complex  for  a  complex  model \nwhich  is  more  sensitive  that  summation  that  we  care  about  is  going  to  be  high   that \nmeans  that difference between the true error and the estimated error is going to be high  \n\f\n \n  \nso   that  is  why  instead  of  minimizing  the  train  error   we  should  always  minimize  the \ntrain error plus some quantity which is linked to the model complexity  this is the basis \nfor all dash methods regularization method  \nso  now  you see where  this  comes from  so  ok  where omega theta would  be high  for \ncomplex models and simple for simple models  ok you get the intuition for this and the \nrest of the lecture  we will spend in taking various cases where we will actually show that \nomega theta would be high and we are trying to control for omega theta  this quantity \nfor the rest of this lecture and for the rest of this course i will assume that we all know \nhow to deal with   \nwe  have  done  enough  of  this   we  have  done  a  lot  of  back  propagation   we  have  done \nenough  derivations  of  the  laws  with  respect  to  the  output  layer  and  so  on  everything  \nright  so  all of us understand how to deal with l train theta where l train theta is this l \nequal to one to m squared error loss or your log likelihood or any of these losses  right  so  \nwe all know how to deal with this today  we are going to focus on this other term which \nbrings in the regularization  \n\f\n \n  \nso  what omega theta does is actually acts as an approximation for this  so  what i should \nhave actually tried to minimize is not just l train theta  but l train theta plus this other \nquantity which was there in my equation  you get this my true equation was that my loss \nis  equal  to  l  trained  theta  plus  this  term   right  which  we  approximated  using  steins \nlemma   so   i  should  have  tried  to  minimize  this  quantity   but  i  do  not  know  how  to \nreally compute this quantity   \nso   i am  going to just substitute it by omega theta and ensure that omega theta is such \nthat  it  is  high  for  complex  models  and  low  for  simple  models   do  you  get  the  recipe  \neveryone  gets  this   how  many  of  you  understand  this   fine  so   we  can  show  that  lone \nregulation ltwo regularization early stopping all of these are actually special cases of this \nparticular formulation that we have  \n\f\n \n  \nand  remember  that  this  is  the  sweet  spot  that  we  were  aiming  for  ok   and  this  gap  is \nactually   this  quantity  because  we  are  making  a  very  optimistic  estimation  of  the  error \nwhereas  there is actually this quantity which we have been ignoring and that is why we \nsee that the validation error is high  ok  so  is the full picture in terms of the diagram and \nall the equations that we have seen  \nso  we should ensure using omega theta that this gap is also minimized  therefore  our \nfunction  should  be  minimized  l  theta  plus  omega  theta   so   essentially  what  we  are \ntrying to do is minimize this gap and hence  the model would generalize better on the test \ndata  is this intuition clear to everyone  \n\f\n \n  \nwhy do we care about this bias variance tradeoff model complexity  this is not a course \non  machine  learning   they  are  highly  complex  models   they  have  many  parameters  \nmany non linearities  in fact  now can you relate this back to the universal approximation \ntheorem  what is the universal approximation theorem say give me any data  i will give \nyou a deep neural network which will exactly over fit the data  right and that is exactly \nwhat  we  want  to  avoid   that  is  why  regularization  is  important  in  the  context  of  deep \nneural networks  fine  it is very easy for them to over fit the data and derive training error \nequal to zero and that is why we need some regularization  \n\f\n \n  \nso   today  we  are  going  to  look  at  different  forms  of  regularization  starting  with  ltwo \nregularization   some  simple  tricks   so   some  of  these  are  going  to  be  mathematically \nmotivated  some of these are just  going to be heuristics or empirical  stuff  so   data set \naugmentation is one such empirical stuff  how many of you tried data set augmentation \nfor the immunized assignment or the back propagation as parameter sharing and tying is \nsomething that  no  i am not   \nplease  do not give me that look  yeah i am not suggesting that  adding noise the inputs  \nadding  noise to the outputs  early stopping   ensemble methods and drop  off  right   so  \nthese are the things that we are going to talk about this and all of this is in the context of \nregularization where you want to avoid some kind of model complexity   \n\f"}
{"audio_filepath": "lec007_005.wav", "duration": 1442.826, "text": "\nltwo regularization \n \n \nso  let us start with ltwo regularization  so  i have seen this before  \n\n \n\fso  all of you see that this is ltwo regularization right  what does ltwo regularization does  \nnow tell me in the context of things that we have discussed today what is this  empirical \nestimate of the train error ok and what is this  is that fine right  so  everything that we \nare going to write is l  because of its w  but fine right ok  now why does this relate to \nmodel complexity  what am i doing here actually by adding this  \nso   they  are  going  to  see  a  very  detailed  analysis  of  this   but  i  just  want  to  see  first \nwhether you get an intuition behind this  so  by doing that what you are trying to do  not \nallow the model to  become very complex right  you do not want  a model  where  your \nweights can take any possible value  you just want the weights to be small  so  you are \nreducing the freedom on the model right  less freedom less complex  you get the intuition \nat  least   we  will  see  this  in  more  detail   but  at  least  you  get  the  intuition  why  we  are \ndoing this  \nso  we are using omega  remember that we are using this omega theta as a surrogate for \nmodel  complexity   so   if  you  add  something  in  all  omega  theta   just  make  sure  you \nunderstand that this relates to model complexity ok fine  and now for sgd what would i \nneed   for  gradient  descent   just  in  case  you  have  forgotten  what  sgd  is   what  do  we \nneed  nothing  we have done it   fl  gradient of this  which is a sum of the derivatives \nof the two quantities of which you know one right  you know this already  and what is \nthe other guy  alpha w right  \n\n  \n \n\fso   you  see  this  ltwo  regularization  right  one  reason  why  it  is  preferred  is  now  imagine \nyou have already written code for gradient descent  all you need to do is change it at one \nplace add this to  your update rule  that is all  you need and  you can think of the vector \nform of this  where you have a vector of parameters  you can think of the matrix form of \nthis  variable  vector  matrix  of  parameters   all  you  need  to  do  is  add  one  term  to  your \nupdate  rule   so  it  can  be  done  with  very  minimalistic  change  and  this  would  be  your \nupdate rule  now  let us see geometric interpretation of this  \n\n \nnow  from here onwards some of you will start getting a bit uncomfortable with some of \nthe math  because of these assumptions that it only works for squared eggs in a vacuum \nright   so   you  will  see  those  kind  of  things   i  will  not  tell  you  upfront  what  is  the \nassumption i am making  because that will just spoil the analysis  you will just not enjoy \nit  as  much  as  you  would  ignorance  is  bliss  right   so   if  you  do  not  know  what  the \nassumptions are  you will probably enjoy it more  \nbut for some of you will pick it up  just keep it to yourself  at the end i will tell you what \nare  the  assumptions  i  had  made  ok   there  are  some  tricky  assumptions  that  i  want  to \nmake  but just live with it and just try to enjoy it while those assumptions last right ok  \nso  now  let us assume that w star is the optimal solution for l w  what is l w  the train \nerror  not our regularized error  just the train error  \n \n\fand  so if w star is the optimal solution what can you take tell about the derivative with \nrespect  to  w  star  or  derivative  at  w  star  sorry   it  is  going  to  be  zero  from  basic  calculus \nright  so  which i say minimize x square  the minima is where derivative of x squared \nwith respect to x is equal to zero right  \nso  now consider one point which is ok  so  what i actually want to consider is that  let \nme just see how to see this  so  let us see my w star ok and i want to consider some point \nin the neighborhood of w star ok  that is what i want do  so  one way of saying it is that h \nis equal to w minus w star  is that fine ok  so  that is what i am going to use in the next \nfew steps  \n\n \nso  suppose i have such an h which is equal to w minus w star  that means  i can move \nfrom w star to some point in its neighborhood by using h  and what does taylor series \ntell  us   this  is  what  taylor  series  tells  us  right   that  the  value  of  the  function  at  this \nneighborhood  point  is  equal  to  this   all  of  you  know  taylor  series  well  now   it  is  that \nfine i do not need to really go over this right  \nthis  is  approximation up to  the second term second order derivative  now  what  was h \nactually w minus w star  so  i will just substitute that and this is what i get  is that fine   \nwhat is this quantity  one minus zero infinity minus infinity zero right we just did that ok  so  \nthat term will disappear  what am i left with  this quantity ok and i have forgotten what \nis next  \n \n\fnow again i am interested in the derivative of this ok  so  what will happen if i take the \nderivative what would i get  i am interested in computing grad l w  what will the r h s \nbe  how many of you fine with this  remember this is a quadratic form right  so  this is \nof  the  form  x  square  that  is   i  mean  that  is  roughly  how  i  remember  it  is  not  correct  \nbecause  of  the  form  x  square   so   when  you  take  the  derivative  one  of  the  x  is  will \ndisappear and this quantity will remain ok  so  everyone gets this ok  \nso  now what do i have is  i have the formula for the gradient with respect to l w and it \nis in terms of the gradient with respect to or rather the gradient at l w star  that is what i \nhave achieved  so   far   but  what  am  i  actually interested in  the  regularized loss   i am  \nwhat  i  am  still  dealing  with   is  the  non  regularized  loss   this  is  just  the  empirical \nestimate  of  the  training  error  that  is  not  what  i  am  interested  in   i  am  interested  in  the \nregularized loss  \nhow many of  you lost at this point   h is the second order derivative oh  so  these are \nbrackets just for clarity  but i see it is making it more unclear yeah  actually we should \nhave used u and then call it u transpose h u  so  it is the brackets here are not indicating \nfunction ok this is just h transpose h  now let us say it i realize how bad it is  so  last \nstep what are we taking gradients with respect to is w right  is it fine  \nso  we have a  so  i mean do not get too confused right  so  up till this point we have a \nformula for l w right  and i am just interested in the derivative of that ok  and all i have \nachieved by this is that  i have ok  in fact  i have one more step right  \n\f\n \nwhat is this quantity zero ok  so  we now know that the derivative of the loss function \nwith respect to w can be written as this quantity  is it ok  and i have just derived it step \nby step  there is nothing great about it  anyone is can  why i am doing this is not clear \nthat  will  become  clear  hopefully   but  what  i  am  doing  is  clear  right   is  that  fine  can  i \nmove ahead   \nnow what we are actually interested in is this quantity  because this is the true loss that \nwe are  going to  deal  with right  and we just saw in  the previous slide that this  quantity \nwhich is  on the  l  h s  is  equal  to  this thing on the  r h s  this  is  what  we saw on the \nprevious slide  can i just go back to the previous slide  because the derivative of this was \njust alpha w  now let us start with this  so  on the next slide  let me just see if there is \nanything else that i need to see here ok  \nso  far everyone is clear what i have derived so far why is not clear  but what is clear  \nwhat is being derived so far  so  i have said that the derivative of the loss function or the \nregular is loss function can be written as this quantity ok  is that fine  where w star is the \noptimal solution for with respect to the un regularized loss function ok  and now i have \nwhat i am interested in this solution with respect to the regularized loss function ok  \n \n\f\n \nnow   let  w  tilde  be  that  solution  for  the  regularized  loss  function   so   that  means   the \nderivative of the loss   the regularized loss function at  w tilde is  going  to  be  zero  nothing \ngreat about this  but i just told you on the previous slide that i can write this quantity as \nthis quantity that is what we derived on the previous slide ok  just take my word that is \nwhat we derived on the previous slide ok  let just  no confidence in me ok that is fine  \nnow can you  are you if i write it as this  just rearranging some terms oh sorry  \nso   i  am  just  grouping  all  the  w  tilde  some  terms  and  this  is   a  matrix  is  needed  here \nright  because i need to  i can only add two matrices  so  what i am just doing is  putting \nthe elements across the diagonal  everyone understands this  everyone gets this step  ok  \n \n\f\n \nso  now i have a formula for w tilde in terms of w star ok  i am going to go a bit further \nand be a bit bold  and compute the inverse also  so  now  i have a exact formula for w \ntilde in terms of w star  so  what is this  actually what is this relation that i am trying to \nestablish  suppose i know the solution with respect to the un regularized loss  and now i \nhave added regularization what happens to the new solution  \nso  i am telling you the new solution would be smaller weights and so on that is what ltwo \nregularization tells you  now you are just trying to make an interpretation for that  so  i \nhave given you a closed form solution that w tilde is actually equal to this quantity that \nyou see on the right hand side ok  why you are doing this is still not clear but right now i \njust  focus  on  the  what  part  of  it  this  is  just  some  mathematical  steps  that  i  am  doing  \nanyone who is not comfortable with this  \nnow notice what would happen if alpha tends to zero what would be w tilde be w star  what \ndo you mean by alpha equal to zero  no regularization right  so  that is just one corner case \nthat i want to do  but that is not what we care about anything what that is stupid to do all \nthis and tell you that if you do not use regularization you will get the same solution  but \nthat is not what i am going to tell you right  we are interested in the case when alpha is \nnot equal to zero ok  so  let us look at that case  \n \n\f\n \nnow  i am going to assume that h is a symmetric positive semi definite matrix squared \negg in a vacuum ok  so  if that is the case then i can write h as this  i have just done the \ndash of h  eigenvalue decomposition all right ok  and i  know that since it is a squared \nsymmetric matrix the eigenvalues are going to be eigenvalues are going to be orthogonal \nyes  eigenvalue vectors are going to be orthogonal and that is why i can write this that q \ntranspose is the inverse of q  \nnow  let us start with whatever we had on the previous slide and substitute what  what i \nam going to substitute  instead of h i am going to use q lambda q transpose ok  so  i \nam doing that  so  is that ok  i will just go over the steps and let me know at any point if \nyou  have  a  problem   what  i  have  done  is   i  have  replaced  this  i  by  this  and  its  valid  \nbecause q q transpose is just equal to i  i have just taken q and q transpose as common \nright  so  this is a c b plus some a z b  so  i have taken a and b out right  is that fine ok  \nnow  what is the next thing i am going to do  this is of the form a b c inverse  so  i am \ngoing to write it as  and the inverses are neat right  \n \n\f\n \nthis is fine  what will happen to this quantity i  what is this quantity q and this is what i \nam left with  but there is still something more i  can do  i  guess let us see ok  so  i can \nwrite this entire thing as a diagonal  matrix  how  many of  you see that it is  a diagonal \nmatrix   because  lambda  is  a  diagonal  matrix   i  of  course   is  a  diagonal  matrix   i  is \nmultiplied by a scalar which is also going to be a diagonal matrix and the whole thing is \nagain multiplied by some diagonal matrix ok  what is the inverse of a diagonal matrix  \nthe reciprocal of the diagonal elements  \nso  i  its fine  so  i have a very neat formula for what w tilde looks like in terms of w star \nok   again  why  am  i  doing  all  this  and  god  knows   but  and  here  d  is  equal  to  this \nquantity  \n \n\f\n  \nso  what  exactly  is  happening  here   in  terms  of  linear  algebra  or  in  terms  of  geometric \ninterpretations   so   let  me  just  see  if  i  have  to  do  something  first  ok   so   what  is \nhappening to w star is getting \nstudent  \n  \nrotated  remember what happens when a matrix where hits a vector  it gets rotated and \nscaled  also   and  then  what  is  this  diagonal  matrix  going  to  do   scale  it   element  wise \nscaling  actually   everyone  gets  this  operation  ok  and  then  i  am  again  rotating  it  by  q  \nagain the same stupid question if alpha is equal  to  zero what would happen  q transpose \nwould rotated by something and then q would rotate it back way  that means  you will \nend up getting the same solution ok  if alpha is equal to zero we understand  \nnow if alpha is not equal to zero  first let us see what does this matrix look like  so  what is \nthis matrix actually  it is a diagonal matrix  what are the diagonal elements  the  what is \nthe  first  element  in  the  diagonal   one  by   everyone  agrees  with  this   what  is  the  second \nelement ok fine  and what is the other matrix that i have  lambda  so  d is equal to the \nproduct of these two things right  so  what is d going to be  what is the first element of \nthis matrix is going to be  how many if you say lambda one by one lambda one plus alpha  this \nmuch is clear  everyone gets this  \n \n\fso  this is a diagonal matrix of the form a b c  let us consider a three by three matrix  now i am \ngoing  to  multiply  it  by  another  matrix  x  y  z  which  is  also  a  diagonal  matrix  right  \nbecause this is also it  so  this matrix i have already told you what it looks like  the other \nmatrix  is  also  a  diagonal  matrix   now  what  is  this  product  actually  a  x   b  y   c  z  and \neverything else has zero  now everyone gets it  now can you say what would this product \nlook like if you can actually make out  it would be a diagonal matrix and what would the \ndiagonal elements be   \n\n \nso   now what  is happening  so   first this  rotation is  happening that no one is  denying  \nafter rotating what is happening  this is a  this product is actually a vector  that is fine ok  \nwhat are we doing to every element of the vector  scaling it  scaling it by what quantity \nthese  quantities  that  every  element  is  getting  scaled  by  the  corresponding  entry  in  the \ndiagonal  in this diagonal right  \nso  the first entry is getting scaled by this  the second entry is getting scaled by this and \nso  on  ok   i  just  want  you  to  take  some  thirty  seconds  and  try  to  figure  out  where  i  am \nheaded from here  \n \n \n\f\n \nlet us see if i can  yeah maybe look at this sentence and see  first of all everyone agrees \nwith this sentence right  is there anyone who does not agree with the sentence  i am just \ntrying  you  to  figure  out  the  implication  of  the  sentence   you  get  it   some  people  are \nnodding their heads just in  because if you scale it right  then there is no guarantee that \nwhat  the  vector  has  changed  ok   what  happens  in  the  following  case   that  means   that \ndimension  will  be  left  as  it  is  ok   but  if  the  eigen   if  this  condition  holds  what  would \nhappen that dimension is almost getting multiplied by a zero right  \nso  see these two extremes  when the eigen value is very large you will end up staying \nwhere you were  so those dimensions will not be affected  if the eigen value is very small \nthen you are almost getting scaled down to zero  so  now  what will happen is actually  only \nthe  significant  directions  larger  eigen  values  will  be  retained   so   what  is  the  effective \nnumber of parameters in your model now  \nsee remember that this w vector is a vector of all the parameters  what am i telling you \nthat some of these are going to disappear  when which condition holds  the third can  the \nthird  bullet  holds   some  of  these  are  going  to  disappear   that  means   the  effective \nnumber of parameters  which remain in your model is going to be less right and you see \nthat it is going to be given by this quantity  right  \nso  that is sometimes known as the effective number of parameters in a neural network  \nif the effective number of parameters in your neural network is decreasing  that means  \n \n\fwhat  you  are  doing   making  the  model  less  complex  right   so   that  is  what  we  have \nachieved  you see that ok  \n\n \nnow  let me end with a pictorial interpretation of this  you see two figures here and there \nis only one figure  but you see two different things here  can you tell me what this is and \nwhat this is  that is the first question i want to ask you  the hint is that in this lecture we \ncare about  the other hint is what was w star  the solution for the  \nstudent  \n  \nunregulated loss which means which loss l theta  you need any more hints  sorry  this \nbox is the contours of l theta  this box contours of omega theta  so  this thing just ignore \nthis part of the figure for now ok  this i have marked as w star  w star was the solution \nwhen  i  only  had  the  un  regularized  loss  ok   there  is  the  solution  when  i  had  the  un \nregularized loss ok  \nso   remember  the  contour  maps  that  we  had  seen   so   this  is  the  minimum  of  that \nparticular  function   so   this  is  the  contour  map  for  l  theta   that  is  clear   now  what \nprobably is not clear is  why is this the contour map of omega theta  let me just go ahead \nactually  \n \n\f\n \nplease do not read this  this is the prestige ok  so  do not read that  so  this is the contour \nmap of omega theta right  because omega theta in the  what is the minima for the omega \ntheta   it  is  a  function  of  the  form  w  square   what  is  the  minima   zero  and  what  does  that \nfunction  look  like  and  what  is  this  point   zero  the  origin  right   so   that is  why  this  is  the \ncontour for omega theta ok  \nnow  what is happening  this was the solution when you had without regularization and \nnow  this  is  w  tilde  which  is  a  solution  with  regularization   so   can  you  make  some \ncommentary  on  this   with  respect  to  not  just  general  commentary   with  respect  to  the \nthings  that  we  saw  in  the  derivation   we  talked  about  rotation   scaling   dimension \nspecific scaling  so what is happening  this was my original solution vector  this was my \noriginal  solution  vector  when  i  did  not  have  the  regularization  term   now  what  has \nhappened   the  rotation  has  happened  and  we  saw  that  there  is  a  rotation  operation \nhappening  more importantly what has happened  scaling has happened  \nmore importantly what has happened dimension specific scaling is happening right  one \ndimension has not  this dimension has scaled down this dimension has not scaled down \nenough  that is exactly what we wanted right  we wanted the less important weights to \ngo  down  and  the  more  important  weights  to  stay  there   we  did  not  want  a  uniform \nscaling down  we wanted a dimension specific scaling down  \n \n\fso  the weight vector has been rotated yes  each dimension after rotation has been scaled  \nsome dimensions  have been scaled down more   the other dimensions have been scaled \ndown  less   how  many  of  you  can  make  this  interpretation  from  the  figure   now  that  i \nhave told you this interpretation  \n\n \nnow  still if you do not how mean if you can still have a doubt with this  you still have a \ndoubt what is doubt fine so  so  this was the original solution vector right  the map told \nus that what actually happens is when you add this omega theta the solution vector gets \nrotated ok  at the same time there is also some scaling down and that scaling down is for \ndimension  \nhow  many  dimensions  do  you  have  here   two  dimensions  right   so   this  is  one \ndimension   this  is  the  other  dimension   now  in  the  original  case  both  these  weights \nactually seemed almost equal right  i mean if you look at the w one coordinate and the w two \ncoordinate they were same  now after this regularization what has happened is  what are \nthe new coordinates for w one and w two  this is the coordinate for w one right  this is the value \nof w one and this is the value for w two  \nboth  of  them  are  admittedly  smaller  than  the  original  values  for  w  one  and  w  two  in  the \nabsence  of  regularization  or  both  of  them  equally  smaller   no  they  are  being  scaled \ndifferently  one rate has been scaled down more  the other weight has been scaled down \nlesser  right  and  that  is  exactly  what  the  math  was  telling  us  that  they  get  scaled  in \n \n\fproportion to those lambda one by lambda one plus alpha and that is exactly what we see in \nthe figure is that fine  \nhow many if you get this interpretation now is that ok  so  all of its elements are shrink \noh  you have a question  so  this final resultant right it is  so  what would have happened \nis that there would have been first rotation then scaling down and then again rotation  so  \nwhat  you  are  seeing  here  is  the  final  rotation  right   so   it  is  not   it  should  have  been \nshowed in three steps by just shown the final step  \nso   its  question  was  that  we  first  had  a  rotation   then  had  a  scaling  and  then  again  a \nrotation   but  i  even  as  explained  in  the  figure  i  spoke  only  about  one  rotation   so   i \nbasically  clubbed  both  the  rotations   and  so  what  you  see  finally  is  rotations  scaling \ndown and again rotation  \n\f"}
{"audio_filepath": "lec007_006.wav", "duration": 372.34, "text": "\ndataset augmentation \nso   how  with  that  heavy  math  i  will  just  interline  with  this   something  very  simple  \nwhich is  something known as dataset augmentation  \n\n \nso  what is dataset augmentation mean  so  you always given some training data  so  in \nthe case of mnist  you had this training data  where you are given these digits  images \nof digits and you wanted to train some classifier  so  in dataset augmentation  what we \ndo  is   so   now   we  have   what  is  happening  here  right   conceptual  is  that  there  some \nseeing   some  training  data  and  try  to  build  a  classifier  and  what  you  doing  actually  is \nminimizing the empirical train error   \nthat mean it will ensure that whatever  you have seen in training is  going to look   it is \ngoing to be perfectly classified  whatever we have seen in training that is going to look \nvery  good   it  is  going  to  be  the  training  error  on   that  is  the  error  of  those  training \nexamples  it is going to be very easy  \n\fnow  my question is this  if a training time you are seeing all this two s which are roughly \nvertically drawn right and a test time   you see at two  which is written like this  which is \nslightly  tilted  what  would  happen   it  will  not  be  able  to  do  a  good  job  on  that   that \nmeans   your  model  is  not  think  of  terms  that  you  have  used  in  this  lecture   not \ngeneralizing  \ncan  you think of  a simple  trick  based on  your domain knowledge of how people right \ndigits to kind of overcome overcome this problem you get the question right  i am telling \nyou that it is possible that someone writes to in a very tilted manner  can you prepare for \neventuality   eventuality  the  title  of  this  module  was  dataset  augmentation   so   what \nwould happen is  are given some training data  \nyou can always generate for training data from that  see here is another training instant \nthat i have created  i have just rotate it to two by some random angle  i took this image  i \njust rotate it and this is a simple operation that all pixels are moving by a certain angle  i \ncould have rotate it more  i could have shifted it vertically  that means  in all my image \nthe two was actually exactly at the centre  i just shifted at a bit vertically  \nso   i am  so  think that you are reading  one of those kyc forms or bank forms  most \npeople would write at the center of the block provided  but some people could write to \nthe extreme right or extreme left right  so  you are preparing for that  they saying that ok  \nall my data  the digits are well written at the centre  but let me just shift them bit  so  that \ni can also deal with people who write it at the corner  \nleft align or  right align  instead of center  align   i could  have  even shift  it horizontally  \nmost people would write at the center  but some people would write at the top or at the \nbottom  i could blur the image  but someone has taken a photo and send it to me and the \nphoto is not very clear or i  could just change  some pixels randomly right   i could  add \nnoise  all of this is dataset augmentation  with the hope that i am capturing with these \nvariations  i am capturing enough variations in the data  \nso   that  i  have a better  chance of doing something better on the test  data   is  that  fine  \nthis is all still training data  mind you  i am still going to compute the empherical train \nerror   it  is  just  that   now   i  have  blown  up  my  data   but  much  more  than  what  i  had \ninitially  do   you  all  see  by  doing  this   you  could  have  done  better  on  the  mnist \nassignment  you could have done better again  i am not asking you to do this  \n\fso  now i will do this then i will have supervised data  because i know that by this small \nvariations   the  label  is  not  going  to  change  and  what  am  i  using   there  i  am  using  my \ndomain  knowledge  right   i  cannot  do  this  always  right   i  hope  you  appreciated  that \nsuppose  that changes the domain a bit and i am given images of defects of motor parts \nright  where i have taken a image and there is a black spot somewhere  which indicates  \ndefect i cannot go about doing the same thing  there i cannot change some other pixels  it \nwill just means that the defects is at a different location right  but in many cases you can \ndo that  \nso   if  you  are  given  picture   because  of  dogs  and  cats   because  the  entire  world  case \nabout classifying cats and dogs then you could do some rotations  you could blur them a \nbit   you  could  occlude  certain  questions  of  the  picture  and  so  on  and  still  generate \ntraining data right  and what you are trying to do is  trying to take care of cases that you \nwould  end  up  dealing  at  test  \n  right   is  that  clear  ok   and  please  be \naware that we are exploiting  some domain knowledge here  \n\n \ntypically   more  data  is  better  learning   works  well  for  image  classification  in  object \nrecognition  these are the task  where this is already been tried out and they have shown \nto work very well in these tasks also shown to work well for speech  where the people \nhave some speech training data  they try to augment it for some task  it may not be very \neasy to generate such data right  \n \n\fso   you  could  think  of  various  nlp  applications   whereas  given  you  a  data  document \nright  because always do what joe does in that friends episode  do you remember what i \nam talking you  see what i am talking about  see you wants to write a recommendation \nletter for monika and chandler ok  and he has a letter written  any replaces  every word \nby  it s  best  synonyms  from  the  thesaurus  \n  right   that  says  a  way  of \ngenerating noisy data and in that case  it was actually noisy right  so  you could think of \ndoing here  but as happened in that case  it will not result in very good transformation  \nnext  for example  i remember something right  they are very warm hearted people got \ntranslated  as  they  have  some  warm  cardiograph  or  something  like  that  which  do  not \nmake  sense   so   it  is  not  very  easy   in  almost  all  application  should  do  it   but  in  some \napplications  typically in vision application  this is easy to do and you would gain a lot \nby doing this right  \n\f"}
{"audio_filepath": "lec007_007.wav", "duration": 81.022, "text": "\nparameter sharing and tying \nthe  next  thing that  i would  like  to  talk  about   and  this  quickly go  over  this  parameter \nsharing and tying  \n\n \n  \n\f \n \n  \nso   parameter  sharing  and  tying   i  will  just  quickly  go  on  this  because  for  the  sake  of \ncompleteness it is there in this lecture  but it should  it would really make sense when i \ndo convolutional neural networks  so  for the time being just take my word for it that in \nconvolutional  neural  networks   you  do  a  lot  of  parameter  sharing  where  as  the  other \nplace that you have seen parameter tying  so  that is again something that i am not going \nto talk about  \nso   this  is  typically  used  in  auto  encoders   where  the  encoder  and  decoder  weights  are \nshared  and  that  effectively  reduces  the  number  of  parameters  in  the  model  which \neffectively  reduces  the  complexity  on  the  model   if  the  complexity  of  the  model  goes \ndown   omega  theta  goes  down  because  that  is  what  which  wise  man  told  us  that  time  \nsteins lemma  \n\f"}
{"audio_filepath": "lec007_008.wav", "duration": 472.093, "text": "\nadding noise to the inputs \nwe go down the next module  which is adding noise to the inputs right  \n\n \nso   we  have  some  kind  of  a  noise  process  and  now  can  you  relate  that  how  that  was \nrelated to regularization  that was exactly the motivation in that case that we could have \nan  over  complete  auto  encoder  which  is  a  very  complex  model   because  it  has  a  large \nnumber of parameters  \nand  to  avoid  that  we  were  adding  this  noise  to  the  inputs  so  that  even  if  it  tries  to \nminimize  the  training  error   it  is  not  actually  minimizing  the  true  training  error  right  \nbecause you have fed some noise to it everyone gets this  right  ok  now  actually we \ncan show that for a simple input output neural network right  that means  you do not have \nany hidden layer you just have a set of inputs and you have the output layer  then adding \nnoise to the input or rather adding gaussian noise to the input  it is equivalent to weight \ndecay  \n\fso this can also be viewed  so  we will do this part right  so  we will just quickly do a \nsmall derivation  where we show that adding gaussian noise to the inputs is the same as \ndoing  a  ltwo  regularization   that  is  a  very  neat  idea   so   this  can  also  be  viewed  as  data \naugmentation  right   exactly  what  i  shown  on  the  previous  slide  you  added  two  you  just \ncorrupted some inputs of it that is the same as adding noise to the data  \nso   the  essentially  augmenting  the  data  right   you  have  some  training  data  and  just \naugmenting it  so  to get more training data is that fine  \n\n \nnow   about  this  smallest  derivation  this  is  again  just  a  set  of  steps   i  will  go  over  it \nreasonably  fast   i  will  give  you  the  set  up  and  then  it  is  quickly  work  through  the \nderivation right  \nso  what i was trying to say is that if you have a simple input output neural network  that \nmeans  you just have inputs and the output you do not have a hidden layer  right  then \nadding a gaussian noise to the input units where the noise comes from this distribution  \nit is a gaussian distribution zero mean  i want to show that doing this is effectiveness the \nsame as doing ltwo regularization  \nnow  again see this is the same thing squared eggs in vacuum because this is not the kind \nof networks that we deal with   but it is good to see what happens at least in these neat \nconditions  because we will never have a simple input output network  at least not in this \n \n\fcourse  we will have a deep neural network always  so  but at least see what happens in \nthe simple case right  so  what we are doing is from the x i\u2019s we are creating a noisy x i \nby just adding some epsilon noise to that  and what is our model going to be  it is just \nan aggregation of all the inputs ok  so  this is what our original model would have been \nwithout the noise fine  \ni would have just aggregated all the inputs  i am assuming there is no non linearity at the \noutput and i am just taking y i is equal to summation of all my inputs everyone fine with \nthis side  or this is too simple for you guys to understand because we have been doing a \nlot  of  deep  neural  networks   so   suddenly  one layer  network  i  do  not  know  what  it  is \neveryone gets it  right  \nand instead of  y hat  now  i have  y tilde because instead of x i  i have x i tilde ok  but \nwhat is x i tilde x i plus epsilon i right  so  i can write it as this just fine  so  actually y \ntilde is nothing but y hat plus some quantity  ok  what are we interested in  always this \nquantity the expected mean square error ok  i mean expected squared error and why not \ny hat  \nso  we have added noise to the input  so now  y tilde are the outputs that we are going to \ntilde  so  let us see what that quantity is  and again just going to be some simple stuff  \nso  i replaced y tilde by this that we just derived on the right hand side  on the left hand \nside ok  so  i am going to take these two terms together  so  i can write it as this plus this \nthe whole square fine  and i am  going to keep this  as it is  what is this quantity  the \noriginal squared error expected squared error right  when i was not adding noise to the \ninputs  ok   and  you  see  how  we  got  these  two  quantities   this  is  just  a  plus  b  the  whole \nsquare is equal to whatever it is equal to right  now let us look at the last term this is a \nsquare of a sum right  \nso   what  kind  of  terms  would  you  have  inside   you  will  have  some  terms  which  are \nepsilon i squares and you would have some terms which were epsilon i epsilon j right ok  \nso   we  will  have  some  expectations  which  are  going  to  be  something  into  epsilon  i \nsquare  and some expectations which are going to be epsilon i  epsilon j  everyone gets \nthis  some terms there  now which of these terms would disappear   \nstudent  \n   \n\fthese terms right  why  because the noises are independent ok  i am not if i have drawn \na noise for one instance  it does not have any influence on the noise that i am going to \nadd to the next instance  if i have taken one x i  corrupted it with some noise there is no \nbearing on the noise that i am going to use for the next epsilon i right  all these features \nare the noise added to the features are independent  \n\n \nso  now  from  these  terms  only  the  square  terms  are  going  to  remain   is  that  fine   and \nsimilarly this quantity  what can you say about this  we just did something similar  why \ni am a saying that this is going to zero  again i can show that this is the covariance between \nthis random variable and this random variable ok  and now are these two random variables \ndependent  what is epsilon i  the noise that i am adding to the input  does it have any \neffect on  y hat no  right  because  y hat does not depend on the noise  what is  y  true \noutput does it have anything to do with the noise  no right  \nso  that is why these two random variables are independent  so  i can again write there the \nexpectation of their product as a product of expectations  and then the expectation of this \nis  going  to  be  zero   because  epsilon  i  was  drawn  from  a  zero  mean  distribution  is  that  fine \neveryone gets that the same trickery that we did earlier  so  this is the quantity that we \nare left with  you see how i got from here to here  this is an expectation of a sum  which \nis equal to a sum of expectations  w i has nothing to do with it is not a random variable  \n \n\fso  it is just the expectation of sigma i square which is nothing but the variance right  so  \ni get this what does this look like  i already told you the answer before starting right  this \nlooks like l two regularization this is the true error  i mean this is the empirical estimate \nfrom  the training error  and this  is  the weight  decay term  everyone  get  this  how  you \nsee that this is an equivalent thing  so  at least in this neat set up you get the intuition \nthat adding noise to the inputs is a same as adding a ltwo regularization term  \n\f"}
{"audio_filepath": "lec007_009.wav", "duration": 280.574, "text": "\nadding noise to the outputs \nso now going on to the next module which is adding noise to the outputs  \n\n \nso   here  when  you  are  given  some  training  data   this  is  the  label  vector  that  you  have \nbeen given  right  where one of these elements  is  one  so  these are like zero to  nine eight  where \nwhich digit it is and in this case it happens to be digit two  so  that element is one right that is \nthe true training data given to you  \n\f\n \nso   what  you  could  do  is  actually  and  actually  what  you  try  to  do  is   minimize  this \nquantity p i log q i  where what is p i  p i is the vector which was given and  what is q i  \nthe  predicted  probabilities  ok   so  now   when  you  try  to  add  noise  to  the  output   what \nyou actually do is  you see that i do not trust the true labels  they may be noisy   \nwhatever data you have given to me that is one way of looking at it  that i do not trust \nit  i will just say that it is noisy  the other way of looking at it is that in some way i am \nensuring that  i do not try to over fit to this label  right  because now my true whatever i \nam trying to optimize  let me just go to that and let us see  so  instead what we will do is \nwe will use soft targets  \n \n\f\n \nso  this is what i mean by soft target  assume that there was some epsilon noise in your \nlabels  so  instead of treating this as one and all zeros  treat the true label as one minus epsilon \nand divide that among the remaining nine entities right that probability mass divided among \nthe remaining nine entities  \nso now when you are trying to minimize this  what is p i  this soft distribution right and \nq i is the predicted distribution  so  you see why this acts as a regularization  why does it \nact as a regularization  what is the aim of regularization  do not over fit on the training \ndata  right   to  over  fit  on  the  training  data   what  should  it  have  done   it  should  have \ntreated  only  the  correct  label   now  if  i  am  giving  it  this  information  then  i  am  not \nallowing it to over fit on the training data right  \nbecause  now  with  this  distribution   this  quantity  will  not  get  minimized   when  q  i  is \nequal to the one hour distribution where all the masses on two  do you get that  so  in some \nsense we are making sure that  now if it tries to over fit on the training data  it will not \nget the minimized error right  so  you have this corrupted the outputs of it everyone gets \nthis  is ok  the trainer no that is the whole point  \nstudent  \n  \nno  \n \n\fso  that is thing right  so  some of these are heuristics based  so now  we have started \nwith  this  whole  derivation   where  we  try  to  show  the  relations  between  trainer  error \ntested  or  not   but  things  that  we  have  seen  some  of  these  things  right   even  whatever \nunfortunately  i tried to prove on the previous slide the weight decay thing  even that is \nonly for these neat networks where you do not have any hidden layer and so on right  \nso  most of these are just heuristics  you are just saying that the principle is that  you will \nnot allow the true training error as computed from the training data to go to zero  if you do \nthat you know that you are going to over fit  so  try whatever you can to avoid that ok  \nthat is the idea  do you agree that doing this is going in that direction   \nstudent  \n  \ntraining data  the hope is that if you do not do that then it will not under fit on the test it \nright  \nthere is no i mean i have you are you looking for a proof  where i say that doing this we \nwill  ensure  that  a  training  error  does  not  go  to  zero   but  the  test  error  comes  close  to  the \ntraining error  there is no such proof right  just a heuristic  it is  going by the principle \nthat if i do not allow the training error to go to zero  then hopefully i will over fit  i will not \nover it as much as i would have otherwise right   \nso  that you can think of it as this way right  so  this is the curve that you are seeing it  \nthis was a training curve  this was your test curve  you are preventing from entering this \nregion where the error is zero  that means  you will end up somewhere here right  and you \nknow that that is a more preferred point as compared to this  that is the intuition that you \nare going right  is that  \n\f"}
{"audio_filepath": "lec007_010.wav", "duration": 707.497, "text": "\nearly stopping \ni will do  will do early stopping where again we will get into some of these eigenvector \nanalysis  so  let us see that   \n\n \n\n \n\fso   the  idea  been  early  stopping  is  actually  very  simple  in  principle  what  needs  to  be \ndone  so we know that  this that this trend exists between the training error and the tester \nright  so in practice  what you will do is you will continue to optimize the training error  \nthe empirical training error which is the sum of the errors on the m training points   \nyou  will  also  continuously  keep  track  of  the  validation  error   that  means   the  same \nquantity you will compute over the n validation or test points  everyone get this  you can \ndo  this  and  you  are  actually  doing  this  in  your  back  propagation  assignment   keeping \ntrack of the training error as well as the validation error and you keep plotting them  ok  i \nwill keep running for various epochs and keep something known as a patients parameter \np   \nso  if you are at the twentyth epoch and if your patients parameter p is equal to phi  and just \ndo a check whether  in the phi last phi epochs has my validation error ever gone down or \nit has been staying the same or has it been increasing ok  now i will give you a condition \nthat it was either staying the same or it was actually increasing is this good or bad   \nwhat does it tell you while your training error was of course  decreasing may the more \nyou train your training error will keep going down  so  what does this tell it is just over \nfitting  you  are  fitting  the  training  error  you  are  just  making  it  zero  or  as  close  to  zero  as \npossible   but  that  is  not  helping  your  validation  error   so   the  validation  error  is  either \nworst case increasing or remaining the same right  \n \n\fso  this is a very commonly used trick which is known as early stopping you keep this \npassions patients parameter  and you make sure that if you have cross this patients right \nand the patients here is that i was waiting for the validation error to go down  but it is not \ngoing down for some p epochs  so  no point in continuing training anymore i will just \nstop it does not make sense  \n\n \nso  and this can also be used in conjunction with other regularizers  right  so  in the quiz \nalso  we  had  this  question  sorry  for  bringing  up  the  quiz   but  we  also  at  this  question \nwhere  you  have  the  sparsity  regularization  and  i  was  asking  whether  i  can  add  the  ltwo \nregularization along with it  so  these regulations can be added or used in conjunction it \nis not that you can only use one of them   \nso  early stopping is a way of regularizing  but you could also use it in conjunction with \nltwo regularization or any other regularization technique that you do not want right so  but \nhow does this act as a regularizer from the picture  it is probably clear and is the same as \nthe  explanation   i  was  trying  to  give  to  his  question  right   that  you  are  preventing \nyourself  from  entering  in  these  regions  and  trying  to  enter  into  more  favorable  stop  at  \nmore favorable regions  \nbut  can  you  think  of  slightly  more  in  terms  of   what  happens  in  gradient   and  what \nwould happen if you stopped it early and so on  can you try it to connect it to the update \nrule of gradient descent  what happens as  you keep doing it for more and more epoch  \n \n\fno  gradient  descent  has  nothing  to  do  with  validation  error  or  backtracking  error \ngradient descent only works on the training data let us think in those terms  \ngradient  star  diminishing  to  zero   so   what  happens  how  does  gradient  descent  progress  \nwhere  do  you  start   i  started  a  random  point  at  every  epoch  which  is  a  collection  of \niterations right  or you go or many training points  what happens to this  i start moving  i \nkeep moving now  if i fix the number of epochs or do not allow it to change any more \nafter  a  number  of  epochs   what  am  i  doing   i  am  restricting  the  boundary  around  the \nweight right  i am not allowing it to grow beyond a certain boundary do you get that  \n\n  \nlet  us  see  that   so   we  will  first  see  an  intuitive  explanation  and  then   go  to  a  more \nmathematical  analysis  are  update   so   the  update  rule  for  gradient  descent  is   i  always \nmake this mistake  this has to be minus oh the t h have disappeared ok is there  so  sorry \nother to have disappear   \nso now  what would actually happen at the t h step is we have w naught three plus or minus \ndoes not  matter   it  just tells  you that  how much it is  going to  change  this  is  what  is \nhappening  actually  at  the  t  h  step  right   you  have  just  subtracted  all  the  previous \nderivatives that you had so far right  from where you started off now  you are looking at \nt  steps   so   at  every  point  you  are  computing  a  certain  gradient   but  had  a  certain \nmagnitude  \n \n\fnow  let me say that  across  all these steps  the  maximum  gradient that  you had   i will \njust call it by tau right  so  that means  in this summation there are t terms  i am saying \nthe maximum of those was tau that was the maximum rate gradient that i got at any one \npoint  \nnow   what  i  am  going  to  do  after  this   i  am  going  to  replace  this  by  something   this \nsummation is always going to be less than or equal to this right  because  i am assuming \nthat each of my steps is less than tau  there are t such steps  so  i could have at matched \nmoved  t  into  tau  right   but  i  would  have  moved  less  than  that   because  tau  was  the \nmaximum gradient that i had  \nso  this is going to be less than equal to is that do you get the change from the equality to \nless than equal  to  ok  so now  what  am  i restricting actually in  early stopping  what  is \nbeing restricted  there are only so many symbols there i just speak one t tau is of course  \nnot in your hands w naught is not in your hands w  so  t is the one right  so  i am only \nallowing  that  many  updates  so   that  means   from  w  naught  you  can  only  moves  that \nmuch this looks  you see that analogy that  this is something similar to you not allowing \nthe weights to really grow a lot  \n\n \nso now  but will not end here you will  of course  do some more stuff on this right ok  \nso   we  now  see  a  mathematical  analysis  of  this   so   recall  that  a  taylor  series \napproximation for l w is the following  the same thing which i wrote a few slides back \n \n\for many slides back everyone remembers this right  and now again i am going to do the \nsame thing that  if i know the optimal w star then the gradient at that point is going to be \nzero   \nso  this term disappears  and now if i take the derivative  this is what will remain  this is \nexactly what we did earlier also right  so  we will have derivative of this and derivative \nof  this   so   the  derivative  of  this  quantity  is  just  this  and  the  derivative  of  this  is  zero  \nbecause  that is exactly what we started off with right that w star is the optimal solution   \nnow  sgd update rule is the following ok  which i can write as this  i just replaced this \nby this ok  i am just rearranging some terms is that ok  how many if you are fine with \nthis  how many feels to tired to even care about this   \n\n \nso  this is what w t would be this is again some simple steps leading to some conclusion  \nthe conclusion is  what matters the steps are very easy you can go back and look at them \nright  so again   i will use the evd the same trick that i did earlier and it will give me \nthis instead of h ok  again i will just do some rearrangements and actually  i can show \nthat if i start with w naught equal to zero  then w two is actually given by this quantity ok  and \nthere is a proof of this in the appendix you can go and look at it   \nnow   what  does  this  look  similar  to  rotation  diagonal  rotation  exactly  similar  to  the \nanalysis  that  we  did  for  ltwo  regularization  right   and  in  fact   if  you  can  you  can  show \n \n\fthat  if we compare this expression with the while we had for ltwo regularization  and this \nis the expression that we had for ltwo regularization right  rotation some scaling and then \nagain rotation right  then we can show that  early stopping is actually equivalent to ltwo \nregularization  if the following condition is satisfied   \nthis  does  not  mean  much  because   god  knows  how  you  will  satisfy  this  condition  \nright  but all it is saying is that there is some equivalence  at under certain conditions \nand  that  is  what  is  the  intuition  was  also  telling  us  that   it  is  somehow  preventing  the \nweights  from  going  large  and  it  is  doing  this   in  this  very  convoluted  way  where  this \ncondition holds for it to be equivalent to ltwo regularization   \nas i said for you and me is going to be very hard to create this condition right  how do i \nmake sure that  something like this is true right  but that does not matter what matters is \nthat  there is some equivalence between them  \n\n \nso  when you are doing early stopping  it is not just a heuristic or a blind thing that you \nare doing  you know that  it is somehow related to ltwo regularization  hence  that you are \ndoing it and hence it also works in practice is it fine  we will that work for all of you ok \nright   so   the  things  to  remember  is  that  early  stopping  only  allows  t  updates  to  the \nparameters  this is the important thing rights  so  now  if a parameter w corresponds to \na dimension which is important for the loss  then what would this quantity be the partial \n \n\fderivative  of  the  loss  with  respect  to  that  parameter  it  is  going  to  be   if  there  is  a \nparameter  \nfor  example   let  us  take  the  amir  khan  an  example  right   that  whatever  weight  you \ngives to whether  the actor was  amir khan or not   if that is very important because  if \nthat feature is on you are lost completely changes and so on right  if you do not learn the \nweight correctly that feature is very sensitive  \nso  for important features the loss would be very sensitive to the changes  in the weights \nof these features  is that intuition correct right  that means  this gradient would be large \nok and if a parameter corresponds to a feature which is not important  what would this \nderivative be  small now  what  is  the net  effect  of this  you have some parameter which \nare important  so  the derivatives are large some parameters which are not important  \nso  the derivatives are going to be small and you are going to only allow t updates  so  \nwhat  is  going  to  happen   the  parameters  which  are  important   we  will  end  up  getting \neffectively  more  updates  right   because   each  of  these  magnitudes  was  higher  and  you \ndid t of those  the parameters which are not important we will end up getting effectively \nlesser movement  \nbecause  each of these gradients were small and you did only t of those right  so  you \nagain see this that  it is a weird way of ensuring that your important parameters get more \nupdates than  your non important parameters right  so  it is very important to see these \nconnections between these different regularization methods  all of you are fine with this \nfine   \n\f"}
{"audio_filepath": "lec007_011.wav", "duration": 463.223, "text": "\nensemble methods \nso  next  we  look  at  on  ensemble  methods   and  this  is  just  to  build  the  intuitions  for \nsomething  known  as  dropout  which  is  very popular  technique  in  deep  neural  networks \nand convolution neural networks and even recurrent neural networks  \n\n \n\f  \n \nso  how many  you have seen ensembles before  seen it in machine learning ensemble \nwas not done in machinery done with ok  ravi did it  so  as a combine so the ensemble is \nessentially just the combining the output of different models  to reduce the generalization \nerror right  why does that make sense  have these different models  all of these would \nhave different biases and variances right  \nso now you are combining them  so  i will end up with a better thing on the test error \nright  so  that is the idea behind ensemble  now the models could correspond to different \nclassifiers  right  for  example   here  i  have  a  logistic  regression  and  svm  and  a  naive \nbayes  i have trained them independently using the same data or different subsets of the \ndata   and  a  test  time  i  am  taking  a  prediction  from  all  of  them  and  then   taking  an \nensemble of those predictions that is the basic idea  \nnow  it  could  be  different  instances  of  the  same  classifier   trained  with  different  hyper \nparameters  i could have the same neural network a three layer neural network  but trained \nwith different hyper parameters  so  the hyper parameters could be learning rate  it could \nbe  batch  size   it  could  be  the  number  of  neurons  in  each  layer  and  so  on  right   so   it \ncould  be  same  classifier   but  different  hyper  parameters   different  features  right   so  \ninstead of looking at all the one hundred features that i have given  i could train these classifiers \nwith different subsets of the features ok or different samples of the training data  \n \n\f\n \nso  bagging is one such ensemble method where you have different instances of the same \nclassifier  which are trained on different samples of the training data ok  so  i have one \nclassifiers  trained  on  a  subset  t  one  of  the  training  data  another  classifier  trained   on  a \nsubset t two of the training data and so on right  and so  each of these model is trained with \na different sample of the data  \n\n \nnow  when would bagging actually work  what would you want these classifiers to be  \nso  each classifier is going to make certain errors  \n \n \n\fwhat do you want these errors across classifiers to be dependent  independent   \nstudent  independent  \nindependent  right   so   if  one  classifier  makes  the  errors  on  certain  test  instances   other \nclassifier makes errors on a different set of test instances and the third classifier makes \nerrors on a very different set of instances  that is the condition that you are looking for \nright  there is errors if all of them make error on the same instance then all of them are \ncollectively going to make an error on the final prediction also right  \nbecause  it  is  like  i  asked  three  guys  all  of  them  gave  me  the  wrong  answer   so   my  final \nanswer is going to be wrong  but at least two of these three guys gave me the correct answer \nthen my  final  answer is  going to  be  correct  right  so   that means  the errors that these \nmodels make  i want these errors to be independent if i treat error as a random variable i \nwant these errors to be independent  \nso   so  consider  a  set  of  k  such  logistic  regression  models   suppose  that  each  model \nmakes an error epsilon i on the test example  now let epsilon i be drawn from a zero mean \nmultivariate  normal  distribution   so   the  variance  is  equal  to  v   and  how  many  such \nepsilons do i have  how many such distributions i am considering  \nstudent  k  \nk  right  because  for  each  classifier  there  is  a  distribution   so   then  i  can  compute  the \ncovariance between these random variables ok  i will add that let that covariance be c  is \nthat fine ok now the true the error made by the average prediction of all the models is \ngoing to be given by this model one made an error of epsilon one  model two made an error of \nepsilon two  \nso   the  average  error  is  going  to  be  given  by  this   now  what  is  this  expected  squared \nerror   this  is  the  error   this  is  the  expectation  this  is  the  square   that  is  the  expected \nsquared error is that fine  again this is a square of a sum  so  it will lead to a lot of terms \nof the form epsilon i squares and what will happen now which terms will go to zero  \n\f\n \nthe terms having epsilon i epsilon j again the same thing they are independent  so  i can \nwrite the expectation of a product as the product of expectations and those expectations \nare zero  so  this is what it is going to look like  what is this  oh sorry actually we had not \nassumed that the covalence  \nwhat is this right  and what is this  covariance i am sorry i have not we had assumed \nthat there is some covariance said wed not assume they are independent right  we would \nwant it to be independent  but in the general case we will assume some covariance and \nthen i will show you the special case where they are independent  \nso  then how many vs do i have here  k right  and how many cs do i have here  \n \n\f\n \nthis summation is k into k minus one right or i equal to one to k and j equal to i plus one to k \nfine   and  so   this  is  what  it  looks  like  now  can  you  make  some  inferences  from  this \nequation  this is what the expected mean square error is going to be  now think in terms \nof  variance   covariance  and  tell  me  when  would  this  be  beneficial   i  have  already  told \nyou the answer  if the errors are independent what would covariance be  zero right  \nso  then what is the mean square error one by k one by k into v right so  that means  bagging \nwould work when your classifiers the k classifiers that you are combining  \n\n \n \n \n\fif  the  errors  are  independent   then  the  mean  square  error  should  actually  have  been  v \nright  for a single classifier it was v right because mean square error is nothing but the \nexpectation of the error expectation of epsilon i square which is nothing but v  \nbut if you are if you are combining k classifiers and if these classifiers are independent \nin terms of their errors  then your mean square error is going to be one by k into v  because \nthis term is going to disappear ok  now if your classifiers are perfectly correlated  then \nwhat would happen  and basically c is equal to v right  is that fine  so now what would \nhappen  what is the net result if i substitute this as v  going to be v right  \nso  if  you are all  your classifiers are perfectly correlated  that is  the other case we had \ntried taken  and all of them are making errors on the same test instances and the same \nerrors  right  then you will not get any benefit of doing bagging  but if you look at the \nother  extreme   where  all  your  errors  are  independent  or  all  your  classifiers  are  making \nindependent errors  then you will get a benefit your expected mean square error would go \ndown from v to one by k into v  everyone gets that  \n\n \nso  this was just to develop an intuition that taking an ensemble helps right  and using \nthis  intuition  now  we  are  going  to  see  at  how  to  do  this  ensemble  in  the  case  of  deep \nneural networks  \n \n\f"}
{"audio_filepath": "lec007_012.wav", "duration": 986.136, "text": "\n dropout \n\n \nso  in this module we will look at dropout now  \n\n \n\fso   the  intuition  that  we  have  developed  in  the  previous  module  which  was  about \nensemble  methods   is  what  that  is  that  ensemble  makes  sense  in  most  cases   because  \nyou  do  not  expect  the  errors  of  these  k  models  that   you  are  using  to  be  perfectly \ncorrelated  and we saw that  whenever they are not perfectly correlated you are going to \nget some advantage  \nnow   how  do  you  do  this  in  the  context  of  neural  networks   so  remember   what  was \nbagging multiple instances of the same network trained on different subsets of the data  \nwhat  is  the  problem  with  this  in  the  context  of  neural  networks   each  of  these  neural \nnetworks is very complex training  each of these is going to take time and i going to train \nk of them is that  fine right  \nso you decide ok  sorry  so  one option that you have is you train several different neural \nnetworks having different architectures right  but this is going to be expensive because  \nyou  have  to  train  k  of  them   the  other  option  that  you  have  is   you  train  the  same \nnetwork  but on different subsets of the data this is also going to be expensive  \nso   whatever  ensambling  sampling  techniques  you  can  think   if  in  the  think  of  in  the \ncontext  of  neural  networks  which  are  essentially  these  two  techniques  different \narchitectures and take an ensemble or train the same architecture on different subsets of \nthe data both of them are going to be expensive right  \nso now how do you go about it  and it is not just training time  expensive it even if we \nmanage to train it at test time again  when you are given a test instance you have to pass \nit  through  all  of  these  complex  neural  networks   each  of  which  is  going  to  take  some \ncomputation  and then take the ensemble of the outputs right  so  even at test time it is \nexpensive it is not just that  that training time it is expense  \n\f\n \nso now  dropout is  a technique which addresses both  these issues  which issues  train \ntime  computation  as  well  as  test  time  computation   so   it  effectively  allows  training \nseveral neural network architectures without any significant computational overhead  so  \nwe will see how that works and it just not training time as i said it also allows us to do \nthis quickly at test time  \n\n \nso again let us see  so again here ok  i will get to it when i know  so  drop out actually \nrefers to dropping out units from the neural network  \n \n \n\fso  this  is  my original neural  network and  i  am  just talking  about  one neural  network  \nforget about ensembles just one neural network is what  i have  now what dropout says \nthis   you  dropout  some  units  from  this  neural  network   that  means   dropout  some \nneurons and when i dropout some neurons  i am also going to drop out the incoming and \nthe outgoing edges  otherwise  where are they headed right  so i am just dropping out  \nso basically  what is effectively happening here  i am getting a new network architecture \nright  at least that is clear that is what dropout effectively does  but i have already made \na case that i do not want so many architectures  that because  it is a headache to train all \nof them and again a test time i have to pass it through all of them right  \nso  i need to still fill that gap  but drop out says that  drop some units and you will get a \nnew architecture  but how does that simplify life  we will see that  and now each node is \nactually  retained  with  a  fixed  probability  for  the  hidden  nodes  and  even  further  input \nnodes  \nso  then we were not wrong in actually dropping out the visible node  because  you can \ndo dropout at the visible nodes also ok  anyways yeah so for  the hidden units you would \ndrop them with  a probability  fifty percent  and the input units  you  will drop them with  a \nprobability of twenty percent typically it again is some hyper parameter that you will have to \ntune  but typically this is what you will do and i hope you see that dropping nodes from \nthe hidden unit  from the input unit is same as corrupting the input data right it is same \nas adding noise to the input data is that fine   \n\f\n \nso this is the idea  now let us see how to actually implement this idea  okso  suppose a \nneural network has n nodes using the dropout idea each node can be retained or dropped  \nan example in the above case i have dropped some five nodes to get a thinned network  \nso  if there are n nodes what are the total number of thin networks that i can get from it  \nand so  that means   i can get two raise to n different neural networks  am i happy about \nthis  or sad about this  sad  there is just too many neural networks  how can i train them \nactually right  \nso how do i do this  i am just creating a lot of suspense without giving you the answer \nok  so  first trick is  share the weights  across all these networks ok  we will see what  \nthat means  and the second trick is sample a different network for each training instance \nok  none of which is clear at this point  i can see i can read your faces i am good at it ok  \nso  let us see how to do that  \n \n\f\n \nso   we  initialize  all  the  parameters  of  the  network  randomly  or  whatever  may  be  used \nand start  training   when  i start training   i will pick up the first  training instance or the \nmini batch or whatever i am doing we apply dropout resulting in this network  \nwhat  will  i  do  and  they  forward  prop   forward  propagation  right  ok   now   ok  we \ncompute the loss and back propagate how  some weights are missing right how do i do \nback propagation now   i  have deliberately dropped up some of these connections   they \ndid  not  participate  in  the  forward  propagation   this  back  propagate   which  are  the \nparameters which will update now  only the ones which actually participated right  \nso  i will just do back propagation  just look at the red arrows  i will just do it over the \npaths which are actually present in my network fair enough right  that is what you meant \nby normally ok  that is normal ok  so  i will just do it over the weights  which actually \nparticipated that is fair enough that is the only thing you could  obviously do  \n \n\f\n \nnow   i  take  the  second  instance   again  i  apply  dropout  and  quite  naturally  i  will  get  a \ndifferent thinned network as you see the figure three in this slide ok  what would i will do \nnow  \nstudent  forward propagation  \nforward propagation  then compute the loss back propagate to compute the loss ok  and \nthen  \nstudent  back propagate  \nback propagate again back propagate only to the  \nstudent  active nodes  \nactive nodes  so these other nodes which will get activated  so  what is happening here  \nis now trying to relate it to what we were doing in bagging right  where we are trying to \ntrain these different networks on different subsets of the training data right  do you see \nsomething similar happening here  there are many such thin networks  each time i am \nsampling a different network and updating it right  \nso  it is equivalent to training these large number of networks on different subsets of the \ndata  right   but  then  the  problem  is  that  some  of  these  networks  may  never  even  get \n \n\fsampled  there are two raised to n of those  my amount of data is definitely to be less than \ntwo raised to n  \nso  some of these networks might just not even get sampled  then  what is happening  \nor they would get sampled very rarely right  for example  what is the probability that \nagain  i will end up with the same network we are computing it  good  it is very less ok  \ni am fine with that at seven hundred and thirty right  \nso  it is  a  very less right  so  it is  quite likely that  this network will never be sampled \nagain   that  means   for  that  network  the  parameters  are  getting  updated  very  few  times  \nam  i  fine  with  it   yes   i  am   why   because  the  same  weights  will  get  updated  for  a \ndifferent network  i am just using the same weight matrix throughout remember that  my \nw matrix or w one  w two is the same throughout  \nit  is  just  that  at  different  depth  subsets  different  instances   i  am  just  touching  some \nportions  of  this  w  one  and  i  am  not  touching  the  other  portions  of  w  one   so  now   what \nwould happen  so i have shown you two training instances right  what would happen to the \nweights  which were active for the first training instance as well as  the second training \ninstance  it will get updated twice and which are active only once  \nstudent  \n   \nonly once right   so  over a period of time  many of these weights  are shared across all \nthese  networks  that   i  am  sampling  right   so   even  though  a  particular  network  is \nsampled  only  a  few  times   its  weights  will  get  updated  many  times  via  these  other \nnetworks which are similar to it  do you get that  how many of you get this  ok  good  \nso what is happening  i will just repeat that i have just one weight matrix  i am sampling \na thinned out network which only uses some of these weights  \nso for that training instance  i will update those weights  now  i know that the likelihood \nof  the  same  network  getting  sampled  again  is  very  less   but  i  do  not  care  about  it \nbecause   i  could  sample  a  different  network   but  i  am  sure  that  some  of  these  weights \nwill again repeat in that right  and in that  i told they will get updated  so  even though \neach  of  these  networks  is  seemingly  getting  very  few  updates   overall   all  the  weights \nshared by these networks are getting updated as much as they should be is that fine  \n\f everyone gets this idea  ok  fine and while i am also taking care that  similar things like \nearly  stopping  or  weight  regularization  ltwo  regularization  where   i  am  not  allowing  a \nsingle weight to continuously grow or something  otherwise because  these weights will \nbe off for some networks  is that fine  you see the connection between early stopping  l \ntwo regularization and this is that ok  \n\n \nand so  each thinned network gets trained rarely or sometimes even never  but i am not \nworried about it  because  it is weights will get updated through some of these other thin \nnetworks  \n \n\f\n \n this is all finite training time  at training time what is happening is this is one of these \nblue  guys  introduce  on  with  the  probability  p   that  means   the  weights  going  out  of  it  \nwho are available with a probability p right and other times they were not available  \nnow  what do i do  it test time  i cannot let me finish this ok  i cannot take an ensemble \nof d ok  the answer would have been that  at test time instantiate all these two raised to n \nnetworks pass the training passed the test example  through all of them and then take an \nensemble right  but of course  that is probablitivly expensive  so  what  will i do at test \ntime  what is the simple trick that i will do  so  he says that just use this network  \nand just use the final net matrix  that you had no  but then you have guessing out of the two \nraised  in  the  sample   some  small  number  of  those  and  do  it   actually   dropout  uses \nsomething very simple than this  what it says is  that each of my nodes was present only \np fraction of the times in the training data ok  that means  one way of looking at it is that  \nso   imagine  that  you  could  think  of  this  as  the  analogy  is  that  all  these  nodes  are \nparticipating in a discussion right where they trying to see how to do this job properly  \nbut with probability p they all sleep off  right   \nso  at the end of the meeting  you will trust each of them only with probability p  so  that \nis the simple trick with dropout uses  it says that just scale their weights by p  because  \nthat is how much i trust this node  it only participated in p faction of the decisions  so  \nthat is the confidence that i have in it  \n \n\fso  if it is saying that with wone weight  do this i will only do it with p into w one weight \ndoes  that  make  sense  ok   and  there  is  again  a  squared  egg  with  vacuum  kind  of \nexplanation for this ok  which was there in the quiz  last year which is very convoluted  \nit does  not  really  give  you the true picture because   you  can derive some math and so  \nthat this is mathematically proper  but that again works in very specific conditions  but at \nleast if you get the intuition that is fine that what we are saying is that  these nodes will \nleave an active a few number of times  so  i will only trust them that much and i will just \nscale their weights by that factor  \nso  at test time  i will just pass my test instance through one network  which is the full \nnetwork with  the weights scaled  according to  the rule  which   i  just said  that is exactly \nwhat dropout does  \n\n \nso   what  dropout  actually  does  is   we  will  apply  some  kind  of  masking  noise  to  the \nhidden units  right  since the same as seeing that you are computing the hidden unit  but \nthen you are masking it off ok  \nso  what is the effect of this  i will give  you the answer and  i like  i like you to think \nabout  it  the answer is  that  it prevents the neurons  from  becoming lazy   what  do lazy \npeople do  they depend on others yeah actually yeah  they depend on others now  so  let \nme answer that give the answer for this and then tell me whether that is still contradict \nok  \n \n\fso  let us see right consider this layer of neurons  all of these are collectively responsible \nfor  what  happens  to  this  guy  right   now  you  see   what  i  mean  by  neurons  becoming \nlazy  i could just see ok  i will not give my input these other neurons will take care of it  \nthey will adjust their weights  \nso   that  they  eventually   it  will  fire  or  not  fire  or  whatever  right  you  see  that  could \nhappen  but now these neurons cannot rely on their neighbors  because they do not know \nwhen their neighbors are going to ditch them right  they will suddenly drop off ok  and \nnow i was waiting for my neighbor to actually do something and he is not going to do it  \nso  i have to be alert always do you get the analogy  \nso   these  guys  are  collectively  responsible  for  something  and  they  know  that  some \npeople  in  the  collection  are  going  to  betray  them   so   each  of  them  has  to  be  more \ncareful   so   the  more  technical  term  for  this  is  that  does  not  allow  the  neurons  to  co \nadapted  \nso  it does not allow them to get into this mutual agreement that you take care of certain \nthings   i  will  take  care  of  certain  things  and  together  we  will  do  the  job  right   you  do \nquestion one  i will do question two  i am ok  it does not allow them to do this  \nso  let us just concretize that intuition a bit for  so  essentially a hidden unit cannot rely \ntoo  much on other units as they may  get  dropped out  at  any time  each  hidden neuron \nhas to learn to be more robust right  it has to do the job as if it is the only guy responsible \nfor the job ok and let us consider one of these neurons h i  \n\f\n \nand let us see that  a h i learns to detect faces  sorry  it learns to detect a nose  so  i am \ntrying to do face detection whether  an image is about a face or not and h i is the feature \nwhich fires   if there is  a face somewhere   if there is a nose somewhere in  the image is \nthat fine  \nnow   if  all  these  guys  start  acting  lazily  ok   this  guy  is  going  to  detect  a  nose   that \nmeans  definitely face will be there  so  i do not need to do anything right  what would \nhappen  now   suddenly  this  guy  is  going  to  go  away  dropped  out   so  then   these  other \nguys need to do one of two things  either add redundancy  that means  one of them should \nalso  take  responsibility  for  detecting  a  nose  or  do  it  in  a  different  way   take \nresponsibility for detecting the lips or the eyes or some other part do you get that  right \nbecause  you know that i cannot co adopted with my other neurons  i cannot say that ok  \nin these front facing faces you just detect the nose and will be done and we will all keep \nquiet right  \ni  do  not  know  whether   you  will  do  your  job  properly   so   i  will  have  to  add  more \nredundancy  you detect a nose  i will also detect a nose or you detect a nose and  i will \ndetect  something  else  which  helps  detecting  the  feature  right   so   that  is  why  these \nnetworks become more and more robust as you add this dropouts  \n \n\f\n \nso  that is all that i had to say i still do not know whether i have answered your question \nor not  all of them try to detect nose  see as long as that helps reducing the final loss it is \nfine  it is just the case that you would have some training images  where the nose is not \nvisible maybe that person is drinking something right  \nso  for at least for those training instances someone else has to take care that you detect \nfrom the other images right  otherwise  a loss would not be zero for that training instance  \nso  as long as you have some training instances see  if all your training instances can be \ndetected just by detecting the nose  then  there is nothing wrong in all of them trying to \ndetect the nose  so  if the training it is like that it will happen  but the hope is the training \ndata is not like that right  is that fine  so we will end here  \n \n\f"}
{"audio_filepath": "lec008_001.wav", "duration": 370.505, "text": "\ngreedy layerwise pre training  better activation functions  better weight \ninitialization methods  batch normalization \nwelcome  to  lecture  nine  of  csseven thousand and fifteen   today  we  will  talk  about  greedy  layer  wise  pre  \ntraining   better  activation  functions   better  weight  initialization  methods   and  batch \nnormalization   so   today\u2019s  lecture  is  more  like  tips  and  tricks  to  make  deep  learning \nwork  \nso  when you are actually experimenting with deep learning in practice  what are some \nof the things that you need to take keep in mind  and it is also my way of connecting the \nhistory that we saw to where we are today right  so  there were certain things which we \nsaw in the history  and now i  will try to bring those back and connect to where we are \nheaded  from  here  right   where  we  have  reached  today  and  where  we  are  headed  from \nhere  \nso  that is with that in module one  i will do a very quick recap of training neural networks \nand not take more than five minutes and i need it for a specific purpose  \n\n \n\fso   we  already  saw  how  to  train  such  a  very  shallow  neural  network   what  was  the \nlearning  algorithm  gradient  descent  and  this  was  the  update  rule  right   in  particular   i \nwanted you to notice that the gradient actually depends on the input  \nso  when you compute the gradient formula you have this multiplication by x  so  it is \nproportional to the input and this is one fact that we will use it  at least a couple of cases \nin the lecture today  so  this was a very shallow single neuron network  what if we have \na wider network  still which algorithm   \nstudent  gradient descent \n  \ngradient descent ok and we just have these three different formulae  and for each of these \nformulae note that the gradient or rather this gradient depends on the input that you are \nfeeding in ok  i did not keep this in mind  \n\n \nand what if you have a deeper network  so  we saw a very shallow network  we saw a \nwide network and i am showing you a deep network what will you do  again gradient \ndescent  \nbut you will apply the chain rule for computing the gradients  and again here in general \nyou will notice that for any of these weights wone  wtwo  wthree the gradient formula will have \nthis h i minus one  what is h i minus one   \n \n\fstudent  \n   \ninput  from  the  previous  layer  right   and  hzero  is  the  actual  input   so   the  gradient  at  any \nlayer is actually proportional to the input from the previous layer and this could either be \nthe input from the hidden layer or the actual input  \n\n \nand finally  we saw this thin  so  we saw a wide network  we saw a thin network  now \nwe will see a wide network and a deep network right  sorry we saw earlier we saw a wide \nnetwork and a deep network  now we see a wide and deep network  and here again you \nhave compute the gradient by applying this chain rule across multiple paths and that is \nwhat  we  use  and  we  call  it  back  propagation   and  remember  again  they  are  the  same \nthing  holds  that  the  gradients  at  some  point  are  proportional  to  the  input  at  that  layer \neveryone remembers that ok  \n \n\f\n \nso  this is important  so  what we have is things to remember from  what we have seen \nso far is that  so  training neural networks is basically a game of gradients right  so  you \ncompute the gradients and everything depends on those how will you update the weights \nand everything from there on is about the gradients  \nand  these  gradients  actually  tell  you  the  responsibility  of  the  parameters  towards  the \nloss   and  you  appropriately  update  them   and  we  saw  a  variant  way  different  sorry \nvarious variants of how to use the gradient  so  we saw the gradient descent  we saw nag \nmomentum and all  \nbut  in  all  of  these  the  underlying  core  thing  was  to  compute  the  gradient  and  then  do \nsome  manipulations  based  on  that   and  the  other  key  thing  is  that   the  gradient  at  a \nparticular layer depends on the input to that layer ok   \n \n\f\n \nso now let us go back and just retrospect a better and see what is it that we have learned \nso far  so  so far what i have taught you gradient descent oh sorry  backpropagation  is \nsomething which was proposed way back in one thousand  nine hundred and eighty six right  \nso in fact  it was existing before that  but it was popularized by this work of rumelhart \nand  others  in  one thousand  nine hundred and eighty six  right   so   but  then  in  the  one thousand  nine hundred and ninetys  or  early  two thousand   if  back  propagation \nalready  existed and we  could  train  deep neural  networks  then  why did  not  we here so \nmuch about deep learning at that time  of course  you guys were busy with school and \nall at that time  but why did the others or older people like me not hear about it  \nstudent  computational power  \ncomputational power is that the only thing  \nstudent  \n  \ncomputation and memory is are the only thing  \nstudent  convergence \nwho said  convergence  ok good  so  actually what  happened right  in  the late eightys and \nearly  ninetys  and  even  early  two thousand   when  you  used  back  propagation  to  train  really  deep \nnetworks  it was not very successful  and what do i mean by not successful  actually  \nwhat are the two things that could happen someone gave the answer already  \n \n\fstudent  \n  \nit does not converge right  that means  you do not reach the optimum solution right  in \nfact  till two thousand and six it was very hard to train very deep networks  \nand  typically  even  a  after  a  large  number  of  epochs  these  networks  did  not  converge  \nthat means  they were still at a very high loss and although in principle everything is fine  \nyou have a deep neural network  you have an algorithm that can train it  but you are still \nnot being able to train it properly and you are not being able to make any practical use of \nthat  \nso  that was the story till two thousand and six  so  today is about what happened in two thousand and six  what it led to \nin the next few years and then where we are currently  right  so  that is the journey that \nwe  need  to  make  ok   and  that  is  why   we  started  off  with  this  quick  recap  of  back \npropagation because that is what i want to tell you that why did it not work earlier and \nwhere are we today  \n\f"}
{"audio_filepath": "lec008_002.wav", "duration": 1486.607, "text": "\nunsupervised pre training \nso  with that we go on to the next module  in which we will talk about unsupervised pre \ntraining  \n\n \nso  this work which i am going to talk about they trying to understand what has changed \nsince the late ninetys or the early two thousand  how did deep learning become so popular despite \nthis problem with training them right this problem was there  \nso   what  happened  to  them  solve  it  right   and  this  field  actually  got  revived  by  this \nseminal work by hinton and others in two thousand and six  \n\f\n \nso  let us see what that idea was  so  this is the idea of unsupervised pre training  in the \noriginal paper they introduce idea in the context of something known as r b m\u2019s  which \nwe  will  do  in  the  last  thirty three  percent  of  the  course   but  we  could  do  the  same  with  auto \nencoders  which we have already done  so  in  this lecture  i  am  going to  talk  about  this \nidea in the context of auto encoders  \n\n \nso consider the deep neural network shown in this figure  so  the a module name and the \nidea  was  unsupervised  pre  training   so   that  itself  is  a  giveaway  of  what  is  going  to \n \n \n\fhappen   ok   so   suppose  this  is  the  deep  neural  network  that  i  have  designed  for  a \nparticular classification task  so  what it is doing is this taking an input which is the red \ncolored neurons that you see at the input it has four hidden layers  that means  it is four layer \ndeep  and then you have the output layer  which tells you whether positive or negative \nright  that is the network that i have and i know that this is hard to train such a network \nthe loss will not converge and i will not get anything meaningful  \nso  what these guys suggested is that forget about the supervised criteria that you have  \nthat means  you are trying to minimize a classification loss just forget about that just take \nthe first two layers of this network ok which is x and h one right  so   you take the original \ninput x  you feed it to some transformations and you get the hidden representation h one \nand now try to reconstruct x from h one  what is this  \nstudent  auto encoder  \nauto encoder ok  what is the objective of the auto encoder  \nstudent  \n  \nit is exactly this  for each of the m training examples look at each of the dimensions of \nyour input and minimize the square difference between the actual input and the predicted \ninput right  is that fine  that is what an auto encoder does  so  this is what they suggested \nok  so  right now i am not telling you why this makes sense and all that that is what we \nwill do later  right now i am just telling you the trick then we will analyze by that trick \nworks  and why is this objective unsupervised  \nstudent  \n  \nbecause we are not using any labels we just giving an input and we just reconstructing \nthe input we only have x\u2019s we do not have y\u2019s of course  eventually we will use the y  but \nat this stage when i am calling it unsupervised pre training i am not using the y  \n\f\n \nnow  at the end of this  what would happen  yeah what would h one learn  \nstudent  \n  \nit  will  learn  an  abstract  representation  of  x   was  that  our  original  task   what  were  we \ninterested in  \nstudent  \n  \nin the classification task  but we are doing something very different  why we will see ok  \nnow guess what would the next step be  does this make sense  \n \n\f\n \nnow  at the end of the first unsupervised pre training   i have ensured that h one which is \nthis  layer  has  learned  some  abstract  representation  of  the  input  right   and  that  i  know \nfrom the auto encoder i mean the auto encoder which we have learned earlier right that at \nlearns an abstract representation of the input  \nnow   i  have  this  so   that  means   given  an  input   i  know  how  to  compute  an  extract \nrepresentation and i am also sure that it captures the important characteristics of the data  \ni will just repeat this process  i know that  i have four layers in my original network  so  i \nwill now take h one try to compute h two and then reconstruct h one from it  so  the in effect \nwhat am i doing in plain english  learning and even more  \nstudent  \n  \nabstract  representation  of  the  input   h  one  was  already  one  abstract  representation  now \nfrom  this  i  am  learning  an  even  more  abstract  representation  and  does  the  objective \nfunction  makes  sense  right   all  i  have  done  is  replaced  x  by  h  one  right   in  both  these \nplaces the rest of it is the same for all the training examples for all the dimensions  and \nthroughout i am assuming that we are n layers  i mean sorry  n neurons and every layer \nincluding the input layer  \nnow  what would the next step be  fix the weights in h one layer fix the weights in it is two \nlayer and now try to reconstruct h two from this h two right  and in this way we will continue \n \n\fand learn all the hidden representations  does that look ok right  so  at least this much \nwe believe it because we know that auto encoder works and you are just using an auto \nencoder  and  we  are  using  it  incrementally   from  every  abstract  representation  learn  an \neven more abstract representation  \nnow  at the end of this what will i do  what was my original task  \nstudent  classification  \nclassification  so  what will i do  \nstudent  \n  \nwhat is a network that i have  when i finish this unsupervised pre training  \nstudent  \n  \nno  tell me of the diagrams that you see on the slide  how much of the network would i \nhave right  everything except the green output layer right because the last step would be \ntake h four or sorry  take h three and reconstruct h three from it and in the process learn h four right is \nthat clear  \nso  i would have learnt till that point  and now what i am going to do is something very \nsimple  \n\n \n \n\fi  will  after  this  layerwise  pre  training  is  done   i  will  add  my  output  layer  now  all  the \nweights in my network for every layer have been initialized  \nand  they  have  been  initialized  in  a  way  that   that  layer  learns  a  good  abstract \nrepresentation  of  the  input  right   that  is  the  thing  that  we  have  achieved  at  the  end  of \nunsupervised  pre  training  that  every  layer  has  learned  and  more  and  more  abstract \nrepresentation of the input right  now i will keep all these weights initialized to whatever \ni learned in the pre training setup does that make sense  \nso  that means  instead of taking this big network with the output layer and initializing \nthe  way  it  is  randomly   i  am  just  going  to  use  whatever  weights  i  learned  using  the \nunsupervised pre training ok  so  can you tell me what has happened in terms of the error \nsurface  and  so  on  or  my  movement  in  the  w  b  plane  or  in  this  case   this  very  high \ndimensional w plane  \ni have reached some configuration for the w\u2019s  where i know that each of these layers is \na  good  meaningful  representation  of  my  original  input  right   is  that  a  fair  statement  in \nenglish   how many of  you agree with  this ah  anyone has  any  questions  at  this  point  \none layer weights that is what you do in answer because if you train all the four then you \nare again entering the same problem which you had earlier right  \nyou cannot back you cannot back propagate through all the four layers because now it is a \ndeep network and we know that does not work  so  at every layer you fix whatever you \nhave  learned  so  far   and  at  a  time  you  are  training  only  one  layer   so   that  is  one \ninteresting way of looking at it right you know that the deep neural network with four layers \nwas not trainable  \nso now we have reduced it to one layer at a time  i knew that one layer at a time works \nright is that fine  now i will add the output layer and what will i do train the weights of \nthe \nstudent  output layer \n  \n i  will  not  just  do  that  i  will  fine  tune  the  entire  network   that  means   i  will  train  the \nweights of the output layer  and i will also fine tune the entire network  but now i am \n\fcontradicting  myself  i  just  gave  an  answer  to  him   that  again  i  am  doing  this  deep \ntraining and i know that deep training does not work  \nbut this actually works  do you get the difference right  one is that when i start from  i \ntake this big network  i start from random weight initialization and try to train it that is \nthe story from one thousand  nine hundred and eighty six to two thousand and six that in most cases these networks did not converge  \nso  now  in  two thousand and six   we  came  up  or  someone  came  up  with  this  idea  of  unsupervised  pre \ntraining where you train the layer network one layer at a time  you do up till the last layer \nnow  you  add  the  output  layer  and  then  fine  tune  the  entire  network   that  means   back \npropagate over the entire network is a set up clear to everyone how many you understand \nthe setup  \nnow  again when i am doing the last step  which is known as fine tuning i have to back \npropagate over the entire network because i am saying i will adjust all the weights  but \nsuddenly this works  as compared to starting from scratch  you see the problem and you \nsee  why  this  is  important  then  because  this  has  now  given  you  a  way  of  training  deep \nneural network i still not told you  why it works  \nwe will delve into it  but not really give any concrete answers because concrete answers \ndo not exist  but we will at least try to get some intuitions behind why it works  so  you \nget the setup now that this is what was happening till one thousand  nine hundred and eighty six to two thousand and six  and now with this \nidea suddenly deep neural networks were being able to train well  \nso  in effect what we have done is  we have initialized the weights of the network using \nthe unsupervised objective right  so now  initial starting with random weights  we have \nsome  weights  which  cater  to  the  unsupervised  objective  that  we  had  and  the \nunsupervised objective was us layer wise reconstruction  so  that is what has happened in \nplain english is that fine everyone gets that  \n\f\n \nnow  the question is  why does this work better  and i give you two options and i want to \nthink about both these options ok  is it because of better optimization or is it because of \nbetter generalization no that is not an option  but i of course  we will relate it to that  but \ngiven  these  two  i  want  you  to  think  whether  there  is  any  difference  between  these  two \nstatements  or  not   that  is  the  first  thing  i  want  you  to  see   how  many  if  you  get  the \ndifference between these two statements  not many why is it so  what is optimization deal \nwith dash data or dash data  \nstudent  \n  \nthe answer you can give dash right  dash one data or dash two data what is optimization deal \nwith  \nstudent  training data  \ntraining data optimization remains on training data what does generalization depend on  \nstudent  it as zero  \nit as zero so  you get the difference between these two questions fine  so  let us try to answer \nthis again here right this is two thousand and six to two thousand and nine period that i am going to talk about  there are \nsome answers and just bear with me i will give you those answers some of them will not \nlook very convincing  but what happened after that or as a result of these investigations  \n \n\fthat  is  more  important  right  whether  these  answers  make  sense  or  not   they  will  make \nsense to an extent i am not saying that we will just be bluffing  \nbut it will not be very convincing because there is no theory behind it right  so  what is \nconvincing if i give  you a proof that this less this is equal to that right then if we give \nyou a proof and everything  you do not have any other questions  that is not what  i am \ngoing  to  give  you   i  am  going  to  give  you  some  intuitions   because  that  is  all  these \nexisting works from two thousand and six to two thousand and nine  had and then i will make a commentary on that which \nwill lead us to some other things  so  just bear with me for a few minutes right  \nstudent  \n  \nthat is the optimization problem if that was the case the  i will just come to that that is \nwhat i want to talk about ok  so  it is  so  these are the two questions that we are dealing \nwith right  and the answer is depends  so  we will see what it is  \n\n \nso  let us first examine the case when it is because of better optimization  \n \n\f\n \nso  let us first understand what is the meaning of this question  when i ask is it because \nof better optimization then the question that i am asking you is that  the first set up where \ni was trying to train everything from scratch  compared to the second set up where i had \nthis unsupervised pre training  is it that the optimization problem becomes easier in the \nsecond set up  now if the optimization problem becomes easier what do i actually mean \nby that that i was able to drive the dash to dash  \nstudent  loss to zero  \nloss to zero right  so  is it that this is the optimization problem that we were interested in  \nso  is it the case that in the absence of unsupervised pre training  we are not able to drive \nthe loss to zero for the training data and hence poor optimization right that if  you do not do \nthis unsupervised pre training even for the training data we cannot drive at loss to zero  that \nmeans  our optimization problem itself is not working properly right i mean the problem \nis fine  \nbut the solution is not good you get that  do you understand what is the subtle meaning \nof this  how many if you get this  so  let us see this in more detail right  \n \n\f\n \nso  the error surface of the supervised objective of a deep neural network is highly non \nconvex  it  looks  something  like  this  or  even  nastier  than  this   and  in  particular  it  has \nmany hills and plateaus and valleys we saw this even in the toy examples that  we were \ndealing with right  and given the large capacity of deep neural networks it is still easy to \nland  in  one  of  these  zero  error  regions   on  what  basis  am  i  making  the  statement  which \ntheorem  \nstudent  \n  \nuniversal approximation theorem that is what the universal approximation theorem told \nus  in fact  there is a study the paper which has been cited  it showed that if the last year \nhas a very large capacity then you can drive the loss to zero even without pre training  do \nyou get the meaning of this  what does is mean  so  i have the input i have a series of \nhidden layers what do i mean by the last layer has a lot of capacity  what do i mean by \nthat  it has a lot of dash  \nstudent  parameters  \nparameters  now  how  do  i  create  these  parameters  i  will  just  grow  the  size  of  the  last \nhidden layer right  and using that then i will predict this one y  \nso  so that is how i could increase so that is exactly what they did they took a very deep \nneural network and made sure that the last layer was given a very high capacity  and then \n \n\fthey showered that even if you do not do an unsupervised pre training  you can still drive \nthe training loss to zero  right  \nso  this was hinting that maybe this is not an optimization problem this is something it is \nstill  not  very  conclusion   but  we  will  just  go  with  these  studies  we  will  just  all  i  am \nsaying is that do not shoot the messenger this is what the study says i am just relaying it \nback to you right  and they will have questions on these which will try to address  but if \nthe capacity of the network is small then the unsupervised pre training helps  \nso  if you do not have these large capacity networks  but you have very deep networks  \nin  that  case  unsupervised  pre  training  helps  and  this  is  all  empirical  observation  right \nthere  is  no  proof  which  says  that  given  a  capacity  k  with  so  much  error  bound  i  can \nguarantee that the loss would be epsilon within the zero loss and so on  it nothing like that \nthat is what it should have been ideally the case in which case life is much easier for me  \nbut that is not the case this is just an empirical study as are most of the studies done in \nthe period of two thousand and six to two thousand and nine  \n\n \nso   that  tells  us  something  about  what  optimization  means  and  whether  this  was  an \noptimization problem or not  \n \n\f\n \nso  let us look at the other question is it because of better regularization  so  what does \nregularization do or you gave the exact answer it constrains the weights to lie between lie \nin some regions  so  it does not allow the weights a lot of freedom right  and so   you \nknow  what  l  one  regulation  does  it  constrains  the  weights  to  this  box  and  l  two \nregularization constrains us to this circle why no why this i know this  but why  \nstudent  \n  \nin why the circle i am pretty sure most of you do not know what you are saying  but you \nare saying the right answers  but anyways i will test this in the quiz  so  i have given you \nanother quiz question on camera  so  yeah so a prevents a loss from taking large values  \n \n\f\n \nso   indeed  pre  training  also  constrains  to  the  way  to  lie  in  certain  regions  of  the \nparameter space  why am i making this statement  what is the meaning of the statement  \nso  i told you that what regularization does and from there i am making this jump and \nsaying that even with pre training the same thing happens that your weights are actually \nconstrained to certain regions of the parameter space  why am i making this statement   \nand what are these regions that the weight is constrained to  think l theta think omega \ntheta  any regulation is of the form l theta plus omega theta  \nlet us see so it constrains the way to lie in regions where the  characteristic of the data \nare  captured  well   that  is  what  unsupervised  pre  training  does  it  is  trying  to  train  the \nnetwork  in  a  way  that  each  layer  actually  captures  the  important  characteristics  of  the \ndata  and this is based on our understanding and belief in auto encoders  so  you could \nactually think of this that the unsupervised objective that you had for all these layers that \nwas actually omega theta  you are first trying to optimize omega theta  \nso  in a normal regulation problem you put l theta and omega theta together and then you \ntry to balance them  but here you have done it slightly differently you first gave it omega \ntheta  which is the lost of reconstruction and you asked it to minimize this loss across for \nevery layer  \nstudent  \n  \n \n\fno is this fine tuning  so now  what  that means  is that see remember that this is a very \nhigh  dimensional  region   where  you  initialize  makes  a  lot  of  difference   so   with  this \nunsupervised pre training you are at least ending up in reason  so  you could think of it \nas  a  constraint  that  ok   move  wherever  you  want  to   but  start  from  here  which \nautomatically  means  that  i  have  i  mean  i  have  how  to  it  is  some  other  regions  in  that \nparameter space you get that  \nstudent  \n  \nas you typically that would be one thing  and it would also mean that you are starting \nfrom  there  so  with  this early stopping and other criteria  you will not  be able to  grow \nmuch  out  from  here  right   so   just  if  that  makes  sense  geometrically  from  here   you \nwould not be able to move all the way there you get that everyone gets this question and \nthe answer  \nso  you see what the answer per is object was and you also see the difference between a \nnormal  regularization  and  this  regularization   in  the  normal  case  you  had  l  theta  plus \nomega theta put together and then you are trying to minimize the sum of these two  it was a \njoint optimization here you have first done omega theta ensured that the weights that you \nlearn  minimize  this  objective   and  now  you  add  in  the  supervise  objective  which  is  l \ntheta  right  \nso now this makes sure that your network cannot be too greedy with respect to l theta \nbecause it has been  constrained   that has to  first  honor the omega theta because that is \nwhere  you started and now from there on it has  to decide how to do  l theta  does that \nmake sense  you see how this is acting as a regularizer is that ok  and that links back to \nyour weight initialization thing right fine  \n\f\n \nso  some other experiments have also shown that pre training is more robust to random \ninitializations  \nnow  what do i mean that mean by that  so  in these two graphs that you see here  so  this \non the x axis you have the number of layers that you add to  your deep neural network  \nand  on  the  y  axis  you  have  the  error  that  your  network  gives   when  you  try  different \ninitializations right  so  this box actually tells you the variance in the error  \nso  that means  i tried training a network with four layers and i tried different initializations \nand the error varied in this range ok  is that good or bad  what would we want typically \nsomething which looks like the plot below right  where all these variances are little  that \nmeans  even once  you do unsupervised pre training  right  it is more robust to random \ninitializations random initializations of what  \nstudent  \n  \nthe  original  random  initializations  from  which  point  you  started  the  unsupervised  pre \ntraining  ok   because  once  you  have  done  the  unsupervised  pre  training  that  is  your \ninitialization  everyone gets that  \n \n\f\n \nso   these  are  some  let  us  see  ok   so   these  are  some  empirical  studies  and  let  me  just \nmake a comment on these  \nso  what happened from two thousand and six to two thousand and nine is people showed that see this is possible you can \nactually train a deep neural network  using some of these tricks  we do not have a very \nclear  answer  for  why  this  works  and  you  could  argue  different  way   so   this  is \noptimization this is regularization and so on  but i do not have any theory supporting it \nthere  is  no  proof  for  why  unsupervised  pre  training  works  all  of  these  are  empirical \nobservations  \nbut what it at least established was that it is possible to do this  so now  if it is possible \nto  do  this  let  me  see  if  there  are  better  ways  of  doing  this   do  we  actually  need  to  do \nunsupervised pre training  oh i think it is better regularization then why not i try better \nregularization  techniques  and  see  whether  that  helps   so   that  led  to  the  evolution  of \nwhich thing that you have already seen yeah which regularization technique that you saw \nin the last class  \nstudent  drop out  \ndrop  out  right   so   drop  out  was  something  specific  to  neural  networks  which  was \nintroduced in the context of neural networks  so  this is because people started believing \nit is possible  so  let us try even better ways of doing that  so  that is how dropout came \n \n\fout  right   then  people  said  maybe  optimization  is  the  problem  maybe  these  earlier \nalgorithms  which up till that point was which algorithm  \nstudent  \n  \ngradient  \n  maybe  that  was  not  good   so   let  us  try  to  decide  and \ndesign  better  optimization  methods  and  that  led  to  the  evolution  of  adam   adagard  \nrmsprop  so  on  right  so  although  these  studies  were  not   so   theoretical  in  what  they \nwere trying to  prove   they created this hope which then led to  a lot of prolific work in \nthat field right  so  at least you get the context now right the some of these might look \noh this is one data set people did experiments on m l s  but i could have taken a different \ndata set and showed that these results do not hold and so on  you could always ask those \nquestions  \nbut  at  least  what  happened  is  people  started  believing  these  and  people  started \nquestioning that ok  unsupervised pre training is one thing what else can i do  and now \nwhat has eventually happened is today no one uses unsupervised pre training right that \nmethod which led to the revival of this field  and you would have hoped that that would \nactually survive for many years that is out  \nnow   hardly  anyone  uses  unsupervised  pre  training  it  is  only  used  in  the  context  of \ntransfer  learning   so   what  i  mean  by  that  is  that  if  you  have  a  model  trained  for  one \nclassification say classification of images on one data set right  \nnow  you have a very small amount of data in some other domain  so  instead of training \na  network  from  scratch  for  this  domain  you  will  just  initialize  it  with  the  weights  for \nwhatever  you  have  trained  on  data  set  one   so   that  is  more  of  transfer  learning  rather \nthan  unsupervised  pre  training   so   that  is  still  very  prevalent   but  this  reliance  on \nunsupervised training to make sure that the network actually trains that is largely phased \nout  \n\f\n \nbecause  what  has  happened  since  two thousand and six  and  two thousand and nine   is  that  we  have  better  optimization \nalgorithms which are  rmsprop  ada  grad  adam  even  so  on right  many various and \neven now that research area is active as i was saying just in december there was a paper \nwhich pointed out some flaws in adam and how to improve it and so on  we are better \nregularization methods the most prominent among those being  \nstudent  dropout  \ndropout  so  these two are things which you have already seen today we are going to talk \nabout  better  activation  functions  this  is  again  something  which  evolved  that  maybe \nsigmoid tanh are not good  so  maybe something else is needed and then better weight \ninitialization  strategies   so   then  people  took  this  inference  oh  one  way  of  looking  at \nunsupervised pre training is  that  it actually initializes the weights  in  a better way  from \nwhere on it becomes easier for me to reach convergence  \nso   why  do  not  i  come  up  with  better  weight  initialization  methods  itself   instead  of \nrelying on this indirect way of initializing the weights  so  you get this  so  get the whole \npicture  now what we have been doing in the past few lectures and how it connects to the \nhistory and these studies which were done from the period two thousand to two thousand and nine  how many if \nyou get the whole picture ok  so  that is where we are now  so  today we are going to \ntalk about better activation functions and better weight initialization methods  \n \n\f"}
{"audio_filepath": "lec008_003.wav", "duration": 1678.207, "text": "\nbetter activation functions \nlet us start with better activation functions  \n\n \n\n \n  \n\fso   before  i  get  into  activation  functions  right   let  me  first  tell  you  why  i  care  about \nactivation functions  why do i actually want to come up with better activation functions \nso   will  start  with  the  following  question   what  makes  deep  neural  networks  powerful \namong other things  what is this one thing which makes it powerful  so  let me give you \nthis intuition  \n\n \nthis  i have a deep network ok  and do not worry it is a thin network  but  i could have \nhad  a  wide  network  also   but  just  for  illustration  i  have  taking  a  deep  network  thin \nnetwork  \nnow imagine that  each of these neurons that you have  if i replace the sigmoid in each \nlayer  by  a  simple  linear  transformation  a  by  the  way  this  is  technically  incorrect   so  \norange is always input  so  this should not be a sigmoid there right  either add one more \nlayer there or let us change the figure  \nso  suppose i replace all these sigmoids by linear transformations  what would y be can \nyou write  y as a function of x  what would it be  give me the function  will we just be \nthis right  so  first we will do w one of x which is this right  then will take w two of that then \nw three of that and w four of that  \nso  i could actually have written this just as y equal to w x where w is equal to w four w three \nw two w one  so  there is no depth here  there is actually only one weight which i could have \n \n\flearned you get that right  if you just have all linear transformations  then essentially you \ndo  not  have  so  many  weights   you  just  have  one  weight  throughout  you  get  that  make \nsense  \n\n \nso  what you are learning eventually  we will just y as a linear function of x and initially \nat some point we started off with such linear functions right  w transpose x in the case of \nperceptron and mp neurons  \nso   what  does  that  lead  to  what  kind  of  decision  boundaries  does  that  lead  to  linear \ndecision  boundary   so   if  you  do  not  have  these  nonlinearities  we  cannot  have  these \narbitrary decision boundaries will only be left with linear decision boundaries  \n \n\f\n \nin particular  will not be able to solve this problem that we had right we were given some \nred  and  blue  points  and  there  was  no  way  to  draw  a  line  such  that  the  red  points  are \nseparated  from  the  blue  points   what  we  needed  is  some  kind  of  circles  or  ellipses  to \nseparate  the  red  points  from  the  blue  points  that  cannot  be  done  with  linear  decision \nboundaries   that  can  happen  only   if  you  use  a  deep  neural  network  with  non linear \ndecision boundaries and we actually have a proof for that  what that proof the universal \napproximation theorem actually towards  right  \n\n \n \n \n\fso  that is why nonlinearities or the activation functions clear a very important role in the \nsuccess  of deep neural  networks  right   hence  you want  to  examine them  very closely \nand  see   what  are  the  newer  kinds  of  nonlinearities  that  have  been  proposed   so   we \nalways start with the basics  so  will start with sigmoid see what are the problems with \nsigmoid  and then see what we can do to solve some of these problems  \n\n \nso  this is what the sigmoid function looks like you have seen it a million times and it \nactually constrains the input to zero to one right  so  it takes some input and it constrains it two \nvalues between zero to one  now since  we are always interested in gradients right  because  \nthe  entire  training  and  that  is  why  i  did  that  precursor  in  the  first  module  the  training \nalways depends on gradients  \nso  it is always important to look at what does the gradient look like  so  we know what \nthe gradient looks like  we have computed this is just sigmoid of x into one minus sigmoid \nof x  so now let us see what happens if you use such a sigmoid neuron in a deep neural \nnetwork  \n \n\f\n \nthis is a deep neural network and without loss of generality i am going to use a thin deep \nnetwork  but the same holds for a deep for a wide deep network also  so  suppose you \nare interested in computing the gradient with respect to w two  right  at some point in the \nchain rule  you will have this term  how many of you are convinced about this  ok  and \nthat will lead to this  could that cause a problem  \nso  at some one of the terms in your chain rule is going to be this dou h three by dou a three  i \nam assuming all of you are convinced about that and i have given you the exact formula \nfor dou h three by dou a three will that lead to a problem  \nstudent  \n  \ngood   so   what  is  the  consequence  of  this  to  answer  this   we  need  to  understand  the \nconcept of saturation  right  \n \n\f\n \nso  a sigmoid neuron is said to have saturated  if it is output is one or zero or rather close to one \nor close to zero ok  what would happen in that case to the gradient  \nstudent  \n  \nit  will  vanish  right   because  sigmoid  of  x  into  one  minus  sigmoid  of  x   so   it  either \nextremes is going to vanish and you do not even need the formula for that  you can just \nsee it from the diagram right  because  the gradient here is going to be zero  that is obvious \nright  it just a what horizontal line  \nso  this gradient would be zero  so fine  why does it bother us  what is our entire training \npremise based on gradients  right  what does our update rule  what happens  if this guy \nis zero  no update e the weights just stay where they are right  that means  the training \ngets stalled right  \nso  think about this right  if all the neurons in your network have saturated  that means  \nall the weights the gradients will be zero  that means  all the weights will remain where they \nare  you  pass  another  input   nothing  is  going  to  change  right   it  still  be  zero  so   if  this \nneurons have saturated your training will just stalled  ok  so  that was one of the reasons \nwhich is to cause problem in training deep neural networks earlier  right  \n \n\f\n \nso that is one of the reason  why it was not converging  because these weights used to \nthese  neurons  is  to  saturate   so   this  is  one  problem  with  sigmoid  neurons  a  saturated \nsigmoid neuron can cause the gradient to vanish  \n\n \nbut  why would the neurons saturated  i mean  what would cause them to saturate  ok  \nthis saturate find their gradients will vanish  but why would they saturate  we should be \nable  to  get  some  hints  from  the  figure  that  has  been  drawn   so   this  is  actually  that  x \nneeds to be changed  \n \n \n\fso  on the x axis we have x quite obviously  but that has to be something else   so  what \nit is  what is happening is  what does the sigmoid neuron do  it takes this aggregate it \nor someone just disappear \n  so  is it very boring today no right  so  \nyou have this aggregated sum of the inputs  once  you have that aggregation you applied \nthe sigmoid now tell me when would it saturated  \nstudent  \n  \nwhen the aggregation is very large  that means  one of the two things could happen either \nthe x\u2019s are very large or the w\u2019s are very large  would the x is x is be large i see a lot of \nyou saying no why  \nstudent  \n  \ngood  we normalize them  right  we make sure they are between zero to one  so  we do not \nallow  those  arbitrary  large  values  of  pressure  density  and  so  on   right   we  make  sure \nthey are between zero to one  so  then the weights can be a problem  right  now  why would \nthe weights be lies move later first  \nstudent  \n  \nif  i  initialize  the  weights  to  a  large  value   if  i  initialize  all  my  weights  in  my  infinite \nwisdom to a large value  what would happen right from the first training example itself  \nw  i  x  i  would  take  on  a  very  large  value  and  your  neurons  will  start  saturating   so  \nimagine if all the weights throughout my network are initialized to large values  \nthen  right  from  training  instance  zero   my  neurons  will  start  saturating  and  i  will  not  be \nable to train anything  how many of you experienced this while doing back propagation  \nand the others did  not  do the assignment  they  copied it please raise  your hands  how \nmany of you experienced it  now many more hands will be raised still now ok  honest \npeople that is a paradox  \n\f\n \nconsider what would happen if you use sigmoid neurons and initialize the weights to a \nvery  high  value   they  will  start  saturating  and  hence   you  will  have  this  problem  of \nvanishing gradients  ok  everyone gets this  so this is a problem at this sigmoid neurons  \n\n \nthe other problem with sigmoid neurons which is very interesting is that they are not zero \ncentered  what do i mean by that  they are not zero center that is  what it ok  so  sigmoid \nis are not zero centered  what do i mean with that  mean by that they are not zero centered  \n \n \n\fthe value is  between zero to one right  so  the average cannot  be zero it is  always  going to  be \nabove zero ok  sigmoid neurons are always going to take on positive values between zero to one  \nso  why is that a problem  so  that is an interesting explanation  oh did i say that did i \nput  the acknowledgements  somewhere  so  all of this  material  that  i have been talking \nabout  it  is  taken  from  andrej  karpathys  lecture  notes   so   here  is  this  interesting \nexplanation for this  \nso now  consider this  particular network ok and  i am  going to focus only  on this part  \nthat means  the output layer and just the layer before that and the layer before that has \nthese two weights wone and w two  i am going to focus on that  \nso   to  update  these  weights   i  need  to  compute   so   what  do  we  need  to  compute \ngradient  ok now  you will answer  so  we need to compute the gradient with respect to \nwone and w two and this is what it is going to look like  what is the red part and blue part  \nwhy red and blue the red part is dash for both common for both  right  \nso  this is going to be common i do not know why i did that  ok \n so  \nthis red part is common for both and what is the blue part actually  what is dou a three by \ndou w one h two one and dou a three by dou w two  \n so  dou a three by dou w one is just h two one and dou a three by dou w two is just h two two ok  so  let me \njust plug in those values and note that h two one and h three are between zero to one  so  can you make \nsome interesting commentary on this interesting  but useful not just philosophical stuff  \nthat these two derivatives are for the weights at a given layer  i have just taken two weights  \nbut i could have taken n weights and the same thing would have hold  \nbecause i know that the derivative is proportional to the input that it gets and the rest of \nthe  part  is  going  to  be  constant  because  that  is  coming  from  the  chain  rule  up  to  the \nprevious layer  right  \nso now  what is happening because of that  just to make fun of you guys  i mean  if you \nget  that  sorry   good  yeah  it  is  not  very  straightforward   but  let  us  see   so   if  the  first \ncommon term in red is positive  right  then  what would happen to these two guys  they \nwould both be positive right  because h two one and h two two are positive  \n\fnow  the first common term in red is negative then  what would happen to these two guys  \nboth  negative  so   that  means   the  gradients  of  the  weights  at  a  particular  layer  where  \neither  all  be  positive  or  they  will  all  be  negative  you  get  that   that  is  because  of  this  \ncommon part and the blue part the blue part we know is positive  \nso   what  matters  is  the  common  part  and  that  common  part  can  either  be  positive  or \nnegative for all of them together right  that means  for a given layer all the gradients at a \nlayer are either positive or they are all negative  so  let us see what is the implication of \nthat right  \n\n \nso  this actually restricts the possible update directions  \nso  which is the quadrant  which has all positive first  ok sorry  for embarrassing yeah \nand all the negative is the third quadrant  that means  your movements can only happen \nin the first quadrant and the third quadrant  so  do you see a problem with this right  so  \nyou are going to actually try to move that your theta  \nwhich is a collection of w one and w two is theta minus eta into the gradient right  and you \nknow that  this vector which is the gradient vector can either be positive  that means can \nlie  in  the  first  quadrant  or  it  can  lie  in  the  third  quadrant   these  movements  are  not \npossible  that means  there are certain turns or certain movements or certain directions  \n \n\fthat  i  am  not  allowed  to  take   so   what  would  this  mean  it  would  take  a  dash  time  to \nconverge  \nstudent  longer time  \nlonger time to converge right because  i am restricting my movement  so  imagine you \nhave to go from destination to destination b and i say that you can never take a right turn \nright   and  there  is  some  going  to  be  some  problem   it  will  take  longer  to  reach  there \nunless the directions are to our left right unless your destination is \n  \nbut that will not happen  \n\n \nso  suppose this is the optimal w star  \n \n\f\n \nand we start with some random initialization because  that is why we are going to start  \nthen the only way i can reach it is  i may by making a series of this kind of movements \nright   as  the  exact  pattern  is   what  will  have  to  take   because  these  are  the  only \nmovements which are allowed or some movements which are allowed and it will lead to \na certain cryptic pattern and i will not be able to have the complete freedom of moving in \nthe direction  which would have directly  taken me to the optimal  \nso   that  is  a  problem  with  something  not  being  zero  center   and  lastly  sigmoids  are \nexpensive  to  compute  because   you  have  to  do  this  exp  right   it  is  not  something  as \neasier  as  something  else  that  we  will  see  in  the  lecture  today  ok   so   these  are  some \nproblems with sigmoid functions   \nstudent  \n  \nso  this  is  some issues  that  were they with  sigmoid  functions  so  this  pointed that ok  \nmaybe we should try better activation functions  \n \n\f\n \nthat is why tanh become very popular  but tanh is not something which happened post \ntwo thousand and six right  so  this was like ninety two or ninety three  when i think yan lacunae had started moving to \ntanh from sigmoid functions right  now again here other inputs are compressed between \nminus point to one ok  where inputs are now zero centered which takes care of this problem \nwhich i mentioned at the end  \nthat  these  directions  of  movements  are  constrained  and  was  the  derivative  of  this \nfunction one minus tanh square right  what happens at saturation even without looking at \nthe formula  the gradient would vanish to zero right  so  the vanishing gradient problem is \nstill there  \nwhat  you  have  solved  is  a  problem  of  zero  centering   and  that  itself  used  to  give  better \nresults  than  just  using  a  sigmoid  function   but  it  is  still  computationally  expensive \nbecause   you  still  have  to  do  these  e  raise  two  components  right   the  you  still  have  to \ncompute these exponential powers  so  it is still computationally expensive  \n \n\f\n \nso   then  in  around  two thousand and twelve  i  guess  is  when  this  relu  was  introduced  in  the  context  of \nconvolutional neural networks  right  and this is what the relu function actually looks \nlike  is this a non linear function  it just looks like a line right  why is it a non linear \nfunction  it is a non linear function  right  because  x is you cannot write x the output as \na function of i mean as a linear transformation  right  so  you have this zero  in fact  if you \ntake two relu functions smartly  \n\n \n \n \n\fyou  can  actually  get  the  sigmoid  i  mean  you  can  get  an  approximate  for  the  sigmoid \nfunction  so  you can go back and check this  right  so  if you take these two functions and \nsubtract one from the other  what is this  this is a relu function this is also a relu \nfunction right  \nso   i  define  relu  as  max  of  zero  comma  x   so   both  of  these  are  relu  functions  some \nvariant  of  that  and  now   if  you  subtract  one  from  the  other   you  will  actually  get  a \napproximation of the sigmoid function right  and this cannot happen if you have two linear \nfunctions   take  any  two  linear  functions   you  will  not  be  able  to  get  this  kind  of  an \napproximation  \n\n \nso  relu is a non linear function  what are the advantages of relu  one is  it does not \nsaturate  in  the  positive  region  right   it  is  computationally  very  efficient  the  output  is \neither zero or x  there is no powers nothing like that right  and it practice it converges much \nfaster than sigmoid and tanh  so  that is what this two thousand and twelve paper show  and now  relu has \nactually become more or less the standard in all convolutional neural networks  \n \n\f\n \nbut there is still a caveat while using relu ok  so  the derivative of relu we can see \nthat if x is less than zero  then  the derivative is going to be zero  right  and if x is greater than \nzero  then the derivative is going to be one and that straight away follows from the definition \nof relu which is zero or x  \nso  when it is zero the derivative will be zero and when it is x the derivative will be one  so now  \nconsider this given network and let us assume and this is not a very far faced assumed  \nassumption it can happen in practice that  at some point a large gradient causes the bias \nb to be updated to a large negative value  so  what i am saying is that something happens \nand b gets updated to a large negative value  \n \n\f\n \nnow  what would happen to this quantity remember this quantity which i have circled is \nactually the input to the blue colored relu neuron that i have  so  i am asking you what \nwould happen to that input  that input would become negative  \nso  the neuron would output zero and i am calling it a dead neuron  why  if the input is zero  i \nmean  is  a  input  is  negative  then   the  relu  functions   output  would  be  zero   what  would \nhappen to the gradients during back propagation zero  that means  what would happen to \nthe weights  \nstudent  \n  \nwould not be updated right now  but that is fine right  if you give some other input this \nwill recover  why am i calling a dead means permanent  right  unless you are in some \nfantasy world  but dead is dead right  so  why am i saying that it is dead  i could might \nas well  i would give it a next  input and then probably things  would be ok  bias is  still \nvery negative because nothing is getting updated right or bias is still very negative  you \nknow that x one and x two are constrained because  you have normalized them right and w one \nand w two have not been updated  \nso  still the situation does not change  so  what happens is that once a relu neuron dies \nbecause   somewhere  in  the  chain  rule  you  got  a  zero   it  will  stay  dead  forever  ok   it  will \nnever be able to come out of that  it will always produce a negative output  that means  \n \n\fthat output will be clamped to zero  that means  no gradients will flow back and  that means  \nall the weights will not get updated connected that neuron  \n\n \nso   in  practice  when  you  train  a  network  with  relu   you  will  observe  that  a  large \nfraction of the units can die if the learning rate is set too high  why this if condition  \nstudent  \n  \nwhat was the assumption that i made  that the bias receives a large negative update  and \nthat is possible if your learning rate is very high  because  you got some small negative \ngradient  but your learning rate blew it up  \nnow  what is the practical implication of this  if a training a network and a large number \nof your relu neurons have died  what does it mean  most parts of your network are  \ndash useless  they are not learning any feature nothing right is all zero  that means  you have \nthis large number of parameters versus getting wasted  because  they feed into a relu \nyou function and the relu function just keeps outputting zero  \n so  if you have n neurons in the particular layer and most of them are zero  that means  you \nare  not  really  learning  an  n  dimensional  feature  representation   you  are  just  learning  a \nmuch  smaller  feature  representation   right   so   can  you  give  me  a  simple  way  of  one \nsimple way of avoiding this among many other ways  \n \n\fstudent  \n  \nno dropout is statistical right  it is probabilistic this is like always dead one thing is to \nupdate  the  weight  to  a  large  to  a  positive  value  and  zero one  mind  you  is  a  large  positive \nvalue  right   later  on  we  will  see  y   but  zero one  is  reasonably  large  ok   so   were  going  to \ninitialize the bias to a positive value  \nso  that even if this  large negative  gradient  flows through there is  still a  chance that it \nwill not become very negative  and hence  it will not mess up the things the way it does \nthat  is   one  solution  to  that  right   but  still  you  will  find  that  even  after  that  the  relu \nneuron  a  lot  of  those  can  die   but  still  in  practice  they  work  better  for  a  deep \nconvolutional neural network ok and we can also use other variants of relu  \n\n \nso   there  have  been  to  avoid  this  dead  neuron  problem   there  are  other  variants  of \nrelu   which  have  been  proposed   and  that  is  what  we  look  at  next   so   there  is \nsomething known as a leaky relu  is it obvious from the equation  what it does right  \nso   instead  of  producing  zero  it  will  just  produce  a  very  small  value  proportional  to  the \ninput  now  what would happen to the gradients  they will not saturate right  will have \nthe gradient would be  if the input is negative what would the gradient be  \nstudent  \n  \n \n\fzero one  right  so  that  means   some  gradient  will  still  flow  through   how  many  if  you  get \nthis   right  so   that  means   if  you  use  a  leaky  relu  neuron  some  gradient  would  still \nflows through  so  just understand this trend right that ah  and this is i mean all this stuff \nis simple there is nothing great in this  but just put it in context right  \nso   in  two thousand and six  to  two thousand and nine  people  realized   ok  now   we  can  trained  networks  and  maybe \nwhatever   we  have  done  with  unsupervised  pre  training  actually  corresponds  to  better \ninitializations or better optimizations or better activations and so on  \nso  now  let  us  try  doing  research  in  that   so   that  led  to  the  discovery  of  relu   now \npeople started observing problems with relu and then proposed a variant of it which is \nleaky relu right  so  that is how this area has now become very prolific and grow right  \nso   we  started  off  with  this  seed  idea  that   it  is  possible  to  train  these  deep  neural \nnetworks and now we are trying to make arrive at better and better ways of doing it  \nmaking  it  more  and  more  easier  to  train  them  and  take  care  of  some  of  these \nirregularities which existed earlier  so  one of them being sigmoid not being a very neat \nfunction to optimize with right  so  that is what all this is about  individually all of these \nare probably easy for you to understand  once you go back and look at the slides you all \nthis is nothing great in this  \nbut  what i want you to really understand is this bigger picture of what is happening here \nas  long  as  you  get  that  time  frame  with   and  of  course   leaky  relu  is  again \ncomputationally very efficient  there is no exponents no squares nothing like that  and it \nis  close  to  zero  centered  and  it  is  still  not  zero  centered   but  close  to  zero  centers  because   you \nhave outputs on both side and then someone came up with a generalization of this  which \nis parametric relu  so  y zero one make it alpha x and alpha will also be a  \nstudent  parameter  \nparameter  it is a trainable parameter it is not a hyper parameter  ok  how many of you \nknow the difference between parameter and hyper parameter  ok  you have used this in \nthe  back  propagation  as  i  am  right   so   it  is  a  trainable  hyper  parameter  it  will  get \noptimized along with your other parameters in the network  \n\f\n \nso   then  someone  said  leaky  relu  fine  parametric  relu  is  fine   let  us  try  to  do \nexponential relu ok  so  it has all the benefits of relu it ensures that  at least a small \ngradient will flow through even when your inputs are negative  that means  it avoids this \ndead neuron problem again close to zero centered outputs  but it is expensive because now \nwe have added this exponential right  \nso  these are all ideas which came out during this period and all of them were shown to \nwork better than the other and so on  and of course  at the end i have to tell you a final \nconclusion right  whenever  i give you so many possibilities  \nso  i have given you sigmoid  tanh  relu  parametric  leaky  exponential  now  what do \nyou use  right this the idea is not to confuse you  but to give you one solution  which \nwould largely work yeah  what regularization  \nstudent  \n  \nyeah   you could have done  yeah that \n there is  exactly  so  a lot of \nthis research right  which has happened in this period  it is not a lot of it is juristic right  \nyou  solve  one  problem  with  relu   ok   the  neurons  and  saturated  ok  just  make  it \nsomething which does not saturated  \n \n\f\n \nso that is there  it is possible that the other solutions would also go there is not that  this \nis  the  only  solution  which  works  now   then  someone  came  out  with  max  out  neuron \nwhich is a generalization of relu and leaky relu  why do i say it is a generalization  \nwhat was relu  that means  w one equal to b one equal to zero  w two equal to one  \nso   it  is  a  special  case  of  the  max  out  neuron   what  about  leaky  relu    this  was \nparametric value  but again what about  so now  what is happening w one equal to alpha b \none equal to zero  w two equal to one b two equal to zero  so you see how it generalizes right  so  this is \nhow these variants keep  kept coming up  \n \n\f\n \nnow   the  problem  of  course  is   doubles  the  number  of  parameters  right  because  you \nearlier had only w transpose x plus b  now  you have w one transpose b one  w two transpose b \ntwo and so on right  so  it is actually doubling the number of parameters that you have  \n\n \nso now coming to the final conclusion of all of this right  what you need to remember is \nthat sigmoids are bad  \n \n \n\fso  no one uses sigmoids in convolutional neural networks  they still use somewhere  i \nam i am sorry  about this relu is more or less the standard unit for convolutional neural \nnetworks  \nso  any standard cnn that you will pick up it will use relu as the activation function  \nif you want you can explore leaky relu max out elu and so on  but it will require a lot \nof careful tuning  say  if you want to use something out of the bulk box relu is just fine \nrelu just works fine in practice despite all this dead neuron and other problems  \nstudent  \n  \nyeah  so then  the argument for that is that  how often when you reach the point x equal \nto zero right  so  the chance of that having is happening is very  very low  and if you get \nthere  you can always approximate it by some epsilon or something and for that training \ninstance  just  go  on  right   any  ways  you  are  making  so  many  approximations  with \nstochastic and mini batch and so on  \nso  this is one more approximation that is how people typically deal with it  but in most \ncases  it will not come in that point appearing is very low  but the question is valid and \ntanh  sigmoids  are  still  used  in  lstm\u2019s  and  rnn\u2019s   which  you  will  see  at  some  later \npoint in the course ok  so  there are a couple of more modules that i need to do  so  we \njust take a break here  \n\f"}
{"audio_filepath": "lec008_004.wav", "duration": 1580.52, "text": "\nbetter initialization strategies \nso  in this module we will talk about better initialization strategies  \n\n \nso  this is where we are in the story right  we saw that deep learning has evolved and at \nleast  these are the four things which have happened  so  we have by the way this slide is \nincomplete  what are the other things which have happened actually which you already \nsaid in the beginning two more things which are not technical  but which happened more \ndata right  \nand more compute  but these are not really technical in the sense that  i mean this just \nhappened we have large amounts of data  that means  more data means  what if you are \nmore data for training  you would have complex networks  but not over fit right because  \nyou  have  so  many   so  much  of  data  right   and  more  compute  of  course   it  speeds  up \nsome of these matrix computations which happen  \n\fso  remember in a deep neural network most of the things which are doing are  matrix  \nmatrix  operations  right   you  are  taking  are  that  is  what  exactly  you  did  in  your  back \npropagation assignment  you did a lot of matrix vector computations and so on  and the \nadvent of gpu\u2019s this became  very very fast rate orders of magnitude fast  \nso   this  two  which  are  here  as  nothing  much  to  talk  about  that  is  just  something  we  all \nunderstand   what  has  happened  they   and  so  now   we  will  talk  about  better  weight \ninitialization strategies  \n\n \nso  let us start  with this question right   we will take this network and we will ask this \nquestion what happens if we initialize all the weights to zero  i like it when you all try to \nvisualize it and ok  so  you have to see what happens right  so  let us start with a one one  \nwhich is w one one x one plus w one two x two  \nso  i always start small l  i do not try to see what will happen everywhere  just start with \none neuron and see what happens take a one two  what would a one month\u2019s value b  if all \nthe weights are initialized to zero zero and a one two again zero right and same for a one three it is all the \nway all the neurons in this layer are going to be zero  till is it  \nso  that means  they will all get the same activation  so  if the a s are same the h s are \nalso going to be same and that is obvious irrespective of what non linearity you use now \n \n\fwhat will happen during back propagation what will delta w one one b  this again i do not \nknow why you do this ok  anyway that will be erase it  into x one  \nso   remember  that  the  gradient  is  always  proportional  to  the  input   and  you  have \nsomewhere  along  the  lines  along  the  chain  rule   you  have  this  h  one  one  and  a  one  one   just \nremember that and what would delta at gradient of w two one b is that fine  \nnow   can  you  see  some  things  on  the  left  hand  side  and  make  some  comments  on  the \ngradients  we have seen that a one one is same as a one two and h one one is same as h one two  that means  \nthese gradients are going to be equal right  that means  the weight started off at the same \nvalue  they are going to get the same updates and again remain at the same or different \nvalue  but  this same  right   then of course  move from  where  you started will not  be zero \nanymore  but they will all be at the same value  \n\n \nboth the weights will get updated with the same value and they will remain equal so  but \nfine  as  i  keep  training  they  will  move  away  from  each  other  right   this  is  what  i  told \nyou   when  you  feed  in  the  first  example  take  a  both  the  weights  remain  the  same   but \nnow if you feed another example and you keep feeding batches there  there is no dearth \nof data that  you have and eventually these weights will move away from each other  \n \n\fthe  update  is  the  same  again   the  weights  are  the  same  again  the  same  situation  will \nhold right  again your w one one x one plus w one two x two is going to be the same as w two and x one \nplus w two two x two and the same argument repeats  \nhow many if you get this  ok  so  once you initialize the weights to zero in all subsequent \niterations  the weights are going to remain the same  i mean  they will move away from \nzero  but they will all be equal ok and this symmetry will never break during training  so  \nwhat actually is happening in terms of the capacity of the network this is same as  \nstudent  single line tying the weights  \nthe  same  as  tying  the  weights  so   this  symmetry  will  never  break  during  training   so  \nasking what is the net effect which is happening  \nso  you have so many weights in your layer  but all of them are moving together will so \nin essence  you do not have the same freedom as you have with n different weights right  \nhere  in some sense unintentionally tied them because  it started off with the same value  \nnow you are all moving at the same values rate you are all going to the same value  so  \nyou  do  not  really  have  the  amount  of  freedom  that  you  would  actually  expect  with  n \ndifferent parameters all of you get this  \nand the same is true for w one two and w two two also  which are the weights  connected to the \nsecond  neuron  and  this  is   in  fact   true  for  all  the  weights  in  layer  two  you  can  actually \nmathematically verify it  that means  whatever this small analysis that i did here  just go \nback and do it for all the weights in the network and you will see that all of them  if you \nare going to initialize them to zero all of them are going to remain equal  \nthis  is  known  as  this  symmetry  breaking  problem  this  is  are  known  problem   this  is \nexisted much before two thousand and six and so on  if we initialize all the weights to zero you will have the \nsymmetry breaking problem  is there anything sacrosanct about zero or would this happen \neven if you initializer to same  but non zero values and that should have been cleared from \nthe iteration  right  because  after the first iteration we were  at  non zero weights and after \nthat the story repeated  right  \nso  even if you initialize it to non zero weights the same story is going to report repeat \nso  that means  as long as you initialize all the weights to the same value  you are going \n\fto end up with this symmetry breaking problem ok  which is not good  so  what is it that \nwe have learnt about initializing weights  \nstudent  \n  \ndefinitely  do  not  initialize  all  weights  to  zero   definitely  do  not  realize  them  to  the  same \nvalue ok  this is the first thing that you have learned  so  we are seeing different ways of \nnot  making  the  light  bulb  and  then   we  will  come  to  a  way  of  making  it   so   zero  and \nequalist no bad yes  some weights will not get updates in that case right  \nso  then that that should be fine so  that is the other thing i wanted to make at some point \nright   these  four  things  right  initialization   optimization   regularization  and  activation \nfunction  these are not independent things they are all tied to each other   \nso  as you said now if you use regularization then probably you could be a bit careless \nwith  the  initialization   even  if  you  had  initialize  the  weights  together  drop  out  would \nhave ensured that  some of these weights are not active at a particular training instance  \nthat means  they will not get weight updates  that means  they will move away from the \nother weights  \nso  that is this is not that only one of these things can be done right  you are going to \nuse a combination of these things  but while analyzing them  we will just look at one of \nthese things  assuming that  the others are not being  right  so  will assume that we are \nnot using drop order anything is that fine  \n\f\n \nso  this at least this in practice you are not supposed to initialize the weights to zeros and \nequal values that is what we have learned so far  now for the rest of the to convince you \nabout some other weight initialization methods  what i am going to do is  i am going to \ntake  a  feed  forward  network   where  you  have  as  input  some  thousand  points   each  of \nthis point is five hundred dimensional  and the input data is drawn from a unit gaussian  what i \nmean by that is  you have this x one two x one five hundred rate for the data instance one  \nso  all of these five hundred dimensions come from  a unit gaussian is that fine  so  this comes \nfrom a unit gaussian  this comes from a unit gaussian and so on ok  that is what i am \ngoing to assume  \n \n\f\n \nand the network has five layers each layer has five hundred neurons  the input is five hundred neurons each \nof  the  five  layers  is  also  five hundred  neurons   and  now   we  will  run  forward  propagation  no \nbackward propagation  no loss nothing and i am not even giving you an objective  this is \njust some input and i just want to see  what happens up to the last layer  i am not even \nbothered  about  the  actual  last  layer   that  means   i  am  not  trying  to  minimize  any  cost \nentropy squared error loss anything  \n\n \n \n \n\fso  let us try a few initialization strategies  so  we realize zero is not good realize equal is \nnot good  so  let us try some random initializations  but small weights ok and this is my \nway of randomly initializing with small weights  \nso  my w is a matrix of size fan in into fan out rate  which is n cross n ok the number of \nweights  coming  in  and  out  rate   so   n  cross  n  and  i  am  drawing  from  a  uniform \ndistribution and then  multiplying it by point zero one which ensures that  all the weights are \nvery small  you get the setup now with this  i am going to start with the input and then \nkeep doing these transformations  \nso   i  will  do  w  transpose  x  plus  b  pass  it  through  a  sigmoid  and  do  this  five  times  \nbecause  i have five deep layers now  this is what happens to the activations across the five \nlayers  so  the first layer remember that we had drawn from a unit gaussian right  so  \nthat is what the data input data looks like  so  this is the first layer which is the input data \nbasically and then this is what happens across the different layers  \nso   what  is  actually  happening  and  this  is  for  the  tanh  activation  function   there  is  no \nvariance  in  the  output  of   so   this  tells  me   so  this  basically  tells  me  that   for  all  the \nneurons what is the average value that  i am getting right and i should ideally get some \nhistogram that for some neurons i am getting the value minus one for some neurons minus \nzero nine  zero eight and so on  but what this is telling me is as  i keep progressing across the layers \nall the neurons have very similar values and they are all close to zero  \nthis  is  what  actually  happens  in  practice   i  have  just  actually  run  it  and  computed  the \nhistogram  \n\f\n \nand if i use sigmoid activation functions  again something similar all the values tend to \nbe close to the center which is zero five  so  this is zero five and although i had started with a nice \ngaussian distribution  \n\n \nnow   what  will  happen  during  back  propagation   so   do  not  try  to  think  for  now  that \nwhy   this  happens  i  am  just  telling  you  have  actually  run  the  code   and  this  is  what \nhappens   now  given  that   this  has  happened   what  will  happen  during  back \npropagation   so   all  the  activations  in  a  layer  are  very  close  to  zero   all  the  gradients  are \n \n \n\fgoing  to  be  close  to  zero   that  means   no  gradients  are  going  to  flow  back   that  means  \nwhich problem are we dealing with vanishing gradient problem  \nso  if you initialize your weights to very small values and this is easy to see in the case \nof tan h  so  for tan h this is my function right and this is zero  now remember that  this is \nwi  summation  wi  xi   if  all  my  weights  are  close  to  zero  or  very  small  values   what  is \nsummation w ix i going to be it is going to lie somewhere here  right  \nso  all these inputs are actually going to be very close to zero  now if my inputs are going \nto  be  close  to  zero   i  know  that  during  back  propagation  at  some  point  my  gradient  is \nproportional  to  the  input   that   i  have  given  and  when  i  say  input  here  i  mean  layer  one  \nlayer two  layer three and so on  \nso   that  means   all  my  inputs  are  very  close  to  zero   now  my  gradients  are  actually \nproportional to the input  so  all my gradients are also going to be close to zero  that means  \nmy  gradients are vanishing right  because  remember that across five layers  you will  have \nthese  products  of  gradients  right   all  of  them  are  very  close  to  zero   so   you  will  end  up \nwith  something  very  close  to  zero  raise  to  five   how  many  if  you  get  this   right   so  our \ngradients are going to vanish  \n\n \nif  you  do  this  very  small  initialization  of  the  weights  and  that  is  exactly  what  is \nhappening  so  this is the histogram for the gradients and i see that  all my gradients are \n \n\factually very close to zero  that means  no effective training is happening  my weights are \nnot receiving any updates this is what happens in practice  if you initialize your weights \nto very small values  \n\n \nnow  let us try to do the opposite of this very small values did not work  so  let me try \nlarge values and for large values i just sample from the uniform distribution  i will get \nsome  numbers  between  zero  to  one   now  can  you  guess  what  will  happen   remember \nsummation wi xi all your weights are large  so  why am i saying that number between zero \nto  one  is  actually  large   it  is  not  by  all  practical  because   this  is  going  to  give  me  this \nfunction is  actually  going to  give me numbers between zero to  one  why am  i  calling them \nlarge weights  \nstudent  \n  \nno  i will i just talked about the weights  assume there is no biases  how many of you get \nthat  answer   remember  there  are  five hundred  neurons   so   if  you  have  five hundred  small  values   that \nsummation is going to be still large right   if  you all of these are zero four or zero five  which still \nlooks small  but if you have two hundred and fifty of these or if  you have five hundred of these the resultant sum \ncould be somewhere of the order of two hundred and fifty right  and that is very large because  if you pass \nthat to a sigma and neuron  what will happen saturation right  so  you get this why i am \ncalling these weights as large  \n \n\f\n \nso  and this is actually what happens  so  when i have these tan h activations across all \nthe five layers  i observe that my neurons saturate i either get minus one as the output or plus \none as the output  and same thing happens  if i use sigmoid activations i either get zero as the \noutput or i get one as the output right  neurons saturated means  what will happen gradients \nwill vanish  right  \nso  even if you initialize the weights to very large values all your gradients are going to \nbe close to zero because  they are going to vanish and again you have a problem  so  what \nhave we seen so far  zero is not good  equal is not good  small weights is not good  large \nweight is not good  then  what do we do   \n \n\f\n \nso  let us see what to do  so  let us try to arrive at a more principled way of initializing \nweights and this again do not should the messenger  i am going to give you a proof under \ncertain assumptions ok  so  just bear with me  i just tell you what those assumptions are \ngoing to be as we go along  so  as i said right  \nso  i mean  you would argue that in practice these assumptions do not hold true  but at \nleast they give us some insights into  what is happening  right  what is the overall idea \nbehind  what  is  being proposed  so  let us start  with  that   so now  consider this deep \nneural network and i am just considering the first layer of it  where i have this neuron s \none one and i am talking about things before the activation  \nso  i know that  seleven is equal to this quantity  right  so  all the incoming weights to the \nfirst neuron which is w one i into x i  now for some reason  i am not telling you why i am \ninterested in the variance of this  can you tell me why i am interested in the variance  \nwhat  did  you  see  in  the  previous  examples   there  was  no  variance  right   there  was \nhardly any variance  so  let us see what happens if you compute the variance of this  \nso  i am just taking the variance formula a variance of a sum is equal to the sum of the \nvariances  right   this  is  of the  form  variance of  a into b  where a is  w one  i and b is  x  i  \nwhat is the formula for this  or if you know it or do not know it do not care  so  this is \nthe formula  \n \n\fso  this is the generic formula for variance of a into b  where you have to assume that a \nis wonei and b is xi  so  this is just a formula  there is no trick  here  no math i mean  no \nnothing fancy here just apply the formula for variance of a b and substitute a is equal to \nw one i and b is equal to x i  \n\n \nnow   i  will  assume  that  all  my  inputs  are  zero  mean  fine   we  have  been  assuming  that  \nforever  and  all  my  weights  are  also  from  zero  mean  ok   what  is  the  effect  of  that  which \nquantities  will  disappear   this  will  disappear  because  mean  as  zero  means  the  expected \nvalue of the weight is zero  so  the square of that is zero an expected value of the input is zero the \nsquare of that is zero  \nso  what am i left with summation  where i variance of xi into variance of wonei  ok now \ni am going to assume that the variance of xi is equal to the variance of x  that means  it is \nthe same for all the i\u2019s  so  i had this remember i had these five hundred inputs  \nso  i am assuming that for all the inputs the variance is the same  they all come from a \nsimilar variance distribution  and i am also going to make the same assumption for the \nweights fine and then i end up with this neat formula that the variance of seleven is equal to \nn  times  the  variance  of  w  into  variance  of  x  right   because   i  assumed  that  all  these \nterms are equal and there are n such terms  everyone is fine with the maths so far with \nthe assumptions that we have  \n \n\f\n \nso  in general for any of these neurons right instead of just seleven  i could take any sonei and \nthis is what the variance is going to be variance would turn out to be  because  i have \nassumed  that  all  the  weights  and  all  the  inputs  come  out  from  the  same  variance \ndistribution ok from a distribution having the same variance  \nnow  let us what would happen if this quantity is very greater than one  the variance of \nsonei  would  be   very  large   right   and  what  would  happen  if  this  variance  tends  to  zero \nvariance would be very  low  so  i am  just giving  you two extremes to  build the intuition \nand let us see what we are going to do with that intuition fine  \n \n\f\n \nnow  let me add one more layer and see  so  i have added one more layer and using the \nsame  procedure  as  above   he  will  arrive  at  variance  of  stwenty one  is  actually  given  by  this \nformula  and  actually  what  has  happened  here  is   that  this  is  si  had  xi  earlier   but  now \ninstead of xi  i have sonei because those are the inputs to this layer  right  \nso  this is exactly the formula that we had arrived at earlier  assuming  zero mean and the \nsame variance for all the weights and the inputs and i am arriving in the same formula \nfor the next layer where  instead of x i have sonei  \nso  this will result in this quantity ok  but i already had a formula for a variance of sonei  \nwhat was that n into this quantity  so  i will just substituted it there  so  i can say that \nvariance of stwoi is actually equal to this  i just substituted this value  \nso  that turns out to be  i have a square here when i have two here  so  you see where i am \nheaded with this  what will be the variance of s k i  this raised to k and is on everyone \ngets this ok  i can just continue the same analysis and  i have assumed that these weights \nand always are the same variance  right  \n \n\f\n \nso  in general i can say this ok  now can you tell me something about when would this \nvariance vanish when n variance of w is  \nstudent  less than one  \nless  than  one  ok   and  which  is  the  thing  that  we  should  aim  for  you  would  want  this \nquantity to be equal to one  in which case it will neither blow up nor shrink fine  so  so it \nto  ensure  that  the  variance  is  the  output  of  any  layer  does  not  blow  up  or  shrink   we \nshould ensure that n into variance of w is equal to one right   so  what is this  this just take \na minute to understand this  i am saying that  i am going to initialize my weights  \nso   i  should  initialize  them  in  such  a  way  that   the  weights  are  coming  from  some \ndistribution  like  we  saw  that  the  distribution  was  a  uniform  distribution  from  where   i \nwas drawing the weights  \nso  they are coming from some distribution i should try to draw them from a distribution \nsuch that  this condition holds  if this condition holds then across layers  my activations \nwill not blow up or shrink though that is exactly  what was happening in the earlier case \nwhen i was doing those bad initializations with small values and large values  \nso   let  us  see  how  to  do  that   so   what  i  am  going  to  do  is   i  am  going  to  consider  a \nrandom variable z ok  where is z comes from a normal distribution ok and i am going to \nscale it is value  i will draw from there and then i am going to scale it by one by square root \n \n\fof n  what is n number  neurons in each layer right here it is the same across all layers  \nbut it could also be different  so  i am considering a particular layer and n is the number \nof neurons in that layer  \nand  now  if  w  is  actually  equal  to  z  by  square  root  of  n  then   i  can  write  that  n  into \nvariance of w is actually equal to this quantity  everyone is fine with this  there is no \ntrickery  here   i  am  just  saying  that   why  i  am  doing  this  is  not  clear  that  will  become \nclear   but  at  least  what  i  am  doing  is  clear   i  am  drawing  the  weights  i  am  taking  a \nrandom  variable  z  which  comes  from  a  normal  distribution  and  then   i  am  setting  my \nweights to whatever values i have drawn  i just divide them by the square root of n  \n\n \nnow  let us see  what is variance of a z a square into variance of z  hey that is a basic \nformula all of us know this  so now  what is variance of z by one in z into one by square \nroot of n one by n into variance of z right  so  the n and n cancel and what is variance of z  \nwhat did i assume about z it came from a normal distribution zero mean and unit variance  \nso  variance of z is one  that means  this quantity n variance of w is going to be one  if i have \ninitialized  my  weights  such  that   they  are  equal  to  this   right  and  now   do  you  see \nwhether the weights are very small very large or what are they some now they made the \nweights dependent on the number of neurons  \n \n\fso   if  you  have  very  large  number  of  neurons   you  are  drawing  drawing  weights  such \nthat or  you are initializing weights such that  it is some normal variable divided by the \nsquare  root  of  n  right   so  now   when  you  do  this  summation  w  i  x  i  your  summation \ncannot blow up because  you have already divided it by n  \nhow many if you get this  so  this is a standard way used for initializing weights  how \nmany if you tried this for your back propagation assignment  why did you try this ah  \nstudent  \n  \nbecause   you are having some problems  with  saturation  i  guess   right  so  this  is  how \nyou should initialize your weights  this is more or less the standard technique and some \nvariant of this right  because  instead of n you would have this fan in and fan in out it  \nhow many weights are coming in  and how many weights are going out  \nso  you make it proportional to the square root of n into k or something like that right  \nso   but  in  general  this  idea  right  of  course   this  proof  we  arrived  at  it  with  lot  of \nassumptions  but we at least got to some principle way of initializing weights and this is \na largely used standard this and some variants of it  \n\n \nso now  let us see if i actually take the same network  that means five layers five hundred neurons at \nevery layer and then initialize it using this  so  this exactly what i had told you right that \ntake  it  from  a  unit  distribution  sorry  a  normal  distribution   and  then   divided  by  the \n \n\fsquare  root  of  the  number  of  neurons  in  that  layer  and  now  let  us  see  what  happens \nacross the five layers  \nyou see what happens we get this good variance in the activation functions  they are not \nall going to zero or one or point five  right  so  this solves the purpose for tan h activation and \nalso for the sigmoid activations  \n\n \nyou  see  a  good  spread  in  the  weights   and  remember  actually  for  sigmoid   although  \nthese values look close to each other  but this is the zero to one range  this is actually minus one \nto zero which will not happen for sigmoid  so  within the zero to one range you get a good spread \nif you initialize the weights this way  \n \n\f\n \nbut it turns out that  this initialization does not work for the relu function  in the relu \nfunction you still see this effect that  you started off with a good spread  but as you keep \ngoing across depths this spread disappears  why would that happen to someone gave an \nintuition for this and is again one of those heuristic things  that in the case of relu you \nneed  to  account  for  this  divided  by  half   because   half  of  the  relu  is  not  active  right \nhalf of the relu is zero  \n\n \n \n \n\fso  you need to account for that fact and do this simple trick that  instead of taking the \nsquare root of the fan in you  take the square root of fan in by two because  you know that \nhalf  the  times  it  is  not  going  to  produce  any  output   right   so   that  is  a  very  simple \nheuristic  that  someone  tried   and  that  leads  to  better  activation  functions  better \nactivations across all these layers  right  so  as you see across all the layers the spread is \ngood now so  the same idea ok  so now  you have a good way of initializing neurons  \nso  this should help you in your future assignments fine  so  this is how  what you have \nlearned about  how to initialize your weights  and it makes a lot of difference to  how \nyour  network  will  behave   right   and  that  is  what  the  i  was  trying  to  show  that  by \ncomputing  these  activations  across  different  layers   and  i  showed  that  as  you  change \nthese initializations strategies you get better activations  \n\f"}
{"audio_filepath": "lec008_005.wav", "duration": 933.729, "text": "\nbatch normalization \nnow  we will end with something known as batch normalization  which is again almost \na defacto standard at least in convolutional neural networks  so  if you are dealing with \nconvolutional neural networks you will use something known as batch normalization  \n\n \nso  let us see what it is  so this is again something which is some method which allows \nus to be less careful about initialization  so let us see why that happens  \n\f\n \nso  to understand the intuition behind this  let us consider a deep neural network ok and \nlet us focus on the last two layers h four and h three  now  typically will use some mini batch \nalgorithm  for  training  right   so  we  will  use  mini batch  version  of  gradient  descent  or \nmini batch version of adam or any of these algorithms  right  \nnow  what would happen if there is a constant change in the distribution of h three no just \nthink about  the question that  i  am  trying to ask  you  so   as  far as these  two layers are \nconcerned h three is the input and h four is the output it does not matter what has happened so \nfar  or  in  particular  does  not  matter  what  x  was   whether  it  came  from  a  normal \ndistribution or whatever distribution  right  \nat this point my input is h three and my output is h four  now i am training it in mini batches \nwhat if across batches my distribution of h three looks very different  what would happen  is \nit a good thing or a bad thing  it is a bad thing  right  so  if you have training data right \njust  think  of  this  as  i  said  just  focus  on  this  layer   if  you  have  an  input  which  is  not \nfollowing  a  fixed  distribution  is  constantly  changing  during  your  training  then  that  is \nalways a bad thing right  because you try to adjust to one distribution and now again the \ndistribution  is  completely  changing   so   that  always  makes  our  training  very  very \ndifficult  right  so  if you have a very fluctuant distribution then a training is going to be \nhard ok  so that is the intuition that i want to build  \n \n\f\n \nso  now this could actually happen  so it would help if the pre activations at every layer \nare you need gaussians because  for the input we made a case that will make the input as \nunit gaussian  right  \nso  that things are very nice they are all coming all the inputs are coming from the same \ndistribution  but we now realize that at every layer we have an input right it is not that the \noriginal input the only input even h three is an input even h four is an input and so on  so  why \nnot ensure that at every layer your inputs or your h one  h two  h three also is something  which \nlooks  like  a  gaussian  distribution  which  comes  from  a  gaussian  distribution   why  not \nensure that  that is the basic idea behind batch normalization and how do you do that is \nthe  following  that  you  had  computed  this  s  i  k  just  as  we  had  done  in  the  derivation \nearlier right  so s i k is one of these guys   \nnow   if  you  do  this  what  are  you  actually  doing  you  just  normalizing  it  right  you  are \nsubtracting the mean and dividing by the variance  so that means you are making it zero \nmean unit variance and that is the intuition which i was trying to build that  why not at \nevery layer have this good distribution which is zero mean unit variance  by even if you \nare feeding it multiple batches for that batch you will ensure that by this subtraction and \ndivision or the normalization process the data will become unit variance and zero mean \nok  so  now at every batch the data is coming from the same distribution even if it was \n \n\foriginally  from  a  distant  different  distribution   but  how  do  we  compute  this  mean  and \nvariance  \nso  did you understand the question that i am asking i am focusing on this s i k i want to \nsubtract  the  mean  of  that  s  i  k   how  do  i  do  that   so   the  name  gives  it  away  batch \nnormalization it cannot be more explicit than that  so  compute the mean for the current \nbatch and the variance for the current batch and normalize your inputs or normalize the s \ni k according to that you get this  so  now end up with a situation where all your inputs at \nevery layer across different mini batches seem to come from the same distribution is it \nfine  the current batch  so you take the average value from the current batch  \nso  then it will become zero mean for that batch and unit variance for that batch and this \nyou are ensuring for every batch  so  every independently every batch you are ensuring \nthat it comes from a zero mean unit variance distribution  right  so  overall the effect is \nthat all the batches are coming from the same distribution no  so  at validation time you \nwill compute the mean and variance from your entire data entire training data once after \nthe training is done right  \nso  now we will computed from a mini batch and this is ensure that across mini batches  \nnow your input always comes from a zero mean unit variance distribution across all the \nlayers  \n\n \n \n\fthis is what a deep network will look like with batch normalization right  so what will \nhappen  is  you  passed  an  input  you  computed  this  tan  h   then  you  will  have  this  batch \nnormalization layer watch is what is the operation that the batch normalization is going \nto do this is the operation that it is going to talk ok  everyone gets that and now it gives \nme a unit normalized distribution sorry it gives me a input coming from a zero mean unit \nvariance distribution and then  i pass  it to  the next  layer again  at  a batch normalization \nlayer  \nso  after every layer you will actually add a batch normalization here  now my question \nis  is this legal  what is legal in this course anything that is differentiable  right  so  you \nhave  to  make  sure  that  if  we  have  added  this  operation  it  should  be  a  differentiable \noperation   so   that  you  can  come  so  now  the  gradients  have  to  flow  all  the  way  here \nright  so that means  i  should  be able to  compute the gradients with  respect  to  this  so  \nnow  this  is  one  of  my  a  i  and  i  should  be  able  to  compute  dou  a  i  with  respect  to \nsomething or rather the loss dou of the loss function with respect to a i by turns out that \nthe operation  that you have done is actually differentiable  \n\n \nyou can actually work that out and it is not important i am not going to derive it because  \nit is just yet another derivative that you will take  but it is a you should get the intuition \nfrom  here  right  what  you  are  doing  is  this  simple  operation  and  this  just  looks \ndifferentiable  right  \n \n\fso   the operation that  you are doing is  differentiable  so that is why  you  can add these \nbatch normalization layers and you can back propagate through this layer  but now what \nis  the  catch  here  it  somehow  ties  to  the  question  that  he  was  trying  to  ask   you  are \nactually enforcing that all your are zero mean and unit variance  right  so  this is again \nsome sort of a constraint that you are enforcing right  what if that is not the best situation \nin which the network can learn  what if to distinguish between some classes it was ok  if \nthe  distribution  was  not  same  across  all  the  batches  they  get  this   they  are  enforcing  a \ncertain consider they are enforcing a certain condition on all the layers and all of them \nhave to be zero mean and unit variance but that may not always be good  \n\n \nso  they do something which is counterproductive  let us see what that is  why not let the \nnetwork decide what is best for it  so  after the batch normalization layer so this is what a \nnormalized s i k was  after you have done that you compute a y k and this is not the final \noutput this is the output at the k\u2019th layer this is  equal to this  why do they do this and \nremember that gamma and beta are going to be learnable parameters  what are you doing \nactually you are again scaling it and shifting it this is the same as adjusting the variance \nand the mean right  \nso  now what happens if the network learns the following you get back the s i k  so you \nhad taken s i k and you had normalized it  but now if you allow these gammas and betas \n \n\fto be there in the network  then the network can decide that maybe at this layer i do not \nwant this normalization i just want to stick to whatever output i was getting  \nso   it  could  learn  the  gammas  and  betas  in  this  way  and  ensure  that  you  get  back  the \nunnormalized s  how many of you get this fine lot of you do not seem to get this  but i \nam pretty sure if you go back and look at it you will get it  right  so  what is happening \nhere is that is why  i said it is counterproductive that you first forced it to make  at unit \nmean and zero variance and now you added no zero mean and unit variance and now you \nadded this operation which is again a scaling and shifting operation  so  remember that \nwhen  you  make  the  data  zero  mean  and  unit  variance  that  is  exactly  what  you  do  you \nshift it  so that it become zero mean and you scale it  so  that it becomes unit variance  \nso  you are again introducing parameters which again introduce the same flexibility that \nyou could learn gamma and beta in such a way that you could get back the original data \nwhich was not normalized ok  so  if the network wants to learn that and if the network \nfees that is the right thing to do  then it has the flexibility to learn those parameters and \nyou can recover si  \ni think the rationale is that your first making is something which is more standard right \nand then from there trying to learn it instead of just trying to let it learn in the way  do \nyou get the difference between the two the first bringing it to all of these things to some \nstandard value  which is between i mean which is the normal distribution and then from \nthere allowing it to learn wherever it has to learn right that is the idea  but it could be the \ncase that the other thing also works here  \n\f\n \nso   now  what  we  will  do  is  we  will  compare  the  performance  with  and  without  batch \nnormalization on mnist data using two layers  \n\n \nso  here in this figure what i am going to draw is the validation loss  am i no the training \nloss as i keep increasing the number of epochs and here i am showing you the histogram \nof  the  activation  functions  at  layer  one   so   i  have  trained  a  deep  feed  forward  neural \nnetwork  and  i  am  showing  you  what  do  the  activations  look  like  at  layer  one  with  and \nwithout  batch  normalization   so   remember  that  we  started  with  this  intuition  that \n \n \n\fwithout batch normalization there would be this constant fluctuation and the data would \nseem to come from different distribution at every training instance  \nwhereas  with batch normalization you are ensuring at your data comes from zero mean \nunit variance distribution right and so that is one thing which i want to see another thing \ni want to see is that how does it affect training  right  so  that is the animation that i am \ngoing to show you  so  focus on all these three things i do not know how you will do it  but \nfocus on this  focus on this and focus on this with two eyes  \n\n \nso  let us see to see what happened right  so this so now look at the focus on the leftmost \nfigure   so   that  does  not  seem  to  change  much  with  respect  to  it  is  mean  and  variance \nright   but  if  you  look  at  the  middle  figure  that  is  constantly  changing  it  is  mean  and \nvariance  right  and you see the effect on the training loss that the first one which was \nwith  batch  normalization   that  converges  faster  as  compared  to  the  second  one  right \nagain an empirical result i am not really proving that this will always happen  this is what \nempirically we observed  \nso  this was the story that we covered from  one thousand  nine hundred and eighty six to two thousand and six where back propagation was \nalready it was already discovered  but was not working well and there was this spark in \ntwo thousand and six  that  showed  that  we  could  do  some  things  to  make  training  really  work  for  deep \nneural  networks   but  maybe  that  something  is  not  sacrosanct   we  could  try  different \n \n\fthings  what  we  tried  at  that  time  was  unsupervised  free  training  which  is  almost \nnonexistent now  \nbut  that  lead  that  led  to  these  thoughts  that  maybe  this  is  because  of  optimization  \ngeneralization  regularization activation functions and so on  right  so  there was a lot of \nresearch in these different areas and that led to a lot of developments which was better \noptimization  algorithms   better  regularization   better  activation  functions   better \ninitializations and batch normalization  right  \nso  these a few concepts that you have seen in the past few lectures one being dropout \nand  the  other  being  weight  initialization  using  this  xavier  initialization  or  he \ninitialization and this batch normalization  right  this is something which is all prevalent \nright  so this is something that  you will see in all deep neural networks that get trained \ndefinitely  in  convolutional  neural  networks  and  more  often  than  not  even  in  recurrent \nneural networks  so  these are the two most popular types of neural networks  so  in both \nof these you will see that these ideas are regularly applied and they always lead to more \nstable training or better generalization  \n\n \nso  now this was all which happened till two thousand and sixteen or seventeen what has happened still since then  \nso   there  is  still  continuous  research  in  designing  better  optimization  methods   so  as  i \nsaid after adam there was this eve which did not become very popular  but there is still \n \n\fpeople  looking  at  better  optimization  methods  and  there  is  something  which  has  been \ndeveloped on adam and came out in december last year  \nnow   people  have  also  started  looking  at  data  driven  initialization  methods  right   so \ninstead  of  having  this  fixed  initialization  which  is  drawn  from  a  unit  or  just  which  is \ndrawn from a normal distribution and then just divided by the square root of n  why not \nthink of data driven initialization methods that  so there are some works on that again not \nvery popular because  most of the shelf things that you will try will not really do any data \ndriven initialization  \nbut if you really think that you are stuck at some point then you could look at some of \nthese works and see how they try to come up with initializations based on the data that \nyou are dealing with and now after batch normalization there have been some other types \nof  normalizations  which  have  been  proposed  which  seem  to  work  better  than  batch \nnormalization  but largely the stable configuration which has kind of prevalent is adam \nin  terms  of  optimization  xavier  or  he  initialization  in  terms  of  initialization  relu  in \nterms  of  activation  functions   what  else  is  there  batch  normalization  in  terms  of  again \nregularization plus initialization and dropouts in terms of regularization  right  \nso  these are roughly the key terms that you will almost see in all the deeply living deep \nneural  network  people  that  you  see  right   you  will  always  see  when  they  describe  the \nhyper parameters  they will say that this is how we initialized is this is the drop out that \nwe use this is the batch normalization and the training algorithm more often than not is \ngoing to be adam  \nso  they have seen some very crucial elements of training deep neural networks over the \npast two to three lectures right and now we will build on these and we will assume that this is \nwhat you are going to do  so  now when i talk about neural networks like convolutional \nneural  networks  and  so  on  i  not  go  back  and  tell  you  use  adam  or  use  batch \nnormalization or assume that you already know these things and you will try to train your \nnetworks using these tricks that we have your learned  \nthe last couple of lectures have been about tips and tricks for deep neural networks and \nfrom here on in the next  lecture will move on to what wordtwovec  because that is what \nyou need for your assignment  so  in the next lecture we will do a word representations  \n\fso  that  is  essentially  seeing  an  application  of  feed  forward  neural  networks  and  from \nthere on we will move on to convolutional neural network  \n\f"}
{"audio_filepath": "lec009_001.wav", "duration": 555.568, "text": "\n so  today we are going to talk about learning vectorial representations for words  \n\n \nso  these are the acknowledgement slash references for where are the things that i have \nreferred to by preparing for this lecture  so  you can just go this some of these are also \navailable as video lectures on youtube can take a look at them also  \nin the first module  we are going to look at one hot representations of words  \n\f\n \nso  as usual we will start with this motivation or motivation  motivating question why do \nwe  need  to  learn  representations  for  words   vectorial  representations  for  words   when \nwords are there right  you can write them using alphabets and characters and so on  so  \nwhy do we need vectorial representations  mention whatever you have seen so far in the \ncourse  i have seen anything like this let us see  \nso   suppose  you  are  given  an  input  stream  of  words   and  it  could  be  a  sentence  or \ndocumented   if  i  say  documented  pretty  much  covers  almost  all  the  text  that  you  see \nright  you can always abstract everything as a document and email is also a document  a \nmanuals are also documents and so on  \nand we  are interested in learning some function  of it  and so   i am  given a document \nand i am interested in the function y hat  say y hat is equal to sentiments of the words in \nthe document or the sentiment of the document itself  \nthis is imaginable this is not something that i am cooking up this is something that  you \nwould want to do you would log on to for example  if you are a movie maker you would \nwant to know once the movie is released people have written reviews about it what is a \nsentiment is positive or negative  similarly  if apple has released a new product or a new \nfeature  you  would  want  to  know  what  are  the  reviews  written  about  this  product  and \nwhat is the feature what is the sentiment coming out of it is positive or negative  \n \n\fnow  sentiment is a binary thing  or it could be rated also right it could be on a scale of \none to ten also  but let us consider it is binary that either people liked it or did not like it  \nso now i am trying to learn this function  which gives me which takes as input words  \nbut as output gives me real numbers  either zero to one or on a scale or one to ten or whatever  \nright and this is not something that we have dealt within the course so far  let how do we \ntake as input word  so  all inputs have already always been numbers  right  they were \neither  coming  from  rn  or  they  are  coming  from  zero  to  one  raise  to  n  or  something  of  that \nsort  \nwe never had the situation when we have words as written  so  right so now how do we \ndeal  with  the  situation  and  also  i  have  made  a  case  for  that  learning  this  function  is  a \nvalid thing to do  you have several news cases where you will need this  \n\n \nso now  if we employ a machine learning algorithm  that some mathematical model  so \nwe  saw  that  we  could  have  several  such  models  logistic  regression   svm  and  neural \nnetwork and feed forward neural networks and so on  right  and at the end we are trying \nto  learn this  function  y  hat  is  equal to  f of x  but  in  our case  the x instead of have be \ninstead of x being numbers it turns out that x is actually a collection of words  \nso  now  how  do  we  reconcile  with  the  situation  where  we  have  suddenly  have  words \ninstead of numbers  so  the way to do that would be we need a way of converting these \n \n\fwords or documents into some number  into some vectorial representation and once we \nhave this vectorial representation  right  so now  we have r  r raised to n and we know \nhow to deal with r raise to n given r raise to n  how to predict r or even r square or rm \nin  general   that  we  know  right   we  can  design  neural  networks  or  any  other  machine \nlearning  algorithm  should  do  that   but  how  do  we  go  from  here  to  here  that  is  the \nquestion  right  and that is why we need to learn vectorial representations of words  \nthis  is  a  motivation  clear  to  everyone   okay   now  let  us  start  getting  with  a   refer \ntime  three fifty two  how to do that right  \n\n \nso  now we will start hearing this word corpus  have you heard this word before  that is \nexactly what you are collecting for the word to like assignment right you are collecting a \ncorpus in specific languages  and you have taken a very toyish corpus for the purpose \nof illustration  so  here is a corpus it just contains four sentences  right  so  think of it that i \nhave a very restricted domain  i have very small set of documents and i just have these four \nsentences with me  this is the valid corpus  \nthe  corpus  that  you  have  constructing  is  probably  much  larger  scale  you  are  trying  to \ncollect  one hundred  thousand  sentences  or  fifty  thousand  sentences  or  something  of  that  order \nright  ah  but we will take this toy example  \n \n\fnow   consider  set  v  of  all  unique  words  across  all  these  input  streams   so   i  just  call \nthem input streams by input streams i mean  sentences or documents or whatever right  \nyou could take it as any sequence of words  and v is set of all unique words across all \nthis input sentences that you have  \nso  can you tell me what v is here  what would can you tell me some elements of v of \nthe set v  human machine interface and so on  so  that is why  in fact this is the entire \nset v which is written on the left hand side right  and v is called the vocabulary of the \ncorpus so  that means  everything in the corpus comes from this vocabulary  all sentences \nare constructed by arranging words from this vocabulary  \nsome  of  you  might  always  i  mean  find  this  very  trivial   but  i  am  just  going  over  the \nbasics   so   that  at  least  the  terminology  is  clear  to  everyone   and  what  we  want  is  a \nrepresentation for every word in v  so  that is the title of the lecture learning vectorial \nrepresentations of words  so  for every word in our vocabulary whatever corpus we are \ndealing with the vocabulary would change  and for every word there you want to learn a \nrepresentation for that word  so  that is what our quest is today  ok  \nand now one very simple way of doing this is right  you tell me you want a vector and \nthat is all you care about  here is a vector i will give you one hot representations  so  if a \ntotal number of words in my vocabulary is v  i just construct a vector of size v ok  and i \nhave assigned a number to every word in my vocabulary  right  so  i will say human is \nequal to zero machine is equal to one interface is equal to two and so on  \nand if you ask me for a vectorial representation of that word  i will just say take this the \nor vector of size v  and switch on the corresponding bit and anything else would be zero  \nhence  one   hot  right  at  any  given  point  of  time  only  one  of  the  elements  in  the  vector \nwould be on  so  that is a simple one hot representation as this is a very simple recipe to \nget a vectorial representation of words  and for every word in your vocabulary  \n\f\n \nnow   what  is  the  drawback  of  this   v  tends  to  be  very  larger   right   so   for  example  \nthere is a standard corpus known as the penn treebank corpus  which is used in various \nnlp applications for various reasons  and that corpus has a vocabulary of fifty k  \ngoogle of course  operates at it is own scale  so  they have a word one t corpus which has \nthirteen  million  words   so   this  is  like  all  the  most  of  the  web  pages  that  they  have  drawn \nconstructed a vocabulary from that  \nso now i am talking about  for every word representing it by a vector of size thirteen million  \ntheory  does  not  good  work  right  there  is  too  much  of  storage  required  for  that   and  if \nyou look at that information in it is so redundant that is all zero except for that one bit which \nis on  \nand the other important problem is that these representations do not capture any notion \nof similarity  other three words that i have shown you which do  you want  to have similar \nrepresentations   cat  and  dog  why   because  both  of  them  are  domestic  animals  right  \nboth of them are mammals  so  there are some things that you would want at least  at the \nminimum that the similarity between a cat and dog is more than the similarly between cat \nand truck  \nor alternately the distance between cat and dog is less than the distance between cat and \ntruck  so now  once i start talking about vectors  i can talk about similarities like cosine \n \n\fsimilarities or i can start talking about euclidian distance  so  once anything i convert it \nto  a vector  i can start asking these questions other two questions  which  i am  asking are \nvalid right what would you expect to be the euclidian distance between cat and dog as \ncompared to cat and truck  \nnow  what happens with the one hot representations  take any two words in your corpus  \nany two what will be the euclidian distance  what will it be  square root of two  right  for \nall the words  take any two words in your corpus what will that cosine similarity  mean zero \nbecause all these vectors are orthogonal  right  so  the cosine similarity is going to be zero  \nbut this is  that means  these vectors are not really capturing any information about the \nessence of the word  right  \nso  remember always we are interested even like that has been our philosophy right from \nauto  encodes   and  so   right  or  even  principle  component  analysis   they  are  always \ninterested in learning meaningful representations which capture something fundamental \nabout  the  entity  that  we  are  trying  to  represent   right   but  here  something  like  that  is \nclearly not happened  ok  so  that is not acceptable  \n\f"}
{"audio_filepath": "lec009_002.wav", "duration": 715.287, "text": "\ndistributed representations of words \nso   what  we  saw  now  was  sparse  representations   or  one  hot  representation  sparse  \nbecause   only  one  bit  is  on  from  there   we  will  move  on  to  something  known  as \ndistributed  representations  of  words   and  you  have  already  seen  that  sparse  is  in \ntheory  but it gives a very simple recipe of converting words to vectors  but it does not \nserve much purpose  \n\n \nso   let  us  see  what  distributed  representations  of  words  are   so   this  around  one thousand  nine hundred and fifty seven  j  r \nfirth made this very profound statement that  you shall know a word by the company it \nkeeps and this of course  a play on some other similar code  but what does this actually \nmean  so it means that  you want to know what does the word bank convey  or what is \nthe  essence  of  the  word  bank   right   what  this  code  says  is  that   if  you  want  to  know \nabout bank  you should say  you should see the company that it keeps  that means  what \nare the other words which appear typically in it is neighborhood  and of course  when \nyou have a large amount of corpus given say the entire wikipedia  \n\fof course  at that time wikipedia did not exist  but any large corpus  and does this led to \nsomething  known  as  distributional  similarity  based  representations   so  to  understand \nthis  we first have to understand the idea of a co occurrence matrix  \n\n \nso  the basic idea is to use the accompanying words  which in this example happen to be \nfinancial   financial  deposits  credit  etcetera  to  represent  bank  and  to  do  that  we  will \nconstruct something known as a co occurrence matrix which looks like this  right  so  a \nco occurrence matrix is a terms cross terms matrix  that means  every row in the matrix \ncorresponds to  a term or a word and every  column in  the matrix also  corresponds  to  a \nterm or a word  \ncan you guess  what how many rows would there be  size of the vocabulary how many \ncolumns would there be size of the vocabulary  right  okay  so here is how we construct \na co occurrence matrix  so  we take a word we are interest interested in constructing the \nrow for that word  the number of columns is the same as the size of the vocabulary  \nnow for every column we will make an entry  which tells us whether or how many times \nthis this word appeared in the context of the target word  right  for example  if i look at \nmachine  i am looking at the row for machine  i am trying to construct the entries in that \nrow   i  know  that  the  number  of  columns  is  equal  to  the  all  the  unique  words  in  my \nvocabulary  \n \n\fso  i look at the first word which is human and in that cell i enter the value  which is the \nnumber of times human appeared in a window of k words around machine  is that fine is \nthat straightforward   and that is how  i will construct this co occurrence matrix  where i \nhave taken the window  size as two   that means   in  any  given cell my entry would be the \nnumber of times human appeared within a two word window of machine is  right the ig\u2019th \ncell  clear   the  definition  of  the  ig\u2019th  cell  clear   so   this  tells  me  that  user  actually \nappeared two times around the word system in a window of two words around it is that clear  \neveryone gets this how i construct the co occurrence matrix  ok  now  you could use the \nsame  so this is known as words and this is known as context  that means  the rows we \nrefer to them as words and the columns they refer to them as  context  now as you said \nthat  the number of rows and the number of columns can be same that we can consider \nthe same words in the context  as well as the same word as the target words  right  \nbut you could also do something different you could say  that i do not want to consider \nall words that is context words   because  for example the word for appearing with any \nother word does not really give me much information because  it is just a stop word right \nor the word the or and or a these are known as stop words in the language  these do not \nreally give me much information  \nso  if  you  go back to  the bank example  financial  credit deposit  are the  words which  i \nreally care about and these are the financial bank or with deposits  and also these words \ndo not really matter a lot ok  do you get the intuition  \nso   you could choose to have  fewer columns  which are only the important  words  that \nyou  consider  and  you  ignore  the  stop  words  ah   in  this  discussion   i  will  alternately \nswitch  between  considering  the  columns  as  the  same  as  the  rows  and  sometimes  are \nrestricting the columns to fewer number of entries  \n\f\n \nso  now   each  row  gives  us  the  vectorial  representation  of  the  word   so   we  have  seen \nhow we have moved from sparse representations to distributed representations  so now \ntake a guess now would this vector be sparse  \nso  we saw the extreme sparse right which was one hot  now  the vectors which you get \nhere  are they  going to  be dense or still sparse  sparse   right  because every word does \nnot  appear  with  every  other  word  right   you  still  have  these  v  dimensional  vector  and \nthere are some words which will appear with very few words  right  so   you expect to \nhave non zero entries in very few columns right  so  these representations are also sparse  \n \n\f\n \nso   there  are  some  problems  some  of  which  are  fixable   so   we  look  at  the  fixable \nproblems first  the first thing as the stop words are very frequent so  these counts should \nbe very large  \nso  if  you take the entire  wikipedia corpus and  you take the word machine or system  \nthen  the words the  and  for and so on would have appeared like more than one thousand times \nin  the  context  of  the  word  machine   right   and  as  compared  to  the  other  words  like \nsystem or user they would have appeared much fewer times  so  this kind of skews your \ncounts right  it is like highly biased in the favor of stop words  so  how do you deal with \nthis  \n \n\f\n \nso there are two ways  one is ignore frequent words right  so  that is the solution which i \nsuggested earlier  that your number of columns would be less than the number of words \nis that fine  so  you do not actually consider frequent words at all  \nthe other is user threshold t so  that means  in these columns like for and with and so on  \nwhatever be the entry  if that crosses a certain threshold then  i will just replace it by that \nthreshold is that clear  \nso   i  am  just  saying  that   this  means  that  the  word  has  appeared  more  than  one hundred  times  \nand  i  am  not  interested  in  keeping  the  actual  count  which  was  more  than  one hundred   i  just \nsaying  that   more  than  one hundred  is  enough  for  me  right   because   i  know  that  all  the  other \nentries  are  going  to  be  much  less  than  this   so   just  like  replacing  it  by  a  very  large \nnumber instead of actually counting that number  \n \n\f\n \n the other solution is instead of count you can use something known as pmi  so  this is \nhow  you compute  pmi   even if  you  do not  know  it does  not  made  a lot  of difference \nbecause  you know that it will always be there on the slide that is why you guys do not \nread  anything   so   pmi  is  computed  like  this   so   intuitively  tell  me  what  does  pmi \ncapture  look i would say focus on this formula rather than the upper one  when would \nit  be  high   the  easier  question  to  answer  is  when  would  it  be  low   remember  you  are \ndealing with a fraction  \nso   if  independently  the  two  words  appear  a  lot  of  times   but  together   they  appear  very \nrarely then the pmi is going to be low is that clear  now if both the words appear one hundred \ntimes and together also they appear one hundred times  that is the best case scenario  that means \nthese are very tightly tight words right  they always tend to appear together  \nso  the pmi would be high for words which are very frequently co occurring  now  so \nthis  is  what  would  happen   if  you  replace  the  counts   the  co occurrence  counts  by  the \ncorresponding pmi  ok  now  if the count of two words is zero we have a problem because  \nthen the pmi tends to be minus infinity  right  so  how do you deal with this situation  \nepsilon or some we will use some hack right as usual  \nso  instead of pmi use something known as pmi zero  which works like this  if the count is \ngreater than zero  then you use pmi  if the count is equal to zero  then you just put the entry zero \n \n\fin the set  make sense  there is also something known as positive pmi  which is slightly \nmore extreme  it says that use the pmi only if the pmi is greater than zero otherwise use zero  \ncan anyone tell me the rationale for doing this  you see the subtle difference between \nthe  three  things  right   one  is  of  course  doomed  because   you  cannot  handle  zero  counts   the \nother  one  is  saying  that   if  the  count  is  zero  then   i  will  just  substitute  zero   the  last  one  is \nsaying that  if the pmi is negative right  then i will replace it by zero  that means  in the last \ncase  all  the  cells  in  your  or  in  your  pmi  matrix  would  be  positive  right   non  negative \nrather  \nso  can you tell me the rough intuition for using this and there is only a rough intuition  \nbut can you tell me  so the very rough intuition for this is  that what does it even mean to \nsay that two words are negatively correlated  i mean  either they occur together right  which \nmeans  there is some relation between them  but a negative relation between words does \nnot make sense that is the intuition behind this  now of course  i could argue that what \nabout  antonyms  and  things  like  that   but  that  is  also  not  the  same  right   because  you \ncould have good and bad in the same sentence  right  \nbut  that  is  the  roughly  the  intuition  that  negative  values  do  not  mean  much   so   just \nreplace  them  by  zeros ok  there  is  no  again  a  formal  reasoning  behind  this   but  just  the \nintuition  so  we have looked at the co occurrence matrix where  we started with counts  \nthese counts were very sparse and there are also some other problems with counts in the \nterms of some frequent words taking a lot of limelight and so on  \nso  we have fixed although then  we have done some very minor and simple fixes  and i \njust very rush quickly rush through them because they are very simple  but these were all \nfixable problems  what a non fixable severe problem with this  what is the problem with \nthe one hot representations  large  \n\f\n \nwhat  about  these  representations   still  large  it  is  still  of  size  v   it  is  still  very  high \ndimensional  still very sparse not as parse as the one hot encoding  but still sparse  and it \ngrows  with  the  size  of  the  vocabulary   so  now   remember  that  penn  treebank  at  fifty  k \nwords  google onet corpus at tha thirteen million yeah  so  it keeps going with the size of the \ncorpus  \nso now how do  you know  how do you fix this  i wish i had that harry potter thing  \nanyone remembers that spell to wipe out your memory  how would you deal with it  so \nyou now see how it connects  so now  again you have ended up in a situation where  you \nhave a very high dimensional matrix  right  and you are looking for ways to reduce the \ndimensions  so we will go back and rely on things that you have learned and one of those \nwas svd right  so  you can use singular value decomposition  \nwhy did i say svd and not pca  because  this is not necessarily a square matrix a  this \ncould be a rectangular matrix and for all practical purposes svd is just a generalization \nof pca  \n \n\f"}
{"audio_filepath": "lec009_003.wav", "duration": 870.849, "text": "\nsvd for learning word representations\nso  in this module we will talk about using svd for learning word representations \n\n\nso  what does singular value decomposition do  yeah these are all possible variants  so \npeople have tried various things and one of the ppmi one is the is the most reliable thing\nthat is what is given  but you can think of  i mean you said one there are ten different\nthings which we can do for the co occurrence matrix  right  but this is the most popular\nand most stable thing to do \nyeah what is the single value decomposition do  can you read it from the slide please  it\ngives the rank k approximation of the matrix  so  let me start defining a few things  so \nfrom now on when i refer to the co occurrence matrix i would mean the xppmi matrix\nright which was the positive pmi which was replacing all negative pmis by zero  and just\ndo not have this nasty variable i will just call it as x \n\fso  from now on whenever i say x  i mean the positive pmi co occurrence matrix ok \nso  that is what this matrix is ok and we know that svd gives us this reconstruction of\nthe original matrix  and fine it gives us the best rank k approximation of the original\nmatrix  and it discovers the latent semantics in the corpus  everyone remembers this like\nthat is what we were by we were using pc and svd and auto encoders it was able to\ndiscover some latent semantics  and we will concretize this intuition with the help of our\ncurrent example  but for now i just want you to recall that it helps in discovering the\nlatent semantics \n\n\nnow  notice that this product and i think i have done this in one of the assignments or\nsomething can be written as a sum of the following products  so  i can write it as sigma one\nu one v one transpose  sigma two u two v two transpose and so on  can you tell me what this sum is\nthis is the rank two approximation of the original matrix and i keep taking more terms i get\nmore and more rank approximations of the original matrix ok now  and we all know that \nok  we all hopefully know that what is the dimension of this  it is a scalar vector matrix \nscalar vector matrix \nstudent matrix\n\fok  now of course  you will say matrix  but what is the dimension of the matrix  why is\nit a matrix  it is an outer product of two vectors right this  what is the size of this  n cross one\ninto n cross one so that  sorry one cross n that gives you n cross n matrix  everyone gets this\notherwise how is it a rank one approximation you have to get the original dimensions right \neveryone is clear with this is an outer product \n\n\nand it belongs to r m cross n  ok and if we truncate the sum at the first term we get the\nrank one approximation and by svd theorem we know that this is the best rank one\napproximation \nnow  what does this actually mean  that this is an approximation  what do we mean by\nthat  so  we will see that on the next slide and similarly in the same way if we truncate it\nin the second term you get the same best rank two approximation \n\f\n\nnow  what do we mean by approximation here  actually and i mean to say\napproximation always in this course at least try to think in terms of compression  how\nmany elements are there in the original matrix  m cross n  that is how many elements\nyou need to describe the matrix completely  if you do a rank one approximation how\nmany elements are you using  m plus n plus one right  so  the original matrix has m cross\nn entries  entries and when you do a rank one approximation you have m plus n plus one\nentry  so  that that is the approximation  right  so  you are trying to really compress the\noriginal data using only these many variables you get that  ok\nand if we do a rank two twice this right  so  as many rank i mean as deeper as you go in\nthe sum you will have that many elements to do the approximation  ok  but what is\nimportant is that the svd theorem tells us that this is not just any random approximation \nbut this is the best approximation that you could have done  that means  if you wanted to\nuse only these many elements  these are the best elements to use  right  everyone gets\nthat \n\f\n\nso  as an analogy consider this right suppose you are given eight bits to represent colors  ok \nand this is how you represent very light green  light green  dark green and very dark\ngreen this is what your representation is \nin this original eight bit representation  there is some similarity between the colors  but it is\nstill a bit latent  but now if i were to ask you to use only four bits to represent these colors \nwhat would you do  the lowest significant bits  if you use the first four no then use only\nget very light that is not the essence of that color right  you need the color to be there  so \nif you compress what would happen is  so  that is what happens in when you go from\ntwo hundred and fifty six bit colors to higher or lower  right  the distinctions between the colors go off \nso  all of them would be compressed to green well that is the most important  important\ninformation in terms of the color right  because you need to be able to distinguish\nbetween green and red  as suppose to very dark and very light that is the more important\ninformation that is there  right  so  when you compress it the most important information\nin that entity should be retained  and that is exactly what svd does when it does a\ncompression  it retains the most important information in the corresponding entries is\nthat clear is the intuition clear fine \n\f\n\nso  let us actually do this  so  this is my original co occurrence matrix x  and i just\nrepeat when i say  x i mean x ppmi  and now i have done svd and i have done a low\nrank approximation of it  i do not know what was the value of k i selected  but some\nvalue of k it was definitely greater than one or two  so now  you see a low rank\napproximation of x  what is the first obvious thing that you notice  it is dense now it is\nthe longest sparse \nnow  can you tell me something about the colored entries  what was happening in the\noriginal matrix x  the word system and machine was never co occurring because of\nwhich their value was zero  same for human and user  but remember there is some\nimportant information in this matrix  which also tells you what are the words with user\nappears with and what are the words with human appears with  and that actually gives\nyou intuition that these two words are actually related right same for system and machine\nsystem and machine both would appear in the context of words like interface  install  run\nand so on  so  you know they are similar it just happens that these two words never\nappeared together  so  this similarity between them was latent or hidden in the original\nco occurrence matrix  now once i have done the svd what has happened because i have\nforced it to compress the data  it has retained the most important in information and\nunder that information these two words have actually come closer to each other  right \n\fso  you see that now you have a non zero entry for the similarity between those two\nword pairs do you get the intuition and can you imagine that this would happen with\nsvd\nand what is wrong in imagining you can  but i guess right that is what is happening with\nthis  so  you think about pca you think about svd you think about auto encoders all the\nintuitions that we had build there the same is being applied here  right  all if you get this\nok fine  yeah  after svd you could have right that is not necessary that it should be\npositive in the original matrix you do not have negative entries \n\n\nnow  here is a question right recall that earlier each row of the original matrix x served\nas the representation of a word  ok  this was my original x ppmi not the rank\napproximation now in that case what would x x transpose give me  what would the ij\nth entry of x x transpose be  so  let us look at this toy example you have this x matrix\nyou have xi and xj now i take x transpose  ok \nnow  this is xi this is xj just standing  now what would be the ij th entry of x x\ntranspose  it will just be the dot product between these two right  is that fine  so  this is\njust the dot product between them and we know that dot product is more or less the same\n\fas cosine similarity module over the normalization right  you just need to normalize it by\nthe norms of x and xi and xj in this case  right \nso  i will just assume that this is a substitute for the cosine similarity  ok  so  every entry\nat every ij\u2019th cell in x x transpose is the cosine similarity between the representations of\nthe i\u2019th word and the g\u2019th word  is that clear to everyone  ok fine  and in the original\ncase which was the xppmi the cosine similarity between human and user was zero twenty one \n\n\nnow  once we do in svd what is a good choice for the representation of the word i \nafter svd what is the dimension of x hat  it is again n cross m because it is a sum of m\ncross n matrix  so  that the dimension of x hat is m cross n  although it has been\nconstructed using fewer information  but the dimension is m cross n right  that means \nwhat is the size of the representation of every word  still high dimensional still the same\nn or v  whatever everyone gets that is there any confusion with that  ok \nnow you could say that ok  i will just take the i\u2019th row of the reconstructed matrix and\nuse that as the representation  because i know that now this representation is better some\nof those zero entries have changed they have captured the latent semantics between the\nwords  so  this is definitely better none is denying that that this compression has given us\nbetter representation because we are only keeping the most important information \n\fnow  if i do x hat x hat transpose  remember x hat is the reconstructed matrix  then\nagain by the same argument the ij\u2019th cell actually gives me the cosine similarity between\nthe i th word and the j th word  and you can see that now the cosine similarity between\nhuman and user has actually increased  so  this is just for me to convince you that we\nhave learned more meaningful representations  so now  what do we choose as the\nrepresentation  i have still while computing this cosine similarity i have still used xi\nwhich is high dimensional which has the entire vocabulary as the number of columns as\na representation  right \nso  there are two things coming out of here one is i really like this cosine similarity i see\nthat\nit has improved \nthat means \nthe representations were computing something\nmeaningful  but on the flip side i am still not happy because the representations are still\nhigh dimensional  so  can you construct a wish list for me based on this  i would want\nthe same cosine similarity to be present as given by x hat  x hat transpose right  but i\nwould like to represent it by fewer dimensions  that is exactly what my wish list is  ok \n\n\nso  let us see how do we do that now for no reason i am going to construct a matrix w\nword equal to u sigma what is u sigma  it is the part of the svd right the svd told us it\nwas u sigma v transpose  so  i am just considering this matrix i am going to call it w\n\ffor no particular reason  ok  now  let me take x hat x hat transpose  i can write it as this\nis that fine  now what is the next step \n\n\nwhat does this mean  i want an answer right  this is that aha moment should be there or\notherwise there is no point  what is how many rows are there in w  the same as the\nnumber of words in our vocabulary  what is the dimension of each row  k  so now  w\nword has low dimensional representations for the words in the vocabulary  but while\ndoing this what have we not sacrificed the cosine similarity  the cosine similarity\nobtained by this is actually the same as this  do you get that  how many if you see this is\nvery  very important  that if you have not understood  this everything is meaningless \nso  you see how from svd we got a low rank or a low dimensional representation for\nthe words right  w word is just to be clear k and k is very  very less than v  right  so \nnow  we have representations for words which are much smaller they are no longer v\ndimensional remember in practice this k would be of the order one hundred  two hundred  three hundred and\nremember your vocabulary was of the order fifty k one thousand k and so on  right \nso \nthe huge reduction that you have got  and you have still been able to learn\nmeaningful representations which give you better similarity between related words  right\n \n\f\n\nso  conventionally w word which is u sigma and belongs to m cross k  so  i am sorry\nfor messing this up  but i have used m n and v are interchangeably  so  you would\nunderstand it from context that m is v  and the other matrix which is v is known as the\nw context matrix right what is the size of w context n cross k or k cross n  right \nthat means it has the representations for all the context words and w word has a\nrepresentation for all the target words right  so  we had these words on the rows and the\ncontext words on the column  so  w word has the representations for the rows and w\ncontext has the representation for the corpus  ok \nso  this what we have seen so far  and this is where we learn today  is what a nlp was six\nyears back right before the advent of deep learning  if you wanted to use word\nrepresentations this is what you would do  you would do con construct a co occurrence\nmatrix try these tricks of pmi ppmi positive negative zero and all those things those\nheuristics  then do a simple svd retain the most important one hundred two hundred dimensions and\ntreat that as word representations and use it for whatever you want to do \nnow  what needs to be seen is  what happened with deep learning and how have this way\nof computing word representations changed over the past few years  right  so  that is\nwhat we are going to see in the next lecture  right \n\f"}
{"audio_filepath": "lec009_004.wav", "duration": 90.868, "text": "\nsvd for learning word representations\n\n\nso  we will start from where we left yesterday ah  so  we did this whole story on starting\nfrom co occurrence matrices  we learnt how to get better word representations and the key\nthing there was we used svd as a dimensionality comp reduction tool  and we came up with\nthis neat result that you could use w word as the representation of the mate of the words  it\nhas m rows and k columns where k is very less than the size of the vocabulary \nso  you have achieved lot of compression and you are still able to learn very meaningful\nrepresentations  which you could use for several downstream tasks  what to use these for and\nhow to use these for you will see that later maybe four lectures from now  i mean i say four\nlectures i mean four two hour lectures right  so  it might be more in terms of actual lectures  so \nwe will get to that  but for now we have a way of learning representations for words \n\f"}
{"audio_filepath": "lec009_005.wav", "duration": 2185.006, "text": "\ncontinuous bag of words model \nso   from  here  on  so   none  of  this  that  we  covered  had  anything  to  do  with  neural \nnetworks  say  but  it was important  to  understand the context  and i will  tell  you why it \nwas important to go over the traditional way of learning word representations and then \nwe  will  see  how  it  ties  to  the  modern  way  or  the  neural  network  way  of  learning \nrepresentations  \nso   we  will  start  with  the  first  neural  network  based  model  for  learning  word \nrepresentation  which is known as the continuous bag of words model  \n\n \nso  just to set the context the methods that we have seen so far are known as count based \nmodels because they rely  on these co occurrence counts for learning representations  of \nwords  and the methods that we are going to see now are called prediction based models \nand it will become clear shortly why the term  prediction  and how they learn the word \nrepresentations  \n\fso  in a way in the original thing there was no learning involved of course  you can say \nthat you were trying to learn these eigenvectors and eigenvalues and so on  but it was not \nin the same way as it be as we have been learning parameters of a neural network and so \non right it was not in the same spirit  but now once i do these second type of models  this \ndistinction would become very clear one why is there a learning involved and why they \nare prediction based models  \n\n \nso  the story is we are going to look at continuous bag of words model  then something \nknown  as  skip  gram   so   this  is  the  famous  wordtwovec  model  which  you  guys  have \nalready started looking at then we look at glove word embeddings which is some kind \nof a hybrid between the count based models and the prediction based models  and then \nwe  see  how  to  evaluate  word  embeddings  and  then  end  with  this  depressing  note  that \ngood old svd is just fine  right   so  all the progress that has happened in the past five six \nyears  you could just use svd and still go by  but if you do that you will probably not get \na job right you have to learn these things  \n \n\f\n \nso  now let us start with the continuous bag of words model  so  consider this task and \njust bear me for a few slides that when why this is connected to our problem and all that  \nso   i  am  going  to  consider  ka  task   we  are  here  to  predict  the  n\u2019th  word  given  the \nprevious n minus one words  right   as this is something that you do regularly sometimes \neven in the class when you are whatsapping or smsing  so  this is what you do right you \nstart  typing  he  sat  on  a  and  you  get  this  prompt  that  the  next  word  should  be  chair  or \nsomething like that  right   \nnow  you can think of this as a classification problem  tell me why you can think of this \nas a classification problem  can you tell me what is  so  remember that we have always \nthought  of this  that  there is  always  a  y there is  always  an x  and then  we are  trying to \nlearn this relation from x to y  so  given this example can you tell me what is x here and \nwhat is y here   \nstudent  \n   \neveryone is clear about that  right   so  this is x and this is  y now  i made a statement \nthat  i  could  think  of  this  as  a  classification  problem   right     so   the  minute  i  say \nclassification what is the y that comes to your mind or dash hot by  \nstudent  \n   \n \n\fy  not  i   anything  else  would  have  been  inappropriate   but   so   one  hot  y  is  what  you \nwould expect there right and now what is the size of this one hot vector  \nstudent  size of \n  \nsize of the vocabulary  so  we are trying to predict one of the words in the vocabulary  \nso  you see why this is a multi class classification problem you see that there are many \nclasses and you want to select one of these class  now the moment i say classification i \ngive you an x and y  i will start asking me who will give me the training data for this  so  \ncan you think of training data for this any corpus similar to the one that you are creating  \nright  \n\n \nin specific what you will do is  suppose you have framed it as the following problem that \nyou are given for words and you want to predict the fifth word  so  in general i have call \nit as that you are given n minus one words and you want to predict the n\u2019th word the n that \ni am considering here is four  \nnow  what is going to be the training data for this  if you take any corpus that you have \nbuilt anything right consider all five word windows from there do not get too engrossed \nin the story  so  there are four the first four words you can treat as x and the fifth word \nwould be your y   so  you can construct many such x comma y pairs from the raw corpus \nthat  you  are  creating   any  five  word  window  and  you  can  keep  sliding  this  window  right \n \n\fwhat i mean by that  this is your first training instance x comma y  this could be your \nsecond training instance  so  this could be these overlapping training instances you keep \nsliding this window and you will get many training instances ok you see that  \nso  this task the advantage is that  given the size of the web and so on at least for popular \nlanguages the training data almost comes for free  right   compare this to mnist or any \nother task where you have to actually acquire these labels that this is an apple  this is a \nbanana and so on here  you get the training data for free just need to scrape it from the \nweb  know   no   window  size  is  something  that  you  will  set  right  whether  you  want  to \nlearn four word windows or what do you mean we do not know the window size no  \nso  this again there is a lot of existing literature in the traditional nlp  where various and \nlot of work has been done to figure out what is the right n  so  in most nlp task right if \nyou want to predict the next word  a three word window is enough actually  if you know \nthe  last  three  words  and  you  can  try  this  as  a  mental  exercise  right   if  you  know  three \nwords  you  do  not  really  need  to  know  the  words  before  that   so   this  is  the  mark  of \nassumption with where this is a trigram dependency in the words  right   \nso   this  n  is  not  really  so  difficult   and  in  the  default  tool  that  you  guys  would  try \nprobably they take the value of n is seven that is an overkill  but that is again it comes from a \nlot  of  existing  literature  in  nlp  right  this  is  not  a  this  task  is  not  deep  learning  broad \nright this task is a simple language modeling task  which has existed for many years right \nfrom probably one thousand  nine hundred and fiftys or sixtys or something  \nso  this is all n word windows in your corpus as i said training data comes for free and \nfor ease of illustration we will now focus on the case when n is equal to two  that means  i \nam given one word and i want to predict the next word  \n\f\n \nand we will see how to model this using a neural network  so  these are the two questions \nwhich i need to tell you how to model this task and what is the connection between this \ntask and our original task of learning word representations these are the  two things  that  i \nam going to answer  \n\n \nso  we will model this problem using a feed forward neural network  what is the input  \none word ok so say the word is sat i am going to represent it using a one hot vector ok \nand  what  is  the  output   i  want  to  predict  a  distribution  over  all  the  words  in  the \n \n \n\fvocabulary  and  i  want  to  predict  i  want  to  pick  the  word  which  has  the  maximum \nprobability  that  is  how  you  did   so   for  example   in  the  case  when  you  had  this \nclassification problem of banana  apple  orange  mango  you predicted a distribution over \nthese four classes and then picked the one which had the highest probability exact same idea \nhere it just that instead of four classes now you have v classes and your v is very large ok   \nit is trying to learn a distribution over there  \nand you know that in this case or the example that you are considering on is the actual \nnext word  so  you type sat and the next word is on  and probably leading to sat on the \nchair  or  something  like  that   so   this  is  what  you  would  want  to  maximize   ok   i  have \ngiven you the input  i have given you the output give me a neural network to model this  \nthere are lot of hints in the diagram itself right you see some space between the input and \nthe output  \nso  what will you put in the middle layer we will put a middle layer there  right   is this \nan ok way of modeling this task  i have an input i want to predict an output  so  i just use \na  regular  feed  forward  neural  network  and  let  us  analyze  these  parameters  a  bit  more \ncarefully  right   so  i am something known as w context  i have something known as w \nword  i  am  already  using  some  notations  from  the  svd  lecture   there  at  the  end  we \nended  with  w  word  and  w  context  right  it  is  not  clear  why  i  am  using  the  same \nnotations  but it will become clear in some time  but let us look at their dimensions   \nso  we have this one hot vector i have a parameter w context which i am going to learn \nright and its size is k cross v  so  what does that mean  this matrix is going to multiply \nby the vector and give me a k dimensional output right is that clear  so  i have this is of \nsize v because always keep surprising me i do not know why you cannot do this r this is \na v dimensional vector  you multiply it by a k cross v vector  so  you do w into x  so  \nyou will get a k dimensional vector  so  this is k dimensional you have a k dimensional \nhidden representation   \nand from there now having captured this hidden representation  you are trying to predict \nwhich is the next possible class  this is the same as any other thing right if you had done \nthe image classification or the mnist digit classification  you had this seven hundred and eighty four dimensional \ninput  vector   you  pass  it  through  a  hidden  layer  and  then  you  predicted  one  of  the  ten \nclasses  there is nothing magic here it is the same thing that you have done seen before  \n\f\n \nhow many if you get this and what are the parameters  w context and w word  ok  and \nwe are going to focus on these parameters and understand what they actually mean  \n\n \nso  what is the product w context into x given that x is a one hot vector  so  i will tell \nyou this suppose the i\u2019th entry is hot here  how many if you say it is the i\u2019th column of w \ncontext   so   it is  simply  the  i\u2019th  column  of  w  context  why   because  you  have  this  w \ncontext matrix you take a one hot vector which has the second entry as hot  if you do this \n \n \n\fmultiplication you basically get the second column of w and you can just see it everyone \ngets this now how many if you get this now  \nso  if you have a one hot vector if its i\u2019th entry is on u multiply it by a matrix  you will \nget the i\u2019th column of the matrix  ok  so  if the i\u2019th word is present in the input then the \ni\u2019th element of the one hot vector is on and the i\u2019th column of w context can be would be \nselected  so  then can what can you tell me about the i\u2019th column of w context  you see \nthere is this one to one correspondence between words in your vocabulary and columns \nof the w context matrix  how many columns has w context have  v columns how many \nwords  are  there  in  your  vocabulary   v   any  one  word  is  on  only  one  column  will  get \nselected and that is a unique column it is not going to change  right   so  there is a one to \none mapping between the columns of w context and the words in your vocabulary  that \nmeans  the columns of w context are the are the vector representations  \ndo you know these vector representations  no these are parameters of your network  so  \nthey  will  they  will  be  learned  how  we  will  see   ok   so   you  see  the  intuition  for  w \ncontext  setting  it  up  this  way   so   now   i  have  set  up  the  problem  in  a  way  that  by \nparameter  matrix  directly  gives  me  the  word  representations   ok   but  any  kind  of \nlearning has to be driven by some objective  so  what is that objective it is already clear \nto a lot of you  but we will just do that in a bit more detail  so  this is exactly what i have \njust said  \n\n \n \n\fnow  how do you obtain p on given sat no no  so  for a given training instance so  when \nyou  so you could so  i will  so  for a given training instance you said that  your corpus \nhas  been  divided  into  those  training  windows   right     so   it  is  possible  that  engineer \nsometimes the word does not and is not the next word  but for this training instance what \nis it  so  that is what you have to predict right is that fine  \n\n \nand at test time so you are saying that what you are saying is more practical that when i \nhave typed sat in the whatsapp message  i do not want on as the always the answer  so  \nwe  get  these  five  options  right  three  to  five  options   so   what  could  be  that  you  have  this \nprobability distribution pick the top five from there and show it as options so  is that fine   \nso  we are done with this now how do  you compute  p on given sat  what is the actual \noperation happening there  what is the appropriate output function  this is a multi class \nclassification problem softmax  \nthis is what softmax looks like  so  the property if suppose on is the i th word in your \nvocabulary   then  i  am  saying  that  the  probability  of  on  given  sat  is  going  to  be  this \nquantity  how many of you agree with that  i mean those who agree is fine i am asking \nwhy the others do not agree what is not clear about this  i do not know how to explain \nthis i mean it is just so plain obvious  what is the softmax function  first of all you will \ndo this aggregation so  you will do this w word into h that is fine  right   so  for you will \ncompute this vector consisting of w word into h fine what is the dimension of that  what \n \n\fis  the  dimension  of  that  mod  this  is  k  dimensional  this  is  v  cross  k  or  k  cross  v \ndepending on how you multiply it  so  what is the output going to be v  so  you have v \nentries  \nthese are dash entries the options are normalized unnormalized  unnormalized now what \ndoes softmax do  \nstudent  normalization  \nnormalization  that  is  exactly  what  this  formula  is  doing   right     you  want  for  the  i  th \nword you see what was the end this product right this gave you a v dimensional vector  \nyou  look  at  the  i\u2019th  entry  there  right  that  is  what  you  are  doing  here  raise  it  to  an \nexponent and divided by the summation of all these entries  come on guys this is highly \ndisappointing  i  cannot  teach  softmax   i had in  the tenth lecture eleventh  lecture  of the \ncourse right what is wrong  how many if you get this now just have to ask of it tangle \nyou  \nso   you  see  this  right  this  is  what  is  happening  here   so   you  get  this  v  dimensional \nvector  and  you  just  con  converting  into  a  probability  distribution  using  the  softmax \nfunction  so  now  this value how did he compute this value actually  you computed this \nproduct  which  is  w  word  into  h  and  then  you  took  the  i\u2019th  entry  of  that  and  then  this \nsome transformation on that  the softmax transformation you see that  \n\n \n \n\fso  now i can say that p on given sat is actually proportional to the dot product between \nthe j\u2019th column of w context and i\u2019th column of w word why am i saying that  \nso  remember that this was the i\u2019th word in your vocabulary and on was the j\u2019th word in \nyour  vocabulary   so   can  you  explain  the  meaning  of  this  sentence  to  me   first  let  us \nlook at the first part what is h  it is a j\u2019th column of w context oh sorry this should be i \nthis should be j  so  this you already saw that h is the jet column of w context because i \nam multiplying a one hot vector with the matrix is that fine and what is the i\u2019th column \nof  w  word   so   why  what  is  this  product  actually  equal  to  if  i  say  w  word  into  h  w \nword into h that is a vector  and then i am taking the i\u2019th entry of that  so  i am saying \nthat is the same as taking the i\u2019th column of w word and multiplying it by h  how many \nif you get this is basically in your algebra  right   \nnow  these four different ways of multiplying matrices i am just using one of those  right   \nso   if  i  multiply  a  matrix  with  a  vector  and  then  take  the  i\u2019th  entry  of  that   that  is  the \nsame as multiplying the i\u2019th column of the matrix with the vector  ok  just go back and \nverify  this  just  take  my  word  for  it  for  now   so   now   what  is  happening  is  that  it  is \nproportional to the product between the j\u2019th column of w context and the i\u2019th column of \nw word is that clear now  everyone gets this  \n\n \nso  p word equal to i given sat does depends on the i\u2019th column of w word  ok  \n \n\fso   now  what  can  you  say   so   earlier  we  saw  that  the  i\u2019th  column  of  w  context \ncorresponds  to  a  particular  word  now  what  can  you  say  about  the  i\u2019th  column  of  w \nword  it also corresponds to a particular word  so  now  why these two correspondences i \nalready had a correspondence between w context and every word in my vocabulary  now \ni  am  saying  that  there  is  also  correspondence  between  w  word  and  every  word  in  my \nvocabulary  how many of you first of all are comfortable with the sentence  that  every \ncolumn of w word has a correspondence with some word in the vocabulary  \nthe second sentences every column of w context has a correspondence with some word \nin the vocabulary do you all of you agree with both these statements  okay that is what \nwe  have  try  to  prove  so  far   so   now   for  every  word   that  means   i  have  two  columns \nwaiting for it  how do i deal with this situation  have you ever dealt with it before  the \nsame thing happened in svd also  right   svd also gave you this u sigma which was w \nword  and  then  v  which  was  w  context   so   you  can  always  learn  two  different \nrepresentations  for  the  words  one  is  when  the  word  appears  as  a  context  word  and  the \nother is when the word appears as the target would you get that  you see why we have two \ndifferent representations fine  \nand as i said hope you see the analogy with svd right you already saw there that there \nwere these two representation  now given all this set up and please do not disappoint me \ncan you learn these parameters with some tweaks to the code that you have written for \nmnist  can you use the same code to learn these parameters  how many if you say yes  \nso   what  is  the  tweaks   what  are  the  tweaks   the  input  changes  instead  of  the  image \ninput you have this v dimensional input  what else changes   \nstudent  output  \nthe output changes instead of a ten dimensional output you have a v dimensional output \nall of you are absolutely clear about this and what is the training algorithm   \nstudent  \n   \nback propagation what is the loss function  cross entropy good  fine  \n\f\n \nso   for  some  i  will  do  some  more  stuff  on  this  because  there  is  some  in  interesting \ninterpretations of the gradient descent update rule here  so  i will refer to the word sat by \nthe  index  c   and  the  word  on  by  the  index  w   ok   and  you  already  saw  that  the \nappropriate  loss  output  function  is  softmax   the  appropriate  loss  function  is  cross \nentropy  so  let me just look at this  right   so  w was the index of the output word  so  \nmy cross entropy formula would just boil down to this everyone is fine with this  ok  i \nwill just try to maximize the w th entry in my y hat  how many of you are fine with this  \nokay and that is nothing  but the probability of the word given the context  \nnow  remember that h is equal to w context into x c  i am going to call that as u c  so  u \nc  is  the  dash  of  the  word  sat  title  of  the  lecture   it  is  a  vectorial  representation  of  the \nword sat everyone is fine with that  because that is exactly what this product is going to \ndo and now my y hat w is equal to this because i already said it is the product of the c\u2019th \ncolumn of w context and the w\u2019th column of w word  \n \n\f\n \nso  now i have a formula for  y hat w what is the training algorithm that you will use  \ngradient descent with back propagation  now let us consider one such input output pair \nand see the update rule for v w  \n\n \nso  my loss function is this  this is actually this quantity  ok  now i can just rewrite it as \nthis i have just expanded the log  so  the log of a by b is log a minus b ok now i want this \nquantity  because this is the parameter of the network right v w is one of the columns of \nw context or is it w word w word  v w is one of the columns of w word and i want to \n \n \n\flearn i want to learn it what are the what are these column entries so  that means  i am \ninterested in this particular gradient  \nso  i will start taking this  so  what is it going to be  so  only u c will remain here  of all \nthese  summation  terms  only  one  of  them  would  remain  and  then  you  can  derive  de \nderivative   right     so   this  is  what  it  is  going  to  look  like  what  is  this  quantity   the \nsoftmax function  so  this is what i get  \n\n \nso  now my gradient update rule is going to look like  everyone is fine with this i have \nderived  this  formula  and  i  have  just  substituted  that  here   and  this  negative  and  this \nnegative  ok  so  now let us look at this update rule  \n \n\f\n \nso   this  update rule  has  a very  nice interpretation which allows us  to  understand  what \ndoes the continuous bag of words model actually learn  now suppose y hat w tends to one \nwhat would that mean  your prediction is very correct right you are almost predicting it \nhas probability as oneok what would happen to the update in that case  there will be no \nupdated  if  it  is  one  there  will  be  no  update  if  it  is  close  to  one  there  is  going  to  be  very \nminimalistic update  that means  you have already learned the v w well enough  ok  \non the other hand if i am very bad  if y hat w is close to zero what would happen  just tell \nme the case when y hat w is actually zero what is the update rule  have you seen something \nsimilar  ever  before   have  you  seen  something  similar  before  where  did  you  see  this \nupdate rule  perceptron what happened when you did this  w and x came closer to each \nother the angle between them actually decreased  so  the same thing is happening here   \nso  what you are trying to do is  you are trying to make your word representation closer \nto  the  context  representation  is  that  clear  how  many  if  you  get  this   it  straight  away \nfollows  from  the  update  rule  right  because  you  are  adding  a  fraction  of  your  context \nvector to your word vector and we know that when we add two vectors they come close to \neach other the cosine between them decreases  that is what we proved in the word to it \nlecture in the perceptron lecture  right   \n \n\f\n \nso  you can go back and refer to this slide on lecture two  now  so  the training objective is \nessentially  ensuring  that  the  cosine  similarity  between  v  w  and  context  word  is \nmaximized  between the word and the context word is maximized  \n\n \nnow  what is the result of this  now i want you to think go back with a starting example  \nwhere we said  that  we  want  to  learn representations  such that cat  and dog  are close to \neach other  but cat and truck are not close to each other  \n \n \n\fi want  you to think whatever you see on this slide  the conclusions that you drew from \nthis slide  how do they help you to relate back to that initial goal  ok  so  now  let us let \nme give you the intuition  right   so  what happens to the representations of two words w \nand w prime which tend to appear in the same context c  so  say dog eats cat eats  right   \nso   dog  and  cat  are  two  words   which  appear  with  the  same  context  eats   so   what  will \nhappen to the representation of dog  it will come close to eats  what will happen to the \nrepresentation  of  cat   come  close  to  eats   not  only  that  dog  will  also  go  close  to  pet \nanimal  sleeps  right  and  so  on  and  cat  will  also  go  close  to  these   so   transitively  what \nwill  happen   dog  is  going  close  to  a  certain  point  or  certain  sets  of  points   cat  is  also \ncoming close to  the same set  of points  so   transitively dog and cat  will come close to \neach other you get this intuition  \nanyone  sees  a  problem  with  this   no   so   known  objective  and  i  said  that  dog  comes \nclose to eats is that what he wanted i mean  why should dog be close to eats  that means  \nif i find the nearest neighbors of dog  i will get words like eats  sleeps  barks and so on is \nthat what i wanted  so  that is exactly what is happening  and based on that i convinced \nyou that dog and cat will come close to each other  but there is a subtle gap here i want \nyou to  close that gap how many matrices do we  have  two  that is  enough  hint  we are \ngoing to either take columns of this matrix as the representations or the columns of this \nmatrix as the representation not mixed  \nso  now can you tell me  so  dog here will come close to eats  sleeps  barks  here word \nwill come close to context word right  cat here will come close to eat  sleeps  and so on  \nright   so  transitively dog and cat will come close to here and this is the representation \nthat you care about not representations across these two matrices ah  so   what i said is \nthat  the  training  rule  ensures  that  the  words  representation  comes  close  to  the  context \nwords representation ok that is what we saw with the training update rule  \nso  that means  dog will come close to any kind of context word that it appears with  so  \ndog  i  would  expect  it  to  appear  with  context  words  like  eats   pet  animals   dog   barks \ndrinks and so on  right   so  dog is coming closer to these words  and i expect cat also to \ncome up here with these words and of course  i do not expect truck to appear with these \nwords  right   so  then cat will also come close to these set of words  dog will also come \nclose  to  these  set  of  words   so   transitively  dog  and  cat  will  come  close  to  each  other \n\fright all of them are coming close to each other  ok which is fine which was my original \ngoal  \nbut my original goal was not that dog and eats should come close to each other  because \neats  and  dog  are  neither  synonyms  when  they  do  not  have  any  semantics  i  mean  they \nhave a semantic relation  but that is not what i wanted  i wanted similar words to come \nclose to each other  but now i have the side effect that dog is coming close to eats  but \nthat is bad was how can i live with that  \nso  the i mean the key thing that you should notice is that  you have one matrix of words  \nthe other matrix is of context words  so  the representation of dog in the word matrix is \ncoming close to the representation of eats  sleeps etc in the context representation on the \ncontext  matrix   the  representation  of  cat  is  also  coming  close  to  these  words  in  the \ncontext representation and transitively because of this dog and cat in the word matrix are \ncoming close to each other and this is the matrix that we care about  \nin this matrix dog and eats  dog and sleeps are not close to each other right is that fine \neveryone  gets  this  now   ok   so   this  is  only  an  intuition  and  this  becomes  very  tricky \nwhen i will blow this up  what do i mean by blow this up  right now what am i trying to \ndo what is the size of n  two right i am taking one word and outputting the other word  \nhence you get all these neat interpretations that you are moving close to that vector and \nso on  the moment i add more words to n these interpretations become more and more \nhard right  but this again i mean this is good to understand that this is what happens at \nleast in the best case  so  this is only an intuition which is reasonable in  my opinion  i \nhave not come across a formal proof  which says that this is what actually happens and \nthat is one criticism for wordtwovec  it works very well  but there is no formal proof which \ntells you why exactly it works  \nas opposed to  svd  right  there we  know there is  a principle behind  it  here that is  not \nvery clear right  but it works very well based on this intuition  ok  so  everyone gets the \nwhole  set  up  how  we  started  with  a  classification  problem  of  predicting  the  n  th  word \ngiven the n minus one words  which had nothing to do with word representations that is a \nsimple language modeling problem which has existed forever  we smartly modeled it or \nsomeone  smartly  modeled  it  using  a  neural  network  such  that  the   parameters  of  the \nneural network end up giving you the word representations  and this network is end to \n\fend  trainable  using  an  objective  function  the  training  data  comes  for  free   for  popular \nlanguages  you have like tons of training data the entire wikipedia entire web whatever \nyou can scrape  that is why with more and more training data you can learn even better \nand better representations  so  for popular languages the representations are really good  \nand then we saw an intuitive explanation for why this works because of this movement \nof  things  closer  to  each  other  and  the  key  thing  to  notice  there  are  two  different \nrepresentation matrices one for the words one for the context  and this is not surprising \nthe  same  thing  happened  for  svd  also   u  sigma  was  w  word  and  v  was  w  context  \nright   so  it is all in the same spirit  right   \n\n \nnow  in  practice instead of window size of  one it is common to  use a window size of d  \neither d could be four or seven i have i have even say and seen eleven actually  but not beyond that  \nok  now let us see what happens if you have two and here itself it should become clear that  \nnow those interpretations are not very neat  so  what i will use suppose i want to take a \ncontext of two words  then i have he sat and now i want to predict the next word  \nso  what is my input now  he and sat  right   so  i will take the one hot representations \nof he and sat  i will just concatenate them sorry i just concatenate them is that fine and \nmy input now belongs to r raise to two v  in general it will belong to r raise to d v ok and \nnow  what  is  the  next  step   do  you  see  something  funny  here   i  have  just  created  two \ncopies of this  ok i  am  telling  you an inefficient  way of doing this  later on it will  be a \n \n\fvery simple thing to do a very efficient way of doing this right  but first just to get the \nmath around i will just do inefficient way of doing it  \nwhy have i staged it twice two words  right   so  now  my h is actually going to be the sum \nof all the columns of w which correspond to my input words is that fine  i have to earlier \ni had just one word as the input  so  my h was just equal to that column of w  now my h \nis going to be equal to the sum of all the columns of w corresponding to the words that i \nhave and i will tell you why  so  i have taken w contexts comma w context which is just \nthe w context matrix staged twice back to back  so  this was my w matrix  this is my two \nhot vector because i have two inputs now  right   so  my vocabulary size is three  so  the first \none hot vector followed by the next one hot vector and now i am going to repeat w w  \nnow  what  is  the  product  of  this   is  the  sum  of  the  two  columns  that  you  see  highlighted \nright and exactly that is what i have written here  \nso  if i do it this way then i can just do this very expensive matrix multiplication and to \ndo a something very trivial which is just taking the sum of two columns  right   but at least \nyou  get  the  operation  and  i  will  just  on  this  next  slide  or  something  i  will  tell  you  an \nobvious simple way of doing that  so  i just get the sum of the two columns  so  that is the \ninput to my network  if i had k words as input if i had my window size four what would it \nbe  i would have these four copies of w context i will have these four one hot vectors and it \nwill just give me the sum of those four columns  ok that is going to be the input ok and the \nrest of the story remains the same  right   once you have this h the rest of it from there \nremains the same and this is the formula for h in general  \nin the special  case it was just  the  i\u2019th  column  in the general  case is  the sum  of all the \ncolumns that are there in your input  \n\f\n \nnow   in  practice  of  course   this  is  a  very  mate  expensive  matrix  multiplication   it  is \nstupid to do it that way  what you will do is you will just slice of those columns from w \ncontext right and then just add them up  so  you do not really need to do that stupid mate \nmatrix multiplication because you know that the matrix multiplication is essentially just \nselecting these columns and adding them  so  just select those columns and add them up  \nso  you do not do that bad matrix multiplication operation  is that fine  \n\n \n \n \n\fnow   what  happens  during  back  propagation  in  this  case  in  the  generic  case   the \nordering does not matter is what you have seen yes it does not matter yeah there is some \nassumption  of  the  model   so   it  is  that  is  why  the  name  of  bag  of  words   you  are  not \nrelying on the sequence  so  this comes from nlp that if you rely the sequence you call \nit sequence  if you just going to take the words in the sequence  you just call it a bag of \nwords  because once you put them in a bag there is no ordering there right that is why \nthe word name bag of words  \nso   and  again  p  on  given  sat  is  given  by  this  softmax  formula   ok  now  tell  me  during \nback propagation and if you give me a right answer to this i really feel  happy that  you \nhave  understood  everything  right  from  the  beginning  of  the  course  so  no  pressure   so  \nwhich are the parameters which are going to get updated during back propagation  which \nare the two large matrices w word and w context  so obviously  the answer is not w word \nand w context otherwise i would not have asked you  the answer is some dash of these \ntwo some subset of these two  which subset let us start with w context which is the input \ndo we are we going to update the entire w context  did it participate the entire w context \nparticipate in the computation  only those columns corresponding to the words  so  only \nthose parameters will get updated   \nso  how many columns will get updated  d columns  right   w word till all the columns \nof w word participate in the computation how many of  you say  yes  how many if  you \nsay no  the others do not care  can you just focus on this circle did all the columns of w \nword participate in the computation  you see the summation at the bottom  it is over all \nthe  columns  of  w  word  all  of  them  participated   so   the  parameters  which  will  get \nupdated are w word and all the columns of the input words and same back propagation \nwill work again is that fine  \nso  remember that and this is i cannot emphasize it enough  whatever i have explained is \nonly for an intuitive explanation  you will never ever do this matrix multiplication right \nand that is why what  you are  going to do is  you are just going to select  those columns \nadd them up and feed them  and the network will take care or rather you will take care \nthat you update those parameters only and you do not update the entire w context matrix \nbecause anyways  there is  no gradients flowing to the other components  so   remember \nthat  in  the  practical  implementation  of  w  of  word  two  vec  do  not  search  for  this  matrix \n\fmultiplication at  the input   or if  you are writing  the code on  your own which is  highly \nunlikely do not do it that way  \nso  if you whatever code that you look at did not have this complex matrix multiplication \ntypically   they  will  just  pick  up  the  columns  and  add  them  and  feed  them  right  and  i \nthink the tensor flow way of doing is you have this word embeddings matrix and you can \nslice  columns  from  there  and  so   so   everyone  understands  this  so  far   now  what  are \nthese problems with this  why is this not as simple in some sense as the mnist data set  \nagain focus on the circle  this softmax computation is a very expensive operation right \nyou have a v cross k sized matrix somewhere there  and unlike at the input here you will \nhave to do this matrix multiplication  right   \nso   we  have  a  v  cross  k  matrix  multiplied  by  a  k  cross  one  vector   and  there  is  no \nsimplification of this  you have to do this multiplication what are the sizes of v that we \nsaw in practice  fifty k  one hundred k and if you had googled thirteen million or something  right   so  \nthis is not feasible we cannot do this expensive matrix multiplication  \n\n \nso   although  all  of  this  works  very  fine  we  need  to  think  of  ways  to  simplify  this \nsoftmax computation  where the denominator requires the summation over all the words \nin the vocabulary  so  you have to do that many matrix multiplications  \n \n\f"}
{"audio_filepath": "lec009_006.wav", "duration": 648.889, "text": "\nskip gram model\nso  with that we will move on to the next model  i am still not telling you how to solve\nthis problem  we will come to that later \n\n\ni am just going to the next model which is the skip gram model  ok and this is the\nfamous wordtwovec which you are trying to implement \n\f\n\nthe model that we just saw was known as a continuous bag of words  it predicts the\noutput given the context  skip gram model does the reverse of that \n\n\nyou are given a word  you want to predict all the context words  so now  i am given the\nword on  i am trying to predict the words which appear on the left and right side of it is\nthat fine  so  how many prediction problems am i solving  how many predictions am i\ngiving you  four in this case  right  so  you see that this is a case where your y actually\n\fbelongs to r four right  of course it is not r four it is four into v  and because you are predicting\nthe entire distribution  but what i meant is that you want these four different outputs you\njust do not want one single output \napart from that does everything else remain same  you have an input word you compute\na hidden representation from that hidden representation you try to predict the outputs \nyou get a probability distribution  what is your loss function  it is a dash of cross\nentropies sum of cross entropies  how many cross entropies do you have  four in this case\nand also notice that i have  i hope i have yeah  i have changed w word and w context\nthey are flipped now is that fine  the role of context and the word has changed \n\n\nin the simple case when you are trying to take one word as the input  and only predict\none word around it  it just becomes the same as the first case that we saw in the\ncontinuous bag of words  there is no difference there because there also you take one\nword and predicts the other word \nso  the entire math\u2019s remains the same how many of you get that  and even when we\nhave multiple context words  our loss function is just going to be the sum of the cross\nentropies for all those d predictions that i need to make or d minus one predictions that i\nneed to make  and then once i see a loss function which is a sum of some things i am not\n\fworried  because i know how to deal with each of these components and gradients are\nadditive  so  if you have the gradient of some  it is just the sum of the gradients  so  as i\nlong as i know how to deal with one of these i can deal with the sum  so  that is why i do\nnot really worry all of you are at that level \n\n\nwhere you do not worry with the sum as a loss function  what are the problems with this\nalready written there same as the bag of words right \nnow  we are doing these four expensive computations at\nthe end  so \nthe softmax\ncomputation is expensive  there are three different solutions  and there are three different ways\nthat we can deal with it one is something known as use negative sampling  the other is to\nuse contrast of estimation and the third is to use a hierarchical softmax  so  we are going\nto see all of these and i will shamelessly continue for a few more minutes  so  first we\nwill see use negative sampling because that is very easy \n\f\n\nso  let d be the sat set of all correct w comma c pairs in the corpus  what do we mean by\nthat  all words which actually appeared in the word comma context pair  so  you can\nlook at the vector  which i have constructed  so  sat on sat or sat chair you can imagine\nthat all of these appeared in the context of each other  so  this is my correct corpus as\nfrom what i got from my data \nnow  let d prime with a the set of all incorrect w comma r pairs in the corpus  and r here\nstands for random  some how am i going to construct this corpus  so  i take a word i\nknow all the words which appeared with it and i know all these other words  which have\nnot appeared with it  so  i will randomly sample a word from there and put it as r  is that\nfine  so  i can compute d prime again d prime comes for free  d was always for free\nnow d prime is also  obviously  for free \nso  i have w comma c and w comma r and i have these corpora d and d prime  and as\nbefore let v w be the representation of the word  and u c be the representation of the\ncontext word  ok  so  v w will try to these two and you see will try to this and u r\nsomething else that we will use for this hopefully  is that fine  okay \n\f\n\nnow  for a given w comma c which belongs to d  which is the true corpus  what are we\ninterested in maximizing  so  let us think of z is a random variable weather which tells us\nwhether this is a true pair or not  so  given w comma c  i want to maximize that p of z is\nequal to one  ok  now  this is what i want to maximize  now it depends on me how do i\nmodel this probability  so  can you guess how am i going to model this  the answer is\nthere in the figure  can you tell me what is the model that i have chosen  can you tell me\nwhat is the formula for z equal to one given w comma c that i have chosen  this stands for\ndot product this stands for the sigmoid function \n\f\n\ni know this is some uc representation  this is some vw representation note that these\nrepresentations are not learned yet i need to learn them using the training objective that i\nset \nbut at the beginning they are initialized to some random values  and the way i am going\nto model probability of z equal to one given wc is that i am going to say that  it is just the\nsigmoid function of the dot product between them how many of you get this are you\ncomfortable with this  this is the modeling choice which i have made or rather the\nauthors of skip gram  right  now how am i going to now what do i want to do for all w\ncomma c belonging to d  i want to maximize this probability is that fine  for all the w\ncomma c pairs which belong to my true corpus which is the d corpus  i want this\nprobability to be high  how many such pairs do i have many many right  so  let us call\nthem as i have n such w comma c pairs \nso  can you tell me what my loss function is going to look like maximize this for the first\npair and for the second pair and for the third pair all the way up to the end pairs \n\f\n\nso  what is it going to look like for every w comma c which belongs to my correct\ncorpus  i want to maximize that probability of z equal to one given that  w comma c pair\nright  and since it is an and i will have this product how many of you are comfortable\nwith this \nso  this as such and this is something that you do regularly you should have done this in\nmachine learning or pattern recognition or somewhere right  that you want to basically\nmaximize the log likelihood of the data  which is saying that you want to maximize the\nprobability of every training instance  which is saying that you want to maximize the and\nof all these probabilities right be you take the and of all of them is that fine \n\f\n\nnow  for the other case wr belonging to d prime  what is it that i want to maximize \nthis probability right because i know this is an incorrect pair  so  i want my random\nvariable to output zero  ok \nnow  what is this going to be the probability one minus the probability  that it was correct \nand that actually if you just simplify a bit it turns out to be this  now for all the elements\nwhich belong to d prime  what is the objective function that i have  i want to maximize\nthis for the first w comma r random pair for the second w comma are random pair and so\non for all the random pairs in my corpus  so  it is just going to be a product of all these\nprobabilities  is that fine  so now  what is my total objective function \nfor every pair in d maximize that  for every pair in d and for every pair in d prime\nmaximize the zero probability  so  what is the total going to be is this fine \n\f\n\nhow many of you agree with this  so  for everything belonging to d i had this and rule\nfor everything belonging d prime  i had another and rule and i am interested in both the\nacts right maximize for d and maximize for d prime of course  different quantities for d\nand d prime ok fine  so  you get this once you basically take the log and so on  so  this\nis a simple set of math operations that i do you will end up with this neat formula ok \nthat for all the w comma c pairs belonging to d  you want to maximize this quantity\nwhich means  you want to maximize what  you want to maximize the when will this\nquantity be maximized  when the dot product between the two is high  that means  again\nwhat are you doing  we are trying to bring the context vectors close to the word vectors\nagain transitively what will happen the words  which appear in the same context will go\nclose to each other  what is the additional thing that you are ensuring here  the words\nwhich do not appear in the same context you are trying to push them apart  why \nbecause of second loss function \nyou see the difference between the two now  in the first case you are only opt i mean you\nare obsessed with bringing things close together  here you are also focusing on the case\nthat where you do not want certain things to be close together because they never appear\n\fin the same context is that fine  so  you see that this is a more powerful loss function in\nthe earlier one  so  that is what the skip gram model does \n\n\nand in the original paper mikolov et al sample k random pairs for every positive pair \nright \nso  that means  if your size of d was n what was the size of d prime b k into n  so  that\nthey had that many positive examples and k times that the number of negative examples \nand this was a hyper parameter which was tuned and they used a value of k such that it\ngave them the best results  also remember that we have this problem of constructing w\ncomma r  now i said that consider all the words which do not appear with your word \nand sample from there and put something there  so  they used a slightly that means  how\ndo i sample that  one is i assign all the words a uniform distribution that every word is\nequally likely  what is a better way of doing that  okay i think i just finished this next\ntime \n\f"}
{"audio_filepath": "lec009_007.wav", "duration": 504.4029375, "text": "\n\n\nso  what i will do is i will quickly  go over what we were doing yesterday  and then by\nthe time people come in we can start with the new stuff  right  so  we were looking at \nso  that is so this needs to be corrected someone who pointed out yesterday  same as bag\nof words  it should be same problems as the bag of words model  so  we are trying to fix\nthis problem where we have this large softmax computation which is very inefficient \nand you wanted ways of getting rid of that  so  the first thing that we were looking at is\nusing negative sampling \n\f\n\nand here the key idea was to con construct this d and d prime  where d prime was the\nrandom corpus and d was a true corpus \nand how do you create this random corpus is something that  was left at the end and\nwhich i need to go over today \n\n\n\fso  i will go over that and then we realize that this actually could be modeled using such\na network  where you take the dot product between the word representations \n\n\nand try to maximize this to dot product for all the correct pairs by setting up your loss\nfunction accordingly \n\n\n\fand try to maximize or rather minimize this dot product  minimize this dot product for\nall the incorrect pairs by again setting the objective function appropriately \nso  we had this objective function where we want to maximize the probability that the\npair is correct for the correct pairs and maximize the probability  that the pair is incorrect\nfor the incorrect pairs  and both these probabilities we had modeled using a sigmoid\nfunction  and inside the sigmoid function we had the dot product between the\ncorresponding representations \nso  the net effect is you either maximize the dot product of the correct pairs  or minimize\nthe dot product of the  or rather and in minimize the dot product of the incorrect pairs\nfine \n\n\nand then so now today the part which was remaining about the comparison between d\nand d prime  so  what i was saying last time is that d prime is actually k times d  that\nmeans  in sample more negative examples than positive examples  so  if you think about\nit actually the number of negative examples in the language is much  much more than a\nnumber of positive examples  let us say if you have fifty k words in your vocabulary most\nof them do not appear together  right  so  that number is actually very  very large as\ncompared to the number of words which can occur together \n\fso  how do you account for this natural imbalance  so  they said that if you keep it\nsame  then we are saying that the size of d prime and d is going to be same  that means \nthe words which appear together and not to appear together we are keeping those two\ncorpora as the same  so  that does not sound reasonable  so  they decided that we will\nkeep it k times  ok  now this k was a hyper parameter which was tuned based on the\ndata that they had  and can you guess  how they would have tuned it  no what do you\ntune your parameters on  what did how did you tune your parameters for the back\npropagation of the word  no using  what \nstudent  validation set \na validation set is it too early in the morning  it fine validation set  so  they might have\nhad some validation set  and if you look at the original word to word code which\nsomeone had posted yesterday  which allows you to compute the distance matrix right \nso  you could what you could do is you could learn these representations  take a few\npairs of words  and take a few pairs of good words right say cat and dog or cat and\nfeline and so on  and also bad words like cat and truck bad combinations rather  and see\nif the distance between cat and truck is much  higher than the distance between cat and\nfeline or cat and dog \nso  you select that k which gives you the best performance on your validation set and the\nvalidation set here would essentially be to find if you get good representations for word\npairs that you care about and for word pairs that you do not care about  ok  now the\nother thing was how do you create this r  so  you have v words in the vocabulary you\nare looking at one of those w \nyou know that some of those have appeared with w in some context  but there is this\nlarge set which has not appeared with w in any context right  so  you are going to draw\nr from this set and the simplest\nthing to do would be to just draw the uniform\ndistribution  that means  all words and let us call this suppose there are capital r words\nhere all of these words could be drawn from using the probability one by r  where r is\nless than v \n\fis that fine  that is one way of doing it just randomly pick any word from the remaining\nwords and put it a pair it with w  but you would also want to account for the individual\nfrequencies of those words  right  if the word is actually very frequent  pair it up more\nwith w  if it is not frequent do not pair it up enough  does that make sense  so  i could\nactually use the frequencies of each of these words  and sample according to that\nfrequency right  instead of using a unigram distribution \n\n\nso  they did something similar  but they had this hyper parameter again  so  basically i\nwas sampling using the probability of r  which is equal to count of r divided by the\nnumber of times number of all the words in the corpus  that is actually the frequency of\nr divided by the total number of words in the corpus  so  instead of just taking that they\nhad this wearied factor of three by four  do you we realize that if you take this three by four you get\nthe best performance \nso  let me just make a few comments on that  so  the original code of or rather the\noriginal skip gram or the bag of words model  actually worked very well and it kind of\nhard a lot of seminal effect or a lot of revolutionary effect on the field of nlp right  so\nnow  everyone started talking about word vectors  and how you can use this meaningful\nrepresentations of words  as features for various down steep nlp thus right \n\fso  at the end in nlp what you are doing is you are collecting of a bunch of words a\ndocument or a sentence or something and trying to do some processing on that  now\nearlier used to construct features out of these sentences using some handcrafted features \nbut now someone said that there is this automatic way of constructing word features\nright  which is using this method  so  people really bought onto that idea and a lot of\nwork started happening  and then later on at the end of the course we will see something\nthat what it eventually led to \nbut later on when people started analyzing this more carefully right  they realized that\nthe original wordtwovec implementation  had a lot of these heuristics or lot of these\nparameters  which need to be really tuned to the core for it to be able to compete with\nsvd right  so  that is what we look at the end  so  svd was already one way of\ncomputing word representations ah  which while popular was not so popular it was used\nfor various reasons  but\nit was not\nlike every npl application is using svd\nrepresentations right  but now it is almost like every npl application is using word\nrepresentations \nso  later on we will see that some of these things like three by four or k the value of k  the\nvalue of learning rate and some other hyper parameters  if you really tuned them very \nvery well\nit\nis only then that as this wordtwovec algorithm can beat\nthe world\nrepresentations learned by svd  or rather the other thing that if you introduce some\nparameters in svd and tune them  because remember for svd there was no tuning right \nwe just got a solution  we just had the closed form solution which is the eigen vectors \nbut you could do some things for creating the co occurrence matrix  if you introduce\nsome factor there which is also looks like this three by four or something like that  or if you\nalso introduced something which looks like a k  then you will be able to get the same\nkind of representations or equally powerful representations from svd as what you get\nfrom wordtwovec  so  that is why i am stressing on these hyper parameters there is some\nsignificance of those \n\f"}
{"audio_filepath": "lec009_008.wav", "duration": 414.345, "text": "\ncontrastive estimation \nso   we  will  move  on  to  the  next  way  of  dealing  with  the  expensive  softmax   so  \nremember that  so  this is known as contrastive estimation   \n\n \nso  remember that this is where we are in the story that  we saw the bag of words model \nwe saw the skip gram model  and we saw that both of them have this expensive softmax \ncomputation at the end  and that is the problem we are trying to deal with  so  we saw \none way of dealing with which was negative sampling  so  you i hope you saw that there \nwas no expensive computation there  \nthe  only  computation  there  was  the  dot  product  between  the  two  words   which  appear \ntogether  or  which  do  not  appear  together   now  let  us  see  what  happens  in  contrastive \nestimation  \n\f\n \nso   here  again  you  use  a  same  idea   so   you  have  a  positive  sentence  or  a  positive \nexample he sat on a chair  you create a negative sentence which you replace the word by \nsome  random  word   now  you  construct  a  feed  forward  network  like  this  which  takes \nthese two one hot representations  basically uses your word context matrix  to give you the \nsummation of these two representations right  that is exactly what we have done in the \nskip  gram  model   now  you  have  this  hidden  representation  which  is  the  sum  of  the  two \nword representations  \nnow from here on instead of doing this softmax computation which we had earlier  we \njust predict a single score ok  we just predict the score for this word pair being of correct \nword  pair   we  do  the  same  thing  with  the  random  pair   so   we  take  sat  we  take \nabracadabra  and  the  add  up  there  word  representations   you  get  this  hidden \nrepresentations  and  you  get  a  score  sr  fine   so   what  is  the  output  computation  right \nnow   what  is  the   is  it  a  matrix  operation   is  it  a  scalar  operation   is  it  a  vector \noperation   what  is  this h  is  equal  to   we  need  to  change  this  to  k  on  the  slide  please \nnote  so  what is this product w into h  just a dot product between two vector  \nw is just k cross one  that means  it is a vector  so  as compared to k cross v earlier  we \njust  have  k  cross  one   you  get  that   how  many  of  you  get  this  we  have  a  very  simple \ncomputation at the end  ok  but now how we set up by loss function  earlier i could set \nup  the  loss  function  as  maximizing  the  log  like  it  of  the  correct  word   but  now  i  just \n \n\fpredicting  two  scores   so   what  is  the  loss  function  what  should  i  try  to  intuitively  do  \nand today we are going to see a new loss function which we have not seen earlier  so  \ntry  to  think  about  this   what  would  you  do   forget  about  the  math  forget  about  the \nmachine learning all that  what  would  you actually  want  what  is  your wish list  that \nshould be easy to characterize \nscore s score sr do you want this or this first one right  you want s to be greater than sr  \ncan you think of making an objective function out of this you want to maximize  \nstudent  \n   \n s minus sr   fine that is a good starting point  so  would you be happy with this  what \nwould  you  want   this  or  this   both  cases  s  is  greater  than  sr  right   what  would  you \nwant  \nstudent  a big margin  \na big margin fine \n\n \nso  we would like sr to be greater than s and not just  so  we could try to maximize s \nminus sr ok but we would also like this difference to be a certain margin  that means  i \nwould want s to be greater than sr  by at least a margin of m  and that m is something i \nwill decide  so  i could say that it should be at least ten points greater than sr or one point \n \n\fgreater than sr  depending on the scores that i have  so  all my scores are between zero to one \nthen probably a margin of zero three or zero four is ok sounds reasonable right  that means  s could \nbe zero six and sr could be zero two does that make sense  \nso   what  i  am  saying  is   what  i  am  trying  to  say  is  that   this  is  my  sr   i  want  s  to  be \ngreater than sr  i am not just happy with that i am saying that even if i add a margin to sr \neven then this condition should hold right  \nand  that  is  the  same  as  saying  that  s  sr  and  there  should  be  at  least  a  margin  of  m \nbetween that  that is the difference that  i accept  i am not if you tell me that s is zero ninety nine \nand sr is zero ninety eight where then you are not really distinguishing much  i want at least s to be \nzero nine and sr to be at least less than zero five or somewhere  \n\n \nso   there  should  at  least  some  gap  between  that  and  that  gap  is  m   so   instead  of \nmaximizing  s  minus sr   i  am  going  to  maximize  s  minus  sr  plus  m   is that  fine   ok  \nnow suppose you are at some point of training   i will have some need some parameter \nconfiguration   that  means   you  have  learned  some  values  for  vc  and  vw   and  you  do \nthis  forward  propagation  compute  s  and  sr   and  we  actually  find  that  this  condition \nholds right  so  right now my loss function is this at some point you are doing this  and \nyou observe that this condition holds  that means  s is actually greater than sr plus m  in \nthat case  what do you want a loss to be  how many of you get the question  \n \n\fi  want  that  s  and  sr  should  be  separated  from  a  margin  of  m   in  the  favor  of  s   i  am \ndoing my training i am at certain configuration for uc s and vw s and so on  i pass it \nthrough the feed forward network and i get s and sr  and i observe that this condition \nalready holds  \nis my network doing anything wrong at this point  it is doing it is job properly  what \nshould  be  the  loss  that  i  back  propagate   zero   again  gets  that  there  is  nothing  to  correct \nhere  i do not need to back propagate any loss  \n\n \nso  then can you give me the full objective function  maximize this  but at this condition \nalready  holds  then  do  not  do  anything  is  that  fine   so   that  is  about  this  so  and  again \nobserve that we have gotten rid of the expensive softmax computation  \n \n\f"}
{"audio_filepath": "lec009_009.wav", "duration": 799.866, "text": "\nhierarchical softmax \n \n \nthe  next  one  is  a  bit  tricky  so   the  third  solution  is  to  use  something  known  as \nhierarchical softmax  this is a bit counterintuitive in the sense it is a very smart trick  \nbut it is not something which is very obvious  so  just pay a bit attention on this it is a \nneat way of handling this large vocabulary thing  and this i think used in various nlp \napplications  where speed is important not often  but wherever speed is important  \n\f\n \nso  this is what our original network was  this was the either you take it as a skip gram \nmodel  or  you  take  it  as  a  continuous  bag  of  words  model   right   let  us  take  it  as  a \ncontinuous bag of words model  \nyou  had  a  word  as  the  input   and  then  you  had  this  large  prediction   and  you  had  this \nsoftmax computation which gives  you the probability  and  you are trying to  maximize \nthis probability for the correct word  right where v w is the correct word   \n\n \n \n \n\fnow  instead of this the hierarchical softmax says that you construct a binary tree such \nthat  your  tree  has  how  many  nodes   v  nodes   it  has  one  node  corresponding  to  every \nword  ok  and there exist a unique path from the root node to every leaf node  every leaf \nnode corresponds to a word and there is a unique path from the root node to leaf node  of \ncourse   there  will  be  overlapping  things  for  example   for  this  word  the  path  is  these \nnodes  and for this word also the path is like there is some overlap in the path  \nbut for every word there is a unique path  how many if you get that setup  now let lw one \nlw two up to lw p be the nodes on this path  so  i am calling this as lwone  lwtwo  lwthree sorry  \nsorry  sorry  sorry yeah actually it is  so  actually this is l on one  l on two  l on three  that means \nthe third node on the path of on  the second node on the path of on and so on  right that is \nhow it is going to be and let pi w be a binary vector \n\n \n   so   what  is  the  size  of  pi  w  actually  binary  tree  log  of  v   right   so   the  size  of  pi  w \nvector is going to be log of v  so  if there are eight leaf nodes  you will have three nodes as the \nsize of the vector  so  for each of these things  this vector takes on a value one so  here the \nvalue would be one  because the path branches to the left if the path branches to right  \nthen the value is going to be zero  right  so  for every node or every word i have this way of \nuniquely defining it is path  i can say that the path is one zero zero  is that fine  for the word on \nthe path is one zero zero  if i consider some other word the path would be different  is that fine  \n \n\fand of course  i have assumed there are only eight words here  right that is why this holds if \nthere are either otherwise i would have a vector whose size is log v  right now my v is eight \nso  it is just three  \n\n \nfinally  each of these internal nodes is associated with a vector  ok so  i have u one u two u three  \nso  how many of these would i have  if there are v nodes at the leaf  how many non leaf \nnodes do you have in the binary tree  v  you all know this  right  \nso  if you have v nodes at the leaf then you will have v nodes internally  so  for each \ninternal node  i have a vector associated with it  so  how many vectors do i have in all  \nu v and my input side is still the same  right  i have this w word or w context depending \non whether it is a skip gram or by or continuous bag of words model  \nso  how many parameters does this model have  is it same as the bag of words model or \nless than the bag of words model or more than the bag of words model  this is how you \nwill  think   you  will  see  how  many  input  parameters  do  the  pool  two  models  have   how \nmany output parameters to the two models are input parameters  same output parameters  \nhow many vectors do  you have  u one to uv each of size k  same as the original model  \nright it is just as an original model i had put everything inside as w context which was k \ncross v  right  so  it is the same number of parameters  \n \n\f\n \nso  the total number of parameters in the network is the same  \n\n \nnow   for  a  given  pair  w  comma  c   which  is  the  correct  pair   we  are  interested  in  the \nprobability p of w given vc  nothing great about this it is the same as i have been saying \nalways that  we want the pa probability of w given c  what we are going to model as w \ngiven vc  because vc is the representation of c  and we model this probability now as the \nfollowing thing  why does this make sense  you just assume this is on  and these are on \nk\u2019s  right  so  on one on two on three  why does this make sense  \n \n \n\fi will get the word on at the output only if the first element on the path was pi on one and \nthe second element on the path was pi on two up to the k\u2019th element on the path was pi on \nk   how  many  forget  that   please  raise  your  hands   ok  right   so   that  is  how  we  are \nmodeling it  is it  but what about pi on one  pi on two  pi on k  how do you model that  at \nleast this form is clear to everyone  right if it is not let me know  because then you not \nunderstand the rest of the stuff  yeah  ok  \nso now see that modeling part is always in your hands  right  you know that you want \nyou are interested in a certain probability  it  depends on you how to model it  so now  \nwhat you have done is you have con constructed a binary tree  now i am interested in p \nof on given some word vc  right or some word vector vc  now i can say that  but the way \ni am thinking about this is that  i get the word on only if the first if i started from the root \nnode   the  first  vector  took  on  the  value  one  or  the  first  branch  took  on  the  value  one   the \nsecond branch took on the value zero  and the third branch took on the value zero  so  that is \nexactly what i am saying here  \nit  is  a  probability  that  the  first  turn  that  i  took  was  a  left  turn   then  a  right  turn  then  a \nright  turn   yeah  the  path  is  you  have  constructed  the  binary  tree   and  the  path  is  fixed \nnow for all the words  how to construct the binary tree is a separate thing  but the binary \ntree has been constructed and every word has a unique path associated with that  so  that \nword  will  occur  only  if  that  path  is  executed   right   so   i  am  just  trying  to  find  the \nprobability of that path being executed  \nnow  i need to tell you what does each  so  how many terms are there in this product  k \nterms  right how do i estimate each of these k terms is what i need to tell you  ok  can \nyou  think  of  it   how  would  i  model  each  of  these  probabilities   remember  that  every \nnode has a vector associated with it  how many if you can think of an answer  i hope i \nare you saying what i think you are saying  \n\f\n \nso  this is what i will do  so  as i said for the on example this is what you want  this is \nthe path that you want to be executed  \n\n \nand i am going to model it as this  \nso  getting a left  turn   i  model it using this that dot  product  between the original word \nvector which was the input word vector which was vc  and the node representation of the \nnode associated with that particular node  does this make sense  so  i will tell you what \n \n \n\fwe are trying to do  so  this path was clear that the probability is going to be a product of \nthese probabilities   \nnow i want how do i get each of these probabilities  so  that is again in my hand  right  i \nam  going  to  say  that   i  am  going  to  train  my  parameters  vc  and  ui  where  ui  is  the \nparameter  corresponding  to  every  node   i  am  going  to  train  it  in  a  such  a  way  that \nwhenever i want this to take on the value one  this should be close to one  ok  because i will \nset up my loss function accordingly  we will see the loss function  \nbut i am saying that whenever i want the probability to be equal to one  i am going to use \nthis to computed  and alternately when i want the probability to be zero  i am going to take \none minus that which is just this  is that fine  okay  let us go ahead a bit and then we will \ncome back if you are still lost  \nso  what does this actually ensure  this ensures that the representation of a context word \nvc will have a very high similarity with the node ui if the path takes a left turn there  and \nit will have a very low similarity with the node ui if the path takes a right turn their  how \nmany if you get this part  based on if you assume that this is how we are going to model \nit  when is this going to be high  when the dot product between vc and ui is high  when \nis this going to be low   \nwhen the dot product between these two is low  right there is a negative  yeah  so  we  ok \nsorry i or rather when is this going to be low  right  so  you get that  so  it is coming so  \nthe word representation which is vc which is this guy would come to the come close to  \nall  these  representations   or  move  away  from  them  depending  on  whether  you  want  to \ntake a left turn there or a right turn there  \nnow  what would happen to words which appear in similar context  the same thing that \nwe  have  been  discussing  so  far   right  they  will  come  close  to  the  node  representations \nwhich are along the path  right is that fine  so  this is the context representation  right  \nthis is actually you are representing every context word by these three representations  now \nif a word appears in the same context  it is representation is going to either come close or \nmove away from these representations   right  so  words appear in the same context  if \nyou have cat  and  you had sleep here  then cat has to come close to this it has to move \naway from this  and it has to move away from this  is that clear  that is how we have set \nup the probabilities  \n\fnow   instead  if  i  had  dog  and  again  you  had  the  context  word  as  sleep   now  the \nrepresentation of dog also has to go close to this  it has to move away from this  and it \nhas  to  move  away  from  this   so   in  effect  again  the  same  thing  is  happening  that  the \nrepresentation of cat and dog are moving in the same directions  so  they will eventually \ncome close to each other  how many if you get this intuition  \n\n \nand  how  many  computations  do  you  need  now  to  compute  the  probability  of  this   so  \nearlier you acquired that complex softmax computation  how many computations do you \nneed   now  you  definitely  need  these  many  computations   and  each  of  these \ncomputations requires a sigmoid over or dot product  right  so  that is much much lesser \nthan so  you just need these many dot products as compared to your expensive softmax \ncomputation earlier  \nso  you see how you get the savings using the hierarchical softmax  so  this is as i said \nthis is not very intuitive it is like a really smart trick  and it takes time to get your head \naround it  but i am sure if you go back and look at the slides you will get it  right  if it is \nif  you  have  just  got  fifty  percent  of  the  idea  here  that  is  typically  how  it  happens  every \ntime  but and i probably not figured out a better way of teaching this  but once  you go \nback i am pretty sure that you will get to understand what is happening  \nso now the question is how do we construct a binary tree  anyone has any thoughts on \nthat  do we need to ensure certain things while constructing the binary tree  okay  i will \n \n\fask this as a quiz question  just note that  there is some subtlety here ah  in practice this \nis what is done  you just randomly arrange the nodes on the leaf nodes  and then you just \nconstruct  a  binary  tree  from  there   right   so   you  have  distributed  all  your  leaf  nodes \nrandomly  and on top of that you have constructed a binary tree  my question is there a \nproblem in doing that which i will ask you on  \n\f"}
{"audio_filepath": "lec009_010.wav", "duration": 468.215, "text": "\nglove representations \nso  now  from here we will move on to yet another way of learning word representations  \nwhich is known as the glove representations  \n\n \nso   the  count  based  methods  rely  on  global  co  occurrence  counts  from  the  corpus  for \ncomputing  word  representations  that  is  what  we  saw  in  svd   they  look  at  these  co \noccurrence counts and from there they build the word representations  \nthe predict based models  set up a learning problem where  you have this feed forward \nnet network and it tries to predict certain things from the given words and then you learn \nthe parameters of that network  and you set up the task in such a way that the parameters \nactually  correspond  to  word  representations   so   this  was  the  difference  between  count \nand  predict  based  methods   now  what  is  the  obvious  next  thing  to  do   like  hear  the \nanswer from a few of you  but i want to hear it from everyone  \n\fwhat  is  the  obvious  next  thing  to  do   you  have  count  based  methods  you  have  predict \nbased methods combine the two  right  so  come up with some kind of a hybrid  so  that \nis exactly what glove does which is known as global vectors  \n\n \nso  i will go back to the co occurrence matrix  so  remember x ij  encodes the important \nglobal information about the word i and j and whether you replace it by pmi or ppmi or \njust keep the counts  it just gives you some information about how many times these two \nwords actually appeared together  \nso  x ij encodes this global information and i call it global  because it is computed from \nthe entire corpus fine  why not learn word vectors which are faithful to this information  \nso  what do i mean by that  suppose v i is the representation of the i\u2019th word and v j as a \nrepresentative the j\u2019th word  which i want to learn i do not have these representations i \nwanted to learn  now  this gives me the dot product between them  which gives me the \nsimilarity between them  why not i set up my task in such a way that this similarity is \nactually proportional to this probability  \nso  what does a similarly tell us  how well these two go together  what does p of j given i \ntell us  how likely j is given i right  so  does that make sense to have this analogy that  \nthe dot product tells me the similarity the other notion of similarity is that how likely j is \nto appear in context of i which is given by p of j given i  so  why not set up my task such \n \n\fthat  whatever  vector  as  i  learn  are  actually  faithful  to  this  global  similarity  that  i  have \ncomputed from the entire corpus how many if you get this intuition  \nhow many if  you see the difference between this and the predict based models  in the \nprediction  based  models  you  are  operating  at  one  word  pair  at  a  time   here  you  are \nlooking  at  these  global  counts   ok   we  are  trying  to  directly  learn  vectors  which  are \nfaithful  to  your  global  similarity  as  given  by  your  co  occurrence  counts   you  get  the \nmerger between the two methods  you should not get it yet because we still have to do \nsomething or at least you get the intuition  now what is p of j given i  it is actually this  \nok  \nso  i can write it as this ok and similarly i can write the other guy v j transpose v i and \nthat is going to be different because that is going to have p i given j instead of p j given i  \nso  i will have log x ij is fine  but instead of x i i will have x j here  \n\n \nnow  if i add these two equations  so  i am going to add this equation and this equation  \nso  the left hand side i just get two times v i transpose v j  because v i transpose v j is the \nsame as  v j  transpose v  i  and on the right  hand  side  i  get  certain  quantities  so   this is \nwhat i would actually want my word vectors to look at look like  i would want my word \nvectors to  be such that  when  i take their dot  product  they  give me the  quantity  on the \nright hand side and this quantity has come based on counts learned from the corpus  \n \n\fso  i have counts on the right hand side  and i have learnable parameters on the left hand \nside   so   you  see  how  we  are  merging  these  two   but  how  do  you  learn  this  problem  \nnow it is ok to say  what i like what i have said now is that this is what i desire  i desire \nthat my word vectors should be learned in such a way that they are faithful to the global \ncounts through the following equation  this is what i desire  desiring something is one \nthing  but now how do i set this up as a learning problem  \nso   when  i  ask  you  what  is  the  learning  problem  what  do  you  need  to  think  about  \nobjective function good that is a good start  so  what is the objective function for this  \nwhat are the parameters of the optimization you are optimizing with respect to what  the \nv i is and the v j\u2019s right all the word representations how many of those do you have  v \neach  of  size  k   so   those  are  your  parameters  for  optimization   now  what  is  the  loss \nfunction  if i give you the loss function it will look very very obvious  but i do not want \nto do that  \nso   just  continue  thinking  about  that   while  i  will  make  some  more  simplifications  to \nwhat we have here now  what is this count  this is the co occurrence count how many \ntimes  these  two  occur  together   what  is  this  count   the  number  of  times  the  word  i \nappear  so  this depends only on i what is this count  the number of times the word g \nappears   so   to  make  the  model  more  flexible   that  means   give  it  some  more  freedom \nwhat i am going to do is instead of log x i and log x j i am going to introduce parameters \nb i and b j  ok \n\f\n \ni am saying that these parameters can also be learned  so  effectively using all these three \ni should be able to get this  this is what i desire now set up the loss function  using these \ntwo things come on that should not be so hard  what is this  this is what you are trying \nto predict  what is this  this is what you know is true because you have computed from \nthe  corpus   now  can  you  come  say  the  loss  function  the  difference  between  these  two \nright   so   you  could  have  this  as  the  loss  function   this  is  the  predicted  value  using \nmodels parameters  this is the actual value computed from the corpus  \nso  think of this that you are trying to learn the parameters in such a way  that you end up \npredicting this and if you predicted this  you know you have done the right thing ok and \nthis you know already because you have computed it from the corpus  so  this is the true \nvalue and this is the predicted value  so  as in any loss function predicted minus true the \nwhole square  does that make sense how many if you are fine with this  \n\n \nso   now  how  will  you  train  in  this  network   gradient  descent   so   i  will  use  gradient \ndescent and you will get these parameters  \nso   there  is  a  bit  more  on  this  which  i  will  not  cover  actually   so   i  will  just  skip  this \nslide  you can go back and take a look at it  it is a some slight modifications to this yes  \n \n  \n\fso  again the same idea that cat will go close to all the  so  here again you will have the v \ni  and  the  uc  s  right   so   you  will  have  cat  will  come  close  to  all  the  words  that  it  co \noccurs with  feline will also come close to the same words  so  maybe i have not used to \nright  notation  here  if  you  need  to  change  it  again   so   we  should  have  v  i\u2018s  and  u  j\u2019s \nright   so   again  you  have  one  word  matrix   word  representations  and  the  other  is  the \ncontext representation then it is fine right that is the problem here how many if you get \nthat right  again we have to have these two things let us change that everywhere  \n\f"}
{"audio_filepath": "lec009_011.wav", "duration": 520.8089375, "text": "\nevaluating word representations \nnow we come to this important part about how do you evaluate word representations  \n\n \nso  there are different tasks that are set up  i hope some of you have read that paper and i \ncan  see  that  none  of  you  have  read  that  paper   so   semantic  relatedness  is  one  way  of \nevaluating word representations  \n\f\n \nso  ask humans to judge the relatedness between a pair of words  so  i construct some \npairs  of  words   and  i  show  them  to  a  human   and  ask  them  how  related  do  you  think \nthey are on a scale of one zero to one  so  it is likely for cat and dog someone would say zero eight or \nat least you would expect values greater than zero six  \nnow   you  have  learned  the  representations  using  your  model   it  could  be  any  of  the \nmodels that we have seen so far  continuous bag of words  skip gram or glove vectors  \nso  these are the three things right continuous bag of words  skip gram which is known \nas  wordtwovec  and  the  glove  representations   and  within  them  you  could  have  this \nhierarchical softmax and other things and so on  \nso   you  could  if  i  asked  you  what  is  the  similarity  between  cat  and  dog   according  to \nyour word representations  you could just use the cosine similarity and tell me that this \nis the representation right  so now  i will have many search words w one w two for which i \nhave the human judgment and i have the model judgment right  so  i will have w one one w two \none then w two one sorry one two and so on  i will have many such word pairs for each of these \nword pairs  i would have the human judgments and  i would have the model  judgments \nright  how close do the humans think they are and how close do the think they are  \nnow   i  can  compute  the  correlation  between  these  two  decisions  or  these  two  random \nvariables  and  i would want that for a good model this correlation should be high  so  \nwhenever  humans  said  that  the  two  words  are  actually  similar  the  models  word  vectors \n \n\fshould also predict a high cosine similarity  and whenever humans said that the two words \nare not similar  the models word vector should also result in a low cosine similarity  how \nmany forget this  \n\n \nso  that is one way of evaluating how good your word representations are right  so  as i \nwas  saying  earlier   how  do  you  tune  those  parameters   so   you  could  have  such  a  set  \nonce  you  have  learned  some  word  representations  and  you  want  to  see  whether \nparameter  k  one  was  better  than  sorry   rather  hyper  parameter  k  one  was  better  than  hyper \nparameter k two  you could just take those  two word representations learned by these  two \ndifferent hyper parameter settings  evaluate them on this corpus and whichever gives a \nhigher correlation you can keep that hyper parameter  how many of you get that  \n \n\f\n \nother task is synonym detection  so  from a resource known as word net or from other \ndictionaries  you could get all the synonyms of a word  so  then people create a corpus \nwhere  you  give  us  in  sin  a  word  and  give  four  candidates  or  some  k  candidates   out  of \nwhich one of these is the correct synonym  the others are just distraction words right and \ndistracting  words   now   what  would  you  expect  your  word  representations  to  do  you \nhave word representations for all of these  what would you want  how would you pick \nup the synonym based on word representations  \nstudents  \n  \nthe one which has the highest cosine similarity  so  again  you will compute the cosine \nsimilarity  you will rank these and you will pick up the synonym right  and now again i \ngave you one hundred such instances  i gave you a word for candidates and i gave you one hundred such \ndifferent  word comma candidate pairs and  you pick the synonym  for everyone and see \nfor sixty of them you got it right then your accuracy sixty percent  so  that tells you how good \nyour word representations are  \nagain if you are given two different hyper parameter settings one gives you sixty percent \naccurate   the  other  gives  you  seventy  percent  accurate  you  will  probably  go  with  the  one \nwhich gives you seventy percent accurate  they are find how you can use this   \n \n\f\n \nthe third is analogy task  \n\n \nwhere you find the nearest neighbor of this operation what should it be  granddaughter \nthis is this analogy teller brother is to sister as grandson is to something right  so now  \nthe  idea  here  is  that  if  i  mean  it  is  like  pretty  weird  right   so   if  i  take  brother  minus \nsister i get something  \nnow  if i add grandson to that then i should get granddaughter  it is intuitive in a way \nright  i mean this is what you expect your word vectors to do right  so  that is how the \n \n \n\fanalogy task works  so  you could set up an analogy task  you could have and you could \nget several such an analogy tasks from online tests and so on  and you would want your \nword representations to exhibit this kind of a behavior right  \nso  again you have these one hundred analogy tasks  for each of these you know the true answer \nand from each of these you predict the answer from your word representations  and first \nsee  for  how  many  of  them  you  get  it  correct   then  you  could  also  have  a  syntactic \nanalogy  so  you can tell me what this would be right  in fact  here again it should be the \nother way round  we works minus  we work  thus  we speak would be  we speaks right  \nso  that is the syntactic thing right  \nso  you are getting a different form of the world  so  your word representations should \nalso have this kind of properties  that is what you desire  so  just evaluating whether your \nword representations show this kind of a property or not  so  we have seen  three tasks  \none is semantic relatedness whether a pair of words how do humans rank it and how do \nthe model how does the model rank it  then the synonymous detection and the analogy \ntasks  in each of these  you do something with the word representations in the first  two \nyou  use  the  dot  product   and  this  last  one  you  use  some  arithmetic  operation  over  the \nword representations  \nso   you  would  want  v  brother  minus  v  sister  is  equal  to  v  grandson  minus  v \ngranddaughter  right  so  v granddaughter  right  so  that is there is a plus minus error \nthere   \n\f\n \nso  now  which  algorithm  gives  the  best  result  right   so   whenever  we  see  a  bunch  of \nalgorithms  same  as  we  did  with  adam  and  \n  and  so  on  we  always \nwant to answer this question  which of these gives the best result   \nso  there was this study done by boroni et  al in two thousand and fourteen  that show that the predict based \nmodels right  which are either which are the predict based models actually  skip gram  \ncontinuous bag of words and even glove for that matter right because it is also a predict \nbased model  these continuously or consistently outperform count based models that is \nwhat they said  but a year later there was a separate study done by someone  and in my \nopinion this  was  a more thorough analysis because the earlier study right   they did  not \nreally give svd a chance to win in my opinion this is all on camera  \nbut the later the second set of guy right they gave svd a chance to win  so  i will tell \nyou one example of how they gave is really a chance to win  so  remember in wordtwovec \nyou had this weird three by four  which you are using to raise the probability right  now what \nthey  did  is  they  said  even  in  the  co occurrence  matrix  actually  these  counts  that  you \nhave  if here  you are using them by three by four in  the case of wordtwovec and getting better \nresults  why not do the same thing in the co occurrence matrix also  \nat the end of the day you are raising the count to three by four right  so  whatever counts you \nhave here based on that you will compute ppmi or pmi or whatever  but first why not try \nto adjust these counts  so  why not have a parameter k such that you can raise the counts \n \n\fto  this  parameter  and  then  do  all  those  computation   and  that  is  fair  because  the \nwordtwovec has a parameter hyper parameter  so  why not give a similar hyper parameter \nto svd  \nsimilarly   they  did  something  to  take  care  of  the  k  negative  samples  which  wordtwovec \nhas  why  not  give  svd  also  similar  chance   right   so   when  they  did  these  kind  of \nadjustments  they found that after these modifications svd does as well as or even better \nthan  wordtwovec  models  for  the  similarity  tasks   but  not  for  the  analogy  task   but  the \nanalogy task was the last task right  brothers to sister\u2019s grandsons to grandmother  right  \nso  in most cases we care about similarity  and in very few cases we care about analogy \nif you are doing nlp application so  that means  in most cases svd would just be fine  \nso  that is what i just said at the beginning   \n\f"}
{"audio_filepath": "lec009_012.wav", "duration": 245.42, "text": "\nrelation between svd and wordtwovec \nnow   and  later  on  actually the  same  guys   they also  came  up  with  this  formal  relation \nbetween svd and wordtwovec which is again under some assumptions  \n\n \n\f\n \nbut  i am  not  going to  do the proof here   i am  just  going to  give  you the intuition   so  \nrecall that svd does a matrix factorization of the co occurrence matrix levy et al showed \nthat wordtwovec also does such a implicit matrix factorization  so  what does this mean  \nso  recall that wordtwovec gives us w context and w word  it gives us these two parameters  \nso  they say that there exist a matrix m such that ok this is wrong  just be the product of \ntwo matrices right  this  is  the product  of two matrices   it  should be w context  transpose  w \nword or just see which way the transpose should be   \nso  it is actually a product of these two matrices that we have learnt ok and what is m m is \nactually  nothing   but  the  pmi  matrix  minus  this  log  k   where  does  the  k  come  from  \nwhat  was  k   the  negative  samples  that  you  have  taken   so   they  actually  showed  that \nwhatever  representations  wordtwovec  runs   it  is  actually  doing  a  factorization  of  this \nmatrix   where  this  matrix  has  a  strong  connection  to  the  pmi  matrix   and  svd  also \nworks with the pmi matrix  \nif  you  take  svd  matrix  and  do  these  modifications  to  it   that  means   you  take  every \nvalue which is the pmi and then subtract this log k from that  and then just do an svd \nof that you will essentially get back the same word representations as wordtwovec  \nthere was some certain assumptions made in the paper  but that is i mean  i do not want \nto  go  into  those   but  the  key  idea  here  is  that   you  can  actually  show  that  svd  and \nwordtwovec are actually connected  and if you think about it at an intuitive level  though \n \n \n \n\fthese methods are relying on the same underlying principle that words appear together  \nbased on that  the word representations get updated or in svd based on there the counts \nget updated and you then eventually end up with certain representation  \nnext the underlying principle is the same  so  there has to be a connection right it is not \nthat  they  are  doing  fundamentally  something  different  both  of  them  are  relying  on  the \nidea of co occurrence or the idea of distribution right  so  they have to at some level be \nsimilar in some ways  right  so  that is what they finally  showed and so  now  but still in \nmost applications wordtwovec is preferred   \nso  one reason for that is  that this is an iterative training procedure right as compared to \nsvd  and  i  come  back  to  your  question   right   how  do  you  do  that   how  do  you \ncompute the eigenvectors of x transpose x  and the answer is  there is no simple way of \ndoing that and you have to do that expensive matrix multiplication  \nand then rely on various very smart libraries for computing the eigenvectors which are \nstill order n raise to two point something or something like that they not order n cube  but \nthey are still order n raise to two point something  means they are still expensive  and then \nof  course   you  have  this  memory  issue  that  if  you  have  a  very  large  vocabulary   your \npmi  matrix  is  going  to  be  very  high  dimensional   and  then  you  need  to  do  the \nfactorization  of  that  high  dimensional  vectors    so   that  runs  into  these  computational \nefficiency issues  \non the other hand  wordtwovec by design is an iterative algorithm because  you are going \nto  grade  gradient  descent  which  is  that  every  times  that  you  are  going  to  update  some \nparameters of the model  you are not learning all the parameters together  you are only \ndealing with  some parameters at  every time set right  so  that is  more computationally \nefficient  especially  if you do the contrastive divergents or the negative sampling or the \nhierarchal sample  so  that is why  perhaps it is still more popular than svd  \n \n \n\f"}
{"audio_filepath": "lec010_001.wav", "duration": 1098.486, "text": "\nso   why  let  us  start   so   far  in  the  course  we  have  looked  at  feed  forward  neural \nnetworks   we  have  seen  how  to  train  them  and  we  have  seen  two  special  cases  of  feed \nforward  neural  networks   one  was  the  auto  encoders  for  learning   representations  or \nlearning latent representations of inputs and the other thing that we had seen was how to \nuse a feed forward neural network to learn word representations where we saw this word \nto wake algorithm and it is different variants  it was continuous bag of words  skip gram \nmodel  graph  and  so  on   so   those  are  all  since  some  since  applications  of  the  feed \nforward neural network  \nand  now  we  will  move  on  from  there   though  we  will  look  at  different  type  of  neural \nnetwork  today   which  is  convolutional  neural  networks  and  we  look  at  some  specific \narchitectures  which have become popular over the past few years  ok  \n\n \nso  with  that   i  will  start  this  lecture  on  convolutional  neural  networks   so   in  the  first \nmodule we will look at the convolution operation  ok  \n\f\n \nso   let  us  see   so   suppose  we  are  tracking  the  position  of  an  aeroplane  using  a  laser \nsensor at discrete time intervals  right  so  you have this ok  so  you have this aeroplane  \nsuppose  it  is  going  from  say  chennai  to  delhi  and  at  discrete  time  intervals   you  are \nseeing the tracking the position of the aeroplane right  how far it is from chennai at this \npoint right may be it is fifty kilometers  one hundred kilometers and so on   \nand  now  your  laser  you  think  that  it  might  be  noisy   it  might  not  be  giving  you  very \naccurate measurements   so  you would be taking these measurements and say intervals \nof course  it is not in practice you would not do that  but just indulge me for the purpose \nof illustration that say  you are taking these measurements every five seconds or ten seconds \nor something like that   \nnow  since your sensor is noisy instead of relying on a single measurement  you would \nprobably want to take the average of the past few measurements that you have taken  so  \nthat would give you a more accurate representation of what the current position is  does \nthat make sense like  you are taking multiple measurements and taking averages of those \nright  and  of  course   more  recent  measurements  are  more  important  as  compared  to  the \nprevious measurements  right   so  this is  suppose  at  time step t  say this  was  t  minus five \nseconds and this was t minus five minutes suppose   \nso  obviously   you  would  not  want  to  take  give  a  very  large  weightage  to  the \nmeasurement that you are taken t   five minutes back right because  the plane would have \n \n\fmoved by a lot by that time  so  it rely more on the recent measurements and less on the \nprevious measurements right  so now  the mathematical way of lighting  this is that you \nknow the positions or the readings that you have taken at time steps one  two  three  up to time \nstep t  you are interested in the revise estimation of this measurement right  so  you have \ntaken some measurement at time step t and you want a revised measurement of that and \nthe way you are going to compute  that is you are going to take a weighted average  so  \nw is the weight of all the previous measurements that you have taken right  \n \n \nso  the measurement that you take a t   one  t   two  t   three all the way up to t minus infinity and \nfor each of them would have a weight associated with this  \nso   this  operation  right  this  thing   you  can  write  it  as  the  following  operation  that  you \nhave a vector of measurements or an array of measurements  which is x and you have an \narray of weights associated with these measurements  the farther the measurement from \nthe current time step hopefully smaller is the weight assigned to that and this operation is \nknown as the convolution operation right  \n \n\f\n \nso   you  have  x  which  is  the  input   w  is  known  as  the  filter  and  the  operation  that  is \ndefined as this equation is known as a convolution operation right  \n\n \nlook   but  of  course   in  practice  you  would  not  do  this  from  infinity  right   you  would \nprobably keep a window  you will say i will rely on the previous six measurements  that \nmeans  whatever i took at t   one second  t   two second up to t   six seconds right  beyond that \nit  does  not  really  make  sense   so  let  us  see   how  this  computation  happens   so  this  is \n \n \n\fweight  array  so  now   what  would  be  the  dimension  of  this  weight  array   how  many \nentries would it have  \nstudent  seven  \nseven right zero to six  so  seven entries ok  and this is what my situation looks like right  so  this is \nthe x the measurements  which  i have taken using the laser ok  so  i have taken some \nmeasurements  now  i am at a particular time step and i want to make a revised estimate  \nso  i have this xt and from that i want to compute st and the way  i am going to do that is \nby  taking  a  weighted  average  of  all  these  previous  measurements   is  the  setup  clear  to \neveryone  ok   \nand now this is what my formula is going to be  so  the revised estimate of ssix is going to \nbe whatever was xsix into wzero  that means  the weight assigned to the current time step  xfive \ninto w minus one  that means  weight assigned to the time step   one  xfour into   two and so on  \nso  you get this ok  so  i have these seven weights and i will multiply with them with the seven \nprevious readings  one is to one multiplication and i will get the weighted average and using \nthat i get a revised estimate  \n \n \nnow  i want to get a revised estimate for the next entry  how will i get it  i will just slide \nthis weight matrix right  \n \n\fso   i  will  just  slide  it  by  one   i  will  again  do  the  same  computation  and  get  the  revised \nmeasurements  again for the next entry  i will slide it by one  slide it by one  slide it by one and \ni  will  keep  getting  these  entries  ok   so   everyone  gets  the  setup   how  do  you  do  the \nconvolution operation  it is basically  a weighted average of the previous entries fine  \nso  here the input as well as the kernel is kind of one dimensional right you  so  you have it \nis  so  you do not have a twod input here  you just have a single dimensional input here  \n\n \ncan you use a convolution operation on a twod input also  do you know of any twod inputs  \nimages  right  so  we can think of images as twod inputs  now again i am trying to do the \nsame thing  the setup is the same  the story just changes from laser to a camera now  so  \ni have taken an image maybe the image was captured and i am not very confident about \nall the pixels that i have captured ok  \nso  now for any given pixel  i want to re estimate it  using it is neighborhood that is what \ni want to do ok  so  this is the pixel  i am going to look at some neighborhood around it \nright  so  every cell here is one pixel  just assume that every cell here is one pixel  so now  i \nam going to re estimate this pixel by taking a weighted average of all its neighborhoods \nright  so now  can you tell me  what is my filter going to look like in this particular case  \nmy  filter  would  be  just  three three   right   so   whatever  neighbors  i  want  to  average  on  for \nevery neighbor  i want a weight  so  if i am going to average on a neighborhood of three  three \n\uf0b4\uf0b4 \n\fthen for each of these  i will want a weight  so  my filter would also be of size three  three  how \nmany of you get this  ok  \nso   we  now  like  to  use  a  twod  filter   which  would  be  m  cross  n   ok   and  in  general  it \nwould  be  m  m   so   it  would  always  be  a  square  filter   but  i  am  just  taking  the  case  \nnow what  this  nasty looking formula is  doing  right  so  i have  a particular pixel   so  \nthis is an image  so  i will refer to this pixel as i ij right  so  it is the ith ijth entry in the \nimage  i want a revised estimate for that i want an sij for that   \nso the way  i am going to do that is i am going to look at m rows and n columns before it \nright   so   i  am  going  to  look  at  this  neighborhood  of  m  cross  n   ok   and  for  each  of \nthese  i would have a weight associated with it  so  if i am looking at say for example  \nthis was four   four  this pixel was four  four then  i will look at four \u2013 one  four   one  so  that would be i three  three  \nso   i  will  look  at  that  neighbor  and  with  that  neighbor   i  would  have  some  weight \nassociated  do you get that how this formula expands  \nso  this formula would have m cross n terms  for every term   you would have a have a \nweight  and  that  weight   you  can  just  represented  as  this  filter  matrix   so   you  get  this \nwhat this formula is doing  it looks a bit nasty  but it is just the weighted average of all \nthe neighborhood that you have and the neighborhood is a two dimensional neighborhood  \nin this case  how many if you get this properly  ok   \nnow this  in this formula actually  i am looking at minus a and minus b  that means  i am \nlooking  at  previous  neighbors  right   now  you  should  have  these  questions  right   why \nprevious  neighbors   why  not  future  neighbors   so   why  am  i  not  looking  at  this \nneighborhood  \n\uf0b4\uf0b4\f\n \nso  there  is  no  correct  answer  here   different  convolution  operations   i  mean  different \npackages  use  different  convolution  operations   but  the  most  standard  one   i  believe  is \nwhen you look at the next neighborhood right  that means  you at this pixel and you will \nlook at this neighborhood  the neighborhood after it right not the before it ok  \nand  in  fact   so  this  is  the  formula  that   i  am  going  to  look  at  plus  j  and  plus  p   that \nmeans  i am looking at pixels in the rows after this and in the columns after this pixel  all \nof  you  get  this  instead  of  before   now  what  is  even  more  natural  to  do   the  names \nsurrounding thing right  so  i will have this pixel and i will look at it is such a way that  \nthis  pixel  is  the  center  of  the  neighborhood  right   so  that  is  what  i  am  going  to  go \ntowards  after  a  couple  of  slides  and  that  is  what   i  will  use  for  all  my  convolution \noperations   but  in  terms  of  textbook  definitions  these  are  the  definitions  that   you  will \nfind in textbooks ok   \n \n\f\n \nso  let us let us apply this to a toy example  so  i have this input  which is two dimensional \ninput  i have a kernel which is a two two kernel  so  my m is equal to n is equal to two  so  i \nam going to place this kernel at this location  ok  and then what will i get as the output  \n\n \na into w plus b into x plus e into y plus f into z right and i will keep sliding this to get the \nother entries  do you observe something about the input and the output  \nstudent  \n  \n\uf0b4 \n \n\fsize  the output size has reduced  why  we will get back to this  \n\n \nso right now  i just you to notice it is obvious nothing great about it  but i will just get \nback to it more formally later on  \nso   for  the  rest  of  the  discussion   we  will  use  the  following  formula  for  convolution  \nwhich  is  the  centered  formula  right   so  \n  to \n   that  means   i  will  be  looking  at  a \nneighborhood  which is  centered on the pixel of interest that is why this    \nto  \n  is \nthat fine  ok   \ntwomtwontwomtwom \n\f\n \nso  this is how i am going to look at it  so  this is how i will place  if this is the pixel of \ninterest  which i want to re estimate  i will replace the kernel such that it  this pixel lies at \nthe center of the kernel  ok  \n\n \nso  we will be looking at both preceding and succeeding neighbors  ok  \n\n \n \n \n\fso  let us see some examples of twod convolutions applied to images  \n\n \nso this is an image  i decide to apply the following convolution operation to edge ok  tell \nme what the resulting image would be  \nstudent  blurred  \nblurred  why blurred  \nstudent  we are taking average  \n \n \n\fwe  are  taking  the  average  right   so   it  would  be  blurred   you  get  the  intuition   so  this \nkernel basically  i have fitted at every pixel and i have computed the average around it \nand place at pixel by that average value and when we are going to take average things are \ngoing to get blurred right  because all the sharpness is gone ok   \n\n \nnow let us look at this kernel  what will this do  sharpen why  because one was blurred \nthe other has to be sharpened  what is happening here  \nstudent  \n  \nit  is  subtracting  the  neighbor\u2019s  right   so  you  are  taking  five  times  the  current  pixel  and \nsubtracting the neighbors from it right  so  if the neighbors are similar  those would get \nsubtracted and this would stand out really right  does that make sense   \nthis will result in a  but this in my on my laptop  this looks like a sharpened image  i do \nnot know why it is looking like this here ok  it is a sharpened image just trust me  you \ncan so actually are common right  so  people who have used adobe or any of these photo \nshopping  software\u2019s  right   so   you  have  this  click  button  and  where  you  say  take  an \nimage sharpen and blur it  so  this is exactly what the tool is doing in the background  it \nis applying this convolution operation throughout the image  \n \n\fso  when you say blur it is basically  placing that convolution operation throughout the \nimage  and  computing  the  blurred  image  and  same  for  sharpening  and  all  these  other \nspatial effects that  you have most of them come out of some convolution operation ok  \n\n \nso for example  the next one  what would this do   \nstudent  \n  \nso  i will give you a hint  when will this result in a zero output  \nstudent  \n  \nwhen all the neighbors are the same as this  right so then  when will it result in a nonzero \noutput  \nstudent  \n  \nwhen there is a difference  when there is a difference  so  looking at this image tell me \none place  where you know that it will result in nonzero output  \nstudent  \n  \nall  the  boundaries  right   so   this  is  basically  an  edge  detector  in  the  slides   it  appears \nproperly  ok   so   this  is  basically  an  edge  detector  and  you  get  the  intuition  that  these \n \n\fboundaries  whether neighbors are not the same as the current pixel  you will not get a zero \nvalue  in this case  when all the neighbors are the same as the current pixel  so you are \ntaking  the  sum  of  the  eight  neighbors  and  subtracting  the  current  value  eight  times   so   that \nwould be zero right ok  \n\n \nso enough of examples  so now  we will see a working example of a twod convolution  so  \ni just want to drill this idea of what happens  when you do a twod convolution  \n\n \n \n \n\fso  what we are going to do is  we have this three three kernel and assume that everything here \nis  a  pixel  ok   everything  here  is  a  pixel   so   i  am  going  to  slide  this  three  cross  three  kernel \nacross this filter  now when i place the filter once on the image how many outputs do i \nget  \nstudent  one  \none  output   so   if  i  keep  sliding  it  across  the  image   i  will  keep  getting  one  one  pixel  in  the \noutput ok  so  what the resulting thing that i get is known as a feature map ok  because it \nis the original input that we have taken  for every pixel  you have tried to approximate it \nor whatever filter  weights  you have applied and it necessarily  does not  mean  that  you \nare taking an average  it could  be some weird average of  your neighborhood right   so  \nyou have extracted some features from there   \nso for example  in the edge detector case  you could think of it that you have extracted \nthe feature that this pixel does not lie at a boundary right  that is why you get the black \npixel  do you get that  you see this way of interpreting a convolution operation that  you \nare trying to extract some features from that neighborhood  \nso   in  this  earlier  example  whenever  you  got  a  black  you  are  basically  extracting  the \nfeature that this pixel does not lay at a boundary  is that ok  fine  so now  you could get \none  such  feature  map  by  using  a  single  three  cross  three  convolution  operation  ok   if  i  use \nmultiple  such  convolution  operations   what  would  happen   i  will  get  multiple  feature \nmaps ok  so  let us try to understand this  what is the dimension of my original image  m \ncross n into three  why is it into three  \nstudent  rgb channels  \nrgb channels ok  rgb is what we will have right  so  we will have this three m n  so  \nwe will return back to this idea and from now this one image by using a single kernel  so \nthis  in fact in for this figure right  i am assuming that the input is one cross m cross and i \nam not assuming there are three channels although  it is a colored image  but just bear with \nme  so  it is a one cross m cross an image and when i apply a filter  i get a one feature map  if \ni  apply  k  such  filters   i  will  get  k  feature  maps   so  one  feature  map  could  be  for  the \nblurring one  one could be the sharpening one  one could be the edge detector and so on \nright  there are various such filters that you could apply  \n\uf0b4\uf0b4\uf0b4\f\n \nnow  in the oned case  we slide a one dimensional filter over a one dimensional input  in the \ntwod case  we slide a two dimensional filter on a two dimensional input  what would happen in \nthe threed case  \n\n \nso  now  we are going to this rgb images right  so  we will have three cross m cross n as \nthe input  what would happen in the threeg  threed case not threeg  \n \n \n\f\n \nso  what would a threed filter look like   \nstudent  box  \nlook like a cuboid  a box basically and we will call it a volume  why volume  because it \nhas a width  it has a height and it will have a depth  so  this is what a three d filter would \nlook like  i will assume that it is depth is the same as the depth of your input  what is the \ndepth of your input in this case  \nstudent  three  \nthree  so  i will assume that the depth of the filter is the same as a depth of the input and the \nwidth and height could be three  three  five  five  seven  seven  anything right  so  we will get into that in \nmore  details  later  on   so   once  again  we  slide  this  volume  across  the  entire  image  ok  \nwhat is the output going to be  twod or threed  \nstudent  twod  \nwhy   so  when  i  was  oned  i  was  getting  oned  output   when  i  was  twod  i  was  getting  twod \noutput  threed again twod output why  because i have assumed that  no not width   \nstudent  \n  \n\uf0b4\uf0b4\uf0b4 \n\fthe depth of the filter is the same as the depth of the input  so now  you just imagine this \nif  you  can   suppose  the  filter  was  of  depth  two  instead  of  three  then   i  would  slide  it \nhorizontally  first   vertically  and  then  across  the  depth  also   so   then  what  would  be \noutput be in that case  \nstudent  three dimensional  \nthree dimensional and it would have depth of two  \nstudent  two  \neveryone gets that right  but for this lecture i am always going to assume that the depth \nof the filter is equal to the depth of the input always right  and that is how it is for all the \nconvolution neural networks  that we will see the depth of the input is going to be equal \nto the depth of the filter  the rather the depth of the filter is going to be equal to the depth \nof  the  input   so  whenever   i  apply  a  threed  filter  i  am  actually  doing  a  twod  convolution \nbecause   i am  moving only  along the width  and the height   i  am  not  moving along the \ndepth   so  the output is  going to  be twod  so now  can  i  have multiple such filters  yes \neach filter will give me a twod output  if i have k such filters i will have a  \nstudent  \n  \nk  twod output right k twod outputs fine   \n\uf0b4\f"}
{"audio_filepath": "lec010_002.wav", "duration": 736.162, "text": "\nok  so  now we will go to the next module where we will try to learn the relationship \nbetween the input size  the output size and the filter size  ok   \n\n \nso  so far we have not said anything about the dimensions of the inputs i have just been \nvery vague that its m \n n and also for the filter i have just said three \n three  five \n five and so on  \nand in fact  i am not even told you what the dimensions of the outputs are  except that i \nbe  got  some  intuition  that  it  seems  that  the  output  dimension  is  smaller  than  the  input \ndimension right  \nso  let us look at these in more detail and see what do these different outputs look like  \nwhy  do  i  care  about  these  things   what  do  the  inputs  and  the  output  sizes  look  like  \nbecause these are your matrices that you will be dealing with this tell you these tell you \nhow many parameters you are going to have  \nthese tell you what is the size of the memory that you need to load this entire network \ninto your memory and so on it  so  that is why this computation is very very important \n\uf0b4\uf0b4\uf0b4\fand i will have some more things to say about it towards the end of this lecture or some \nlecture ok   \n\n \nso  if you first define the following quantities  so  we have the input which has a width \nwone  height hone  and depth done  so  if you are looking at the original image then the depth \nis going to be three in most cases it is going to be rgb ok  there is something known as \nthe stride s  i will come back to it later on  \nbut  i  am  just  defining  it  here   or  others  just  introducing  it  here   and  then  you  have \nnumber  of  filters   so   i  said  that  every  filter  that  you  apply  you  are  going  to  get  one \nfeature map which is two dimensional  if you have k such filters  you will get k feature \nmaps each of them is twod right   \nso   we  will  have  something  known  as  number  of  filters  k  and  then  you  will  have \nsomething known as the spatial extent of these filters  so  that is the number three \n three  five \n five \nwhich i have been saying  so  that is known as the spatial extent i am going i am going \nto refer to it as f ok  and we are going to always assume square filters  \nso  it is always going to be f  f  is that fine  ok  and the depth of the filter is one more \nthing which i need to worry about  but i have already said that i am going to assume that \nthe depth of the filter is the same as the depth of the input ok   \n\uf0b4\uf0b4\uf0b4 \n\fnow  the output is again a volume which is of size wtwo  htwo  and dtwo  and my quest is to \nfind out how do i compute these wtwos  htwos  and dtwos that is what i want to figure out  it is \nnot very difficult  but i just want to do it properly  so that is what the setup is right  so  \nwe have defined the sizes of everything on the input and the filters now you want to look \nat how do we get the output sizes ok  \n\n \nso  let us compute this for one example  so  this is my original image  so i am looking at \na two dimensional input which i believe is seven \n seven  and i am applying a three \n three filter to it  \nso   every  time  i  slide  the  filter  i  will  get  one  pixel  in  the  output  and  i  got  the  entire \nfeature map  \nnow it is obvious that the size of the output is less than the size of the input why is it so  \nbecause there are certain pixels at which i cannot place the filter why  you will go out \nof the boundary right  so  i cannot if this is my pixel of interest  i cannot place my filter \nthere   because  then  the  filter  will  go  outside  the  image  and  i  do  not  know  what  the \naverage to come to how to compute the average right those values are undefined do you \nget that  ok  \n\uf0b4\uf0b4 \n\f\n \nso  in general for let me see for the three cross  in fact  this is true for all these pixels which \nhave  been  shaded  or  any  of  these  pixels  i  cannot  place  the  filter  because  you  will  go \noutside the boundary  so  now  for a three \n three filter what is the reduction in the size of the \noutput compared to the input  the width decreases by two and the height decreases by  \nstudent  two  \ntwo right  so  can i be bold enough and say that the width and height decreases by not yet \nok   \nso  let us see if we had a five \n five kernels ok  then which are the places at which i will not \nbe able to place the filter  these two shaded boxes and both these boxes i cannot place \nthe filter because the filter will go outside the image   \n\uf0b4\uf0b4 \n\f\n \nso  now can  you tell  me how many  so  in  this case the size reduce is  by  how much  \nthe width reduces by  \nstudent  \n  \nno the width reduces by four sorry and the height reduces by  \nstudent  four  \nfour  so now can i say that the width and height actually reduce by f minus one  where f is \nthe size of the filter is that ok  how many of you get this  so  you did not get this  no  \nyou  did  not  get  this   ok   so   in  the  three \n  three  case  you  see  that  that  is  one  row  and  one \ncolumn on each side left and right which i cannot apply ok  \nso  let us focus on the columns  so  columns give me the width right  so  when i have a \nthree \n three filter there are two columns which get subtracted  because these are the boundary \ncolumns   when  i  have  a  five \n  five  filter  how  many  columns  get  subtracted   two  on  the  left \nhand side  two on the right side is a total of four  \nif i have a seven cross seven filter  three on the left hand side  three on the right hand side  so total of six  \nso  you see the relation it is always f   one right  so  your new width and height is always \ngoing to be wone   f   one which is w one   f   one is that ok  everyone gets this and same for \n\uf0b4\uf0b4\uf0b4 \n\fthe  height  the  width  and  height  are  going  to  be  symmetric   because  the  filter  we  have \nchosen to be symmetric it is f \n f ok   \n\n \nnow   but  what  if  we  want  the  output  to  be  the  same  as  the  input   what  do  we  do  \npadding ok  you can use you know something known as padding  so that means  now i \nwill have a boundary of zero s  so  i am saying that this is my original image and outside it \nthere is  a black border or a  white border  i  do not  know whether zero stands for black  or \nwhite it is embarrassing  but  \nstudent  black  \nblack ok  so  it is a black border outside the image ok  and now i am going to take an \naverage that way  \nnow   this  was  the  pixel  earlier  on  which  i  could  not  place  the  filter   but  now  i  can \nartificially place on it assuming that it there is a black boundary around it  so  now what \nwould be the output size  \nstudent  \n  \nsame as the input  so  now can  you tell  me  so   i have wone   i have  f  and now  i have \nsomething  known  as  p  also  ok   so   i  know  that  w  output  rather  wtwo  is  the  output  is  one  \n\uf0b4 \n\fnow when i add the padding what would the formula be  two p is that fine  everyone gets \nthat  ok   \n\n \nso  now if  i  have a five  cross  five filter  and if  i  want the output size to  be the same as  the \ninput size  what is the padding that i should use  \nstudent  p  \np  is  equal  to  two  right  that  is  clear  from  the  example  that  we  saw  that  there  were  two \nshaded columns and rows which were problematic  so  i need to do a padding of two and \nthen if i substitute in this formula you can just see right  so  five   four   one right  so you will \nget back wone is that fine is that ok  how many if you get this  how the formula works \nwith the padding  how many of you do not get this please raise your hands  you do not \nget this ok   \nso  remember in the three \n three case there was one column on the left hand right which was a \nproblem  so  when i say a padding of one i add one column to the left  one column to the \nright  one column to the bottom and one column to the top  \nand that is exactly the problematic region in the three \n three case right  so that means  this was \nmy  original  formula  ok   now  the  new  width  is  going  to  be  plus  two  times  the  padding \nwhich i am going to use  because i have used one padding here  and one padding here \nright  now  in the three \n three case that is ok  \n\uf0b4\uf0b4\uf0b4 \n\fnow in the five \n five case how many columns are problematic  two so  that means  i have use \na padding of two  when i say a padding of two i add two on all the sides  so  now again if you \nsubstitute in this formula so you would have wone   five   one   four  so  that will give you back \nwone right  so  that is how it is right   \n\n \nnow  the question is you have if you have taken care of filter size and padding  now the \nother thing that we need to look at is stride ok  so  the stride defines the interval at which \nthe filter is applied  now what do i mean by that  \nso   remember  that  stride  is  basically  a  step  right  the  same  definition  as  it  is  applies  to \nwalking rate  so  right now what we were doing is we placed the filter at as this pixel at \nthe center  then this pixel has the center  and then this pixel has the center  instead of that \ni  could  take  a  that  means   my  stride  is  one   i  am  taking  one  step  at  a  time   so   if  i  do  a \nstride of two what would happen  \nstudent  \n  \ni will apply two alternate pixels right  so  this is how i will move  so  then what would \nhappen to my output size if my stride is two  \nstudent  \n  \nwhat would happen  \n\uf0b4 \n\fstudent  \n  \nit will become half ok  so  what would the formula be then  so  so far my formula was \nnow if i have a stride of two what would the formula be  \nstudent  \n  \nthey divide the whole thing by two  \nstudent  \n  \nby s right  if it was s was three then this would have become one third roughly right  if s \nwas four this would have become one fourth  so what should i divide by  \nstudent  s  \ns  so  i should divide the whole thing by s  \nstudent  \n  \ns ok   \n\n \nso  it turns out that is not exactly that  but you get the intuition and you can work out the \nformula   so  you  do  not  divide  this  by  s  and  you  will  figure  it  out   it  is  easy  to  see \nbecause of some ceiling and flooring and things like that  so  you can go back and check \n \n\fthat out and basically you could just think of this that  this was your original weight in \nthe absence of stride  or rather than the stride was one  \nso  now if you are going to take longer strides you have two account for that if you take \na stride of two  stride of two your width is going to become half  you should take a stride of four \nyour width is going to become one fourth is that fine  do not worry about this additional \nplus point you can go home and figure it out  \n\n \nfinally coming to the depth of the output what would the depth of the output be  so  let \nme just see right now all our formulas were wtwo  htwo in the presence of filter padding and \nsize  a stride sorry  but we never had a formula for  dtwo  so that is what i am asking  \nnow what is the depth of the output  same as the every filter is going to give you one \ntwo dimensional output  if you have k filters  \nstudent  k  \nyou will get k two dimensional outputs  that means  the depth of your output is k right   \n \n\f\n \nso  the depth is very simple it is just equal to k the number of filters that you have  so  i \nwant you guys to note down these three formulas  \n\n \nnow  let us do some exercises  so  this is my original input which is two hundred and twenty seven \n two hundred and twenty seven \n three  i \nhave decided to apply eleven cross eleven filters and i am not going to tell you the depth of the \nfilter  because it is going to be the same as the input ok  \n\uf0b4\uf0b4 \n \n\fand i have ninety six such filters i have decided to use a stride of four there is no padding  can you \ntell  me  what  is  the  output  volume  going  to  look  like   what  are  the  dimensions  of  the \noutput volume  ok  so  dtwo is simple  what is dtwo  ninety six  what is wtwo  \nstudent  fifty five  \nfifty five  htwo  ok you guys have the last class fine  so  similarly you can do it for so actually \nthe  computation  which  i  had  that  this  was  just  not  some  random  computation   this  is \nactually  the  first  convolution  layer  from  alexnet  right   so  one  of  the  popular \narchitectures that we are going to cover later on right  \nso  this is what aalexnet does at its input  it takes the rgb input and gives you a volume \nof this slice this side and then there is something else with this volume right  so  we will \nsee that later on  there is one more exercise you can do it later on  i do not want to do it \nnow ok   \n\f"}
{"audio_filepath": "lec010_003.wav", "duration": 1007.127, "text": "\nok   so  now   we  will  go  to  the  next  module   this  is  for  the  camera  and  this  is  on \nconvolutional neural networks  \n\n \nso far  we have done what we have just talked about a convolution operation  you just \ntaken some input boxes and converted them to output boxes  what does this anything of \nthis have to do with neural networks  i keep saying that is a course on neural networks \nright  so  everything has to link to that  so  what is the connection  \nso  we will try to understand this by taking the example of image classification and i will \nuse the same trick to  get everyone\u2019s  attention  so  the next  few slides  are  going to  tell \nyou  the  difference  between  machine  learning  and  deep  learning  ok   so  now   everyone \nwill pay attention   \n\f\n \nso  this is the task you have give given an image and you want to classify it into one of k \ncategories and i am considering four categories here car  bus  monument  flower ok  what \nis the simplest thing that you can do  suppose this is a twenty cross twenty image you know the \nsimplest thing   \nstudent  sir  \ngiven on the slide  you will just take this as a four hundred dimensional input feature vector right \nand you will treat it as a four class classification problem  train some multiclass svm or \nanything  on  that  right   so   you  have  a  simple  input  so   you  are  given  some  one  million \nimages  all of these are four hundred dimensional and they come from one  two  three or four  these are the four \nclasses  which is car  bus  monument and so on   \n \n \n \n\fso  you can just treat this as an input feature vector and do your classification right that \nis the simplest thing that you do or else what you could do is  you could do some kind of \nfeature  engineering  right   you  could  say  that  actually  this  entire  blue  sky  is  not  really \nhelping me in deciding anything  these entire green lawns and all this is not helping  if \nmonument  car  bus and flower are the classes  what i care about is the shapes  i do not \ncare about the details inside the shapes  i am not trying to decide whether the car is of a \nblue color or what model the car is and so on right  all i want to see is that this particular \nshape of a car is present or not  \nnow  what kind of filter gives you the shape of the image  \nstudent  \n  \nedge detector right so  i could use edge detector  so now  this is something that i have \nused based on my domain knowledge  that for these four classes actually  just detecting the \nshape  is  important   so   i  will  ignore  everything  else   so   there  is  a  lot  of  details  there \nright  so  i have actually sparcified my entire input   i have just looking at the edges in \nthe input and now this is a better refined feature as compared to the earlier feature  how \nmany of you agree that this is a more refined feature representation right   \nbut this was handcrafted  i actually hand coded the edge detector kernel  because i knew \nthat it is eight at the center and minus one everywhere else right  that is how i thought of it that \nthat is what an edge detector is or at least i read about it somewhere right  so  that is how \nyou would do it  so  this is feature engineering   \n \n\fso  what  is this  this is  how  you do machine learning right   you take an input  you do \nsome feature engineering and then you train a classifier on top of that  but now you could \nbecome even more  creative with  the feature engineering and that is  what the computer \nvision  community  was  doing  largely  before  two thousand and twelve   come  up  with  different  ways  of \ncapturing better and better features from images  so  too popular in features from that era \nand that is i am just talking about two thousand and twelve not some like five hundred years back  but from that era \nwas sift and hog features  which actually tell you how do the gradients of these pixels \nchange across the image right   \nso   this  is  again  try  to  capture  something  like  how   what  is  the  variation  in  the  image \nfrom pixel to pixel right  so  that is the essence that how is you do not care about these \nentire blue patterns  because they are just telling you sky it is redundant right  if you have \nseen some ten pixels or twenty pixels  which has sky you know that a large part of it is going \nto be sky  \nso  these try to  capture  some abstractions from  the image and these are better than the \nedge detectors and these features were extremely popular  so  what you would do is you \ntake your original input  this is a deterministic algorithm  you apply the hog algorithm \nor  the  sift  algorithm  and  it  gives  you  a  transformed  representation  for  the  image  and \nyou can use this transform representation to do classification  and a lot of work prior to \ntwo thousand and twelve  two thousand and eleven show that these features work extremely  well across a wide variety of across \na wide variety of image tasks ok   \nso  again   what  was  happening  here   this  was  feature  engineering  because  someone \nrealized that what i care about is this gradation in the input images and i can capture this \nby  this  nice  algorithm  called  sift  or  hog  of  course   someone  came  up  with  that \nalgorithm  but it is still kind of feature engineering right   \nso   this  is  how  the  learning  is  to  happen  is  you  are  given  some  input   you  do  a  static \nfeature extraction no learning  so  feature extraction is deterministic you take the input \npass  it through one of these algorithms   either the edge detector or the blur detector or \nsift or hog and you get some representations for the input  and the only learning that \nhappens is on top of this transformed input  so  you now have a transformed input and \non top of that you are going to train a classifier and you are going to learn the weights of \nthe classifier  so  the only thing that you learn is the weights of the classifier  \n\f\n \nso   that  is  equivalent  to  learning  only  the  soft  max  layer   in  case  of  a  feed  forward \nnetwork that is the output layer right \nnow   instead  of  using  these  so   this  is  the  question  instead  of  using  these  handcrafted \nkernels  or  features   such  as  edge  detectors  or  blur  detectors  or  what  not   can  we  learn \nmeaningful kernels in addition to learning the weights of the classifier  do you get this  \nquestion  at  least   whether  the  answer  or  not   but  you  get  the  question   so   what  i  am \nasking is that why should i hand code this edge detector ok  \n\n \n \n \n\fwhy not have after what is the edge detector  it is like a three cross three matrix right and i have \nseen tons  of such matrices in  my feed forward neural  networks   i have dealt with  very \nlarge matrices  which were called parameters of the network   \nso  why not have a three cross three or a five cross five or whatever dimensional matrix and try to \nlearn   what  should  be  the  right  values  in  this  matrix  instead  of  hand  coding  the  edge \ndetector matrix  do you get the idea  how to do that as still far  but at least do you  get \nthe idea that is what i am we are trying to do ok  \nso  now instead of just learning the weights of the classifier  we also want to learn the \nweights of the kernels  that means  instead of using handcrafted features  i am now going \nto  \nstudent  \n  \nlearn  the  features   so   that  is  the  difference  between  deep  learning  and  machine \nlearning   so   you  had  handcrafted  features  there  and  now  you  are  going  to  learn  the \nrepresentations also by treating them as additional parameters in your network how you \nwill  do  that   we  will  see  and  it  is  very  simple  given  that  you  understand   how  to  train \nfeed forward networks  \n\n \nbut then why the stop there  why just have one feature representation for the input  can \ni learn multiple such kernels right  i could have one three cross three matrix  which learned this \n \n\ffirst representation another  three cross matrix  which learned this another representation and \nyet another three \n three or five \n five or seven \n seven matrix  which learns this different representation  so  \ninstead  of  learning  one  static  representation  from  the  input   i  could  learn  multiple \nrepresentations from the input  \n \n \nin fact  why not why just stop there  what is the next thing that i am going to try to do  \nmultiple  layers  of  features  right  so   that  means   at  the  first  layer  i  learned  this \nrepresentation  now i could take this and try to learn an even more abstract representation  \nand then keep doing this to make it deeper and deeper  do you get this  ok  \nso   at  every  stage  now  i  have  these  parameters   which  are  helping  me  learn  the \nrepresentation of the input  i am learning multiple representations at every layer and then \nhaving  multiple  layers  of  these  representations  right  and  everything  is  learnable  end  to \nend ok  so  you get the difference between deep learning machine  learning now there is \nno  handcrafting  of  features   you  are  learning  the  feature  representation   i  know   i \nunderstand there is some confusion about how you would do this  \n\uf0b4\uf0b4\uf0b4 \n\f\n \nbut we will get to that just trust me on that that you will be able to figure out how to do \nthis ok  \nand all of this we have some weight matrices here  we have some weight matrices here  \nthese are the layer one weight matrices  the other layer two weight matrices and these are the \noutput layer matrices and you see this layer wise  arrangement of these weight matrices \nand  they  are  very  comfortable  with  this   because  we  have  done  feed  forward  neural \nnetworks  where we had these multiple layers and we knew how to back propagate from \nthe last layer to the first layer   \nnow  what i am trying to say is that  i would like to adjust these weights of filters in such \na  way  that  my  classification  loss  is  minimized  and  what  is  the  loss  function  that  i  am \ngoing to use here  \nstudent  \n  \ncross entropy  because this is a multi class classification problem ok   \n \n \n \n\fso  just hang on with this intuition and we will make it more clear fine  \nso  such a network which has these multiple convolution  learned convolution operations \nat every layer and then multiple such layers is known as convolutional neural network  \n\n \n ok  fine   so   get  this  idea  that  we  need  to  learn  kernel  filters  by  just  treating  them  as \nparameters of the classification model ok  but how is  this different  from  a regular feed \nforward  neural  network   you  could  have  taken  a  regular  feed  forward  neural  network \nand i will show it to you on the next slide and what is the difference between that and a \nconvolution operation  \n \n \n\f\n \nso  if you understand that then  you would be done for this lecture  yeah  \nso  we have an input  so  let us say now  i will take back the eminist case  where you are \ngiven an input as an image and these are digit inputs and you want to classify them into one \nof ten inputs and i am going to assume that  my input is four cross four  that means  i have sixteen \npixels ok   \nso  the simplest thing that  i could have done or the feed forward neural network way of \ndoing  this  is  that   i  would  just  flatten  out  this  image   i  will  get  sixteen  inputs   i  need  ten \noutputs  at  the  output  layer   so   i  have  an  output  layer   which  will  have  one  of  these  ten \nclasses  and  then   i  add  as  many  layers  that  i  want  in  between  ok   this  is  what  a  feed \nforward  neural  network  would  look  like  and  if  i  consider  any  one  neuron  in  the  first \nlayer   it  takes  inputs  from  all  the  sixteen  inputs  right   that  is  how  a  feed  forward  neural \nnetwork is  you have these extremely dense connections  where every output depends on \nevery input at every layer ok   \n \n \n \n\fnow   so   this  is  the  same  story  which  i  have  said   now   let  us  look  at  what  a \nconvolutional neural network looks like  so  again you have these sixteen inputs  i am using a \ntwo cross two convolution ok  now if i use a two cross two convolution  if i place it here  then i \nam  using  pixels  one   two   five  and  six  and  computing  one  value   so   you  see  the  difference \nbetween  this  and  a  feed  forward  neural  network   in  a  feed  forward  neural  network  heleven \nwould have depended on  \nstudent  \n  \nsixteen values  sixteen inputs and a convolutional neural network it is depending on  \nstudent  \n  \nonly four  only four neighbors  ok  and similarly htwelve  i am using a stride of two by the way right  \nso   i  am  not  placing  the  filter  here   i  am  just  skipping  one  step   htwelve  would  depend  on \npixels three four seven eight   ok and so on right  \nso  one thing is clear that as opposed to a feed forward neural network  you have sparser \nconnections here  is that clear  and why do we have sparse connections  because we are \nexploiting our knowledge about images that in an image  you do not really care about the \ninteractions between on between a pixel at the leftmost left top most corner and the right \nbottom corner right  so  there is sky here  there is ocean here or there are trees here  you \nwould want to capture the neighborhood around that pixel not really capture it with the \n \n\fentire image  that is why you do not want all of these sixteen inputs to contribute   you only \nwant a small neighborhood to contribute  do you get that intuition ok   \n \n \nso   this  is  the  first  property  of  a  convolutional  neural  network  that  it  has  sparse \nconnectivity ok  but its sparse connectivity really good  i just made a case for that  now \ni  am  going  to  counter  argue  right   is  it  really  good  that  you  have  these  sparse \nconnections  because you are losing out information right  you are using out interactions \nbetween certain pixels  so  why can  why is that good  \nstudent  \n  \ni  am  hearing  a  lot  of  interesting  answers   but  remember  that  you  are  always  going  to \nhave multiple layers ok  so  consider these two pixels  in the first layer these two pixels did \nnot  interact   because  htwo  only  dependent  on  these  three  and  hfour  only  dependent  on  these  three  \nthere  is  no  a  there  is  no  unit  here  which  depends  on  both xone  and xfive   is  that  obvious  \nbecause i am just using a window of size three   \nbut now once i go to the next layer  once i go to gthree  gthree depends on htwo  hthree  hfour here  which \nin  turn depends on xone xtwo xthree xfour xfive right   so   even though at  this  layer xone and xfive  are not \ninteracting with each other as you go deeper these interactions become obvious  do you \nget that  right  so  that is why you will always use a deep convolutional neural network  \n \n\fwhere all the pixels get to interact at a deeper layer  but at the more image  it layers you \njust want to capture the interactions between a neighborhood   \nso  it is like you take this neighborhood find out something  then take neighborhoods of \nneighborhoods and then try to find out something at the next layer and keep continuing \nin  this layer  how many  of  you  get  this  right  ok  so  this is  what sparse connectivity \nlooks like  \n\n \n \n \n \n \n\fanother  characteristic  of  cnns  is  something  known  as  weight  sharing   so   let  us  see \nwhat it is  so  remember i had considered this two cross two kernel and  i was placing it at \nthese four pixels  which is pixels one  two  five and six and i was pacing another kernel at these four \npixels  which is pixels eleven  twelve  fifteen and sixteen right  these four pixels  and i have used different \ncolors  for  them  indicating  that  these  filters  are  different   so   they  are  both  two  cross  two \nfilters  but i am assuming at the values inside them are different  does this have to be the \ncase  just think what a filter is trying to do  \nstudent  \n  \nit is striding across the entire image  at every location i want to do the same operation  \nremember  when we are doing blurring or edge detection or sharpening i had the same \nfilter  which i was applying at every location  so  i want to see  what is the effect of this \nfilter  throughout  the  image   so   i  do  not  really  want  to  change  this  filter   that  means \nthese four weights would be the same as  \nstudent  pink weights  \nthe  pink  weights   how  many  of  you  get  this   so   this  is  a  question   do  we  want  the \nkernel weights to be different for different portions of the image  so  imagine that we are \ntrying to learn a kernel that detects edges  so  the same kernel configuration is required \nthroughout the image because  that is the kernel configuration  which will detect an edge  \n\n \n \n\fso   you  want  the  same  kernel  to  be  there  everywhere   so   we  are  going  to  share  these \nweights  they should not be these pink and orange weights  we will just have the same \nweights everywhere ok  so  this is known as weight sharing   \nso  now this is something ridiculous  if you think about it now how many weights do i \nhave in layer one  \nstudent  \n  \nfour weights that is all that looks too less right it would lead to  \nstudent  \n  \ndash fitting  \nstudent  \n  \nunder fitting because we have very few parameters so  how do i deal with the situation  \nstudent  \n  \nyou  will  have  multiple  kernels  right   so   you  will  have  another  kernel   which  takes \nsomething else you will have one more kernel  which takes something else and you can \nhave as many such kernels right  so  the more the number of kernels will have you will \nhave  that  many  into  four   as  the  number  of  parameters  and  that  many  outputs  at  layer  one  \nhow many if you get this  ok good   \nso   these  are  the  two  important  characteristics  of  convolutional  neural  networks   one  is \nsparse connections and the other is weight sharing ok  \n\f\n \n so  so far we have focused only on the convolution operation  now let us see  what does \na full convolutional neural network look like or maybe i will do this next time  i think \nthis is  \n \n\f"}
{"audio_filepath": "lec010_004.wav", "duration": 1048.644, "text": "\n \n \n \n \nso  we will start from where we left  yesterday  so  this is what we had seen yesterday  \nwe saw what\u2019s the difference between a convolutional neural network and a feed forward \n\fneural  network and  we focused on two main properties   one is  sparse  connectivity and \nthe other was weight sharing  that is about it and then we saw that this representation of \ni  mean   we  saw  this  diagram  about  how  to  you  could  have  multiple  kernels  and  each \nkernel  would  apply  across  the  entire  image  and  the  weights  would  be  shared  for  that \nkernel   \n\n \nso  far we have only focused on the convolution operation and even when you have seen \nthe  neural  network  or  the  convolutional  neural  network   we  have  only  seen  the \nconvolution layers   right  so  there is something more in a typical convolutional neural \nnetwork and that is what i was about to start yesterday  so  we just continue from there   \n \n\f\n \nso  this is what a full convolutional neural network looks like  so  ignore these things for \nnow  all these parameters etcetera as a just ignore for them  i will just walk you through  \nwhat is important in this diagram  right    \nso   your  input  is  an  image  and  the  tasks  that  you  are  dealing  with  here  is  digit \nrecognition or handwritten digit recognition right and what you see here is that you have \ntaken an input which is a two dimensional input and then the next layer you actually see one  \ntwo  three  four  five  six outputs  so  what does that mean  \nstudent  \n  \nyou have use six filters apply them  throughout the filter  throughout the image each filter  \ngave  you  one  feature  map   and  so  in  this  layer   you  have  six  such  feature  maps   so   the \noriginal  two  dimensional  input  has  now  become  six  two  dimensional  outputs   ok   after  that \nthere is something known as a pooling layer  we will see what a pooling layer is in detail   \nnow  what i want you to understand is lets assume that  what the pooling layer does is it \ndoes some kind of a shrinking  it takes the original output and shrinks it  how it shrinks \nit  we will see in a while  but let us see what happens after that  so now  you have one  two  \nthree  four  five  six as  your input  so now  you have this is  your input  this volume is now   your \ninput  i  call  it  a  volume   because  it  has  depth  height  and  width  and  now  you  are  again \n \n\fgoing to apply convolutions to that and what do you see  how many outputs do we have \nnow  we have sixteen outputs   right  so  what does that mean  \nstudent  \n  \ni took this threed input applied sixteen threed filters on it each threed filter give me one feature map  one \ntwod feature map  why one twod feature map  \nstudent  \n  \nbecause we are doing a twod convolution  we are taking a threed filter  but we are doing a twod \nconvolution   right  and then after that again this becomes my input and then do a max \npooling on top of that and then something else happens after that  so  there as which we \nwill come to later  \nso  right now i just want to say that there is this input  then you come out will come with \nsome output after applying convolutions  now this becomes the input for your next stage  \nwhere you have done pooling  now the output of pooling becomes the input for the next \nstage  so  you will take this as the input  apply some convolution on that get some output \nand continue in this way right and we will come back to what these are   \n \n \n \n \n \n\fso   now  let  us  dive  deeper  into  what  is  pooling   what  does  the  pooling  layer  actually \ndo  so  here is your input again  it is a volume and now when i say input  you should not \njust think of the input as this  remember that  all of these can be inputs right  so now  at \nevery stage once   you have  got  an output for the next  stage that becomes input  that is \ntypically how it was even in the feed forward neural network right  once you compute a \nhidden representation that is the input to the next layer   \nso   i  have  some  input  at  one  of  these  layers  either   the  input  layer  or  any  of  the \nintermediate layers and i apply a filter on that and that filter gives me some twod output  it \ngives me one feature map and let us say  this is what the feature map looks like   \nnow  what does the pooling operation do  so  i would apply two \n two pooling with a stride \noff two  so  let us see what that will do  that means  i look at this two \n two region  i will pick \nup the max value from there that is why  this is max pooling  ok  so  the max value is eight  \ni will just keep that then  i am  going to  do a stride of two   that means   i am  not  going to \nplace it on this block  i am just going to shift to the next block  again  i will take the max \nfrom there  which is four right and i continue this and i get this is the output  so  you see \nwhy the shrinkage happens because  i am taking a two \n two area and i am shrinking it by a \npicking up only one value from there right   \nso  it actually kind of half the width and half the breadth so  total of one by four reduction is \nwhat you get  but you could also use a filter with stride off one  so  this is what it would \nlook like  so  you will place it here  take the max value then  the max value then  the max \n\uf0b4\uf0b4\uf0b4 \n\fvalue and so on right  so  in that case you will get a lesser reduction and instead of max \npooling   you  could  also  do  average  pooling   that  means   instead  of  taking  the  max  of \nthese  four   you  could  take  the  average  of  these  four   is  it  fine   so   is  the  pooling  operation \nclear and how it results in the reduction of the size of your input  ok  \n\n \nso now  what we will do  is now that we have some idea of what a full convolutional \nneural  network  looks  like   so   it  looks  like  alternating  convolution  and  max  pooling \noperations  we know what a convolution operation looks like  in particular we know that \na threed filter applied to a threed input results in a twod output  because we are not applying the \nconvolution  along  the  depth   we  just  applying  the  convolution  along  the  width  and  the \nheight right  so  that is what we know so far  \n \n\f\n \nand  based  on  this  knowledge   now  we  are  going  to  see  some  success  stories  of \nconvolutional neural network right  \nso   we  will  start  with  the  first  one   which  is  lenet five   so   this  was  already  the  fifth \nversion  this was around ninety seven or ninety eight or something and i had mentioned this  when we were \ndoing the history lecture   \nso  this is the input  now you have decided to apply six filters  you have said that the stride \nis going to be one  that means  you are going to place at every location  the spatial extent is \ngoing to be five and the padding is going to be zero   now  the question that i have for you is  \nhow many parameters does this convolution layer have  what are the parameters here  \nstudent  the weights  \nthe  \nstudent  \n  \nthe filters  the weights in the filters  how many filters do have  \nstudent  six  \nsix  how many ways does each filter have  \n \n\fstudent  twenty five  \ntwenty  \nstudent  five  \ntwenty five right  five cross five is a twenty five  so  the total number of parameters is one hundred and fifty  now  i want you to \nappreciate something here  so  the input was actually thirty two cross thirty two  which i believe one thousand and twenty four \nand the output was twenty eight cross twenty eight right  that is what the output you got that is  i guess seven hundred and eighty four \nyeah  so  in a feed forward neural network  if you had x belonging to rone thousand and twenty four and your h \nbelonging  to  r  seven hundred and eighty four   how  many  parameters  would  you  need   one thousand and twenty four \n  seven hundred and eighty four   how  many \nparameters do you have here   \nstudent  one hundred and fifty  \none hundred and fifty much much smaller right  this is because of \nstudent  \n  \nboth  sparse  connectivity  as  well  as  weight  sharing  right   so  now   you  appreciate  the \ndifference between the two ok  and thats one of the reasons that even  before this is for \nexample  from ninety seven  ninety eight  one thousand  nine hundred and ninety seven  one thousand  nine hundred and ninety eight right  so  even before the deep learning wave of two thousand and six \nor the revival after two thousand and six right  convolutional neural networks must still being trained for \neven deep networks four to five layers  because they had much fewer parameters and that is \nwhy  it  was  relatively  easier  to  train  them  as  compared  to  a  very  dense  feed  forward \nneural network  \n\uf0b4\f\n \nso  i want you to appreciate that fact  now  after this i have the pooling layer  when i am \ngoing to  use a stride  of one and f equal  to  two   that means   i  am  going to  pick up  ok  so \nnow  i am going to use a max pooling layer  where i am decided to use a stride of one and \nthe max pooling will happen  in the region of two \n two and again k six  because there are just \nsix  filters   so   max  pooling  happens  on  every  feature  map  independently   it  does  not \nhappen across the depth  that means  i am not going to pick the max along these six layers  \ni am going to pick the max along each of these feature maps  so  it is a per feature map \noperation  ok  \nand this since it is a two \n two filter  it will result in a size reduction and from twenty eight \n twenty eight  i will \nget to fourteen \n fourteen  how many parameters is the max pooling layer have  \nstudent  \n  \nwow good  there is no parameters in the max pooling layer  because you are not having \nany weight matrices  just taking the input and applying a simple max operation on that  \nthere  is  no  w  transpose  x  or  any  kind  of  a  transformation  happening  there   now   this \nbecomes your input  what\u2019s the size of this volume  \nstudent  thirty two  \nfourteen cross fourteen cross  \n\uf0b4\uf0b4\uf0b4\uf0b4 \n\fstudent  six \nsix  so  all the filters that i am going to use from now on  what is the depth of those filters \ngoing to be  \nstudent  \n  \nwhat is the depth going to be   \nstudent  six   \ni want to everyone to answer  \nstudent  six  \nsix right  because we are always going to assume that the depth is equal depth of the filter \nis equal to the depth of the input   \n\n \nnow  here they decided to use sixteen filters and by the way  you did hopefully notice that  \nthis twenty eight  how did you get it  from which formula  \nstudent  w n minus \n  \nwn minus f plus twop plus one right  so  that is the formula   \n \n\fso  now we have a fourteen cross fourteen input and you have sixteen filters  so  what is the depth of the \noutput volume going to be   \nstudent  \n  \nwhat is the depth of the output volume going to be  \nstudent  sixteen  \nsixteen  right  and a  ok so  you have sixteen and you have a spatial extent of five \n five  just a minute \nspatial extent of five \n five  how many parameters does this layer have  i want everyone to \nsay it  \nstudent  four hundred  \n \n \nfour hundred ok  fine so  that is  \nstudent  \n  \nthere are sixteen of these each is five \n five  \nstudent  \n  \noh into six into d ok ok good  then we made a mistake here also  no there the depth was one \nare  you   do  you  get  it  how  you  got  two thousand  four hundred  right   we  forgot  about  the  depth   so   each  of \n\uf0b4\uf0b4\uf0b4 \n\fthese filters is five \n five \n depth right  what is the depth  six the same as the input  right  so  \neach of these filters is twenty five \n six  which is one hundred and fifty \n sixteen yeah fine  is that ok  did i confuse you \nor everyone back on track  pooling layer  edge should have been two  because of that half \nreduction using  \nstudent  \n  \nyeah  maybe   can  we  just  check  this   i  think  it  should  be  two   ok   yeah  there  was  a \nquestion  \nstudent  \n   \nstudent  \n  \ngo from the pooling layer to the next layer from here to the next layer  ok  \nso now  what is the depth of your input volume  six and what is the width and height  fourteen \ncross fourteen  so now  every filter that i going to apply at the next layer is  going to have a \ndepth of six  so  i have decided to apply sixteen such filters  so  what is the depth of this layer \ngoing to  be  the depth of the output is  equal  to the number of filters  so  the depth  is \ngoing to be sixteen and all my filters are five \n five  but  what we forgot is that  when i say the \nfilter  is  five \n  five   it  is  actually  five \n  five \n  six   because  the  depth  is  also  there  right   so   it  is \nwidth into height into depth  so  that is this number of parameters in each of my filters \nright  and  that  is  one hundred and fifty  and  i  have  sixteen  such  filters   so   that  gives  me  a  total  of  two thousand  four hundred \nparameters  is it fine  ok  ok  \nso  now   we  have  a  volume  of  size  sixteen  cross  ten  cross  ten   now  i  am  going  to  do  max \npooling on that maybe  again this should be two  there was the same doubt you had fine  \nso  it will result in a reduction in the output and now what is the volume  what is the \nsize of this volume  five \n five \n sixteen and the parameters is zero  max pooling layer does not have \nany pooling   \nnow  after this what we have is something known as the fully connected layer  ok  so  \nnow  as i said the size of this volume is sixteen \n five \n five  its arranged in these feature maps  \nbut  i can always  flatten  it to  get  one single vector  do  you  get  that  so  from  these sixteen \nfeature maps each of five \n five size  i can flatten it out and get the single vector of size four hundred  \ndo you get that  ok  so  thats what  i do in the fully connected layer  \n\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\fso now  i am going to flatten this treated as a single vector and then fully connect it to \nthe next layer  what do i mean by fully connected  dense connections  no more sparse \nconnection  so now  we have a feed forward network from  this  point of view  so   you \nhave four hundred and that connects to a layer of size one hundred and twenty  so what are the number of parameters   \nstudent  four hundred into one hundred and twenty  \n \n \nfour hundred into one hundred and twenty  plus we will have one hundred and twenty biases  so  that is what this number is  fine  so  this \nis one fully connected layer of size one hundred and twenty  after that i have another fully connected layer of \nsize eighty four  so  the number of parameters would be one hundred and twenty into eighty four plus eighty four and after that i have \nthe output layer  so  the output layer this was twenty six or whats the output  \nstudent  \n  \noh  but  the   this  is  digit   this  is  alphabet  recognition  right   ok   so   probably  they  have \ndone  the  computation  using  ten   but  it  should  have  been  using  twenty six  as  the  output  layer  \nbecause you want to predict one of the twenty six alphabets  so  you can assume this is twenty six  so  it \nwould be eighty four \n twenty six   twenty six right that is the size of the output layer right   \nnow  do you observe something immediately is a something very striking immediately  \nin terms of the number of parameters   \nstudent  \n  \n\uf0b4 \n\fthe fully connected layers clearly dominated right  here  we were dealing of order two thousand  four hundred \nand max and here we just start with forty eight thousand itself right  so  just keep this in mind that the \nfully connected layers have the largest number of parameters  that you have  and we will \ntry to come back to this and see if we can solve this problem  ok  \nso  now   when  you  see  a  convolutional  neural  network   you  should  be  able  to  reason \nabout the following things  at each layer  what is the size of the input volume  what is \nthe size of the output volume  what are the number of filters being used  and what are \nthe  number  of  parameters  in  that  layer   right   if  you  can  reason  these  things  and  you \nhave  really  understood   what  is  actually  happened  and  unless  you  can  reason  these \nthings  i do not see how you can efficiently code it up right  so  you should be able to \nknow that  this is the size of the input  this is the size of the output and so on and i guess \nall of us are comfortable with others   \n\n \nso  now   how  do  we  train  a  convolutional  neural  network   what  is  the  answer   your \nnodding   that  means   you  do  not  know  or  you  know  it  is  too  trivial  to  even  ask  this \nquestion  how will you train it  \nstudent  \n  \nbut  how do  you back propagate through  a convolution  operation  that  is a very  nasty \nlooking operation   \n \n\f\n \na  cnn  can  be  implemented  as  a  feed  forward  neural  network   with  what  being  the \ndifference  \nstudent  \n  \nonly some of these weights would be active  all the gray weights will not exist only the \ncolored weights will exist right  now can you back propagate to this network have you \nseen something similar before  \nstudent  dropout  \ndropout right so  if you could do that you can do this also  so now  if you take this view \nof a convolutional neural network where so  this is just to give you an intuition or make \nyou  feel  confident   that  once  you  know  the  backpropagation  algorithm   there  is  no \nnothing much different from training a convolutional neural network operation  ok  \nso  everyone is fine with that  how many of you agree with that  that you can actually \ntrain using the same algorithm with some smart coding required to make sure that these \nweights  are  not  active  and  so  on   but  in  practice  of  course   you  will  not  do  this   in \npractice  you will define the convolution operation  you will also define the derivative of \nthe  convolution  operation  and  then  use  that  right   because  that  would  be  much  more \nefficient   this  is  very  inefficient  right  because   you  are  assuming  that  there  is  a  fully \n \n\fconnected network and then some things do not exist there  so  that is not thats the whole \npoint  i mean you wanted to avoid such dense connections right   \nbut  in  principle  you  could  have  just  used  this  and  trained  the  convolutional  neural \nnetwork   in  practice   you  do  not  need  to  worry   because  you  just  need  to  define  your \nforward convolution operations and people like google and all who release tensor flow  \ntorch and all will do the hard work of doing the back propagation for you right  so  you \nnever  have  to  write  back  propagation  in  your  life   apart  from  what  you  have  already \nwritten in the assignment right that is why i make you go through that torture once   \nso  now  afterwards  afterwards  whenever  you  use  any  of  these  platforms  or  pytorch  or \ntorch  or  tensor  flow   the  back  propagation  comes  for  free   you  just  need  to  write  the \nforward progression  that means  you just need to write convolution operations and you \ndont need to worry about how the derivatives will be computed  but i what i want you to \nunderstand is that conceptually  it is the same  you can still use or in fact  you still use the \nsame back propagation algorithm  to train a convolutional neural network also  everyone \nis  fine  with  this   pk   ok   so  now   we  know  how  to  trained  a  convolutional  neural \nnetwork also   \n\f"}
{"audio_filepath": "lec010_005.wav", "duration": 1238.282, "text": "\nso   now   we  will  go  to  the  next  module   we  will  talk  about  some  success  stories  on \nimagenet  right   so   this  is  the  challenge  which  actually  made  convolutional  neural \nnetworks very famous back in two thousand and twelve   \n\n \nso  they are going to look at some algorithms in fact  two more hopefully today  \n\f\n \nso   this  is  the  story  right   so   there  is  this  challenge  or  competition  called  imagenet \nlarge scale visual recognition competition right that is what ilsvrc stands for  and \nthis was a data set created which had one thousand categories  it actually has ten thousand categories  \nbut in the competition we use only thousand of those categories yes   \nso  one thousand plus one thousand i think  the roughly the data set size is one million  and so  that is what \nwas used for training a classifier  and i am talking about two thousand and ten  the pre deep era right  i \nmean so  of course  deep era networks existed at that time  but the participants and these \nchallenges and that time were relying on the classical machine learning approaches  so  \nwhat was that approach  take the image   \nstudent  \n feature   \nextract features  which features  \nstudent  \n  \npredominantly  \nstudent  sift and hawk  \nsift and hawk features  were the predominant  features at  that time and then  you train  a \nclassifier on top of that  and then  you use things like on symbols or better handcrafted \nfeatures and things like that certain more tricks on top of that  so  that  with that on this \n \n \n\fdata  in  two thousand and ten  the  error  was  twenty eight two  percent   that  means   if  i  give  you  a  test  set  of  one thousand \nimages you will make two hundred and eighty two errors on that right  that is what this means  then  in eleven there \nwas still some progress  this was again pre deep era and there was this error came down \nto  twenty five eight   and  then   in  two thousand and twelve  there  was  this  alexnet   which  was  a  deep  convolutional \nneural  network  applied  to  the  task  of  image  classification  and  it  gave  a  dramatic \nreduction right from twenty five eight to sixteen four   \nand was i think absolute  in absolute terms eight to nine  eight to nine percentage better than the best \nsystem in that competition that year  ok  so that was in two thousand and twelve  in two thousand and thirteen  there was further \nimprovement  on  a  different  architecture  for  doing  this  and  that  give  a  further  error \nreduction of eleven seven  then in two thousand and fourteen there was vgg net  so  these are all three that we are going \nto  see today  which  give a further error reduction of seven three  then  google  decided to join \nthe party  and they make it six seven and as  i have said before then afterwards microsoft got \ncrazy  and they brought  it  out  in three fifty seven and this is when we started making claims that a \nconvolutional neural network has become better at this task than humans right   \nbecause  if you show these one thousand images to a human  even a human is bound to make a \nthree five percent more than three five percent error  that means  because some of these images would \nbe blurred  so  i would not be very sure whether  this is a bulldog or a different type of \ndog or something like that right  so  i even a human cannot really recognize it correctly \nand  that  is  the  whole  hype  around   how  convolutional  neural  networks  have  beating \nhuman level performance on this particular task right   \nand  let  us  see  so   this  was  all  the  shallow  pre  deep  era   the  first  architecture  was  eight \nlayers  and  i  think  this  was  called  a  varied  no  this  probably  not  this  yeah   the  second \narchitecture was also eight layers  then  we had nineteen then  twenty two and then one hundred and fifty two right  thats how the \nprogress  has  happened   so   these  are  all  the  architectures  that  we  are  going  to  look  at \ntoday or at least some of them today and the rest maybe tomorrow  \n ok  so  we will start with alexnet and i am going to tell you the exact architecture of \nalexnet  what was refused  what did it actually use   \n\f\n \nso  the input was an rgb image  so  it had a depth of three and it was two hundred and twenty seven \n two hundred and twenty seven that is \nwhat the data set input was  all the images in the data set were to two hundred and twenty seven \n two hundred and twenty seven cross three  so  \nthe  first  thing  that  they  did  was   they  decided  to  use  ninety six  filters   can  you  read  that  \nanyways i will say it outright   \nso   they  resided  to  use  ninety six  filters  with  a  spatial  extent  of  eleven  cross  eleven   a  size  of  four   and \npadding of zero  ok  so  the moment you see size  a stride of four  what do you know is going \nto  happen   there  is  going  to  be  some  shrinkage   roughly  by  how  much   one fourth  \nright  so  now  can you compute these three things  what was wtwo  what was htwo and what \nwas the number of parameters in this layer  we will do it for a few of these layers and \nthen i will just rush through that  so  whats wtwo going to be  you have already done this \ncomputation right  the exercise that we did was exactly this computation  so  there was \nfifty five cross fifty five and what is the depth going to be  i want everyone to say that  \nstudent  \n  \nok  and what is the number of parameters  \nstudent  \n  \nninety six into  \nstudent  eleven  \n\uf0b4\uf0b4 \n\feleven into  \nstudent  eleven  \neleven into three  don\u2019t forget the depth  the depth is three here   \n\n \nso  that is the number of parameters that they had in this layer  ok  eleven into eleven into three into \nninety six  now  what is the next layer going to be  a max pooling layer   \n\n \n \n \n\fok   so  they  had  a  max  pooling  layer   where  they  used  a  three  cross  three  max  pooling   that \nmeans   you are going to pick up max from a three cross three grid  and the stride was two  that \nmeans we are going to get half the output  and now can you tell me  what wtwo htwo would \nbe roughly  half of fifty five fifty five right so  twenty seven twenty seven   \n\n \nand what is the number of parameter is going to be  don\u2019t be lazy  everyone be say it   \nstudent  zero  \nzero right so  that is the max pooling layer   \nnow  what is the size of your input volume at this point  \nstudent  twenty seven  \ntwenty seven cross twenty seven cross  \nstudent  ninety six  \nninety six  as  opposed  to  the  original  input  which  was  two hundred and twenty seven  cross  two hundred and twenty seven  cross  three   so  as  you  keep \nprogressing   your width  and height is decreasing  but  your depth is increasing because  \nyou are using more and more filters to capture more and more patterns in the images   \n \n\f\n \nnow  so you have twenty seven \n twenty seven \n ninety six  then they decided to use two hundred and fifty six filters  each of size five \n five \nwith a stride of one and padding of zero  ok  is it right  so how many parameters do you have \nnow  \nstudent  \n  \ntwo hundred and fifty six into   \nstudent  five into five  \nfive into five into  \nstudent  ninety six  \nninety six  \n\uf0b4\uf0b4\uf0b4 \n\f\n \nso  thats the number of parameters that you will have and the size since would decrease \nonly by one right because  you have a stride of  it will decrease by two  because a filter size is \nfive and you have a stride of five  ok  \n\n \nso   these  are  the  number  of  parameters   we  had  zero six  million  parameters  in  this  layer  \nwhat is the next layer going to be  pooling   \n \n \n\f\n \nso  you do a max pooling  again you do a three \n three  you do a stride of two  so your width in \nheight  is  going to  decrease  the depth  does not  change  remember in  max pooling  the \ndepth  does  not  change  because  the  max  pooling  operation  is  per  feature  map   it  is  not \nacross the depth fine  then use a three \n three filter and three hundred and eighty four of those   \n\n \nso  how many parameters would you have  \nstudent  \n  \n\uf0b4\uf0b4 \n \n\fthree hundred and eighty four into three cross three into two hundred and fifty six  the depth   \nso  now you guys get it  so  i will not bore you anymore   \n\n \n and then you have a convolution operation again  which is a three hundred and eighty four convolutions each of \nsize three \n three and so many parameters  followed by a convolution operation again  followed \nby a max pooling operation  then  followed by a fully connected layer  \n\n \n\uf0b4 \n \n\fso  what would i do to this two hundred and fifty six \n two \n two  i will fatten it  so  i will get what dimensional \noutput  \nstudent  one thousand and twenty four  \none thousand and twenty four  two hundred and fifty six into two into two  so this one thousand and twenty four dimensional vector i am going to fully connect it to \na four thousand and ninety six dimensional vector  how many parameters  four million right  four into ten raise to six \nright so  roughly four million   \n\n \n then  you  have  another  four  million   another  four thousand and ninety six  vector  fully  connected   how  many \nparameters   \nstudent  sixteen million  \nsixteen million  then  you have the one thousand classes that you are interested in   right  so again \nfully  connected   so   you  get  the  full  architecture   anyone  has  any  questions   no  one \nwants to know why this particular configuration among all the possible configurations  \nwhy  not  ten  layers   why  not  first  eight  cross  eight  filters   why  not  nine  cross  nine  filters  \nunfortunately  no one knows   laughing  \nstudent  \n \nso  this i mean see this what this is what would happen right  now we get into something \nknown  as  hyper  parameter  tuning  right   so   what  are  the  hyper  parameters  in  this \n\uf0b4\uf0b4 \n\fnetwork  the kernel size is and the number of filters right  so  you would have tried a \nlot  of  these  things   evaluated  on  the  validation  set   seen  which  one  gives  the  best \naccuracy  and  then  chosen  right   so   that  is  probably  what  would  have  happened   but \nthere is not enough insight into how this particular architecture came up   \napart from some things right that three curves three neighborhood sounds reasonable  initially  \nwhen you have the full image  you use larger filter sizes because  you want to capture a \nlot of things there but  once the image has shrunken you use smaller filter sizes  so  those \nare some rational decisions which look reasonable  but why these three convolutional filter \nlayers back to back instead of convolution max pooling convolution max pooling and so \non right  \nso   the  some  of  those  things  are  not  clear  so   just  in  case  you  are  wondering   do  not \nwonder   this  is  just  the  architecture   this  is  known  as  modestly  named  as  alexnet   so  \nthat is  laughing  yeah and so i said that this has eight layers  but you clearly see more than \neight  layers  here   so   why  did  i  say  that  has  eight  layers   which  are  the  layers  we  are  not \ncounting  \nstudent  \n  \nwhy   \nstudent  \n  \nbecause  they have no parameter right  so when you count the number of layers you only \ncount  those  layers  which  have  parameters   so   you  have  five  convolutions  and  three  fully \nconnected layers   \n\f\n \nthen  so the total number of parameters in this network is twenty seven fifty five million parameters and  \nok at this point i will and  obviously  you notice that most of these parameters were there \nin the fully connected layer  so   you had four million here  then sixteen million here and then \nagain four million here right   \nso   roughly  twenty four  million  of  the  twenty seven  million  parameters  were  there  in  the  fully  connected \nlayer  you see that skew in the number of parameters  ok  \n\n \n \n \n\fand  i  will  just  look  at  the  fully  connected  layer  again   so   the  last  max  pooling  layer \nactually  gave  you  a  two hundred and fifty six  cross  two  cross  two  output   you  just  flatten  it  to  get  a  one thousand and twenty four \ndimensional vector and then you connected fully to the four thousand and ninety six vector right  so  that is what \ni mean by a fully connected layer  why do you move max fully   \nso  the reason for that is basically  to shrink the size of the image right because  after that \nif   if  you  keep  working  with  this  size  right  then   the  number  of  parameters  is  going  to \nreally blow up  a by using a larger stripe yeah both of them are feasible right  so  now  \nsee from here remember that we had the original image sizes two hundred and twenty seven cross two hundred and twenty seven and by the \nend we were just left with two cross two  \nand then adding a fully connected layer on that makes sense right  if i had not done this \nshrinkage throughout either by increasing the stride of the convolution layer or by doing \nmax  pooling  right  then   you  would  have  left  with  something  of  the  order  of  two hundred  cross \ntwo hundred here and then  you have to do a fully connected on top of that is just infeasible right  \nit  just throws away all the hard work that  you have done by doing weight  sharing and \nsparse connectivity right  so  that is not feasible   \nthere  are  also  papers  with  say   which  i  think  it  is  titled  fully  convolutional  neural \nnetwork which does not have any max pooling layers and they show that that also works \nfine   in  fact   when  we  see  vgg  net   we  will  see  that  it  has  back  to  back  convolution \nlayers  and  very  few  max  fully  layer  right   so   these  are  all  things  which  people  have \ntrained   \nnot so  many  years  two  years the challenge came out  in  two thousand and ten and in  two thousand and twelve this was  used \nright  so  it is like not really a large gap right  and if  you read the original paper they \nhad to do a lot of tricks to actually make this work  it was not as simple as i am showing \nit  of course  now with  all the stability which comes from these platforms  tensor flow \npytorch  you can probably just go and implement this as it is and you should be able to \nreproduce the results  but six years back that was not the case  there was a lot of hard \nwork  involved  in  getting  this  too  work   and  they  this  was  also  the  paper   which \nintroduced the relu non linearity in the context of convolutional neural networks right  \nso  they had to change from sigmoid or tan edge to relu  \n a  lot  of  these  small  small  things  which  they  had  done  and  at  that  time  it  is  also  not \npossible with the existing hardware to train this on the given gpus that you had at that \n\ftime  so  they had to do some splitting across gpus and so on  so  it was not as simple \nas  it  is  today  with  all  the  hardware   as  well  as  api  developments  or  platform \ndevelopments around this right  so  probably that is why it took two years to yeah sure   \nso  each of these things  so after you do the convolution operation  you pass it through \nthe  relu  non linearity   ok  so  what  does  that  mean  is  that   the  convolution  operation \ngave you a feature map  every entry here was just a weighted average of the neighbors \nright  you take this entry or rather you take this feature map and create a new feature map \nwhere every entry here is the sigmoid of every entry here  do you get that or not sorry \nsigmoid  some non linearity and they use the relu has the non linearity  so  you do get \neveryone gets this  so  all the convolution layers are followed by a relu non linearity \nlayer  \nso  you get this volume  pass it through the relu and get a new volume  but  i have just \nshown that as a single operation  it is before pulling so this was the fully connected layer  \nso  now  we look at the next architecture which is zfnet  \n\n \nnow  i am going to compare zfnet with alexnet  so  on the top you will see alexnet  \non the bottom you will see zfnet ok  so  again the input was the same  two hundred and twenty seven \n two hundred and twenty seven \n three  \nnow instead of eleven cross eleven filters  zfnet decided to use seven \n seven filters  and their rationale \nwas that you do not need such large neighborhoods  you do not need as small as three \n three  \n\uf0b4\uf0b4\uf0b4\uf0b4 \n\fbut probably you need at least as much as seven \n seven  you do not need eleven \n eleven  so  that is the \nfirst change that they did and that would also result in some parameter pruning right  \nbecause  the number of parameters now would be seven cross seven into three  so  the difference in \nthe  number  of  parameters  at  this  layer  for  zfnet  which  is  at  the  bottom  and  alexnet \nwhich  is  at  the  top   would  be  this   how  many  of  you  get  this   ok   so   thats  in  the \ndifference  in  the  number  of  parameters   so  now  the  output  volume  still  remains  the \nsame  its fifty five \n fifty five \n ninety six   \n\n \nthen again they had the same max pooling operation  this layer there was no difference \nbetween  zfnet  and  alexnet   and  then  after  that  you  had  again  layer  three  which  was \nexactly the same as alexnet   \n\uf0b4\uf0b4\uf0b4\uf0b4 \n\f\n \nthen  layer  four  again  the  same  as  zfnet   afterwards  layer  five  instead  of  three hundred and eighty four  filters   they \ndecided to use five hundred and twelve filters  the rest of the thing remains the same  that means  the size or \nthe spatial extent of the filter remains the same  that again results in some difference in \nthe parameters  so  thats the number of parameters that got added in zfnet as opposed to \nalex net   \nand  of  course  the     oh  sorry  sorry   oh  sorry   the  bottom  one  is  a  zfnet  yeah  that  is \ncorrect sorry  so  in zfnet you had five hundred and twelve filters as opposed to three hundred and eighty four filters in alex net  ok  \nis it fine   \n \n\f\n \nand then the next layer again instead of three hundred and eighty four filters  they had one thousand and twenty four filters   \n\n \nthen  again instead of two hundred and fifty six  they had five hundred and twelve filters and then  a max pooling layer then the \nsame dense fully connected layers  ok  \n \n \n\f\n \nso  everyone gets this  this is the difference between the two architectures and this led \nto that difference in the error of around three to four percent is that we have seen earlier  \n\n \nso  difference the total number of parameters was one forty five million and of course  zfnet had \nmore parameters because  is that  it has these more filters in the deeper layers ok  so  we \ngo to the last point which is may be more in that vgg net   \n \n \n\f\n \nso  again in the case of vgg net  the input was  ok  so i just want to  i  will not see it \n\n in   so  the input was again  the same  it was rgb cross  two hundred and twenty seven cross \ntwo hundred and twenty seven   \nso   this  is  what  the  vgg  architecture  looks  like   they  have  so  in  vgg  network \nthroughout  ok   wait   so   how  many  layers  this  zfnet  have   eight   so   you  only  count  the \npink boxes because  the those has ones which have two parameters  now  vgg net has \nslightly more number of layers but  in all the convolution layers they use three cross three filters  \nright   from  the  beginning  they  use  three  cross  three  filters   ok   so   you  have  the  first \nconvolution  layer   then  another  convolution  layer   another  convolution   another  max \npooling layer followed by two convolution layers  then a max pooling layer  followed by three \nconvolution  layers   max  pooling  just  keep  adding  box  is  writing  just  because   you  can \nand then  you have the fully connected layers   \nso  again there is not much intuition for why sixteen  in fact  later on someone came if this is \nthe vgg sixteen architecture because  it says sixteen layers  later on some of someone came up \nwith the vgg nineteen architecture  which has nineteen layers right  so  a lot of this is data center \neven right  so you try your best to get the best possible accuracy on the imagenet data \nand that is the architecture you came up with right   \nbut  as  long  as  how  many  of  few  feel  comfortable  with  what  is  happening  right  and  i \nmean   when  i  say  comfortable   i  mean  that   you  really  understand  the  gory  details  of \n \n\fwhat  is  happening at  each layer in  terms  of input volumes   output volumes   number of \nparameters  how are you going to train this network end to end  can you see how are you \ngoing to train this  so  you will get some loss here that is going to propagate all the way \nback to the first layer  right  and this propagation is going to happen over some sparse \nconnections  that  fine   now   this  is  one  very  important  point  that  i  have  skipped  and \nwhich none of you is questioning  is everything that is happening here differentiable  \nstudent  \n  \nwhat  happens  to  max  pooling   is  max  pooling  or  differentiable  operation   so   i  am \ngoing  to  ask  you  this   how  are  you  just  note  this  down   how  are  you  going  to  back \npropagate  to  the  max  pooling  layer  because   you  need  to  see  whether  the  max  pooling \nlayer is actually a differentiable layer or not   so  here i just some statistics about vgg \nnet  everyone is writing that down  laughing  this perhaps  means i will not ask it  \nthe  kernel  size  is  three  cross  three  throughout   the  total  number  of  parameters  in  non  fully \nconnected layers is sixteen million  the total number of parameters in fully connected layers \nis one hundred and twenty two million  so  you see that this fully connected layer is really a problem it like really \nhogs all the lime that it has the maximum number of parameters there right  and so and \nthe most number of parameters are there in the first fully connected layer  because you \nhave this five hundred and twelve \n seven \n seven  you remember then alex net and zfnet  the last layer was two hundred and fifty six \ntwo \n two  which has definitely more manageable than this layer which has grown almost eight \ntimes in size  but not even eight actually four into four into two right  sixteen thirty two times in size right  \nso  that  is  really blown  up the number of parameters in  the first  fully  connected layer  \nso  you just imagine the i mean  you have such a deep layer and then you realize that all \nthe main number of parameters are there in  this one particular layer  everything else is \nmuch  fewer  parameters  or  orders  of  parameters  less  number  of  parameter  is  less  then  \nthis one fully connected layer  \n\uf0b4\uf0b4\uf0b4\uf0b4 \n\f"}
{"audio_filepath": "lec010_006.wav", "duration": 166.727, "text": "\nthis  is  where  we  left  off  in  the  last  class  so  we  look  at  three  networks  for  image \nclassification  starting   with  alex  net   then  zf  net   and  then  vgg  net   vgg  net  in \nparticular  had  sixteen  layers  including  convolutions  and  fully  connected  layers   and  one \nthing that we saw that a large number of parameters are there in the first fully connected \nlayer  \n\n \nbecause you are connecting a five hundred and twelve \n seven   seven volume to a four thousand and ninety six dimensional vector right so  \nthat is one thing  the other thing that i would like to kind of mention right now so  that it \nbecomes useful for the later part of the lecture is that  if i look at any of these pink boxes \nhere right or even these things which are known as the fully connected layers  and if i \njust flatten them out and view them as a vector  what does that vector actually capture  \nit captures a  it captures an abstract representation of the image right  \nso  now imagine what would happen is  suppose you have trained one of these networks  \nalex net  vgg net or any of your favorite networks  and by what i mean by training is \nthat you have been tracking the cross entropy laws and you have run it for several epochs \n\uf0b4\fwith some patience and so on  and i was satisfied with whatever training error you are \ngetting and you have stopped training now right   \nnow   after  this  if  i  pass  images  through  these  net  through  this  network  and  i  take  the \nrepresentation from any of these boxes or from the fully connected layer  what is it that i \nhave essentially got now  i have got an abstract representation of the image that i have \nbeen feeding it right  so  just remember this and this is something that we will use  \nso   this  is  very  common  to  do   so   you  have  a  trained  image  net  many  people  have \nreleased different models for image net the ones which we have covered being included \nthem   and  now  for  him  any  image  task  if  you  want  to  do  some  processing  then  it\u2019s \ncommon to take the strain network pass your image through that  \nso   you  can  train  any  you  can  use  any  image  trained  image  net  and  pass  that  image \nthrough it  or sorry  any trained convolutional network trained on image net and pass the \nimage through that and you can get a representation for that image  and these are known \nas the fc representations and these are as the convolution representations ok  any of the \nconvolution layers fine   \n\f"}
{"audio_filepath": "lec010_007.wav", "duration": 1359.931, "text": "\n\n \nso  we will go to the next module where you wanted to look at two more architectures \nfor image classification  these are googlenet and resnet   \n\n \nso  here is a question right  so  consider the output at a certain layer of a convolutional \nneural network  so  you have this layer after layer of convolutions and max pooling and \nso on and  you are at somewhere in the middle and this is what your volume looks  like \nthis  is  what  your  output  volume  looks  like   now   after  that  you  could  apply  a  max \npooling layer  you could apply a one \n one convolution  you could apply a three \n three convolution \nor you could apply a five \n five convolution right   \nand so  far we saw that all these architectures they do one of these things  they either do \na max pooling or they do a three \n three convolution or they do a five \n five convolution or a seven \n seven  \neleven \n  eleven  right  any  convolution   but   they  are  all  uniform  they  are  all  either  three \n  three  all \neither five  five or either seven \n seven right  so  why choose between these options  why not do all \nof this at every layer  do you get the question that i am trying to ask right  \n\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\fso  far what we were doing is that you have this volume  this volume at a certain layer of \nthe convolutional neural network and after that you are either using all three cross three filters  \nso  you are using two hundred and fifty six three \n three filters or five into three \n three filters or using seven \n seven or using five \n five  \nyou are never using a mix of all these right  so  why not use a mix of all these  why the \nwhy take a decision on that i only want three \n three  because  it is possible that you want to \ncapture  interactions  at  different  levels  so   you  should  have  varied  size  filters  at  every \nlayer   \nso  how many of you get the question and the intuition that i am trying to ask   ok   so  \nthe idea here in googlenet or in the inception net is that why not apply all of them at the \nsame time and then concatenate the feature maps right  so  i will also do max pooling  i \nwill  also  do  three \n  three  feature  maps   i  will  also  create  five \n  five  eleven \n  eleven  and  then  just \nconcatenate all of these together so  let us see how to do that right   \n\n \nnow   one  problem  with  this  naive  idea  is  that  it  could  result  in  a  large  number  of \ncomputations so  let us see what i mean by that  so  suppose the padding is zero  the stride \nis one then  if you have a w \n h \n d input as the volume and if you have an f \n f \n d \nfilter  then the output would be of this size we all agree with this  this is the formula that \nwe have been looking at  so  this is the size of the output volume  now  every entry in \nthis output volume requires how many computations  to get a single entry in this output \nvolume how many computations do i have to do   \n\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4 \n\f student  f \n f \n d  \nf \n f \n d so  how do i get a single value  i apply a convolution at that value and then i \ndo those many computations  and  here the number of computations is that i am going \nover this block of f \n f \n d  doing a weighted multiplication and then adding them up \nright  so  you need that many computations  everyone is clear with this  ok   fine   \nso  each element of the output requires these many computations and we have so many \nelements  in  the  output  right   so   you  are  doing  really  those  many  number  of \ncomputations right  so  can we do something to reduce the number of computations right \nso  that is the key idea that we need to focus on  so  all of us buy the idea that doing this \nmulti granular or multi sized filters is a good idea because  you are capturing interactions \nare different layer  but i showed you that this is a problem with this  you guys just apply \nmultiple filters so  let us see what we do  \n\n \nso  we what we do is one \n one convolutions  what is the one \n one convolution do  what does \nit make sense  \nstudent  \n  \nhow does the one \n one convolution make sense  you have a pixel  i fit a one \n one convolution \non that what will i get  i will get back the pixel   \n\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4 \n\fstudent  \n  \nalong the depth right so  remember it is not one \n one  it is one \n one \n depth  \nstudent  depth  \nright so  what does a one \n one convolution do  it actually aggregates along the depth  so  \nthis is what my one \n one convolution looks like it is one \n one \n d  so  i just fit that block on \nthat pixel and do everything along the depth and get a single value right  so  from a threed \noutput using a one \n one convolution  i can go to a two \n twod feature map  everyone gets this \nok   \nnow  i could use several of these one cross one operations one cross one convolutions  in fact  i \ncould  use  done  of  these  such  that  done  is  less  than  d   so   what  effectively  happens   the \ndepth of the output reduces  so  i take a certain output volume whose original depth was \nd  now i take done one \n one convolutions right  so  i get an output whose depth is smaller \nthan the depth of the original output  is that fine everyone gets this ok    \nand  you see how this will save computations right because  remember that this was f \nf \n  d  and  now  i  have  reduced  d  to  done  ok   so   it  is  going  to  reduce  the  number  of \ncomputations   so  that is what  the idea is  you  reduce it from  f \n f \n d to  f cross of \ncross done right  so  thats this particular network or this paper introduced the idea of this one \ncross one  actually it did not introduce it used it  but it made it popular probably   \n\n \n\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4 \n \n\fnow  once you have done this so  this is how i am going to proceed now  i have a certain \nvolume i have applied one \n one convolutions to it  using that i have reduced the number of \ndimensions   now   i  am  going  to  apply  three \n  three  convolutions  as  well  as  apply  five \n  five \nconvolutions  on top  of that  right  because  that is  the motivation  that  i had started with \nthat i want to apply kernels of multiple granularity   \nnow  can you think of some refinement to this  you see this branching over here  why \nuse the same one \n one convolutions before feeding to three \n three as well as five \n five  i could use a \ndifferent set of one \n one convolutions and feed it to a five \n five and use a different set of one cross \none convolutions and feed it to three \n three  is that fine  what is the problem with this  \nstudent  again  increasing the number of computations  \nagain  increasing the number of computations right  but they found out that the tradeoff \nbetween  this  is  fine   even  if  you  are  doing  more  one  cross  one  operations  it still  is  ok   the \nnumber  of  computations  are  still  manageable  ok   and   then  you  could  also  do  a  max \npooling because  we were choosing between these things right five \n five  three \n three  seven  seven and \nmax  pooling   so   we  will  do  all  of  these  in  parallel  and  we  also  do  some  one \n  one \nconvolutions  so  how many different types of operations we have done  we have done \none \n one  three \n three  five \n five and max pooling followed by one \n one convolutions   \nnow  all of these outputs that we have got  we have got a bunch of feature maps now  \nthis  is  one set  of feature maps  this is  another feature maps  this  is  another and this is \nanother  all of these four  we are going to concat together to get a single output volume  do \nyou  see  what  is  happening   right   it  is  not  very  mechanical  there  is  nothing  really \nprofound  about  what  is  being  done   the  only  two  profound  ideas  are   one  is  apply \nmultiple kernels of different sizes and the other is to use one \n one convolutions to make the \nwhole computation manageable  that these are the only to main ideas  the rest of it is not \nvery  different  from  what  we  have  been  doing   how  many  if  you  get  this  operation \ncompletely  \nso  this block is called the inception module ok  this entire thing is called an inception \nmodule   so   in  subsequent  slides  when  i  put  an  inception  module  then  you  know  that \nthese  parallel  operations  are  happening  right   so   far  whenever  we  had  seen  a \nconvolutional  neural  network   it  was  all  serial  right  so   you  started  with  one  operation \nthen another operation  then another operation and so on  now  you have an output or an \n\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\finput volume   you apply multiple operations in  parallel  and  get  one single output right \nso  it is a parallel serial combination ok  \nso   you  will  now  see  the  full  googlenet  architecture  so   his  question  was  basically  three \ncross three would result in a different sized feature map right because of an five cross five would \nresult in a different sized feature map  so  i will use appropriate padding so  that all of \nthis becomes equal ok   \n\n \nso  this is how googlenet looks like so  you have the input again rgb and same two hundred and twenty seven \ntwo hundred and twenty seven or two hundred and twenty nine \n two hundred and twenty nine  then you apply a convolution layer followed by a max pooling layer  \nconvolution   max  pooling  then  you  have  this  inception  module  with  a  very  specific \nconfiguration   so   they  have  ninety six  one \n  one  convolutions  before  feeding  to  one hundred and twenty eight  three \n  three \nconvolutions  sixteen one cross one convolutions before feeding to thirty two  five five convolutions and so \non  and  i do not really see much point in going into the details of these numbers  there \nis hardly any intuition behind them   \n i again guess that it\u2019s you try a bunch of things and this is the one which probably gave \nthe best output  so  the key idea is that of course  you have this inception module which \nis a parallel module which does a lot of operations in parallel  this is again followed by \nanother inception module which has a different configuration followed by max pooling  \nthen again a few inception modules  in fact  five of them again max pooling then inception \nand this is the other interesting idea that they came up with   \n\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4 \n \n\fso  at this point remember in vgg net at the final layer you had an output of size five hundred and twelve \nseven \n seven right  and  we said that this was a problem how many of you remember this  why \nwas this a problem  \nstudent  \n  \nbecause  i need to connect this to a  \nstudent  fully connected  \nfully connected layer right and that fully connected layer was of size four thousand and ninety six right  so  what \nthey said is that what you could do is instead of taking five hundred and twelve cross seven cross seven  for each of \nthese five hundred and twelve feature maps that you have take the seven cross seven and just do an average pooling \nfrom there  what does what do i mean by that  \nstudent  average  \ntake these seven cross seven values  take an average of that  so  now instead of five hundred and twelve cross seven cross \nseven  how many values will you end up with  \nstudent  five hundred and twelve  \njust five hundred and twelve and in their case instead of one thousand and twenty four cross seven cross seven you will just end up with one thousand and twenty four \nvalues  right   so   instead  of  looking  at  these  dense  connections  with  every  pixel  in  the \noutput volume  you just take the average of those pixels and then do a dense connection \nfrom  there  so  from  this  volume  you just go to a vector of size one thousand and twenty four which is  exactly \nthis  vector  shown  here  and  from  there  on  life  becomes  easier  right  because   you  have \ndone a fifty percent  sorry  fifty times reduction in  the volume  so  this was  one thousand and twenty four cross forty nine  \nnow we just have one thousand and twenty four cross one so  you have a forty nine times reduction in the size and that is a \nhuge parameter reduction   \nand  that actually worked very  well in  practice they of course   add these dropouts  and \nother  things  and  then  you  have  your  fully  connected  layers   and  finally   the  soft  max \nlayer  at  the  output  to  predict  one  of  the  thousand  classes  right   so   this  is  the  full \nstructure  of  googlenet  or  inception  net  or  with  multiple  inception  modules  right   so  \njust  remember  that  key  takeaways  are  three   one  is  half  filters  at  multiple  granularity \napplied  in  parallel   the  other  is  use  one  cross  one  convolutions  to  reduce  the  number  of \n\uf0b4\uf0b4 \n\fcomputations  and  the final one is to use this average pooling to make sure that you do \nnot have this blow up of parameters at the output ok  so  these are the three main ideas \nthat you need to do right ok  \n\n \nso  this is exactly what  i explained so  instead of having this nasty looking connection \nwhich would have been fifty thousand  one hundred and seventy six cross one thousand  you just take the average from this grid  and  \njust get a one thousand and twenty four dimensional vector which results in a much smaller weight matrix at the \noutput  everyone gets this so  ok  yeah  so this is fine  \n\n \n \n \n\fso   this  has  twelve  times  less  parameters  than  alexnet   it  has  two  times  more  computations \nright  so  that is what i meant by the tradeoff so  the number of parameters has reduced \nsignificantly  of  course   a  large  amount  of  this  savings  happen  in  the  fully  connected \nlayer   its  not  the  ingenuity  in  the  inception  module  which  led  to  the  fewer  number  of \nparameters that actually leads to more number of parameters right   \nbut  the reason they could afford more number of parameters in the convolution layers is \nbecause  they reduced a lot of parameters in the fully connected layer  do you get that  \nso  they did  this  tradeoff and it has two times more computations then alexnet   but  it is \nstill acceptable because you see that there are many many layers as compared to alexnet \nright  so  let us actually count the number of layers that we had here so  one two three four five six seven eight nine \nten eleven twelve thirteen fourteen right  so  it has fourteen layers and each of these inception modules is again \nlike split layer right it has this parallel components there   \nso  having two times more computations was still an acceptable tradeoff  and  it of course  \nled to much better accuracy as compared to alexnet or zf net or vgg net right that we \nhad seen in the original trend graph ok  so  now we will go on to the last architecture that \nwe will discuss for image classification which is resnet  \n\n \nso  here is the idea behind resnet or here is the motivation right  now  suppose we have \nbeen  able to  train  a shallow neural  network well  now again  my definitions of shallow \n \n\fare relative this is by no means shallow  there are many layers here right  so  you have \nsome eight layers here   \nnow  if i have been able to do this properly  that means  what i mean by that is that using \nthis  network  at  least  i  was  able  to  reduce  the  training  error  to  zero  or  close  to  zero  some \nacceptable  value   and   i  was  able  to  get  some  reasonable  generalization  performance  \nthat  means   on  the  test  that  i  was  able  to  get  some  reasonable  accuracy  that  is  what  i \nmean by i was able to make this network work well   \nnow  suppose i add a few layers to this network and i have carefully added some layers \nin between here and in between here or over there right  now  intuitively i could argue \nthat if the shallow network was working well  right then for the deep network this is exist \nat least one solution which can directly come from the shallow network  can you tell me \nwhat that solution is  \nstudent  \n  \ni want all of you to kind of digest that idea  what the deep network could have done is  i \nknow that this shallow network works why not  i just behave like that and i learn these \nparameters in such a way that i just end up copying from here to here  how many if you \nget  this   right  so   there  is  a  case  for  the  deep  network  to  do  at  least  as  well  as  the \nshallow  network  and  it  could  do  the  same  thing  at  this  point   all  of  you  get  this  idea \nright  \nso  in other words the solution space of the deep network or rather the solution space of \nthe shallow network is actually a subset of the solution space of the deep network  there \nwas  one  solution  for  the  shallow  network  which  could  have  been  used  as  it  is  for  the \ndeep network  of course  for the deep network there are several other solutions because  \ninstead  of  the  identity  here  you  could  learn  different  things  there   but  at  least  that  one \nsolution exists  so  i should at least if i do use this in practice ex i should expect that this \nwould work as well as the shallow network right  is that argument fine with everyone   \nstudent  \n  \nor which has only one  yeah yeah of course  \nstudent  yes  \n\fi mean so it those arbitrary things would not work  but here what there it is the for the \nexplanation intuition right you are using some reasonable things  and  you are just trying \nto make it compatible with whatever you have so far  so  the argument is valid right so  i \ncannot expect that  i had a volume whose depth was one hundred and twenty eight and then  i suddenly decide to \nuse only one filter in the next layer right   \nthat means  i have compressed everything and now i expect it to be able to recover from \nthere  that  is  not  going  to  happen  right   so   that  is  a  fair  argument   but  the  argument \nwhich  i  was  trying  to  make  or  at  least  for  the  illustration  purpose  is  that   if  you  do \nreasonable things and that  is  what  people were trying out  right   these this is  the exact \nnetwork that someone was trying out and this did not work with well  i will tell you what \nit is  so  do you get his doubt and my clarification on that  is that fine  ok   \n\n \nso  this is what was happening in practice right  so  you have a twenty layer network or thirty two \nlayer network or forty four layer network and a fifty six layer network  and  you see that the training \nerror of deeper networks is much higher than the training error of the shallow networks  \nthat means  this argument which i was trying to make that the deep network should at \nleast do as well as the shallow network was not working well in practice right  and  it is \nif you think about it is not very surprising because  this argument hinged on the fact that \nit should be able to learn this identity mapping  but  this identity mapping is one of many \nsolutions right   \n \n\fso  for it to be able to narrow down on that solution  it is easy for you and me to think \nabout it  but for the network it does not have this intuition right  that i can just copy it \nfrom  there  to  here   do  you  get  that   the  solution  space  is  really  really  large  and  like \nfinding that needle in  a haystack right   you have these many solutions  possible  and   i \nam trying you to arrive at a solution where you end up with the identity solution  is that \nclear and it is not so  easy for the network to do that  everyone gets this  how many of \nyou get this idea   \n\n \nso  why not explicitly try to do something of this sort  where  the network can actually \nlearn  some  kind  of  an  identity  function   so   now  consider  any  two  layers   you  know  by \nstack layers i mean this is a convolution layer and followed by a convolution layer right \nso   these  are  two  convolution  layers  back  to  back   so   from  i  what  do  i  actually  end  up \ndoing here i had a certain input x and i am learning some transformation of x  through \nthese convolution layers right  i am trying to learn x and then i sorry i was given x  i am \npassing it to convolution layer so  i will run some transformation of x   \nand   my  argument  was  that  if  it  could  learn  to  directly  copy  x  here   the  deep  network \nshould at least work as well as the shallow network  so  why not i explicitly ensure this  \nso  why not i do this  that in addition to these connections i also explicitly connect x to \nthe output  do you get this  so  now  what i am trying to do this  is hx is equal to f of x \nwhich is the transformation that i learned for x and in addition i also add x  so  what am \n \n\fi doing  i am explicitly feeding it the identity function right  how many if you get the \nintuition  for  this   so   what  i  am  trying  to  do  is  i  have  a  sense  that  if  i  could  have \ntransferred this x as it is across layers  then there is a chance that i should be able to do as \nwell as the shallow network right   \nso  now i am  going to explicitly ensure that that you learn these transformations  but  i \nwill also feed you x at every stage at a reasonable time right  so  these are known as skip \nconnections  so  after every two layers i will feed back the x or you could try after every three \nlayers i will feed back the x  so  i am trying to maintain the original copy of x after every \ninterval  ok  fine  so  why would this help so  this follows back from our argument and \nit is the same thing which i said before  \n\n \nso   using  this  idea  of  using  these  skip  connections   these  authors  were  able  to  train  a \nreally  deep  network  of  one hundred and fifty two  layers  right   and   this  gave  on  multiple  vision  challenges \nright  one  being  imagenet   it  gave  sixteen  percent  better  results  and  the  best  network   and  \nthis is the one which reads that near human performance  then imagenet localization is \nanother challenge  where  you need to  localize the object   so  there they  get  twenty seven percent \nbetter  than  the  best  results  and  there  are  these  bunch  of  other  vision  tasks  detection \nsegmentation and all of them  and  in all of them this significantly outperformed the best \nsystem using a very very deep network  of course  the downside is that you have a very \n \n\fvery deep network  it will take its own time to train and so on  but of course  if you have \na microsoft or google you can afford to do that   \n\n \nso   that  is  the  current  theme  right  i  mean  the  one  with  the  largest  computational \nresources wins everything right  so  and they also are there is some other bag of tricks \nwhich  is  not  i  mean  it  is  not  very  difficult  to  understand   so   they  used  batch \nnormalization  every  after  every  conversation  layer   have  you  heard  of  batch \nnormalization  ever  in  your  life   ok  good   they  used  xavier  by  two  initialization   ever \nheard of that  xavier by two was the same as  \nstudent  he initialization  \nhe  initialization   right   then  they  use  sgd   not  any  of  the  fancy  adam  or  adagrad  or \nanything  with  a  momentum  of  zero nine   learning  rate  was  set  to  zero one  and  divided  by  ten \nwhenever the validation error plateaus  the mini batch size was two hundred and fifty six  they use a weight \ndecay  of  one  ht   what  is  weight  decay   weight  decay  is  in  the  context  of  which \nregularization  \nstudent  ltwo  \nltwo and what does this mean  weight decay of one e raised to minus five  \nstudent  lambda \n  \n \n\fthe lambda was set to one e raised to minus five  all of you remember these things right  we \ndid it in some previous course  in some previous life and no drop out was used right so  \nsince i have this here i will just say something more on this  so  in your reading papers \non  deep  learning  right  focus  on  the  experimental  section   where  all  these  hyper \nparameters  are  described  so   these  are  known  as  the  hyper  parameters   these  are  not \nrelated  to  the  parameters  of  the  model   these  are  related  to  hyper  parameters  which  is \nwhat the batch size is whether you used l two regularization  what was the learning rate  \nwhat was the optimization and so on  \nso  turns out in many cases if you do not stick to this you will not be able to reproduce \nthe results of the paper right  so  you might be wondering that this network i understand  \nthis is just one hundred and fifty two layers and i can just keep adding skip connections  i can easily code this \nup  but  i am not getting the same results as the original authors of the paper   \nso   this  is  where  you  need  to  dig  up  them  right   you  need  to  look  at  the  experimental \nsection  where  most  good  authors  provide  these  details  of  how  they  have  trained  the \nnetwork   how  many  epoch  that  they  use   what  was  the  patients  set  and  all  that   if  you \nfollow  those  the  chances  of  reproducing  are  much  higher  still  not  guaranteed   but \ndefinitely much higher ok  so  that is where we end the lecture on convolutional neural \nnetworks and imagenet classification   \n  \n \n\f"}
{"audio_filepath": "lec010_008.wav", "duration": 385.389, "text": "\ndream  deep art  fooling convolutional neural networks \nso  in  this  lecture  we  will  look  at  various  ways  of  visualizing  convolutional  neural \nnetworks  and  although  it  is  not  very obvious  at  this  point  as  we  go  along  we  will  see \nwhat i mean by that  so  let us start this lecture  \n\n \n  \nso  i forgot to add the acknowledgments slide  so  a lot of the material that i am going to \ncover today is based on some content by andrey karpaty in his online course  stanford \ncourse  we will add the appropriate acknowledgments and a link to the course ok  \nso  with that i will start module one  which is visualizing patches which maximally activate \na neuron ok  so  what are we trying to do here is we are trying to  the quest today largely \nis going to be able to understand what a cnn has actually learned right and what i mean \nby that is we said that there are these filters  which try to detect edges  which try to detect \nblurs and so on and then there are these neurons which fire for certain things and so  on  \n\fso  we want to see different ways of finding out what a convolution neural network has \nactually learned or what have the filters actually learned or what are the different neurons \nin  the convolutional neural  network actually capturing  what  do they fire for what  are \nthe kind of images that make them trigger and so  on right  so  that is the first thing that \nwe are going to look at how do you visualize patches which are causing a neuron to fire  \n\n \nso   this  is  again  our  vgg  network  just  put  it  vertically   say  have  passed  an  image  to \nthat  and then at every layer you are applying convolutions and then match pooling and \nso  on   right  up  to  the  last  layer  right   now  we  consider  some  neurons  in  one  of  these \nlayers  so  i am considering this neuron and i want to find out what exactly is this neuron \ntrying to do right and which is the same as asking what kind of images does this neuron \nfire for  \nso   i  have  thousand  different  classes  i  have  cats   dogs   cars   trucks  and  so  on   i  am \ninterested  in  figuring  out  what  are  the  different  kinds  of  classes  that  this  neuron  fires  \nand  this  is  more  from  say  i  am  already  getting  some  output  accuracy  and  i  am  either \nhappy  with  it  or  not  happy  with  it  in  either  case  i  just  want  to  see  what  is  it  that  my \nnetwork is learning is there any scope for improving  is that that there are no neurons in \nthe  network  which  actually  fire  for  the  dog  class   did  not  should  i  do  something \ndifferently  was  it  that  most  of  the  neurons  fire  for  all  classes   that  means   they  do  not \nhave any discriminative power  so  what exactly is going on right   \n \n\fso  that is why we are that is why this study is interesting and you will do something of \nthis sort in your cnn assignment ok  so  and by now we are clear that if i am focusing \non  any  neuron  and  any  layer   i  can  always  go  back  and  trace  the  patch  to  which  it \ncorresponds in the input image  everyone is fine with that  right  so  we saw that if i am \nsomewhere here  then every neuron here corresponds  to  some sixteen cross sixteen patch in  the \noriginal  image  and  the  same  is  true  for  every  layer  right   i  can  always  this  is  a \ndeterministic  process   i  can  just  find  out  which  are  the  original  image  pixels  which \ncontributed to the computation of this particular neuron in any layer ok   \nso   i  can  do  that   so   now   what  i  am  going  to  do  is   i  will  send  as  many  images  as \npossible   whatever  images  are  there  in  my  training  data   test  data  whatever  images  i \nhave  i will pass these images through the convolutional neural network ok  and for the \nneuron of interest i will note down  which when does it fire and where ever it fires and \nby  fire  i  mean  it  is  a  output  is  close  to  one  or  it  is  a  output  is  high  because  these  are \nrelu  neurons   i  look  for  high  output  they  do  not  saturate  at  one  right   so   this  i  look \nwhich images for which this neuron had an high output and for those cases i will go back \nand trace the image and see which patch of the image actually caused this to fire  \nso  i want to see whether my neurons are actually learning things like noise detector or \neye detector or something right  \n\n \n \n\fso  let us look at the results of one such experiment done by a group of researchers  so  \nthey considered some neurons in the pool five layer and they did this experiment that they \npass a lot of images and whenever this neuron fired they went back and saw what was \nthe patch in the image  which was causing this neuron to fire   \nso  that they found that one set of neurons is actually fires for people places  so  if you \ngo back and trace which is the image  which caused is to fire or which is the patch  then \nit is largely centered around a persons face or which is something which is very clearly a \nperson ok  another set of neurons fires for dogs  another set of neurons fires for flowers \nall sorts of flowers and different orientations different maybe colors are same here  but \nthey are all different thing right somewhere inside a bouquet somewhere inside a flower \npot some somewhere on a table and so  on  but expected of that these neurons are firing \nfor any flowers that appear in your input image and the fire only for that patch nothing \naround it  \nso  it is very is actually able to localize and fire  there are some images which fire for \nthis images the digits and alphabets written in the image  so  these are some addresses or \ndates or billboard signs or something like that and whenever there are these characters or \nnumerals there and this neurons fire  \nand  some  neurons  fire  for  houses  and  then  some  neurons  fire  for  shiny  surfaces   so  \nthere is this different sets of neurons which fire for different sets of things right  so  also  \nthat means  your convolutional neural network is trying to learn specific characters of the \ninput characteristics  of the input and this is  one way of visualizing  so  this is  not  like \nanything tricky here it is just that its good you can think of this as debugging tools for \nyour  convolutional  neural  network  right   because  in  your  you  i  guys  are  used  to \nprogramming where you give different inputs and see what is the output and then try to \ndebug it   \n so   this  is  one  way  of  trying  to  figure  out  whether  your  network  has  learned  does  it \nreally need more training is there a certain class of images for which it is not firing at all \nor is it confusing between two classes and so  right  so  that is one way of visualizing   \n\f"}
{"audio_filepath": "lec010_009.wav", "duration": 380.467, "text": "\nso  now  this was visualizing the neurons inside the convolutional neural network  so  \nneurons remember are the outputs right  these are not these are the feature maps   what \nabout the weights itself  what are the weights in a convolutional neural network  \nstudent  \n  \nthe  filters   the  filters  themselves  are  weight   have  you  ever  tried  to  visualize  weights \nbefore  when  \nstudent  auto encoders  \nauto encoders and what was a trick there  how did we  \nstudent  \n  \nvisualize what was the optimization problem that we solved  \nstudent  \n  \nhow many of you went and looked at the prerequisites  how many of you looked at the \nprerequisites  ok  \n\f\n \n  \nso  we had done something similar while discussing auto encoders  so  because that we \nhad done something similar while discussing auto encoders right  so  we were interested \nin  knowing  that  there  is  a  particular  hidden  neuron  inside  the  auto  encoder  and  we \nwanted to see that  what does this neuron capture  so  if i give it emnist digits then what \nkind  of  patterns  does  it  fire  for  and  if  you  remember  we  had  solved  this  optimization \nproblem and realize that  this neuron will fire for an input  which looks like this where \nwone or all the weights which are connecting to this neuron ok  what was the dimension of \nthe input if you are dealing with emnist digits  \nstudent  \n  \nseven hundred and eighty four  what is the dimension of this a one thing which i have circled here   \nstudent  \n  \nseven hundred and eighty four right  it is written x equal to so  it has to be seven hundred and eighty four  why is it seven hundred and eighty four because there are seven hundred and eighty four \nweights  connecting  each  of  the  input  pixels  to  that  neuron  right  so   that  means   this \nweight matrix itself we can visualize it as an image and thats exactly what we had done if \nyou remember we had this grid of images that we were analyzing and in some images we \nsaw  that  some  dark  element  fires  here  and  each  we  were  arguing  that  this  is  the  curve \nwhich exists in two or nine or eight and that is the one which is capturing   \n\fand in some cases there was a cusp here which was firing and we were arguing that this \ncould be for the three or for a nine or for a eight or something like that right  so  we were trying to \nvisualize these things and the way we had plotted it was just treating this weight matrix \nor weight vector as an image and seeing what causes the neuron to fire right   \n\n \nso  we can do something similar for convolutional neural networks  i want you to think \nhow would you do that i will give you some hints   the answer is there on the next slide  \nbut i just want you to think about it right  so  remember here you have dense connections \nok   that  means  your  weight  vector  was  the  same  dimension  as  the  input  vector   what \nabout filters in the case of cnn  they are smaller they are three \n three  five \n five or seven \n seven much \nsmaller than your original image  \nso  then what do these filters correspond to  just think of the animation that we had seen \nright we had this image and we were taking a filter and applying it at different places  so  \nwhat does the filter correspond to  what is the filter overlap  with  patches in the image \nright  so  now  what kind of analysis can you do  \nstudent  dense  \nwhat  kind  of  patches  does  this  filter  fire  for  or  what  kind  of  patches  does  the  neuron \nconnected to this filter fire for  does that make sense everyone gets the intuition  how \nmany if you get the intuition please raise your hands  thank you   \n\uf0b4\uf0b4\uf0b4 \n\f\n \n  \nso   now   recall  that  we  can  think  of  a  cnn  as  a  feed  forward  neural  network  and  in \nparticular  when  you  have  a  filter  it  actually  interacts  only  with  few  pixels  right   so  \ninteracts with say pixel one two five and six  so  that is the patch that it interacts with  \nand now i want to see when does this neuron fire  so  that is the same as asking what do \ni put in one  two  five  six for this neuron to fire or similarly what do i put in three  i do not know this \nwas one  two  five  six  i  guess   so  three  four  seven  eight  for the same different  neuron to  fire right   but  all \nthese neurons fire because they are connected to the same filter   \nso  that means  i am interested in these patches  which will cause the neuron to fire and \nthose patches can appear anywhere in their image  is that fine that is the whole point of \nconvolution neural networks  wherever there is a nose whether it is at the top corner of \nthe image or the center or the bottom it should be able to detect right that is the whole \npoint of weight sharing and sparse connectivity ok  \n \n \n\f  \nso   we  are  going  to  do  exactly  the  same  thing   we  will  have  a  three \n  three  filters  or  five \n  five \nfilters  or  seven \n  seven  filters  were  just  going  to  visualize  as  them  as  images   but  unlike  the \nearlier  case  where  the  image  actually  correspond  to  the  full  mnist  image  here  these \nimages are just corresponding to those three cross three or five cross five patches and you want to see \nwhat kind of patches causes the neurons to fire ok  and the solution is still the same we \nwill have this w by w the normalized weight filter weight  which is causing the input to \nfire  how many if you are fine with everything at this point please raise your hands high \nup  \n\n \n\uf0b4\uf0b4\uf0b4 \n\fso   this  is  what  we  get  right  and  we  observe  certain  things  which  like  we  had  earlier \nmade  a  case  for  that  these  filters  try  to  detect  certain  types  of  patterns  or  textures  or \nedges   so   you  can  see  that  right  this  is  capturing  these  slanting  edges  this  is  trying  to \ncapture  some  horizontal  sorry  vertical  edges  then  some  edges  oriented  differently  and \nalso  some  colored   patterns  some  texture  right   so   you  see  something  like  a  checkbox \nhere or chess box here and so on  \nso  these filters  are actually firing for different  kinds  of patches  so  they are trying to \ndetect different things from the images  so  you could visualize this and unless you see a \nlot of variety in  this   that means  something is  wrong  right  because  your  filters  are not \nbeing  trained  to  be  discriminative  with  terms  of  different  patterns  that  they  can  detect \nand so  on right  so   you want these variety of patterns to occur ok  and i am  going to \nmake a claim that this is only interpretable for the first layers in the convolutional neural \nnetwork why is  it so   i am  seeing some half complete answers  so   i will ask  this  as a \nquiz question   \n\f"}
{"audio_filepath": "lec010_010.wav", "duration": 370.668, "text": "\nok   the  another  thing  that  you  can  do  is  to  figure  out  whether  things  are \nworking properly or not  so  you can do something known as an occlusion experiment  \nso  these \nare  all  your  debugging  tools  sort  of  to  say  if  you  are  working  in  vision  or \ncomputer vision  where  you  are  using  a  convolutional  neural  network  and  this  is  to \ngain  more \ninsight said  most of you will get away by just taking an off the shelf convolution \nneural network training it on your data getting some accuracy and reporting it  \nbut   for  those  of  you  who  want  to  really  understand  what  is  happening  and  how  can \ncompare  whether  a  five \n  five  filter  would  have  been  better  than  a  seven \n  seven  filter   then \nyou could have observed what these filters are actually learning and in your data does it \nmake \n\uf0b4\nimprove  things  further   so   this  could  actually  tell  you  for  example   if  you  want  to \n five filter versus seven \n five filters are not \nsense to have a five \nso   this  is  for  people  who  really  want  to  get  into  the  know  how  of  how  things  are \nbeing able to distinguish enough  but  if you had used a smaller or a larger filter things \nworking  otherwise  most  of  you  i  do  not  really  expect  you  to  do  this  is   but  this  is  an \nwould have been different  right  \nimportant  set  of  tools  to  have  and  i  would  strongly  encourage  everyone  to  experiment \n seven filter  because maybe the five \nwith them and some of this you will do in the assignment  ok  \n\uf0b4\uf0b4\uf0b4\uf0b4\f\n \nso  here is the idea of behind occlusion experiments  so  we are interesting knowing that \nwhat patches in the image are actually causing the output to belong to a particular class  \nright  \nso   i  have  here  the  figure  of  a  dog  and  the  class  being  probably  predicted  is  a \npomeranian  and  i  want  to  know  that  what  patch  of  the  image  actually  resulted  in  this \noutput  right  so  have you tried doing this in any other context  if you want to know if \nyou have several features or several things several factors and you want to decide which \nactually influenced the output  how would you do it  so  what you could do is you could \ndrop one factor  right and see whether your output would have drastically changed if it \ngoes from positive negative then that maybe that was  the factor which really mattered  \nright  \n so  if for example  it is a movie review classification  right  so  when you drop certain \nwords  from  your  review   so   you  drop  the  word  amazing   great  and  so  on  and  keep \neverything else the same  now  it is quite likely that your probability of the review still \nbeing tagged as a positive review will at least drop earlier maybe with these words it was \ngetting tagged as a positive review with zero nine probability it would come down to zero six  but \nnow if you drop words like the  and  for and so on then you do not expect the output to \nchange  much  because  these  words  are  not  really  important  indicators  of  positive  or \nnegative  ok  \n \n\fso   the  similar  thing  that  you  do  here  is  you  occlude  certain  patch   patterns   a  certain \npatches of the image  so  i have shown one occlusion here so  i have replaced that patch \nby a gray patch and i again feed the image to the convolutional neural network and i see \nwhat  is  the  probability  of  the  pomeranian  class  right  now   ok   and  i  do  it  for  all  such \npatches in the image  i can do for as many patches as i want  \n\n \nand   i  create  something  known  as  a  heat  map   so   the  red  portions  here  are  the  ones \nwhich  do  not  cause  a  large  drop  in  the  output  probability  if  you  occlude  them  and  the \nblue  portions  are  the  ones  which  cause  a  large  drop  in  the  output  probability  if  you \nocclude  them  and  it  is  pretty  obvious   because  what  is  happening  is  when  i  cover  the \nface of the dog the probability drops drastically and that is what you would expect  right  \nso  this is also an indicator that your network has actually learned something meaningful  \nit is being able to detect this based on the facial features and not just randomly guessing \nthat this is a dog  right and see the similar experiment  \nso  for example  if there is a car  sometimes these results  are not very at  least to me it \ndoes not look very intuitive  so  i would have expected that the wheel would have been \none of the deciding factors  right  so  if i occlude the wheel the probability should drop \ndrastically   but  the  other  way  of  looking  that  it  is  that  its  really  learning  a  lot  of \nredundant features  so  it is not heavily relying on the wheel unlike in the dog case even \n \n\fif the wheel  is  occluded it is  relying on certain  other features  which look  like cars and \nhence the probability is not dropping drastically  right  \nso   this  allows  you  to  interpret  what  kind  of  things  it  is  running   so   if  its  heavily  for \nexample  for face detection if its heavily relying on nose to detect the face to say that this \nis the face  the moment you block the nose it will drop its probability of detecting this as \na  face   but  thats  not  good  right  because  you  want  these  redundant  features   remember \nwe had discussed this at some other point where it should try to detect the face not only \nfrom the nose  but also from the ears  from the hair  from the eyes and so on  \nso   if  your  occlusion  is  not  drastically  reducing  your  probability   that  means   it  has \nlearned some redundant features which are still allowing it to operate well even though \ncertain  portions  of  the  image  are  not  there   that  means   it  is  more  robust  noise  in  that \ncase  right  and  here it looks like it is not so robust because it is probably heavy this is \nthe rearview mirror of the car  so  it is probably heavily relying on that feature to detect a \ncar  ok  \nthen this is another thing where the true label is an afghan hound and for some reason \nif you occlude the face of the woman its probability decreases  now let us not comment \non  that   but  if  you  go  back  and  look  at  the  image  you  might  be  able  to  make  some \nobservations  right  so  these are things so  this is an indicator that is probably not really \nlearnt  it  well  maybe  all  the  afghan  hound  images  that  it  saw  maybe  a  woman  was \ncarrying the dog always  right  \nso  its learn this wrong association that when i see a woman with some object it that is \nthe portion which is the dog which is bad  right  so  now  you can see that your network \nhas  not  learned  something  interesting  and  you  would  want   so   if  you  look  at  one \nnetwork which is predicting a dog based on this kind of a occlusion and another network \nwhich is  predicting  a dog based on this occlusion then  you would prefer the other one \nand so  this is a very interesting experiment to do   \n\f"}
{"audio_filepath": "lec010_011.wav", "duration": 362.007, "text": "\nso  now  so far what we have done is  we have seen the influence of neurons on the or \nwhat  image  patches  cause  a  neuron  to  fire   then  we  have  visualized  the  weights   so  \nneurons  have  been  visualized   weights  have  been  visualized  then   we  have  done  some \nocclusion  experiments  on  the  input  image   now   we  will  take  this  further  and  we  are \ninterested in  seeing that  what  pixels in  the image actually help  in  the output or in  any \nneuron in the intermediate layers and we will find out some principal way of finding this \ninfluence  right  and we are  going to  use  back propagation   that means   we are  going to \nuse gradients ok  \n\n \nso  we can think of an image as an m cross n inputs  going from xzero xone all the way up to x \nm \n n nothing great about that  and we are interested in finding the influence of each of \nthese  inputs  on  a  given  neuron  ok   now   what  is  one  way  of  computing  influence  that \nyou  have  learned  in  this  course   what  is  the  hero  of  this  course   gradients   right   so  \ngradients tell you the influence  so  now  can you tell me if i want to compute what is the \ninfluence of this neuron or this input on this neuron  what would you do  \n\uf0b4\fstudent  \n  \nhj  xi  but can you compute that  how will you compute that  how do you compute the \ngradient with respect to the input  we have always stopped at the last hidden layer and \nthe weights before that  so  how will we do that  how will you do this ok  this is a trick \nquestion just a hint  is there a restriction on the chain rule or can you do it  you can just \nkeep  adding  links  to  the  chain  right   so   what  is  so  difficult  about  that   you  already \nknow how to compute the gradients till this point and in fact  you will also know how to \ncompute the gradients till this point   \nand what is stopping you from doing it up to this point  what if i just call this h instead \nof x then  you would not have a problem right  and actually  we call it h right we call it \nhzero  we can do it right  it is straightforward  so let us see  \n\n \n\uf0b6\uf0b6 \n\f\n \nif  i want to compute  hj  xi  i can see that if the if  hj  xi is zero  that means  this pixel \nhas zero input on the neuron  if it is large then  it has a high influence if it is small then it \nhas a low influence  so  this is  how  i will see whether  a pixel  has an influence of the \ncertain  neurons  in  the  in  some  of  the  hidden  layers   and  this  is  not  restricted  to \nconvolutional  neural  networks  as  you  can  see   i  am  just  actually  treating  it  like  a  feed \nforward neural network with parse connections ok  \nso  we could just compute these partial derivatives and visualize this gradient itself as an \nimage  so  what do i mean by that is  i am going to compute  hi  xzero   hi  xone all the \nway up to  hi  x mn right  so  i am going to compute this m cross n entities and i can \njust  visualize  this  as  an  image   now  what  do  you  expect  this  image  to  look  like   if  zero \nrepresents gray colour  what do you expect this image to largely look like  what would \nyou  actually  hope  for   this  image  should  be  largely  gray  because   most  of  the  input \npixels should not be influencing a given pixel in the hidden layer right  that pixel should \ninfluence by only a small number of pixels  \nso  that we can say that  this is the patch which causes it to fire and not that every pixel \nin the input is causing it to fire because  that is meaningless  so  that does not that is not \nsomething that we care about  how many if  you get this please raise  your hands  so  i \nwill  just  repeat  it   if  a  pixel  fires  for  every  pixel  if  a  neuron  in  the  hidden  layer  is \ninfluenced by all the pixels in the input  that means  it is not really discriminating  it is \n\uf0b6\uf0b6\uf0b6\uf0b6\uf0b6\uf0b6\uf0b6\uf0b6\uf0b6\uf0b6 \n\fnot really specialized right  we want neurons which fire only for certain patches in the \ninput  so that we know that this neuron is responsible for this kind of a pattern ok  \nso   if  i  plot  this  as  a  image   i  would  want  most  of  these  entries  to  be  close  to  zero  right \nbecause  i want the influence to be zero ok  \n\n \nnow  the question is how do we compute these gradients  so  we will just treat them as a \nfree forward neural network  we already know how to do back propagation across these \nroots and we just need to add one more term to the chain right  so  i will just show you \nwhat we will do here  so  i am interested in  hthirty two  xtwo  so  i will observe that there are four \npaths which go from hthirty two to xtwo or rather from xtwo to hthirty two  so  i will just sum up the gradients \nalong these four paths right and if i solve it i will just be left with this ok  so  that is how i \nwill visualize  so  this is very simple we have done a lot of gradients in class  so you can \njust go back and check this and it should work out well ok  \nso  you can just see this and this way we can just compute the gradients for all the input \npixels  \n\uf0b6\uf0b6 \n\f\n \nand now i am going to plot it as a image and this is what my image looks like  do you \nsee what is happening here  its  all very murky right  most of it is  great that is fine we \nexpected it  but  there is  nothing really standing  out  right  even in  this patch where  you \nhave some non gray pixels  it is almost like the entire cat region is appearing as non gray  \nthe influences  are not coming out  to  be very sharp  we would have wanted something \nlike   only  the  eye  pixels  cause  some  neuron  to  fire  or  only  the  ear  pixels  cause  some \nneuron  to  fire  and  that  is  not  really  happening  ok   so   it  does  not  produce  very  sharp \ninfluences  so  someone proposed  something known as guided back propagation  which \nwe are going to see next and that helps you to better understand the influence of the input \npixels  \n \n\f"}
{"audio_filepath": "lec010_012.wav", "duration": 281.503, "text": "\nso   we  will  see  what  guided  backpropagation  is   so   idea  here  is  a  bit  hacky  a  bist \nheuristically  but it still works very well so  let us see what it is right  \n\n \nso  suppose you feed an input image to a convolutional neural network  that image will \ngo through all the convolution layers and say it one convolution layer this is what your \nfeature map looks like  i am operating at a very small scale  i am just considering a two \n two \nfeature map ok   \nnow  we consider one neuron in some feature map at some layer  ok so  we will consider \nthis particular neuron  and we are finding interested in finding the influence of the input \non this neutron  so  this is what i will do is  i will set all the other neurons in this layer to \nzero because  i do not care about them  i only care about this particular neurons  i just focus \non that   \n\uf0b4\f\n \nand  we  now  back  propagate  all  the  way  back  to  the  image  right   that  means   i  will \ncompute if i call this as htwo then  i will compute  htwo\nioneitwoithree and so on ok   \nnow  recall that during forward pass what happens is because you have relu neurons  any \noutput which was negative that was clamped to zero  in the forward pass any output which \nwas negative was clamped to zero  so what would happen to the gradients when they flow \nback  through those neurons  you already did this if an relu neuron is dead  the gradients \ndo not flow back right  so  the gradients will not flow back through these neurons  that \nmeans  that only the so  only these gradients will actually flow back  which correspond to \nnon negative entries in the image before it or in the matrix above it right  is that fine   \nso   now  these  guys  use  this  interesting  idea  that  in  the  forward  pass  you  dont  allow \nnegative  things  to  go  forward   so   the  backward  pass  also  do  something  similar   dont \nallow the negative influences to go back  that means  any gradient which is negative just \nclamp it to zero ok  so  what i am going to do is  all these negative elements in the gradient  \ni am going to set them to zero  you see that  so  this is just taking the same idea which you \napply  that  forward  propagation  that  relu  clamps  the  output  to  zero  if   the  influence  was \nnegative and the backward pass also do the same  any gradients which are negative just \nclammed them to zero   \nso  the intuition  here was that maybe there  was  a pixel  which is  really influencing the \nparticular  neuron  and  it  stands  out   but  because  there  are  some  positive  and  negative \n\uf0b6\uf0b4\uf0b6 \n\fgradients flowing back  they seem to cancel each other  and all these influences tend to \nbe zero because  thats what we observe that image was largely gray with very few non gray \npixels   \nso  this is very heuristically because  the reason i call it a heuristic is because   you are \nmessing with the math right  the math tells you that the correct gradient has to go back \nirrespective  of  whether  its  positive  or  negative  but  they  give  this  justification  that  on \nbased on two things  and the forward pass you are not passing the negative gradients  a \nnegative  outputs   so  in  the  backward  pass  also  kill  them  and  this  should  avoid  this \ncanceling of positive and negative output  \nso  this is known as guided back propagation because  you are meddling with the actual \nback propagation  you are doing something different   \n\n \nand   so  the  idea  was  to  neglect  all  the  negative  influences  and  when  they  apply  this \nguided back propagation  this is what the influence looks like  so  you see that it is much \nsharper now  it is actually very nice its focusing completely on the eyes and you can see \nthe layout of the cat much more clearly as in the earlier picture earlier image right   \nso  this is a popular technique to use to for various things it is also among other things  \nfor in for understanding what your convolutional neural network is doing right  so  this \nlecture is entirely about understanding what are the neurons learning  what are the weight \n \n\fmatrices learning  what are the kernels learning and so on  so  these are all again tricks \nthat  you  need  to  have  in  your  repository  to  be  able  to  do  something  more  than  just \nreporting  accuracy  ok  i  will  get  seventy  percent  accuracy  on  this  status   refer  slide  time  \nfour fifteen  right  so  this guided back propagation is one algorithm that you will implement \nas a part of the assignment so   \n\f"}
{"audio_filepath": "lec010_013.wav", "duration": 617.496, "text": "\nok  the next thing that we are going to do is optimization over images  so  this is again \ninteresting  and  it  eventually  led  to  this  whole  field  of  adversarial  deep  learning  or \nadversarial machine learning in general right  so  we will see what this is   \n\n \nsuppose i have a trained convolutional neural network ok and now i want to figure out \nwhat kind of image should i pass through this  so that  it gets recognized as a dumbbell  \nwhy we want to do i would not want to have such a weird objective  can you think of a \nreason  why would want such a weird objective  i know there is a convolutional neural \nnetwork  which can distinguish k classes these classes could be anything   \nnow  i want to deliberately create images  which get passed as the dumbbell class  why \nwould  i  want  to  do  this   ok  you  are  going  into  your  details   so   i  will  give  you  a \napplication right suppose  this network is supposed to do face detection and the k classes \nwhich are there are k people right   now   you want  to  see what  kind  of image should  i \nfeed to this so that  i get recognized as amitabh bacchan right  so  now  that could have \ncertain benefits and various high places and so  on its i would want to do that right   \n\fso  thats the whole idea behind adversarial learning  so  now  i am asking this question \nthat  i want and here its in of course  a toy setup there is no reason  i why i would want \nto generate dumbbells  but say if i am going to if its an automatic verification whether  \nmy  product  looks  like  a  dumbbell  or  not  i  might  want  to  do  this  right   so   you  could \nthink of all sorts of reasons why you want to do this  so  what we will do is  the question \nthat we are interested in is that  i have a blank slate with me  it just contains some pixels  \ni want to be able to modify this pixel so that  my class dumbbell class gets fired   \nnow  we have done enough gradients  enough back propagation everything in this class  \nso  i will ask you to give me a solution for this  and the hint is  treat the image itself as a \nparameter matrix  the second hint is assume that all of this is going to remain constant  \nyou are not going to change any of this  and you have initialized your parameters  which \nis the image pixels to zeros  that means  you are started with a gray image   \nnow i will change the question a bit  only a bit and all of you will be able to answer this \nok  suppose my network is strained and now  i want to change the weights in this layer \nso that  my accuracy improves  so  that when its a dumbbell class  it predicts dumbbell  \nhow  will  you  do  that   it  will  pass  the  same  image   what  will  you  do   how  will  you \nchange  the  weights  in  this  layer   back  propagation  what  is  the  update  rule   say  the \ngradient descent update rule  say that the gradient descent update rule  \nstudent  \n  \nw is equal to ok  you guys actually unanimously said gradient is an update rule ok  so  w \nis equal to w minus oops oops oops  ok  minus eta into  ok  \nstudent  \n  \nthats what you will do  now if i ask you the question for this you can answer it  but if i \nask you the same question here  why cannot you answer it   \nstudent  \n  \nso   here  what  were  you  doing   computing  the  gradients  of  the  loss  with  respect  to  the \nweights what will you do here  \nstudent  \n  \n\fit put respect to each of these pixels and then  update this pixel by using what formula  \nstudent  \n  \nione   thats  the  first  pixel  is  equal  to  ione  minus  eta  gradient  ione  where   what  is  gradient  ione \nactually  everyone gets the intuition right you can do it now   \n\n \nso  we could pose this as an optimization problem where what we want to do is  given an \nimage   we  want  to  maximize  the  score  of  the  output  class   and  i  also  want  some \nregularization because  whatever i get i want it to look like an image right  \nso   we  will  see  different  types  of  regularization  for  doing  this   some  very  simple \nregularizations but this is the overall idea right  so  any generic loss function is always \nthe training loss plus the regularization so  i have just kept both  the training loss as well \nas the regularization  whats my training loss  the score for the class that i am interested \nin   and  what  are  the  parameters  of  this  object  of  this  optimization  problem   the  input \npixels right   \nso  far we had already be always been doing w  b but instead of w  b you now have i as \nthe parameters of your optimization problem  is that fine  \n \n\f\n \nand now  we can just think of the entire image as a collection of parameters  and we can \nnow update the weights of this matrix  which is the image matrix ok  \n\n \nso  let us see how we will do it  so  we start with a zero image as i said  set the score vector \nto all zeros and one for the class that i am interested in ok   \nnow  compute the gradient of this score vector with respect to ik  its i want this quantity \nto  be  maximized  everything  else  to  be  zero   so   thats  what  my  loss  function  is   so   i  am \n \n \n\fgoing to compute the gradient of each of the pixels with this  now i am going to update \nthe pixel using my gradient descent rule  which i just explained brief previously  \nnow  i  again  do  a  forward  so   now  instead  of  this  zero  image  i  have  a  modified  image  \nslightly modified image because  the pixels i have moved away from zero update based on \nthe gradients  now this image i will pass back through the network and what will i do \nnow  again change  so this is the same as the weight matrix right  so you should be able \nto visualize it exactly the same way as you would have visualized this  you had certain \nweights here  you change them a bit  again did the forward pass  again did the backward \npass change them a bit and keep doing this till  \n student  \n  \ntill convergence right  whatever is your definition for convergence till you are satisfied \nand instead of score of one you are at least getting a score of zero nine or zero ninety five or something like \nthat right  so  we will keep doing this right till convergence  at the end you will you all \nof you can imagine that this image will keep getting modified ok   \n\n \nso  now let us see if  we learn run this score or the run this code for certain classes  so  i \nmean interested in  the dumbbell  class and  i have ran that algorithm starting with  the zero \nimage and this is the kind of image that i end up with  you see a dumbbell here  without \nme drawing it right  if you go back and look at it  you will see that there are a lot of these \n \n\fdumbbell  like shapes which have actually  appeared here  the colour is  of course  very \nmuch different i dont think dumbbells are of these colours ever  but you can see that its \nactually trying to produce that shapes  which will cause the dumbbell output to fire   \nnow  what is interesting is that  its being very redundant  so  its not trying to generate a \nsingle  dumbbell   a  generating  a  lot  of  dumbbells  of  different  orientations   so   i  just \nkeeping its basis covered so that  some of this should actually fire and cause a dumbbell \noutput to be maximized ok   \n\n \nnow  let us see if we take a cup and this is like the trophy cup i believe  so  this is what \nis  appearing  here   there  is  one  more  cup  here  and  there  is  one  more  cup  here   its  a \ngenerating these cups so that  you cant be  you would not be able to see it  its different oh \nit really looks like i am manipulating it  but i am not  you can go back in check it  those \ncups are there ok   \n \n\f\n \nand  then   for  dalmatian  actually   this  at  least  you  can  see  some  white  and  black  spots \nright at least thats fine  \nso  dalmatians are these dog which have these  white and black spots  so  and  you can \nalso  see  some  kind  of  a  shape  here  right   which  with  my  drawing   so  it  is  actually \nproducing that doglike shape and its producing multiple of those  so  its being redundant \ni am trying to compute that right   \n\n \n \n \n\fand  now  you  see  right  with  these  very  arbitrary  images   which  to  you  and  me  do  not \nknow nowhere close to we will fire will classify this as dalmatian  but for the machine \nand  is  classifying  this  as  a  dalmatian  and  this  is  bad  right   this  is  not  good   there  is \nnothing  to  be  impressed  about  this  is  actually  bad  because  i  can  give  it  these  horrible \nimages and still get away by something called as a dalmatian   \nso  if i want to sell some a dalmatian on olx  this is what i can do right  i can upload \nthis image and a machine would trigger it and some one would buy it ok  so and this is a \nbell paper so  you can go back and see you see a lot of bell papers here and similar for \nlemon and so on right  \n \n \n \n\f \n \nso  various classes  you can see that  its actually trying  to produce those shapes  but its \nnowhere actually producing a clear image  which is undoubtedly of that object right  is \ngenerating something which can later on be used to fool the network right  which is not a \ngood thing ok  and we can actually do this for any arbitrary neuron  so i was trying to \nactually fire this neuron  which was the output layer  but maybe i want something else to \nfire here so  i want to actually see what is  it that causes this neuron to fire  so  i could \nrepeat  the  same  algorithm  by  setting  something  here  as  high  and  then  again  back \npropagating  the  gradients  only  from  here   and  reconstructing  the  image  every  time  so \nthat  this neuron then five is right   \n \n\f\n \nso  these are what the updated images look like  which excite certain neurons and some \nlayers  so  what does this look like  its actually like a pirates ship if its not very clear  \nyou have these multiple layers of things and something like this ok  so  its some neurons \nare actually firing for this kind of a pattern  there are some other neurons which are firing \nfor different kinds of patterns and so on right   \nso  you can just create images which cause certain neurons to fire and all these are lot of \nfun to do  so  you should i would encourage you to do this  i will get more insights into \nwhat your network is   \n \n\f"}
{"audio_filepath": "lec010_014.wav", "duration": 369.577, "text": "\nthe next thing that we will see is how do you create images from embedding  so  let me \nsee what that means   \n\n \nso  remember that each of these things can be thought of as an embedding of the image \nright because  you had this original image which was two hundred and twenty seven two hundred and twenty seven dimensional and now you \nhave a four thousand and ninety six representation for that or a two hundred and fifty six \n seven \n seven representation for that  so you could \njust flatten it as an out as a vector and you could treat that as a embedding for the original \nimage right  \nnow  for any kind of embedding or hidden representation  what do we always want from \nthat  representation   think  auto  encoders   it  should  capture  all  the  important \ncharacteristics of the original image and in particular i should be able to dash the original \nimage from it  \nstudent  \n  \n\uf0b4\uf0b4\uf0b4\fwe construct the original image from it right  so  thats what i would want from a good \nembedding   so   let  us  see  if  we  can  do  this  right   so  find  an  image   this  is  the \noptimization problem that  i am interested in   find an image such that  its embedding a \nsimilar  to  a  given  embedding   what  do  i  mean  by  that  is   suppose   i  take  a  monkey \nimage and pass it through all these layers and compute all these embeddings right  \nnow  again i start with a blank image and my optimization problem is such that  for this \nblank image i want to modify it  so again this blank image is my parameter matrix and i \nwant  to  modify  it  such  that   the  embedding  that  it  produces  should  be  similar  to  the \nembeddings that the monkey image produced  so  how can you set this as a optimization \nproblem  what would your loss function be  so  lets call the original monkey images ione \nand let us call this as embedding of ione   \nnow  can you tell me what the objective function would be  for the new image that you \nare trying to create  this entry the first entry in its output that let me call that e i two ok  so  \ne i two one and ei one one  that means  the first dimension of the embedding they should both be \nstudent  very \nvery close  so  in such cases what is the error function that we will choose  \nstudent  \n  \n\f\n  right   so   you  have  to  get  comfortable  with  designing  these  loss \nfunctions right  so you have seen  you have seen this loss function before  we just have to \nbe able to related to the problem that you are trying to work on  \n\n \nso  let phi zero be the embedding of the image of interest  let x be a random image and we \nwill report the repeat this  forward pass using x and compute phi of x right  that means  \nwere computing the embedding of this random image that we have started with  then we \ncompute this loss function and add appropriate regularization for that and that propagate \nand update what  what will you update  \nstudent  \n  \nimage  right   you  will  update  your  x  matrix  right  and  you  will  keep  doing  this  till \nconvergence  \n \n\f\n \nand let us see what happens  so its suppose so now  what i am trying to do is  this is my \noriginal  image  and  i  have  the  convolution  one  embedding  of  it   so   in  this  i  am  using \nconvolution  one  as  the  embedding  and  then   i  am  trying  to  solve  this  optimization \nproblem to recreate x such that  its very close to the original image  \nso  let us see what are the different outputs that i get  so  this is the original image and \non the right hand side you have the reconstructed image such that  the conv one embedding \nof  both  the  images  is  the  same   so   you  can  see  that  when  i  am  trying  to  do  a \nreconstruction from the conv one layer  i get almost the same image back  now  if i keep \ndoing it from different layers what do you expect it to be  if i do it from conv two  conv three \nconv four and so on  \nstudent  \n  \nit  wont be so accurate right  so  let us see what  happens if   i try to reconstruct it from \nconv two \n \n\f\n \nah relu one \n\n \nmax pooling \n \n \n\f\n \nnorm one  \n\n \nconv two  \n \n \n\f\n \nrelu two  i am keep i am going deeper and deeper into the network  so  what i am trying to \ndo  here  is  remember  that   i  have  different  choices  for  these  embeddings   so   the  first \nthing which i showed you was  when i was trying to the first thing was when i was trying \nto set my objective function such that  i am trying to map this embedding  the second \nimage  that  i  showed  you  was   when  i  was  trying  to  map  this  embedding  and  the  last \nimage  that   i  will  show  is  when  i  was  trying  to  map  these  to  embedding   so   my \nobjective function was to create an image such that this embedding of the created image \nis the same as this embedding of the original monkey image right  so  thats what  i am \nprogressively trying to do  \n \n\f\n \nas you can see as i keep going ahead  i get more and more abstracter reconstructions and \ni dont really get the monkey back  \n\n \nand once i go to the last fc six or f seven layers  i get very weird looking reconstructions  \n \n \n\f\n \nand  thats  expected  right   because  by  that  layer  they  have  completely  abstracted  it  out \nright  you have just probably captured there is something like a nose  something like eyes \nor some for here and there  but you have loss the entire shape and other characteristics of \nthe original image right  from the deeper layer the construction would not be that good \nand thats kind of expected right  \n\n \nin spite of having the maximum no you could right  a maximal operation is just another \nembedding  which is that the compression there is much more because  you have ignored \n \n \n\fthe four entries and just taken the max value  so  it becomes harder and harder to do that  \nbut  mean  you  i  wouldnt  call  this  as  a  reconstruction  right   what  you  see  here  is  not \nexcept  for  the  conv  one  and  conv  two  layers   the  rest  of  the  things  were  not  really  such \naccurate  reconstruction   so   just  says  that  you  are  losing  a  lot  of  information  in  that \nabstraction or maybe not  i will do it next time   \n\f"}
{"audio_filepath": "lec010_015.wav", "duration": 668.464, "text": "\nso  we  will  start   so  we  were  in  the  threerd  lecture  on  cnn\u2019s   where  we  were  looking  at \ndifferent visualization tools for understanding what your convolutional neural network is \nlearning  and we did a bunch of things and now you move on to the next module where \nwe talk about something known as deep dream very interestingly titled  but i am sure \nmost of you have already seen this or read about this  \n\n \nso  here is the idea right so  far we were seeing that if we start from a blank image  then \nwe could suitably modify it by constructing an optimization problem whose parameters \nare the pixels of the image  and we can modify the image so that it starts looking like a \ncertain class of interest right  but now suppose instead of starting with a blank image  i \nstart with a natural image right  say a sky or any image that you have in your dataset  \ni  start  with  this  and  then  i  focus  on  neurons  in  some  layer  of  the  convolutional  neural \nnetwork  i am focusing on these neurons say any one of these neurons i am focusing on  \nand i want to change the image so that these neurons so when i say neurons i actually \nmean only a single neuron  but for illustration i will show multiple neurons  so  i want to \n\fchange the image so that this neuron fires even more  so  how would we achieve this  \nwhat will we do   \nso  say this is the neuron which i want to fire even more   so  what is my optimization \nproblem  first of all what are the parameters of the optimization problem  \nstudent  \n  \nthe pixels of the image  that is clear  now i want this to fire even more  so  what is the \nobjective function  what  you are going to maximize  lets call this neuron as hij  what \nyou are going to maximize   \nstudent  \n  \nsorry  \nstudent  no \n   \nno i want this neuron to fire more  \nstudent  \n  \nmaximize hij right  i mean that is i mean why so that sort of a thing  ok  \nbut of course  we will do something so that it is a neat differentiable thing and so on  so  \nyou want to maximize the activation of one such neuron hij  so  we could just formulate \nthe following optimization problem that i want to maximize hij\ntwo  ok  and of course  the \nparameters of the optimization are the image pixels  and if  i consider one such pixel in \nthe image then i essentially need to compute this gradient  \nthe gradient of the loss function  which is hij\ntwo with respect to this image pixel and i can \ndo it in these two parts  the lead ability of the  loss function with respect to hij and the \nderivative of the hij with respect to the image pixel  this we have seen a million times \nwhile doing back propagation of course  you are not gone all the way back to imn  but \nwe  saw  last  time  that  it  is  just  one  more  term  in  the  chain  rule   and  this  again  looks \nstraightforward right the derivative of the loss function with respect to hij looks straight \nforward  so  i have a very simple way of computing the derivative of the loss function \nwith respect to any pixel of the image   \n\f\n \nso  now  i can apply gradient descent and i can update the image  so  i started and now \nremember  that  the  my  original  i  mn  was  not  blank  or  random  or  zero  or  anything   it  is \nactually the sky image so maybe it was blue or cloudy or whatever pixel that i have in \nmy original image and that pixel i am changing   \nso  i have started with the sky image  i have changed a bit based on this gradient update \nrule   gradient  descent  update  rule   and  now  i  feed  it  back  to  the  network   what  will \nhappen  what will happen to hij   \nstudent  fire a bit  \nit  will  fire  a  bit  more  because  that  is  exactly  how  you  have  changed  the  image  with \nexactly that objective function right  \nand now if  i keep doing this what  will happen  so  remember  what  does  h ij actually \ncapture now this is where  so  if you understand this right you will really understand and \nappreciate  everything  about  convolutional  neural  network   and  i  will  be  sure  that  you \nare actually understood the details and not just these boxed architectures right  so  what \nif this happens right then what does actually hij capture  it captures certain  \nstudent  patterns   \npatterns in the image right  now if hij is firing  that means  these patterns have started  \n \n\fstudent  \n  \nappearing in the image we started with a sky image  \nand  now  hij  is  firing  more  and  more   that  means   it  is  now  the  image  is  suddenly \nbecoming  more  and  more  or  containing  more  and  more  patterns  for  which  h  ij  should \nfire  does that make sense  ok  yeah  so  let us run this algorithm we will start with this \nimage and we will run this algorithm  so  i will run it before that i want some guesses  \nwhat kind of patterns do you think will start appearing here and this is deep dream is the \ntitle right ok so fine  \nso  let us see  so i will run this algorithm so what i am doing is i am starting with this \nimage  and  running  exactly  what  i  showed  you  that  i  will  compute  the  gradient  with \nrespect  to  one  of  the  neurons   and  i  will  keep  updating  the  image  so   that  it  becomes \nmore and more like the patterns that i am trying to capture  so  lets run this and observe \ncarefully it is almost a magic trick  i hope this does not disappoint  what do you see  \nstudent  \n  \nmost of them are what  \nstudent  \n  \nthey  are  dreaming  so  they  are  literally  building  castles  in  the  air  right   so   what  is \nhappening   why  is  this  happening   everyone  sees  castles  right  that  is  the  first  thing \notherwise   \nstudent   laughter   \nok  good  why is this happening  have you seen the disney logo the castle what does it \nhave in back background  how many of you find this interesting  how many think this is \nok   expected   ok   why  is  this  happening   think  about  training  data   think  about  what \nwould have happen or you missed the magic show  so  what is the convolutional neural \nnetwork actually trying to do  \nstudent  \n  \n\fi will give you a hint its being over enthusiastic  how many of you get that  ok  so here \nis  what  is  happening right  should  i explain it or no   i am  not  going to  ask  you a quiz \nquestion i am just saying that i have some more images to show  ok i will explain it first  \nso  this is what is happening right  \nso  in  the  training  data  whenever  the  castle  appears  it  is  typically  has  the  sky  as  the \nbackground  ok   so  now  the  convolutional  neural  network  started  drawing  these \ncorrelations   so   whenever  it  sees  a  sky  it  is  trying  to  find  a  castle  somewhere   but \nbecause  it  knows  that  most  of  the  times  whenever  i  see  a  sky  there  is  a  castle  in  the \nforeground  \nso  those neurons are firing a bit and then now you are trying to fire them even more and \nmore   so   that  keep  trying  to  change  the  image  till  this  castle  actually  appears  on  the \nimage  how do you how many if you get this explanation please raise your hands  ok  \nso  let us see some more examples right  so  now guess what will happen here  ships  ok  \nagain a generation which thinks of  \nstudent  \n   \na ships is ok  i shouldnt comment on that  \nstudent  birds   \nfishes  \n   \nstudent  birds  \nbirds  what else but there are also mountains   \nstudent   \n   \nice  ok interesting  \nstudent   \n   \nnow our expectations are increase   let  me just run this and see what  happens oops oh \nno  \nstudent  \n   \n\f i have my final trick ok  \nstudent   laughter   \nthe prestige is gone ok  yeah so what do you see here  so  actually if you go back and \nlook at it carefully right this is very interesting a lot of fish eyes actually start appearing \nhere  and  some  shapes  like  fishes  actually  start  appearing  here   go  back  and  look  at  it \ncarefully and all on the mountains and the green regions a lot of birds and animals start \nappearing right which is again expected  because in your data set you would have seen \nbirds and animals with a green or this kind of a background right whatever you call it a \nmix of green and brown background right  \nso   now  it  is  trying  to  find  those  things  even  though  they  do  not  exist  and  as  it  try  to \nforce it more and more it starts creating those images as you start asking to dream more \nand  more  right  and  since  this  is  about  dreams  i  could  not  let  this  go  it  has  to  had \ninception in that  so  what will happen here now  \nstudent  \n   \nthere is actually nothing interesting is this for my own sake that i put this  unfortunately \nnothing interesting happened with this  \nstudent  oscars  \nwow  \nstudent   laughter   \nif only  but thats the point right this is so data set specific that it cant really generalize it \ncannot dream beyond the data set  actually nothing interesting happens  it is just a lot of \nthese  men  are  wearing  brown  suits  and  in  the  data  set  unfortunately  all  brown  things \nwere  dogs    laughter   so  this  is  what  will  happen   we  will  start  seeing  dogs  appear \neverywhere  you see one here  \nstudent   laughter   \nyou see many here actually  \nstudent  \n   \n\fit  is  like  a  few  more  and  this  would  have  turned  into   laughter   something  unpleasant \nright  so  that is what is happening actually see a lot of dogs here  here in many places \nright  \nso  this is  its still running  so  what exactly is happening here the same thing that i had \ndetected  right  the  network  has  been  trained  to  detect  certain  patterns  dogs   cats   birds \netcetera which appear frequently in the imagenet data and with these backgrounds that i \nam trying to do or these textures that i have in my images  it starts seeing these patterns \neven  when  they  hardly  exist  and  now  as  i  start  focusing  on  these  neurons  which  are \nfiring and try to modify the image to make them fire even more  it will start producing \nthese pixels or these images in the original image right  \nso  you can read this explanation which is from the google blog on this they have some \nreally some code and something on this  so  you can just read this explanation if a cloud \nlooks a little bit like a bird  so  that will make it look more like a bird  this in turn will \nmake the network recognize the bird even more  strongly on the next  pass  and so forth \nuntil out of nowhere a bird actually starts appearing in the image right  so  that is exactly \nwhat is happening so this is deep dream  \n\f"}
{"audio_filepath": "lec010_016.wav", "duration": 338.067, "text": "\n\n ok now we will go to deep art  now here any questions on that  ok  \n\n \nso  now here is what  here is a again an iq test right  so what will happen ok  so  this is \ndeep art ok  someone wanted to try this that if you take natural images or camera images \nand if you have art from various famous artists and i want to render this original image in \nthis art form  and how can i do  so  i will explain this  the bit of a leap of faith in what \nis happening here  but just indulge me right  so let us see  \n\f\n \nso   to  design  a  network  which  can  actually  do  this   we  design  we  first  define  two \nquantities  one is the content targets  so  i call this image as the content image because  \nthis  is  the  content  that  you  are  interested  in  right   i  want  my  final  content  to  look  like \nthis  for the content  we would want the following thing that if  i am able to create a new \nimage   when  i  pass  it  through  the  same  convolutional  neural  network   we  want  these \nhidden representations to be equal  right because  that is the assumption here is that  the \nhidden representation actually captured the essence of the image which is this face and it \nis various attributes right  \nso  if i create a new image in a different style  still this content should be present in it  \nand my way of ensuring that or rather the way of the author\u2019s way of ensuring this was \nto make sure that  the embeddings that i learn for the new image and the original image \nare the same ok  so  i want these to be equal and i have just shown one for illustration  \nbut  you  could  have  the  same  objective  function  for  all  the  representations  right  \nremember that we learn multiple representations and a convolutional neural network  \nso  this is what my objective function would be for the content  i would want that this \ntensor  which  is  the  volume  ijk   every  pixel  or  every  feature  value  in  the  tensor  for  the \noriginal image should be the same as the generated image ok  and again my optimization \nproblem  is  with  respect  to  what  image   i am  going to  change the image  and this is  the \nloss function that i am interested in  is that fine  ok  fine  \n \n\fso   i think x is  my  original  image  and p is  the new image which  i  am  going to  create \nright   \n\n \nnow   next  and  here  is  where  there  is  a  bit  of  leap  of  faith   we  want  the  style  of  the \ngenerated image  to be the same as the style image  so  i gave you one content image and one \nstyle image  so  for content the loss function is clear  now  for style how do you capture \nthe style of the image  so  the explanation given here and i am not very sure about this  \nbut maybe it comes from some traditional computer vision literature  but i just take it on \nfaith  that  if   you  have  this  volume  here   which  is  say  sixty four \n  two hundred and fifty six \n  two hundred and fifty six  or  any  other \ndimension right then  v  t v which is a sixty four \n sixty four dimensional image or matrix  captures \nthe style of the image  so  this is what has been written in the original paper  i am not \nreally dug deep  but my feeling is it comes from some of the traditional literature from \ncomputer vision right  so  that is not important  we will just take it for granted that that \ngives the image and here is the illustration for that  as you go deeper and deeper  so this \nis if you plot the sixty four cross sixty four image that you got  then you get different styles as you go \ndeeper and deeper you get a better representation of the style of the original image right  \nso  that is the argument made in the original paper  \nnow  if you assume that this is correct then  can you design a loss function for the style \npart of it  i want the style of the created image to be the same as the style of the style \nimage  \n\uf0b4\uf0b4\uf0b4 \n\f\n \nso  how would i do that  so  this is the content image  this is the actual oh sorry this is a \nstyle image correction ok  so  i would just want that this vt v  which captures the style \nand i could do it for any one of the layers or all layers depending on what i want to do  i \njust want that this style should be as close to each other  \nso  i can have a similar matrix squared error kind of a function right  so that is what this \nis trying to capture  these are the style gram  so this is v t v for the style image and this \nis v t v for the generated image  if i pass it through the convolutional neural network  i \nwant both of these to match  \n \n\f\n \nso   i  want  the  content  to  match  i  want  the  style  to  match   so   then  what  is  my  total \nobjective function going to be  \nstudent  sum of these  \nsum of these right  so this is what my total objective function is going to be  i want the \ncontent to match and i also want the style to match  so  i will use an objective function \nwhich tries to balance between these two and alpha and beta are some hyper parameters ok  \nand if you do this and train the algorithm and try to modify the pixels along with some \nother bunch of tricks then  you will get this gandalf rendered in this style that you have \ngiven right  so  this is  again some code is available for this   you can go and try it out  \nand it is interesting it is in a very interesting idea that you could have taken these two \nthings and now you could be imaginative right   you could do all sorts of things with if \nyou have two different images  how do you want to combine them and so on right  so  \nthat is the basic key idea here   \n \n\f"}
{"audio_filepath": "lec010_017.wav", "duration": 392.611, "text": "\nwith  that  we  go  on  to  the  last  module  which  is  fooling  deep  convolutional  neural \nnetworks  \n\n \nso  turns out that using this idea of optimization where  we are able to actually change \nthe image to suit our needs right  and these needs were  one was we wanted to change \nthe  image  so  that   it  fires  for  a  particular  class   the  other  was  deep  dream   where  we \nwanted to change the image so that  it is starts seeing patterns which were otherwise not \nobserved  in  the  image   and  the  other  was  d  part   where  we  trained  the  image  or  we \noptimized over the image so that  we could produce some artistic images and these are \nthe different optimization problems that we have seen  \nbut the same idea can actually also be used to full convolutional neural networks and i \nhave already hinted at this earlier  so  let us see how to do that  \n\f\n \nso  now suppose  we feed an image to a convnet and i know this is the bus image right  \nbut  now  what  i  do  is   this  is  a  trained  convolutional  neural  network  and  what  i  do  is \ninstead of setting the cross entropy loss to maximize bus  i will set up the cross entropy \nloss to maximize ostrich  and then i will back propagate through the network  i will not \nmodify any of these weights or parameters and i am only change the image right  \nso  what i am trying to do is  i know that this is the bus image  but now i am setting the \nobjective that  it should fire for the ostrich class  so  now  i am going to back propagate \nand change this image so that  the blog the likelihood of the ostrich class increases  you \nget this set up  its very straight forward  ok  and turns out that if you do this with very \nminimal  changes  to  the  image   you  can  actually  fool  the  convolutional  neural  network \nok  \n \n\f\n \nso  this is the change right  you have the original image  the second image is actually the \namount of change you made and the third image is the original image plus this change  \nnow  to the human eye there is no distinction here right  you would all of first would still \nthink this is a bus and in fact  i do not even see that  there is a noise in the third bus that \nyou see  same for some other class they have taken some bird or something like that and \nadded  some  noise  to  it  and  a  temple   and  in  all  of  these  cases   the  network  actually \npredicts that the modified image is an ostrich right or some very random class from the \noriginal class  so  why is this happening and before asking that question let me just finish \nand it need not be that you start with an original image and then try to modify it  \n \n\f\n \nactually   you  can  start  with  a  blank  image  and  do  the  same  experiment  where  you \nmodify the image minimally so that  p of robin becomes one or close to one  and you will get \nsome very arbitrary noisy looking images  which no in  which to at least you and me do \nnot  look  like a cheetah  or robin or  armadillo   but  the network thinks that  these are the \nclasses that these images belong to  \nnow  this is definitely a risky  how many of you appreciate that  it is bad ok  now and \na network is not just predicting it  is predicting it with a very high confidence right ninety nine six \npercent confidence  so  why is this happening  can even think of a reason for that  \nstudent  \n  \nno  but ok  in that case i would have been fine if there are one thousand classes it should have \ngiven  one one thousand  probability  to  all  the  classes  right   but  this  is  like  worse  than  random \nclassifier right  it is saying with ninety nine percent confidence that  this is a ostrich or whatever \nclass  that  is  so   why  is  this  happening   and  the  interesting  thing  is  that   this  in  some \nsense  ties  back  to  the  universal  approximation  theory  or  at  least  some  ideas  with  that  \ncan you think of why this is happening ok  \nso  let us try to see a very intuitive explanation for this so on   \n \n\f\n \nso  this explanation is due to andree karpathi  we need to put the acknowledgments  this \nslide  does  not  have  any  acknowledgments  actually   so   remember  that  images  are \nextremely  high  dimensional  objects  right   they  are  two hundred and twenty seven two hundred and twenty seven  which  is  a  very  high \ndimensional object  high dimensional space  and no matter how much training data you \nhave  you see a only a small sample of this high dimensional space right because  its real \nnumbers two hundred and twenty seven two hundred and twenty seven  just imagine the number of possibilities out there  no matter you have \none million  samples  ten million  samples  for training  this  is  much smaller than the  actual \nnumber of samples which exist in this space  of these only a few are images right  \nso  now  think of all two hundred and twenty seven \n two hundred and twenty seven matrices that you can make and how many of them are \nactually going to be natural images  the probability of natural images is very small  most \nof  these  are  random  things  right   they  are  just  matrices  which  do  not  make  any  sense \nwhich actually look like these images that you see here right   \nnow  using the training images  we fit some decision boundaries and this is the decision \nboundaries that we fit right that  this is class one  the rest of the green part is class two and so \non   and  in  fact   we  are  doing  these  decision  boundaries  for  some  one thousand  classes  while \ndoing so  we actually end up taking decisions for a large number of points that we have \nnot seen  we have not seen any points in this space  but i have made a decision for them  \nthat all of them belong to the green class  i have not seen any point in this space  but i \nhave ended up taking a decision for them that all of them belong to the red class right  \n\uf0b4\uf0b4\uf0b4 \n\fso  in particular what i have done is  i saw a cheetah class  image from a cheetah class  i \nsaw a few images from the cheetah class and i drew some boundary around it to say that \nthis is the cheetah class  but my boundary also contains images like this  because  this is \na  very  high  dimensional  space  and  in  that  boundary  a  lot  of  points  actually  fall  in  and \nsome  of  these  points  are  these  random  points   which  have  no  relation  to  cheetah  right  \nbut  i  have  been  so  aggressive  in  fitting  to  the  training  data   that  i  have  drawn  these \nboundaries which also include a lot of these points and now  all i need to do starting with \nthese rend random images is that  go somewhere inside this boundary and then i am all \nset right it will start detecting it as cheetah because  the boundaries have been drawn by \nthe  classifier   how  many  if  you  get  this  explanation   good   so   that  is  the  intuitive \nexplanation  for  why  this  happens   so   this  is  where  we  will  end  the  discussion  on \nconvolutional neural networks   \n\f"}
{"audio_filepath": "lec011_001.wav", "duration": 513.366, "text": "\nthrough time \n  vanishing and exploding gradients  truncated bptt \nin  this  lecture  we  will  talk  about  sequence  learning  problems  and  in  particular  some \nneural network architectures which deal with sequences  so recurrent neural networks is \nwhat we are going to see  so  we will start with the first  module which is on sequence \nlearning problem  \n\n \nso   what  are  sequence  learning  problem   so   so  far  we  have  dealt  with  two  types  of \nnetworks  one  is  feedforward  neural  networks  and  the  other  is  convolution  neural \nnetworks and both these networks the input was always of a fixed size  \nso  what do i mean by that is  if you take a convolution neural network you are feeding \nthirty two \n thirty two images to it or two hundred and twenty seven \n two hundred and twenty seven images to it and this size will always fixed  all your \ntraining  images   all  your  test  images  were  always  scaled  or  cropped  to  this  particular \nsize   ok   similarly  when  we  used  feedforward  neural  networks   so  one  example  was \nwordtwovec  the size of the input was always fixed we had this input of size two v  right or k \nv in general  if you are looking at the k word window  right   \n\uf0b4\uf0b4\fso  this input was not varying from one training instance to another training instance or \none  training  instance  to  the  test  instance  or  anything   and  secondly   each  input  to  the \nnetwork was independent to the previous or future inputs  so  i pass an image of an apple \ni  get  the  prediction  apple   then  i  pass  some  other  image  to  the  network  and  i  get  a \ndifferent prediction  it does not matter whether my previous image was a apple or a car \nor  a  mango  or  whatever  it  just  reads  each  of  these  inputs  independently   there  is  no \ndependence between the inputs and the size of the inputs is fixed   \n\n \nbut in many applications the input is not of a fixed size  so  and also successive inputs \nmay not be independent of each other  so  let us understand this with the example of auto \ncompletion that all of us are used to while typing smss or whatsapp or other things   \nso  given the first character d  i want to predict the next character which is e then once i \nhave predicted e   i  want to  predict the next  character  again  and so on till  i  get  the  full \nword  ok  this is what my task is   \n \n\f\n \nso   let  us  notice  a  few  things   first  successive  inputs  are  no  longer  independent   if  i \nknow that the previous input was d and the correct input is e then i know that only a few \nthings are possible  right  in particular if you know that the previous input was a z and \nthe  correct  input  is  a  e  then  most  likely  the  next  is  going  to  be  a  b   right   but  if  you \nignore the previous input which is z  then after e there are many things which can appear  \nright  so  the inputs are no longer independent of each other   \nand the second thing is  the length of the input is not fixed because words could be of \narbitrary sizes i am trying to type the word deep that is four letters or if i am trying to type \nthe learn which is five letters machine which is seven letters and so on  right  so  the input size \nis no longer fixed and the inputs are now dependent on each other  right  there is some \ndependence between  so  now  this is very different from what we saw in convolutional \nneural networks and feedforward neural networks  so  how do we deal with this   \nand the third thing here is that  each network now i am calling this as a network and i \nwill just clarify some notations also soon  each network is actually performing the same \ntask  it is taking as input a character and it is producing as output one character  and now \njust remember that these networks i have drawn them vertically  you are used to seeing \nthem as this  so  this is input  this is your hidden layer  and this is your output  so  this is \nthe  green  part   this  is  the  blue  part   and  this  is  the  orange  input  and  this  is  the  fully \nconnected layer  right   \n \n\fso  each of these boxes is actually this network  ok i have just drawn it more concisely \nbecause i need to draw many such networks  so  everyone gets that  just remember this \nmind that each of these orange  blue  green  structures is a fully connected network like \nthis   \n\n \nso  these problems are known as sequence learning problems where you have a sequence \nof  inputs  and  then  you  need  to  produce  some  outputs   and  each  input  actually \ncorresponds to one time step  so this is the input at time step one  time step two  time step three  \ntime  step  four  and  so  on   so   let  us  at  some  more  examples  of  such  sequence  learning \nproblems   \n \n\f\n \nso  one classic example is the task of predicting the part of speech tag of every word in a \nsentence  right   so   i am given a sentence man is  a social animal  and for every word  i \nwant to predict whether it is a noun or an adverb or an adjective or a verb or any other \npart of speech type  right and this is how it happens   \nnow  notice that once we see an adjective in this case social we are almost sure that the \nnext what is going to be a noun or at least we are sure that the next word cannot be an \narticle or most likely it will not be a verb  right  there is a very high prior that the next \nword is going to be a noun  so  that is why these inputs are actually dependent on each \nother   \nso  the current output not only depends on the current input it is also actually depends on \nthe  previous  input   right   unlike  the  case  of  convolutional  neural  networks  where  i \nfeeded  an  apple  it  is  no  dependence  on  whether  the  previous  input  that  i  pass  to  the \nnetwork was an apple or a car or what not  and the size of the input is not fixed because \nthese  sentences  could  be  of  arbitrary  lengths  i  could  have  sentences  as  small  as  three  to  four \nwords or as long as twenty five to thirty words  right  so  average wikipedia sentence for example  \nis twenty five words  roughly twenty five words   \nand notice that and this case we are interested in producing an output at every time step  \nbecause  for  every  input  i  want  an  output   and  each  network  again  this  orange   blue  \n \n\fgreen structure is performing the same task  its taking as input a word and its producing \nan output  what is producing as an output  part of speech tag  \n\n \nso  here the two examples that we saw we were having an input and every time step and \nan output at every time step  but there could also be cases we are interested in producing \nthe  output  only  at  sometime  steps  or  at  the  final  time  step   so   let  us  consider  task  of \npredicting the polarity of a movie review  right sentiment analysis   \nso   i  am  given  a  movie  review  and  after  i  have  read  the  entire  review  i  should  give  a \nprediction   right  otherwise  it  would  be  incomplete   i  cannot  actually  look  at  only  this \nword and give a prediction does not make sense  i does not also make sense to make a \nprediction here at this point  because there could have been the movie was boring  but i \nstill loved it or but the action was amazing or something like that it because it could have \nalready flipped after that  so  i need to look at the entire sentence and make a prediction  \nbut you are not interested and prediction as these intermediate time steps   \neven in this case you can actually assume that every network is performing the same task \nits taking a word as an input and it is producing some output it just that till the end you \ndo not care about you outputs you care about the output only produced at the final step  \nyou  do  not  care  about  what  the  outputs  are  at  this  time  step   right  that  is  one  way  of \nlooking at it   \n \n\fso   again  at  every  time  step  we  have  the  same  network   but  you  are  only  interested  in \nsome time steps of the network  ok   \n\n \nfinally   it  is  not  always  necessary  that  sequences  are  composed  of  only  words   what \nother  kinds  of  sequences  are  you  familiar   popular  sequences   speech  is  one   video  is \nanother  \nso  a video could  be treated as a sequence of images and now  you could  have a video \nwhere some  someone is performing surya namaskar  and as you can understand that i \nneed to look at the entire sequence and only then be able to make a prediction  right  if i \nstop at this point if i only consider this  this is only namaskar  no surya namaskar  right  \nso  you have to look at the entire sequence and then decide what the output is and you do \nnot care about the intermediate outputs  i do not care what is the prediction till this point  \nthis  of  course   is  again  some  aasan  but  i  do  not  care  about  that   i  care  about  the  full \nsequence that i am dealing with  it is just to motivate that sequences can be of all types   \nand  i  apologize  to  the  speech  people   i  do  not  really  understand  much  of  speech \nprocessing  so  i never give speech examples  but video is something i understand  so i \ncan give examples on that   \n \n\f"}
{"audio_filepath": "lec011_002.wav", "duration": 577.407, "text": "\nso  we have seen sequence learning problems  now  we are interested in the question of \nhow to model these  right  so  we look at something known as recurrent neural networks  \n\n \nand our question that we are interested is how do you model tasks which involves such \nsequences  ok  \n\f\n \nso  here is the wishlist that we have  what will model will come up with should account \nfor the dependence between inputs  because that is the strong case that we have made  \nthat  the  output  actually  depends  on  multiple  inputs  and  not  just  a  single  input   you \nshould also account for variable number of inputs because a video could be three hundred seconds  \nit could be twenty seconds  twenty five seconds  a sentence could be of arbitrary lines and so on  \nand  it  also  makes  sure  that  the  function  at  each  time  step  is  the  same   right  but  every \ntime step they are trying to do the same activity  ok  so  we will focus on each of these \nitems from our wishlist and then try to arrive at a model for dealing with such problems  \n \n\f\n \nso  first let us ask this question what is the function being executed at  each time step  \nwhat is the function being executed at each time step  either should come after dealing  \nyou have an input your ability to go blank on me is just amazing actually  you have a \nhidden  representation  and  then  you  have  an  output   the  first  time  we  are  seeing  this \nsituation  in  the  entire  course  where  we  have  an  input  a  hidden  representation  and  an \noutput  \nwhat is the function being executed  remember the output is always  a function of the \ninput  lecture two or three  i do not know  definitely not lecture fourteen  so  what is the function  \ncan you write y i as a function of x  for my sake if not for god sake  you can  ok  what \nis it  ok  first tell me what is s one  u s one  no nonlinearity  no bias which all that plus bias \nthen no nonlinearity  who cares about nonlinearities  ok  and then what is yone  ok  some \noutput function  ok  for some reason i have written sigma here  i will just call the output \nfunction as o always maybe in this case sigma would work but o is what  i will call it \njust to make the this thing clear  ok  is that fine  \nso  this is the function being executed at this every time step we can just write it using \nthese two equations which are seeing for a first time  and i is a time step  since we want \nthe same function to be executed at each time step we should share the same network at \nevery time step  that means  what do i mean by share the same network  share the same \nparameters good  so  this is the same as this because u  v and b and c are the same  ok  \n \n\fso  that is an easy way of taking care of the requirement that i want the same function to \nbe executed at every time step  \n\n \nand  this  parameter  also  sharing  also  ensures  that  the  network  becomes  agnostic  to  the \nlength of the input  because now whether i have a word which has ten characters or twenty \ncharacters it does not matter  because at every time step i am going to execute the same \nfunction  that is why it is important that at every time step we have the same function  \nso   since  we  are  going  to  complete  the  same  function  the  number  of  times  it  does  not \nmatter and we can just create multiple copies of this network that we have and for any \narbitrary length n we can still compute the output  ok  still not quite there  we still need \nto  take  care  of  a  lot  of  things   but  we  are  just  slowly  addressing  each  item  from  our \nwishlist  \n \n\f\n \nnow  how do we account  for dependence  between inputs  or rather  actually the  right \nway of asking this is how do we account for the case that the output actually depends on \nmultiple inputs and not just the current input  ok  how do we account for that  feed in \nthe ok  good  \nso  let us first  see an infeasible way of doing this   ok  so   you  are  given the first  time \nstep xone  you have a network which predicts one from xone  you know at the same second time \nstep you also want to look at the previous inputs  so why not just feed it xone and y xtwo both \nand then try to predict y two  at the third time step feed in x one  x two  xthree and predict ythree and so \non  forever   is  this  fine   probably  the  word  infeasible  is  there   so   y  is   so  what  is  the \nproblem with this  yeah good  so  i am looking in terms of the conditions that we have \non  the  wishlist   which  condition  does  is  violet   what  is  the  function  being  executed  at \neach time step  ok  so  let us see  \n \n\f\n \nthe function being executed at every time step is different  so  yone is function of x one  y two is \na function of x one   x two  remember that this is not just saying that you are passing to inputs \neverything changes  because you now you need to have uone and utwo here you need to have \nu one  u two  u three  right  so  everything changes  it is not the same function how many of you \nget this it is a different function being executed at every time step  \nso  now  if i have a sequence of length one hundred what happens  i need y one hundred which takes f x one \nto x one hundred as inputs  and has how many parameters  u one to u one hundred  right  you could you could \nshare u one to u ninety nine  for y ninety nine and y one hundred but we still need those many of that  right  so  that \nnetwork is  now sensitive to  the length  of the input and on the length  of the input goes \nyou will have to construct more and more functions  right  \nand  imagine  that  if  the  training  time  the  maximum  sequence  length  that  you  had  seen \nwas  twenty five   and  suddenly  a  test  time  you  get  a  sentence  which  is  of  length  thirty  you  do  not \neven  know  how  to  compute  that  because  you  have  not  train  any  parameters  for  doing \nthat  \n \n\f\n \nso  then the final solution is actually to add a recurrent connection in the network  why \ndoes this work  ok  before that  now can you tell me what is the function being executed \nat every time step  assume there is a s zero here  these are a s one  s two  s three up to s n and there is \na s zero  \nnow  what is the function being executed at every time step  can you write it down  if \nyou  it would help if you think in terms of y two and not in terms of y one  y one is the boundary \ncase was special case the thing in terms of y two or any other of the y\u2019s  and first think of \nwhat s two is from s two y is straight forward  how many of you can write the function  so  s i \nin general s i is u into x i plus w into s i minus one plus b  how many of you get this  and \nthen what is y i  again this has to be output functions  ok  but how does this solve our \nproblem  does it take care of everything on the wishlist  one the way we have written it \nin  terms  of i   which is  the time step definitely the same function is  getting executed at \nevery  time  step   there  is  no  doubt  about  that   right   modulo  this  boundary  case  of  s  one \nwhere will assume that there is an s zero  ok  \nso   same  function  being  executed  at  every  time  step   can  you  deal  with  inputs  of \narbitrary length  yes  as long as you ensure that the same function is executed its fine  \ndoes it ensure that the output is actually dependent on the previous inputs  how  \nstudent  \n  s i  \n \n\fthrough s i minus  right  so that is an interesting thing that this guy actually depends on \nthis guy which depend on the previous input and also on this guy which in turn depends \non the provision inputs  so  recursively you can see that you depend on all the previous \ninputs that you had  ok  that is a very neat way of ensuring that your output depends on \nall the previous inputs and you do not blow away the parameters blow of the parameters \nby sharing this recurrent connection  and that is this is a compact way of writing is that \nyour y i is now function of x i s i and has these parameters w  u  v and b and c  so  s i is \ncalled the state of the network at times step i  \nand what is see here  this is just for the sake of completion this is known as a recurrent \nneural network because of this recurrent connection and s i is a state of the network at \ntime  step  i   so   as  when  you  start  working  in  deep  learning  and  you  are  dealing  with \nsequence problems state of rnn or state of lstm or state of grv something that you \nwill be hearing or reading often  so  this is what you mean by the state of the recurrent \nneural network this is the current state which kind of encode everything that is happened \nso far  right  it has a encoding of all the inputs that you had seen so  \nthe  parameters  of  the  network  are  w   u  and  v  which  i  shared  across  time  steps  i \nobvious forget the biases  and the same network is getting executed every time steps i \ndo  not  need  to  worry  about  whether  i  am  computing  y  one   y  two   y  three  or  y  one hundred   right   so  \neveryone agrees that this solution takes care of all the things that we had on our wishlist  \nhow many of you agree with that  \n\f\n \nand this is a more compact way of representing that that you say that you compute s i \nand  then  you  are  feeding  it  back   so   this  is  just  more  compact  way  of  representing  a \nrecurrent neural network  \n\n \nso  let us now revisit the sequence learning problems that we have seen  so  now  just \ncorrect  each  of  these  networks   so   what  would  happen   each  of  these  things  i  was \nthinking of all the inputs as independent  so now  what will i do  what is the only thing \nto be done  just add the recurrent connection  right  \n \n \n\fso  once you add the recurrent connection  now you can go back and relate to all these \nproblems that one i am trying to predict the character which appears after e  i also have \nthe information of d and e  and same argument you can make for all the other examples \nthat you have  but i am trying to predict this final state i have the information of all the \nprevious inputs here  ok  \n\f"}
{"audio_filepath": "lec011_003.wav", "duration": 853.004, "text": "\nso  that was recurrent neural networks  now  whenever we propose a network what do \nwe do next  training  right  so  what we will look at it back propagation through time  \nthis is not the title of a fiction and movie or anything  this is an algorithm that we will \nsee  \n\n \nso  before we proceed  right let us look at the dimension of the parameters that we have \nand i expect you to tell me the dimensions  so  i will define somethings for you which \nare  very  hard   so   xi  belongs  to  rn   so  let  us  be  clear  about  that   si  belongs  to  rd  that \nmeans  the si is a d dimensional vector and yi belongs to rk which has k classes  ok  \nso  now what is u  what is v  d \n k  is it d \n k i am asking soham  now i mean we \nhave be written it as d \n k  and w is  \nstudent  \n  \n\uf0b4\uf0b4\uf0b4\fd \n  t  sure   everyone  sure   ok   right   so   these  are  the  dimensions   why  am  i  talking \nabout  these  dimensions   whenever  we  talk  about  gradients  what  we  talk  about  partial \nderivatives or gradient or something  we need to know what is the size of the parameter \nwith  respect  to  which  we  are  taking  the  gradient  because  that  is  what  the  size  of  the \ngradient matrix is going to be  right  that is why i am asking you to focus on this  \n\n \nnow  how do we train this network  title of the module   \nstudent  backpropagation  \nbackpropagation  ok  how  why do i have a module if i am only going to tell you about \nbackpropagation   do  you  see  any  problem  with  this   why  cannot  you  just  apply  the \nstandard backpropagation algorithm  ok  so  we will try to understand this with the help \nof a concrete example and we will go back to our example of predicting characters  ok   \n\uf0b4 \n\f\n \nso  this is the auto completion task and for simplicity we will assume that english has \nonly  these  three  characters  d   e   p  and  then  a  stop  to  indicate  that  the  word  has  been \ncompleted  ok  this is what you are going to consider that my vocabulary size is just four \nthat means  i can only predict one of these k four classes  k is equal to four  ok   \nand at  each time say  i want to predict one of these things  what is the suitable output \nfunction for this task  can everyone say with probability ninety nine nine percent  \nstudent  half max  \nhalf max  ok  what is the suitable loss function for this task  small pleasures in life that \nis all i get  ok  \n \n\f\n \nsuppose  we  initialize  u   v   w  randomly  and  networks  predicts  the  following \nprobabilities  ok  so  let us understand what is happening  i fed it d as the input i have \njust  started  training   so   my  u   w  and  v  are  all  some  randomly  initialized  weight \nmatrices  right now  and so it has predicted this as my probability distribution  this is the \npredictions that i have got from the network   \nand  i  also  know  what  is  the  true  probability  distribution   what  is  the  true  probability \ndistribution  for the first time step  zero one zero zero and so on  right you can see it  second times \nthat is also zero one zero zero  third is zero zero one zero and the last one should have been zero zero  so  given the \nsituation and before i talk about learning algorithms  what is the first thing that i need to \ndefine   objective  function   right   so   what  is  the  objective  function  here   how  many \nerrors do i have  i mean i can make my errors at four places  whether i making an error or \nnot is the separate case but i can have four loss functions  \nso  then these are the two questions that i am interested in  what is the total loss made \nby the model and how do we back propagate this loss and update the parameters of the \nmodel as usual i am ignoring the biases which is w  u and v  so  we can answer these \ntwo questions then we are done  right  if you can do this then we are done  \n \n\f\n \nso  the total loss  what is the total loss actually  take a guess  sum of all the loss  right \ngood  so  just going to  be the sum of the loss over the times steps that you are i mean \nvery logical and what else would it be  and we know that the loss at every time step is  \nso this is the loss at time step t hence y t  and what is c actually  the true class at time \nstep t  right  so  it is would be e at first time step  e at second time step  then p and then \nstop  ok  so that what c is  \nso  this is we all comfortable with is this is the cross into p loss and i am going to sum at \nover all the t time setup that i have  now  for back propagation what we need is we need \nto be able to compute the gradient of this loss function with respect to w  u  v   \n \n\f\n \nif i give you your formula for the gradient the rest is straight forward you will just apply \ngradient as well  ok  so  let us look at each of these parameters  we will look at the easy \none  first  which  is  v   so   what  is  the  derivative  of  the  lost  function  with  respect  to  v  \nhave you ever done this in life  \nstudent  yes  \nyes  \nwhen  \nstudent  \n  \nnow  i am asking the date  ok  so  you have done this when you doing backpropagation  \nthis  is  the  gradient of the loss function with  respect  to  the  weights  in  the output layer \nand  we  know  how  to  do  that   right   that  is  very  straightforward  and  there  is  no \ncomplication there  and you will see what i mean by complication later on  \nso  all i need to do is take this loss function and compute its gradient with respect to v  it \nis  very  simple  chain  rule  which  i  can  update  there   apply  there  and  i  can  compute  it \nseparately for all these guys and i can just sum it up  right  so  this is the easy part  this \nis very straight forward  so  where one parameter we are all set  we know how to do that  \nright  we can just add up all these gradients  the some lose notation here this is actually \n \n\fan addition of four matrices  right  each of this i hope is a matrix  is that a matrix or a scalar \nor a vector or a tensor  \nstudent  matrix  \nmatrix   ok   so   we  have  already  seen  how  to  do  this  back  propagation   and  this  is  a \nsmallest chain possible in the back propagation and we have enough confidence in doing \nthis   \n\n \nnow  let us considered the derivative of the loss function with respect to w  just take a \nminute and see if it is complicated or if it  is straight forward to see a lot of w\u2019s in the \nfigure  ok  so  let us see how to do that  right   \nso  again the loss with respect to w or the derivative with respect to the loss derivative \nof the loss with respect to w is going to just be the sum of these four or t derivatives  and \nby changed of derivatives we can just sum the derivative across all the paths which lead \nfrom  the  loss  function  to  w   is  that  fine   right   whenever   you  want  to  compute  the \nderivative of the loss function with respect to any parameter a recipes to look at all the \npaths which go from the loss function to that parameter and some of the gradients across \nthose  paths   how  many  if  have  fine  with  this   what  are  the  paths  which  are  actually \nconnecting the loss function to w  \nstudent  \n  \n \n\fthere will be t  paths   good  so  let us  see we will  consider  lfour \n   this is  the last time \nstep  \n\n \nso  lfour \n  actually depends on s four  s four depends on what  w and s three  s three depends on what  \nw and s two  s two depends on what  an s one depends on w and s zero  always assume there is s zero  \nwhat kind of a network is this  what kind of a function is this  what did i ask to revise  \nthis is not an order derivative  what kind of function is this   \n\n \n\uf071\uf071 \n \n\fso  we have an ordered network with i will give it to you and it is not be to  say in an \nordered network each state is computed one at a time  right  so  we will first compute s \none   then  we  will  compute  stwo  because  s  two  depend  on  sone  there  is  no  other  way  we  can \ncompute stwo  then sthree  s four and then finally  the loss function  \nso  now  we have the following situation that the derivative of lfour \n  with respect to w \ncan be written using this chain rule which is the derivative with respect to s four  and then \nthe derivative of s four with respect to w  and that is that looks manageable there is nothing \nfancy  here  or  is  it  i  see  a  lot  if  people  that  looks  manageable   right   everyone  is  not \n\n   \nstudent  \n  \neven though you have done the assignment everyone is not  even though you have revise \nthe assignment everyone is not \n  so  this part we have already seen  \nthis is not the tricky part  lfour \n   sfour is straight forward because it only depends on this v \nand so its fine that part we have seen  this is same as computing the gradient of the loss \nfunction with  respect to  the hidden layer  but now  let look  at  the derivative of sfour with \nrespect to w   \n\n \n\uf071\uf071 \n\fwhat  is  sfour  actually  \n   so   now   if  i  want  to  compute \nsfour  by  let  me  just \nremove the sigma  right  i mean we can always get back the nonlinearity  so  i want to \ncompute \nsfour  w  so  it will just be s three  s three is again  \nstudent  depend on w  \ndepend on w  right  so  that is the problem with an ordered network  in such an ordered \nnetwork you cannot compute the gradient of a s four with respect to w assuming that sthree is a \nconstant   sthree  is  not  a  constant  its  again  a  function  of  w  and  w  is  the  parameter  with \nrespect to a computing the derivative  right  that is the problem here   \nso  in such networks the total derivative has two parts  what are these two parts  okay  \nhow many if you have revise this  what are the two parts called  explicit and i mean at \nleast your language model should be fine at  explicit  and what else can it be  think on at \nleast  have  that  much  smartness   either  you  do  not  read  its  fine   so   that  is  going  to  be \nexplicit and implicit  what do we do in the explicit case  if  you can read the slide  we \ntreat all the other inputs as constant  right  an implicit is summing over all the indirect \npaths from sfour to w  so  let us actually try to derive this whole thing  right  \n\n \nso  this is what the total derivative looks like  all of you are comfortable with this  right  \ni mean this is all we have done this in the assignments i will not go into the theory and \nfour  wsb\uf073\uf02b\uf0b6\uf0b6\uf0b6 \n\fall that  you should be comfortable if you have not revised  you have to be blamed sorry \nfor that  but i cannot go into the details of that but i still derive the whole thing   \nso  this is what it looks like  the plus here indicates that we are going to treat everything \nelse as a constant  and just take the derivative with respect to w  and then the implicit \npart would be this  we are going to sum across all the paths  so  this is a path  ok  \nnow   here  again  we  have  a  total  derivative \nsthree     w   so   what  am  i  going  to  do  for \nthat   again  explicit  and  implicit   again  i  have  this \nstwo  by  two  by  w  which  is  again \nexplicit plus implicit  again \nsone    w is that fine and then this is finite because s one does \ndepends on s zero which has no connection to w  so  this is what your entire formula looks \nlike  now  this  sum slide abuse of notation here because what is each of these actually  \nscalar  vector  matrix  \nstudent  \n  \ns four is   \nstudent  s four is vector  \nvector  w is  \nstudent  matrix  \nmatrix  the derivative of a vector with respect to a matrix is  \nstudent  tensor  \ntensor  you cannot do this in your head is it  these three sentences is one after the other  ok  \nso   for  simplicity  what  i  am  going  to  do  is  i  am  going  to  short  circuit  some  of  these \npaths  right  so  let us i will just tell you what i am going to short circuit  so  i am going \nto write just for ease of coming up with the generic formula  the first term i am going to \nwrite as this  and this is fair because this is just one  right  the second term also is fine  \nthe third term i am going to short circuit this path i am just going to write as \nsfour   \nstwo \nand then \nstwo    w  and again i am going to short circuit these paths and just write it as   \nsfour   \nsone and then this  \n\uf0b6\uf0b6\uf0b6\uf0b6\uf0b6\uf0b6\uf0b6\uf0b6\uf0b6\uf0b6\uf0b6\uf0b6\fthe reason i am doing this then i can write it as a very simple summation  where i have \nsfour by sk  where k goes from one  two  three  four and then i just have the explicit derivative of sk with \nrespect to w  just stare at this for the minute and not a minute actually just ten second or \nsomething  if you have any problems with this let me know  i will use my standard trick  \nif  you  do  not  understand  this  you  will  not  understand  anything  afterwards   no  one  is \nfalling for that  ok  everyone is comfortable with this  ok  \nso  we have a formula for \nsfour    w  and we have dealt with the tricky situation where \nwe  have  these  multiple  paths  in  an  ordered  network  and  hence  we  are  to  split  into \nexplicit and implicit derivatives  so  we have done all that \n math and \nyou have come up with the simplified formula for this   ok  so  finally  this is  what  we \nhave  \n\n \nyou noting it down  right   \nstudent  \n  \n laugher  i do not see you noting it down  ok  so  now  let us look at \nsfour    w  that is \nexactly what we have derived on the previous slide and that was a summation of t terms  \nand for us t is equal to four  ok  and in general lt by  the this was for lfour so in general if i \nwant to do lt then it is going to be this which i am replaced by t  and this which i have \nreplaced by this formula  everyone is fine with this  what were this means  everyone is \n\uf0b6\uf0b6\uf0b6\uf0b6 \n\ffine with this formula  right  this is generic formula with respect to any time step  the \nonly thing is that on the previous slide we are derived with respect to sfour  now i have just \ncome up with the generic formula  ok  \nso   this  algorithm  is  called  backpropagation  through  time  because  now  we  have  taken \ncare of this ordered network and  you have a way of computing this gradient  once  you \nhave  this  gradient  your  life  is  simple  because  now  we  can  just  supply  the  gradient \ndescent  update   ok   so   we  have  dealt  with  v   we  have  dealt  with  w  and  as  the  name \nsuggest who will deal with u  you ok  fine  so  you will to find out what it is for u  ok  \nby its going to be something very similar  and i do not want to do it because that is not i \nmean going to be something very similar you can do it on your own  but i want to focus \non something which is important   \n\f"}
{"audio_filepath": "lec011_004.wav", "duration": 653.568, "text": "\nand that takes us to the problem of vanishing and exploding gradients ok  so  you want \nto  see what  is  a problem  with  this back propagation  through time  which  could  lead to \ncertain interesting situations  \n\n \nso   we  will  focus  on  this \nst    \nsk  and  let  me  just  go  back   so   remember  that  this \nformula had this \nst   \nsk  right  where st could be the last time step and sk could also be \nthe first time step because you are summing over all the time steps  right  \n\uf0b6\uf0b6\uf0b6\uf0b6\f\n \nso  you could have a term which is st capital t which is the last time step the first time \nstep and the derivative of the last time step with respect to the first time step  right  so  \nthat is a situation that we are dealing with  so  we will consider one such generic element \nwhich is \nst   \nsk and we will just try to expand it  so  remember i have done this short \ncircuiting  so i am now just going to expand it again  so  this is going to be t   t   one  t one  t  \ntwo and so on up to k  one  sk  ok and i can write it as this generic formula  everyone find \nwith this  i have just replace this as a product and written it more compactly  \nnow  let us look at one such term here \nsj  \nsj one  now  just to confuse you guys from \nnext slide i will go over to \nsj  \nsj   one or not confuse you i just did not pay attention to \nthis  so  instead of s plus one and j and i am going to do j and j minus one  right  it remains \nthe same does not matter  \n\uf0b6\uf0b6\uf0b6\uf0b6\uf0b6\uf0b6 \n\f\n \nso  we are interested in this particular quantity  so  let us see what this derivative is  and \nremember  that  in  the  final  formula  we  have  a  product  of  these  quantities   so   i  am \nlooking  at  one  such  term  in  my  final  product   so   just  to  jog  a  memory  a  j  is  the  pre \nactivation which is given by this and then s j is the hidden representation after activation \nafter the nonlinearity which is given by  so  let me just write it down as s j by s j minus one \ncan be written as this chain rule which is first compute s j with respect to a j and then a j \nwith respect to s j minus one  everyone is fine that so far at this point  please raise  your \nhands if you find ok   \nnow   let  me  just  write  down  a  j  and  s  j  explicitly   so   remember  that  a  j  is  this  d \ndimensional  vector  which  are  the  entries  a  j  one   a  j  two   up  to  a  j  d  and  s  j  is  the \ncorresponding activation applied vector which has these entries sigma a j one  a j two and so \non  ok   now   first  question   what  is  this  quantity   scalar   vector   matrix   tensor  \nnumerator is a  \nstudent  \n  \ndenomitor is a  \nstudent  \n  \nthat is why it is a matrix ok  so  that is the matrix that i am interested in  if i can give \nyou that matrix and we are kind of done so  it help me filling in this matrix  \n \n\ftell me what this matrix is going to look like even before we start filling it ok  you are \nright   but  it  does  not  matter  because  you  will  have  u  x  and  then  you  are  taking  the \nderivative with respect to s j minus one  right so  this does not matter ok  so  everyone gets \nthat  you will have a u x j here  right but that does not matter because you are taking a \nderivative with respect to s j so  that is a constant  \nso  \nsj    aj  is what  what does this matrix look like  how many of you see a diagonal \nmatrix  ok good  so  it is straightforward  right  what is the first entry it is going to be \nsjone   ajone  what is that going to be  it will be something  but let us look at the second \nentry \nsjone   ajone what is this going to be  what this going to be  \nstudent  zero  \nzero  because it does not depend on that  right  so  now  you can see how the full matrix will \nlook like all the of diagonal elements are going to be zeros and diagonal elements are going \nto be sigma primes  everyone fine with this  ok  so  this matrix i am going to just call it \nas diagonal sigma prime a j  this is a diagonal matrix which i have  and what is dou a j \nby dou s j minus one  scalar  vector  matrix  scalar  \nstudent  \n  \nmatrix  which matrix  \nstudent  w \n  \nw  right ok  so  now  for some reason i am interested in the magnitude of this  why i am \ninterested in the magnitude of this  for some reason i am interested  let us see  why we \nwill become clear that for some reason i am interested in  \n\uf0b6\uf0b6\uf0b6\uf0b6\uf0b6\uf0b6\f\n \nand here i will write how i will write the magnitude of this  right  so  this is the norm \nthat i am interested in  so  i have already said that this is actually equal to whatever is \ninside this norm  so  i can just write it as this norm so  i have norm of c is equal to norm \nof a b which is less than equal to  \nstudent  norm a  norm b  \nnorm a norm b  ok  this is fine ok  now  let us look at the norm of this  now  going to \nsay that sigma a j is actually a bounded function because  we are using sigmoid or tan h \nor something so  it is a bounded function ok  so  that mean sigma dash a j is also going \nto be bounded  actually  can you tell me what is the bound for the logistic function for \nsigma dash a j   \nif sigma is logistic function what sigma dash what is the bound for sigma dash  if i say one \nby four how many of you will agree with that  how many of you have a problem with that  \nif you do not understand this you not understand anything after that ok  still do not have \na  problem   so   for  the  logistic  function  the  bound  is  actually  one four   the  maximum \nderivative that you can get if you have this curve so  then that would be one four  ok  \nwhat about the tan h function  and that actually happens at this point  right zero five  so zero five \ninto  zero five  is  one four   what  about  the  tan  h  function   the  bound  is  one   right   so   this  is   this \nclearly  an  upper  bond  on  these  things  the  derivative  is  going  to  be  an  upper  bounded \n \n\fthing that means  this magnitude is actually going to be upper bounded by something and \ni will just call it as lambda sorry as gamma  so  this quantity is bounded and i am going \nto call that bound as gamma  \nwhat about our weight matrix  it is again bounded  right we have real weights we do not \nhave like blowing we do not have very large weights it is all bounded  so  it is still going \nto be some upper bound on this and i will call this magnitude as gamma  right  so  this \nquantity  on  the  left  hand  side   i  can  say  that  it  is  less  than  equal  to  some  gamma  into \nlambda  \nnow  let us look at the product  so  this is a quantity that i was interested in and this is \nactually a product of various such quantities  so  what is it going to be now  can you go \nto the next step  it will be gamma into lambda raise to t minus t minus k  right  t minus  \nit basically as t minus this product as t minus k terms  right  so  it will be gamma lambda \nraise to t minus k  now  if gamma or lambda  or rather gamma into lambda if it is greater \nthan one  what will happen  what will happen to the series  explore  if it is less than one  \nstudent  \n  \nit will vanish  right so  you get that  so  that is why you have this vanishing an exploding \ngradients problem ok  but why what if this vanishes what vanishes  let us go back  so  i \nhave shown you that this quantity could vanish right if this vanishes the entire gradient \ncould vanish  and if the gradient vanishes what would happen  \n\f\n \nstudent  no updates  \nno  updates  and  you  just  stuck  where  you  are   if  the  gradient  explodes  what  happens  \nthink  in  terms  of  the  wb  plane   you  suddenly  have  a  very  large  gradient  what  will \nhappen is just gone way far from where you are right  now  because your update is w is \nequal to w minus eta into this gradient and this you have got a very large value now  \n it just going to move somewhere very far from where you are and that is never go where \nyour  suddenly  jump  to  a  different  universe  ok   so   that  is  the  problem  in  training \nrecurring  neural  networks   you  could  have  this  problem  of  exploding  or  vanishing \ngradients   and we have  done a mathematical derivation  of why  you have this  problem  \nok  \n \n\f\n \nso  one trick to  do that  is  to  avoid  this is  remember these are t minus k terms  and the \nproblem appears when your t minus k is or rather you are t is close to capital t  and k is \nclosed to one  right  in those cases you will have many terms in the product you will have \nas  many  as  t  terms  in  the  product   so  even  if  your  product  is  even  if  this  product  is \nslightly  less  than  one   if  you  raise  it  to  capital  t  it  is  going  to  vanish   right   so   can  you \nthink of solution for this  \nand the last module in the title of this lecture was truncated back propagation  can you \nthink of a solution for this  so  you do not back propagate through all the time steps  yes \nuse an approximation that if you are at time step n  we are just going to look at n minus \nk time steps and we are not going to look all the way back  right  that is the common \ntrick used to avoid exploding and vanishing gradients  \nwhat  is  the other thing that  you could do to  avoid  exploding  gradients  so  remember \nthat you have some gradient  right  to think in terms of vectors we have some gradient \nvector w whose magnitude is very large  what will you do to avoid exploding gradients  \nin gradient descent your always interested in the direction so  what can i do   \nstudent  \n  \njust normalize it  right  so  you can just do this so  typically what is done is that you can \nit  is  a  normalizing  it  you  can  just  say  that  you  will  clip  the  gradient  so  that  it  is \n \n\fmagnitude is less than a certain k  right  so  normalize it in such a way that it is grade it \nis  magnitude  becomes  k   so   this  is  something  typical  that  you  will  see  when  you  use \ntensor  flow  where  you  have  something  with  says  clip  the  gradients  to  a  certain \nmagnitude  and there are different ways of doing this  so i just give you an intuition that \nthis  is  what  is  used  for  magnitude  but  there  are  other  things  that  you  can  use  for \nmagnitude  so  just go back and look at that ok  \nso  that is a back propagation through time with exploding and vanishing gradients and \nthen the solution for that or a part for that is truncated back propagation ok  we have we \nhave not  yet done with this problem  we will again look at other solutions for handling \nthis  which  will  lead  us  to  lstms  which  is  long  short  term  memory  cells  and  gated \nrecurrent units  so that we will do in the next lecture   \n\f"}
{"audio_filepath": "lec011_005.wav", "duration": 407.53, "text": "\n\nsome gory details \nbefore  that  i  will  just  go  into  some  more  gory  details  about  the  math   this  is  not  to \nscare  but just to make you more comfortable that we can actually deal with something  \nwhich is not very straightforward or very neat as compared to what you are seen so far  \n\n \n\fso   i  just  go  back  to  the  formula   which  i  had  for  computing  the  gradient  of  the  loss \nfunction with respect to w  and i cannot repeat enough times that  all these notations are \nactually a bit of abuse of notation  because these are gradients and not partial derivatives  \nso  i should actually be using this notation  but for ease of explanation  i use i stick to \nmy original notations ok  \nso now  let us look at each of these quantities here and tell me the dimensions of these \nquantities  let us start with the left hand side  what is the dimension of this  w was what \nmatrix  what is i am talking about the circle entity  what is the magnitude  what is the \ndimension of that  k \n d  \nstudent  \n  \nd \n d  \nstudent  \n  \nd \n d  someone n \n d  \nstudent  \n  \nn \n d and n \n k are the two options  which are left  w is the recurrent weight  so  w is \nwhat dimension  \nstudent  \n  \nd \n d  \nso   what  is  this  gradient   d \n  d   ok  what  about  this   fast   st  what  the  hidden \nrepresentation   so  that  was  d  dimensional   so   what  is  this  d  one   ok  what  about  this  \nwhy do you guys still struggle with this  \nstudent  \n  \nd \n d and this d cross  \nstudent  \n  \n\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\fd cross it is very straightforward  right  what is the dimension of numerator  what is the \ndimension of denominator thats all right  \n\n \n so  you see the kind of multiplication that you are doing here  say of d cross  done d  d \nd and then d  d \n d ok  let us look at each of these quantities and see if you are actually \ncomfortable  in  implementing  these   are  you  comfortable  with  this   the  loss  function \nwith  respect  to  the  hidden  representation   we  have  done  this  enough  times  in  back \npropagation  what about this  we just saw a formula for this right  so  we know how to \ncompute this quantity  we have seen this in back propagation  this is the derivative of a \nscalar with respect to a vector and we are very comfortable in computing this  \nthis was slightly tricky  but we just derive this formula on the previous slides  everyone \nokay with that  what about this  this is a tensor  how do we compute this tensor  what is \nour standard recipe focus on  \nstudent  \n  \nthe little guy  one element of this tensor and then you can generalize somewhere right  so  \nthis is the tensor and we will just see that  this just to make you all comfortable is this not \nlike just to intimated you with all these large sized tensors  but i am just trying to show \nthat  this is all easy  this is not hard ok  so how do we compute this  all the other terms \nare covered  \n\uf0b4\uf0b4\uf0b4\uf0b4 \n \n\fthis is the only one  that we do not know  \n\n \n so  we will just look at one element of this tensor and it is going to be skp wq r  \nso  let us just see that  you have s k as this vector and you have w as this matrix  so  i \nam  considering  one  such  weight   which  is   w  p  comma  q  and  one  such  element  from \nhere  which is s k sorry  so  q r and i am considering one element from this  which is s k \np  so  i am trying to compute the derivative of one element of the vector with respect to \none element of the matrix  so  this is going to give me one entry  in my tensor and that \nentry is going to be what  p q r  how many of you are fine with this  ok fine  \n \n\f\n \nso  now recall that a k was equal to w into s k minus one plus b and s k was sigmoid of a \nk  i think again  i have miss that u into x k  but that will not matter because  that is not \nthere in the derivative ok  you are fine with so far  \n\n \nso now  let us look at this  because the other two terms do not matter  so  i just look at a k \nis equal to w into s k minus one  so  this is the matrix way of writing it  ok  now i am \nlooking at one of these elements which actually comes  from the multiplication of a row \nand a column  the highlighted row and the column  everyone gets this  ok  \n \n \n\fnow so  i can write it as a k p is actually equal to this summation  which is nothing  but \nthe dot product of this row with this column ok  now s k p is just the sigmoid of that  so \nnow  if i want to compute s k p with respect to w q r  i can just write the chain rule that \ns k p with respect to a k p which is straightforward and then a k p with respect to w q r \nand  i  already  have  a  formula  for  a  k  p   how  many  of  you  are  fine  so  far   please  raise \nyour hands high up  if you are fine ok  so  what is the first term going to be  sigma  \nstudent  \n  \nsigma prime of a k p and what is the second term going to be  this is what  the second \nterm is  now  what this is lot of terms here  which of these terms would actually remain  \nonly the once where  only the terms where i is equal to  \nstudent  \n  \nr and  \nstudent  \n  \np is equal to q  so  only that term will remain  in that case  it would be this right and in \nthe other cases going to be zero right  so  now  you have one element of this tensor and you \nhave it as a very generic formula  you can just fill in all the elements of the tensor right  \nso  what does this tensor look like  it is a very  \nstudent  \n  \nsparse  tensor  right  that  is  all  i  wanted  to  convey  ok   so   this  is  again  the  same  thing \nright  is that fine  so  even though it is a nasty looking tensor  if we just break it down to \none  element   it  is  going  to  be  very  easy  and  now  from  this  element   you  can  just \nreconstruct the entire tensor  do not worry  i am not going to ask you to implement this  \nbut if someone were to  maybe at some point  then you should be able to do it right  that \nis where we will end today   \nso  we have finished recurrent neural networks and the next thing that  we are going to \nlook at is lstms and gated recurrent \n ok  \nthank you  \n\f"}
{"audio_filepath": "lec011_006.wav", "duration": 547.407, "text": "\nso  we have been looking at a new kind of or a different kind of neural network which is \nrecurrent neural network  and last class we spent some time on showing that it is how to \ntrain  these recurrent  neural  networks because of  the specific problem  of  exploding and \nvanishing gradients  in particular we saw that if you want to propose  if you want to back \npropagate the gradient from time step t  the final time step to an arbitrary time step k then \nyou have this multiplicative term in the back propagation which could explode or vanish  \nso  today  we  are  going  to try to see if we are trying to  focus on something which can \nhelp  us  solve  this  problem  to  whatever  extent   right   so   that  is  why  we  will  look  at \nlstms  a long short term memory cells and gated recurrent units  ok  so  let us start \nwith that   \n\n \nso  first we will introduce the idea of selective read  selective write and selective forget \nand then we will try to build on this intuition and see how could you could realise it by \nusing  lstms  and  gated  recurrent  units  and  whether  that  help  in  solving  the  vanishing \nand exploding gradient problem  \n\f\n \nso  recording the state of an rnn records information from all the previous time steps \nthat was the whole idea that you add this recurrent connections  so  as you keep going on \nat this time step the cell is not only recording information from the current time step but \nit also has some kind of an accumulated history from all the previous time steps  right  \nbut now the issue is that this state or this blue coloured vector that you see is going to be \nof  some  finite  size   right  we  will  say  that   it  is  a  one hundred  dimensional  vector  or  a  one thousand \ndimensional vector but whatever be the size it is going to be some finite dimension  \nnow  as you keep writing information to that cell you are morphing the information that \nyou  had  written  at  the  earlier  time  steps   right  do  you  get  that   so   now   that  is  the \nproblem   right   because  on  one  hand  you  are  saying  that  you  want  to  record  the \ninformation from  all the previous time steps  but the at  the same  you at the other hand \nyou just have a finite amount of memory to deal with  so  it is bound to read over ridden \nand  the  information  will  get  morphed  so  much  that  it  is  completely  impossible  to  say \nwhat  was  the  original  contribution  at  time  step  one  or  time  step  two  once  you  have  reach \nsome  time  step  twenty   thirty  or  so  on   right   so   that  is  the  problem  with  recurrent  neural \nnetworks   and  we  will  tie  it  back  to  the  problem  that  we  had  with  the  vanishing  and \nexploding gradients  right  \n \n\f\n \nso   in  fact   the  similar  problem  occurs  when  you  try  to  when  the  information  flows \nbackwards during back propagation  right  it is very hard to assign the responsibility of \nthe error caused at time step t to arbitrary time steps before it  right to very far away time \nsteps it is very hard  and that is the vanishing gradient problem  right  because we have \nthis multiplicative term and the gradients vanish  so that that is very hard to do  so  both \nduring  forward  propagation  the  information  vanishes  and  even  during  backward \npropagation the information vanishes  right  and we saw a formal argument of this while \ndoing  vanishing  gradients   so   this  is  just  an  illustrative  diagram   but  we  also  saw  that \nformally the guardians do vanish under certain conditions  right  \n \n\f\n \nnow  let us see an analogy for this and from here on we will build on some ideas which \nwill  help  us  arrive  at  lstms  and  gated  recurrent  units   right   so   the  analogy  is  a \nwhiteboard  so  you have a whiteboard and it is always of a fixed size  right i mean the \nwhiteboard  is  not  infinite   you  just  have  some  size  for  the  whiteboard  and  you  keep \nwriting information on that  so  at every time steps i keep going and writing something \non the board and i am trying to derive something or just try to make a story or anything  \nright  i just keep trying to write information on the board  \nnow   since  the  whiteboard  is  fixed  size   at  every  time  i  am  essentially  morphing  the \ninformation  which  was  written  at  the  previous  time  step  and  after  many  time  steps  it \nwould be impossible to find out that now whatever my state of the board is how did time \nstep one contribute to that state  right  because  it is written all over the board i do not \nknow where i started what i did and so on and its going to be very hard for me to find \nand  this  happens   right   when  you  do  these  long  derivations  on   on  the  whiteboard  it \nbecomes very hard to track  where did i start where was this variable defined and so on  \nright because it is a fixed size you cannot really i will end of deleting some terms and so \non  right  \n \n\f\n \nso  and we will make this more concrete with the help of an example  right  suppose i \nam  trying  to  drive  an  expression  on  the  board  and  typically  what  we  do  is  we  use  three \nthings  we use selectively write on the board  selectively read the already written content \nand selectively forget or erase some content  so  let us see what i mean by these three ideas  \nok  \n\n \nso   first  we  look  at  selective  write   so   this  is  my  problem  this  is  the  derivation  that  i \nwant  to  do on the board and  i have a very small whiteboard which allows  me to  write \n \n \n\fonly  three  steps   ok   that  is  the  situation  in  which  i  am  operating   ok   so   i  am  given  the \nvalues of a b c d and i want to compute this expression ac\n  ad  ok  now  the \nfirst thing that i am going to compute is ac  right that make sense because that is the first \nterm that i need  so  i will write ac equal to five  and the second thing i will write is bd   \nthirty three  ok  so  now  why did not i do the following  i could have done this a equal to one  c \nequal to five  then ac   five  then b is equal to something  then b into d is equal to something   \nso  what am i doing here while writing on the board  why did i write only two steps  \nbecause  i  know  i  have  a  finite  size  for  the  whiteboard   right   so   i  am  only  trying  to \nwrite information which is important  right  i am not writing everything because i know \nthat  i  am  going  to  run  out  of  memory   right   so   that  is  why  i  did  not  write  these \nintermediate steps that a   to one  c   five and then a c   five and so on  right  i juts wrote the \nresults  which  are  important   so   i  am  selectively  writing  to  the  board  because  i  am \ndealing with a finite size memory  and this is exactly what we do while writing in a note \nbook or a whiteboard or anywhere  right   we just do not  write everything one because \nyou are lazy but second is also because we do not have enough space  right  so  that is \nabout selectively writing  \n\n \nnow  selectively reading i already have something stored on the whiteboard  so  this is \nthe state of the whiteboard at this point  now  at the next time step i need to read some \ninformation  but i do not need to read everything that is on the whiteboard i just need to \n \n\fread some information  what is the information that i need to read for the next time step  \nbd  i do not need to read ac  right  so  i am just doing a selective read of the information \nor the state which is already stored on the whiteboard  \nnow  what is happed is i have exhausted my space where i had just three steps that could be \nwritten on the whiteboard and i have done that  now  what do i do for the next thing  \nnow  i need to compute ac \n  i will have to selectively erase  so what will i erase  \nbd  right  \n\n \nso now as the whiteboard is full and i will have to selectively delete some information \nand as this obvious in this trivial example that you can get rid of bd because  you have \nalready  encoded the information in  bd    a  and  now the next  step which is  ac into bd \nplus a can do on the whiteboard and this is how you keep doing at every time step  you \nselectively write at every stage   \nhere again note that i am not written ac\n would be something like i do not know \nwhat was it five x thirty four   one hundred and seventy  right  i am not using two steps i am just writing everything in a \nsingle step because i do not have enough space  right  so  selectively writing  selectively \nreading   and  selectively  forgetting  things  which  are  there  in  a  constant  memory  is \nsomething that we do regular  right  \n \n\f\n \nand  then  other  ways  of  motivating  this   so  you  could  even  think  of  our  brain  as \nsomething which can store only a finite number of facts  right as we keep going or if we \nwill learning more and more things we can only retain a finite number of facts  \nand what  happens inadvertently is  that  you erase some of the steps not  consciously of \ncourse  you do not have a delete button or anything but  you erase some of these things \nthat you forget a lot of things which had happened a year back or so  and also at various \ntimes if i ask you what was this which i have done in last class most of you forget to do \nthe selective read but that is what you do  right you always do selective forget but that is \nwhat we typically do in our brain also  right  any time when you are dealing the finite \nsize memory you will always have this three operations either they are explicit or implicit  \nbut the intuition is that you end up doing this  ok  \nso   now   since  the  rnn  also  has  a  finite  state  size  can  we  do  something  like  this \nselective read  write and forget  so that one during forward pass even if the information \ngets morphed  it gets morphed in  a principle manner  right  so  even in the whiteboard \nexample i was morphing the information  i was deleting the information written at time \nstep  one   time  step  two   but  i  was  being  a  bit  smart  about  that   i  was  retaining  some  good \ninformation and only deleting what was not required  so  can we do this in analogy  so  \nthis  analogy  really  sets  it  up  but  now  the  solution  is  not  going  to  live  up  to  the \nexpectation  but it will have some it will be something in this direction  ok   \n \n\f"}
{"audio_filepath": "lec011_007.wav", "duration": 1869.086, "text": "\n\n \nso   with  that  motivation  let  us  go  to  the  next  module   where  we  will  talk  about  long \nshort term memory and gated recurrent units ok \n\n \n\fso now  all this  was  fine in  terms  of ok   i  gave  you a derivation on the  board and say \nthat  this is not required  but can i give you a concrete example  where rnns also need \nto selectively read  write and forget right only then you will be convinced that this kind \nof morphing is bad in the case of rnns  so  i will start with that example and then once \nwe  agree  that   we  need  selective  read   write  and  forget   how  do  we  convert  this  into \nsome  mathematical  equations   right   because  conceptually  it  is  fine   but  you  have  to \nwrite  some  equations   so  that  the  rnn  can  do  some  computations   where  you  have \nselective read write and forget right  so  that is what we are going to do over the rest of \nthe lecture  \n\n \nso  first let me start with the concrete example  where you want to predict the sentiment \nof a review using an rnn  so  this is the rnn structure  we have done this in the past \nthat you have a sentence  one word at a time is your every time step  you will feed this to \nthe rnn and at  the last  time step   you will make a prediction   and as  i  said  the rnn \nneeds a document from left to right and by the time it reaches the end  the information \nobtained from the first few words is completely lost right because  it is a long document \nand you are continuously writing to the same cell state   \nso   you  will  lose  the  information  that   you  had  gained  at  the  previous  time  step   but  \nideally  we  want  to  do  the  following   we  want  to  forget  the  information  added  by  stop \n \n\fwords  like  a   an   the  these  do  not  contribute  to  the  sentiment  of  the  i  can  ignore  these \nwords and still figure out the sentiment of the document  \ni  want  to  selectively  read  the  information  added  by  previous  sentiment  bearing  words  \nso  when i have reach the last time step  i should be able to read everything else  which \nhad some sentiments  before it and focus on those words just  i want  to  selectively  read \nfrom  these  sentiment  bearing  words  and  also  i  want  to  selectively  write  the  new \ninformation  so  i have read the word performance  now i want to selectively write it to \nthe memory  whether i should write it completely or should i only write parts of it or not \nthat is what i need to decide  so  that is fair  this is a typical example  where rnn also \nwhen  it  is  dealing  with  long  documents   it  needs  to  understand  what  is  the  important \ninformation in the document that needs to be retained and then selectively read  write and \nforget ok  \nso  i am spending a lot of time on this analogy  because  you need to really understand \nthat this is important and this is where rnn suffer right  if you are using them for very \nvery long documents  if we have document of the size one thousand words  which is not comm  \nwhich  is  not  uncommon  right  because  wikipedia  pages  have  much  more  than  that  per \ndocument  so  it is going to be very hard to encode the entire document using an rnn \nnot  that  it  is  going  to  become  significantly  easier  with  lstm  or  grus   but  to  certain \nextent it will become easier ok \n\n \n \n\f \n \nnow the next part is how do we convert this intuition into some mathematical equations  \nright  so   let  us  look  at  that   so   in  this  diagram  recall  that  the  blue  colored  vector  is \ncalled the state of the rnn  it has a finite size  so now i will just call it as s t belongs to \nsome  r  n  and  the  state  is  analogous  to  the  whiteboard  and  sooner  or  later  it  will  get \noverloaded with information and we need to take care of this right  so now  our wish list \nis selectively read write and forget ok  so  let us start with that  \n\n \n \n \n\fso   what  we  want  to  do  is  that  and  this  is  the  problem  definition  now   that  we  have \ncomputed the state of the rnn  this is a blue colored vector although  it is not blue  but \nthis the blue colored vector from the previous diagram  where the state of the rnn was \ncomputed  i know what the state is at time step s t minus one  now i want from here to here \ngo from here to here  that means  from s t minus one  i want to compute the new state of \nthe rnn right  so  i had something written on the whiteboard  i want to write something \nnew  i want to change the state of the whiteboard and this is the new information that has \ncoming to me right the x t is the new information at time step t  \nand while doing this i want to make sure that i use selectively write  read and forget  so  \nthese  three  operations  have  to  come  in  somewhere  in  the  between   so   that  i  am  true  or \nfaithful to the analogy  which i have been giving right that is the this is the our problem \ndefinition  now going from s t minus one to s t and introducing these three operations along the \nway that is what we are interested in doing  \n\n \ni  will  go  one  by  one   we  will  implement  each  of  these  three  items  right   so   we  will  start \nwith  selective  write   so   recall  that  in  rnns  this  is  what  happens  at  every  time  step  t  \nyou take the previous time  step previous cell state   you take the current input  do you \nrecognize  the  operation  here   how  many  of  you  recognize  the  operation  here   raise \nyour  hands  ok   so   this  is  nothing   but  the  following  operation  and  as  usual  i  have \nignored the bias  ok  is that fine  so  that is what i am representing it as  \n \n\fbut now so  one way of looking at it is that  when i am computing st  i am reading or i \nam  taking  the  whole  of  st one   so   once  i  have  computed  st one   i  am  writing  this  to  my \nwhiteboard  and  then  whole  of  it  would  be  used  to  compute  the  next  time  step  ok   but \ninstead of doing this what i want is  i want to read only selective portions of s t minus one \nor rather i want to write only selective portions of st one  once i have computed s t minus one  \ni  do  not  want  to  write  the  whole  of  it   because  then  the  whole  of  it  will  be  used  to \ncompute  the  next  cell  state   i  do  not  want  that  i  just  want  to  selectively  write  some \nportions of it ok  \nnow   in  the  strictest  case  since   i  know  that  s  t  minus  one  belongs  to  r  n   it  is  an  n \ndimensional  vector in  the strictest  case  what  i could  have done is  i could have used a \nbinary decision that of all these n entries  i am going to read some entries and ignore the \nothers  so  all the other entries i am going to set to zero fine  that is the strictest thing  that \nyou could have done  now for any of these strictest things  what is the soft solution  so  \nfor binary what is the soft solution  binary zero to one  so  what is the soft solution for that  \nbetween  \nstudent  \n \nzero  to  one  so   read  some  fraction  of  each  of  these  dimensions  right   so   let  us  try  to \nunderstand what i am trying to do here ok  so  and the third bullet some of these entries \nshould have gone to zero right ok  \nso  instead of doing this  what we want to do is we have this vector  which has n entries \nthis is the cell state at t minus one  i do not want to write the entire vector onto the final cell \nstate  what i want to do is  i will take some fractions of it is say zero two of this zero three of this zero four \nof these and then write only that  do you see the operation that i am trying to do right  i \nwant to take some fractions and write only those to the cell and as i said this is the softer \nversion of the hard decision  which would have been zero for this one  for this again zero for this \nand so on right  \n\f\n \nhow to do this  why to do this  all that is not clear  i am just telling you the intuition  \nhow and why will become clear later is that fine  ok  so  we want to be able to take st one \nand write only selective portions of it or pass only selective portions of it to s t  \n\n \nso  whenever we compute st  we do not want to write the whole of s t minus one  just want \nto use selective portions of that  so  what we do is we introduce something known as a \ngate  and   so   this  gate  is  ot one  ok   we  take  the  original  cell  state  st onedo  an  element  wise \nproduct with a gate  which is known as the output gate and then write that product to a \n \n \n\fnew vector  which is st oneok  so  initially this will look confusing  but it will become clear  \nby the end of this lecture ok  so  is that fine  this is what i am trying to do  again how to \ndo this is not clear  but this still matches the intuition  which i have been trying to build \nthat i want to write only selective portion of the data  which i already have  is that fine \nok   so   each  element  of  ot one  gets  multiplied  by  the  corresponding  element  of  st one  and  it \ndecides what fraction is going to be copied  and this o t minus one is going to be  between \nzero to one  \nbut  how do  i compute o t  minus one  how does the rnn know what  fraction of the cell \nstate to  get to the next state  how will it do it  we need to learn something whenever \nyou want to learn something  what do we introduce  everyone  \nstudent \n  \nparameter  sorry   what  did  you  guys  say   back  propagation   back  propagation  will  do \nwhat  it will work in the air or propagate to what  \nstudent  \n  \nwhenever you want to do some kind of a learning  i want to learn some function  what \ndo i introduce  \nstudent  \n parameter  \nparameter  right  so   that  is  what  we  are  going  to  do   we  are  now  going  to  introduce  a \nparametric  form  for  ot one  right   and   remember  this  throughout  in  machine  learning  \nwhenever  you  want  to  learn  something  always  introduce  a  parametric  form  of  that \nquantity and then learn the parameters of that function  do you get this how many of you \nget the statement  ok this is what we have been saying day from right from class two or \nclass three right  \n\f\n \nalways  introduce  a  parametric  function  for  your  input  and  output  and  learn  the \nparameters of this function  so  that is exactly  what i am going to do  i am going to say \nthat ot one  is actually this function  i am just giving some time to digest this  so  this is at \ntime step t   one  so  it depends on the input at time step t minus one  it also depends on the \noutput  at  output means  whatever comes out  of this right   so  the same operation  what \nhave  happened  at  time  step  t   two   so   whatever  was  the  output  at  that  state  it  will  also \ndepend on this  \nyou just take a while to digest   this equation  you will see at  least  six more equations of \nthis form in this lecture  so  if you are comfortable with one all of them  would be clear  \nso  try to connect the whole story  i have st one  i do not want to pass it as on pass it on as it \nis to st  so  i am computing some intermediate value  where i will only selectively write \nsome  portions  of  st one  and  selectively  write  in  the  strictest  case   it  should  be  binary   but \nthat is not  what we are interested in  we introduce fractions if the fraction has to learn \nbinary let it learn  but we will make it fractional  that means  we will make it between zero \nto one   \nhence  the sigmoid function right   remember in  one of these lectures  we had said  that \nsigmoids  are  still  used  because  in  rnns  and  lstms   remember  in  we  had  said  that \nsigmoids are bad use stanage \n or use relu  but we had ended with \n \n\fsigmoids are still used in the case of recurrent neural networks  and lstms  so  this is \nwhere they are used ok  how many should get that connection  ok good fine  \nso  we use sigmoids  because we want the fraction to be between zero to one and we also want \nsome parametrization right and this is the particular form that  we have chosen  there are \nvarious equations possible various things  you could have done here  in fact  there are ten \nto fifteen different variants of lstms  i am covering the most popular one  which uses the \nfollowing equation right  so  it is says that this is how you will compute the output gate \nand that gate will regulate  how much of the cell state should be passed on from t minus \none to the next state  ok  everyone clear with this ok  so now  if you are clear with this \ngive me an equation for h t minus one  \nstudent  \n  \nloudly  everyone  s t minus one is that fine right  so  this is the equation that we will have  \nso   we  have  done  selective  writing  and  these  parameters  are  no  special   they  will  be \nlearned along with the other parameters of the network ok  so  let us spare some thought \non that  you got a certain loss at the output ok  earlier  you just had these parameters w  \nu  v which were the parameters of rnn  which you are adjusting to learn this loss  now \nin addition  you also have the flexibility to adjust these parameters   \nso  that if the lost could improve by selectively writing something then  these parameters \nshould be updated accordingly right  may be you are being over aggressive and making o \nt minus one to be all ones  that means  you are passing everything to the next state right  \nnow  it  has  the  chance  because   they  have  introduce  parameters   if  it  helps  the  overall \nloss  it better make these fractions more appropriate  so  that only selective information \nis passed to the next state  how many you get this intuition  so  that is why anytime  you \nintroduce parameters  you have more flexibility in learning  whatever you intend to learn  \nthere is remember  one clear difference here right and that is where i said that  while i \nwas giving the analogy  i was really setting up things  but here there is one distinction  \nwhat is the distinction that is there  ideally what would  i have wanted  suppose i take \nthe  example  of  the  review  ok  and  the  review  was  say   the  movie  was  long   but  really \namazing ok  now which is the word here  which is actually trying to mislead  so  overall \nsentiment  is  positive  right   everyone  agrees  with  that   but  which  is  the  word  which  is \nmisleading  \n\fstudent  long  \nlong right  that means  i need to do what with that word  \nstudent  \n  \nforget that would  right  now ideally  i would have wanted someone telling me retain  \nretain  retain  forget  retain  retain  retain  i would have a label for each of these words \nand  then  i  could  have  a  loss  function  which  tells  me   whether  my  gates  were  actually \nathering to this decisions or not  so  remember my gates are learning some distribution \not one  which tells me what fraction to retain  and at this particular time step  i would have \nwanted o t minus one to be all zero\u2019s ok  i would have wanted to forget  but this kind of not \njust o t minus one  this will become more clear  when i do all the other gates also  so  what \ni am trying to say  is that you should have had some supervision  which tells you which \ninformation  to  retain  and  which  information  to  forget   but  you  do  not  have  this \nsupervision right no one is telling  whether these are the important words these are not \nthe important words  \nso  that is the difference between the whiteboard analogy  there you knew exactly which \nstep  is  important  and  which  step  is  not  important   here  you  do  not  know  that   all  you \nknow is  that you have a final loss function  which depends on plus or minus  whether the \nthis prediction is close to positive or close to negative and what is the loss and that loss is \nwhat  is  being  back  propagated   but  the  difference  now  is  that  you  have  introduced  a \nmodel   which  can  learn  to  forget  some  things  right   earlier  you  did  not  have  a  model  \nwhich  could  learn  to  write  or  read  or  forget  selectively   now  you  have  introduced  a \nmodel  this is a better modeling choice right  so  the same as we have had arguments that \nyou  could  do  y  is  equal  to  w  transpose  x  or  you  could  do  y  is  equal  to  deep  neural \nnetwork  of  x  right   you  are  making  different  modeling  choices  here  and  with  the  hope \nthat one modeling choice is better than the other choice  \nso  just as rnn was one modeling choice now you are using a different modeling choice \nwhere again with the help of these gates and all you can definitely write a function of y is \na function of the input and that function is going to be lstm function  which we will see \nin  detail   so  this   one  part  of  that  function  and  while  doing  this  you  are  just  making  a \nbetter modeling choice  which allows you to learn more parameters and along the way  if \nimportant do selective write  read and forget is that clear  right so   you would see the \n\fdifference  what would have been the ideal case and what is it that you have  the ideal \ncase  would  have  been  explicit  supervision  for  what  to  forget   read  and  write   you  will \nnever have that  but you are still making a modeling choice  which allow you to do that  \nso   if  it  required  to  model  while  back  propagation  should  be  able  to  learn  these \nparameters  so  get you are able to do that  \n\n \ni know i am repeating myself  but it is very important that you understand this situation  \nhow many of you get this now  and as i said these parameters will be learned along with \nother parameters and o t is called the output gate  because it decides what to output to the \nnext cell state  ok  still you see that there is a lot of gap here  we have not reached st yet \nwe are still at st one  we have computed some intermediate value  but we have not reached s \nt  yet and along the way we had three things selective write  read and forget  we have only \ntaking care of selective write so  far ok  \n \n\f\n \nnow let us look at selective read   so  what this selective read  do  you are going to  get \nnew  information  at  time  step  t   which  is  x  t  right  and  now  instead  of  this  original  cell \nstate   you  have  used  the  selectively  written  cell  state  because  that  is  what  you  have \nwritten now  so  that is what you should use  \nnow  using a combination of these two  i am going to compute  some intermediate value  \nok   and  just  stare  at  this  equation   this  equation  form  is  very  similar  to  the  rnn \nequation form  right  only thing is that instead of s t minus one  i am using h t minus one and \nfor good reason because  i know that h t minus one contains only selectively written values \nfrom  ht one   is  that  fine  and  x t  is  the  new  input   still  there  is  some  gap  here   i  have  not \nreached st yet i am still at an intermediate value  so  this is the new input  which i have \nreceived  now what should i do with this new input  selectively read this input i do not \nwant to take all of this input because  may be the input which i have got now is a stop \nword and i do not want to read all of it right  do you get that  \n \n\f\n \nso  now it captures all the information from the previous state as well as the current input \nand  we  want  to  selectively  read  this   so  now   what  would  you  do  to  selectively  read  \nagain the same situation that  you have a \nthe answer is already here  you have s tilde \nand you do not want to pass it on as it is to st  this is \n  st is somewhere here  which you \ndo not know how to get to  but you know that you do not want to pass on all the input \nthat  you have read  you want to selectively pass it on  so  what will you do now  again \nintroduce a  \nstudent  gate  \ngate and this gate will be called  \nstudent  read gate  \ninput gate or the read gate right ok  \nss \n\f\n \nso now  what  can  you  give me an equation for the gate i  t  is  equal  to  sigma of that is \ngood because  sigmoid is what we need it is going to be a fractional thing  let me add \nthe easy part  w into  \nstudent  ht one  \nht one   that  is  telling  you  what  has  happened  so  far  and  u  times  xt   you  see  the  same \nequation  same form the parameters have changed  so  these we will call as wi ui and vi \nand  they  are  depending  on  the  input  as  well  as  the  previous  state   previous  temporary \nstay that we had computed ok  so  that is exactly  what your input gate is going to be and \nnow  this  operation  is  the  selectively  reading  operation   how  many  you  are  fine  at  this \npoint  ok and then this product is going to use to be it is will help us to read selectively \nfrom this temporary value that  we have constructed or the input that we have taken ok  \n \n\f\n \nso  far what do we have  we have the following  we have the previous state  which was s \nt minus one  then we have an output gate  which was o t minus one using these two  we have \ndone  selective  write  right   we  have  taken  the  previous  state  and  the  gate  and  then  a \nselective  write   is  that  fine   ok   we  need  to  check   if  the  sigmoid  should  come  here \nbecause   sigmoid  is  already  there  in  the  computation  of  s  t  minus  one  right   oh  it  is  not \nthere   so   this  already  has  one  sigmoid  right  yeah   so  then   again  a  sigmoid  on  that  is  it \nthere  ok we will figure it out  just check the equation right   \nso  there may or may not be the sigmoid  the sigmoid already applied to s t minus one  but \nwe can figure that out  ok  so  this  is  the selective write portion then   you compute the \ncurrent temporary state ok  and just look at the similarity between these equations  then \nyou have an input gate and using these two  we have done a selective read ok  so  you have \ntaken care of selective write and selective read  but you are still not reached s t  i still do \nnot have an arrow here  i still need to figure out  how to compute the st finally  ok  so  \nwhat is the operation which is remaining now  selective  \nstudent  forget  \nforget ok  so  what do you think should we forget  we want to find new st  so  let us see \nwhat we will forget right  \n  \n\f\n \nso  the question now is that you had this s t minus one and now you have a temporary state \nt  which  is  here   how  do  we  combine  these  two  to  get  the  new  cell  state   ok   so   the \nsimplest  way  would  be   that  you  say  that  st  is  equal  to  whatever  was  there  in  st one  plus \nselectively reading from the current input  is that fine  this is the one way of doing it ok  \nbut now what am i doing here  what is the problem here  i am reading  i am taking st one as \nit is right  so  what should i do  i should forget some parts of st one  so  what should i do \nfor that  introduce a  what gate  \nstudent  forget gate  \nforget gate right  so  we may not want to use st one as it is  but we are want  to forget  so  \nthere is at this point all of you should get some confusion  if you do not then  i would be \nworried  if you are getting some confusion good right  you should all get confused at this \npoint   why  are  you  confused   because  you  already  did  selective  write  and  now  again \nyou are doing a selective forget also right  but there is a difference because  the selective \nwrite  was  then  used  to  compute   how  to  read  the  information  right   but  now  once  you \nhave read the new information   you want  to  see how to  assimilate it back with  the old \ninformation that you had right  so  that is why you introduce a separate gate  so  think of \nit as this way that you are keeping these functions separate input  output and forget  so  \nthey can separately learn things ok  \ns \n\fso  whatever you want to selective write let it be a separate function  these h t minus one \nis not going back to s t right  let us just by use so that  you can compute these temporary \nstates   so   that  is  what  is  being  passed  to  the  next  temporary  state   let  i  t  only  decide \nhow much of this input should be read  ok and then when you want to combine these two  \njust  use  a  separate  gate  and  this  exact  idea   which  is  confusing  all  of  you   why  have  a \nseparate  write  gate  and  a  separate  forget  gate   led  to  something  known  as  gated \nrecurrent units  where they merged these to gates we will get back to that ok   \nso  at this point it is fine  i am just telling you the original equations for lstm and this \nwas the motivation that they had  so  as i said there are at least fifteen to twenty different variants \nof  lstm  which use different  equations   they tie some of these weights   so  one thing \ncould be that forget is the same as one minus remember  right  or output could be same as \none  minus  input  right   you  could  have  tied  these  gates  instead  of  learning  separate \nparameters for that  \n \n \nso  in the most parameterized form you have a separate parameter for all of these ok  so  \nwe introduce the forget gate again  can you tell me the form for this forget gate ft is equal \nto first term  \nstudent  wf  \n \n\fwf  second term uf  what will be there in the second term xt and the first term ok  so  \nthis is what it will look like ok  so  if you remember one of these equations  you will be \nable  to  write  all  of  these   not  that  i  am  going  to  ask  you  to  write  them  in  quiz  or \nsomething  but why take a chance  so  and then once you have completed the forget gate  \ninstead of this equation  can you tell me what is the equation and you are going to use  \nwhat is the first term going to be it is st one here  what is it going to be now   \nstudent  ft into  \nf t into  \nstudent  st one  \nst one that fine ok  \n\n \nso now  we have the full set of equations for lstm  we have certain gates and certain \nstates  what are the gates  output gate  \nstudent  input gate  \ninput gate  \nstudent  forget gate  \n \n\fforget  gate   why  do  you  guys  has  this  momentary  amnesia  like  suddenly  you  forget \neverything ok  so  output gate  input gate and forget gate all of these are the same form \nwith  different  parameters  ok   what  about  the  states   which  are  the  states  that  we  have \ncompleted  one was st the other was ht and the third one was \nt ok t from \nt  we get st \nand from st  we compute ht ok  \n\n \nso  in  the  diagram   that  you  see  here  at  the  top   tell  me  which  are  the  computations  \nwhich are happening at one time step  at time step t  which are the computations  which \nare happening  is it i will give you the options right  is it this or is it this let us call this one  \nlet us call this two or this three or this four  which are the computations happening at one time \nstep  and you see the order also here this should be straight forward right why  \nstudent  \n  \nhow many of you say four  that is the one right because  you start with selective reading \nright and you can just go by this right these are all indexed by t right is that fine ok  so  \nthese  are  the  computations   which  happen  at  time  step  t  and  these  are  exactly  the \ncomputations  which  were  written  right   so   we  have  the  three  gates   which  you  need  to \ncompute  at  every  time  step  and  you  have  the  three  states   which  you  need  to  compute  at \nevery time step  is that fine  and this s t minus one is not being computed  it is just taken \nfrom  the  previous  time  step  is  that  fine   so   you  have  these  six  computations   which \nhappened at every time step and the output final output of an lstm  so  when you use \nsss \n\ftensor flow or something the output of an lstm  would give you two things  it will give \nyou ht  st because  these both the states that are being computed  one is the running state \nand another one is the current state  which is being computed ok  \nand i choose the notation s because  that is what we have been using for rnns  but in \nlstm in all the literature instead of s  you will find it to be ct because it is called the \ncell state  so  that is why st ok  so all these equations wherever  you see an s when you \nare reading some standard blogs or things like that you will see c instead of s  so  you \njust do this mapping in your head ok  \n\n \nso   lstm  actually  has  many  variants  with  include  different  number  of  gates  and  also \ndifferent arrangement of the gates  so  as i was saying that you could say that input is one \nminus output or input is one minus  forget  or things  like that and also  why this particular \nparametric  form  right  why  not  make  w  zero  into  st one  instead  of  ht one  and  so on   so   the  all \npoints  of things that you could do or all of these are valid these are all valid variants of \nlstms  \nso  there is this paper called lstm  a search space odyssey  so  you can go and look at i \nthink  we  link  it  in  the  in  the  reading  material  right  ah   so   you  can  see  that  there  are \nactually  many  variants  of  lstms   but  this  is  the  most  standard  and  default  variant  \nwhich  you  will  find  in  most  platforms  on  tensor  flow  or  pytorch  \n \nform  \n \n\f\n \nand  there  is  another  very  popular  variant  of  lstms   which  is  called  gated  recurrent \nunits   so   we  will  just  see  gated  recurrent  unit   so   i  will  just  give  you  the  full  set  of \nequations for grus  so  you have gates  but unlike lstms  you have only two gates  you \nhave an output gate and you have an input gate  you do not have the forget gate ok  \nso  what am i going to do for the forget gate  so  this is what i am going to do  you see \nthe  last  equation   so   instead  of  forget  gate  i  am  just  saying  that  this  is  what  you  are \ngoing to selectively input from the current temporary state  so  the rest of you rest of it \nyou take from the previous state right  so  i have just tied the input gate and the forget \ngate  any  other  changes  do  you  see  in  this   so   earlier  we  had  s  t  minus  one   everywhere \nright now we have s t minus one itself is that fine ok  so  the basic idea these equations are \nmany and you could think of your own equations  you could say that i will not really use \nthis input information at all or i will choose to use it differently or what not right there \nare  several  things  that  you  could  do   at  a  very  abstract  level  this  is  what  you  need  to  \nwhat is this   \nstudent  \n  \nso  these parameters could then make a difference right  they could adjust it accordingly \nand so on right  so  that is what i was coming  so  the there are various ways of realizing \nthis  right  at  the  abstract  level   you  need  to  understand  that  the  original  problem  was \ntrying to store all the information from time step one to time step t capital t right  which is \n \n\fnot  feasible   because  of  this  finite  size  that  you  have   so   along  the  way  we  built  this \nintuition that it should be good to have these operations  which allow you to selectively \nread write and forget right  how do you mathematically realize these operations  there \nare various choices for doing that and we saw a few choices for doing that right  there are \nmany others you could have done  but this is largely  what whenever you say that i have \nused an lstm  most likely you are using the set of equations which i saw  which we saw \non the previous slide and whenever you are using a gru  these are the set of equations \nthat you will be using ok  \nand again remember this  that there is no explicit supervision here  it is just that we have \na better modeling choice we are just introduce more parameters  so  that if required these \nparameters  could  be  adjusted  to  do  a  selectively  read   write  and  forget  right   so   it  is \noften  it is often valuable  if you are doing some task with rnns or lstms  you should \nvisualize these gates right  you should see that at time step t  if you thought that it should \nhave  forgotten  everything  that  it  has  learnt   so  far   because  suppose  you  had  this  the \nmovie was long  but i really loved it because  the direction was superb and so on  \nnow  this  word   but  actually  changes  everything  right  because   it  whatever  was  written \nbefore  it  does  not  matter  anymore  right   so   is  it  really  learning  those  kind  of  gates  \nwhere  everything  before   but  was  forgotten  right   so   it  would  be  helpful  to  visualize \nthese  output  gates  and  see  what  kind  of  values   they  are  learning   what  kind  of  things \nthey are remembering forgetting and selectively reading and so on right  so  as i said  i \nwill just again summarize the key thing here  is the intuition and then the realization in \nthe form of equations  there are multiple choices  we have seen a few of those right that \nis what i will end with  and  in particular in grus  there is no explicit forget gate and \ninstead of ht one you use st one everywhere   \n\f"}
{"audio_filepath": "lec011_008.wav", "duration": 480.011, "text": "\nso  that was lstm and grus  \n\n \nnow  the issue is that  i have given you a very explanation that why you selectively read \nwrite and forget should work  but  you have not actually formally proven or even given \nan intuition for with these sets of equations  how are we sure that the gradients will flow \nback right  we introduced a bunch of equations  remember in the case of lstms sorry in \nthe case of rnns  the problem was because of the recurrent connections right  because \nyou  had  these  recurrent  connections  this  w  which  was  the  recurrent  parameter  right  \nwhich was connecting cell state st one to cell state st   \nthis was repeatedly appearing in your gradients right and that was causing the problem \nbecause  when you had this multiplicative factor lambda into w and then if you compute \nthe  and  this  was  some  raise  to  t   so  then  if  you  compute  this  magnitude  then  if  the \nmagnitude  of  w  blows  up  then  the  whole  thing  will  explode   if  the  magnitude  of  w \nvanishes then the whole thing will vanish right  that is the problem that we had  \n\f\n \nso  this was because of the recurrent connections  do we have recurrent connections in \nlstms or grus for that matter  do you have recurrent connections  yes or no  \nstudent  yes  \nyes  so then that problem could still occur right  i mean if you had that the crux of the \nproblem  for  the  vanishing  gradient  was  this  recurrent  connection  which  is  getting \nmultiplied   and  hence  reading  to  problem   but  we  still  have  recurrent  connections  the \ncase of lstms also and why should things become any easier in this case  how many if \nyou get the question  how many if you can give me the answer selectively  that is a good \nanswer  so  can you think of what is happening here  so first thing that we going to do \nnow so i will go on to the next module   \n \n\f\n \nwhen i going to give you intuition for what is happening and then we will do slightly  in \nfact   a  rigorous  proof  of  why  it  actually  solve  the  vanishing  and  exploding  gradient \nproblem  ok   so   let  us  look  at  the  intuition  first   how  lstms  avoid  the  problem  of \nvanishing gradients  i am only focusing on vanishing gradients  exploding gradients are \nactually easier to deal with  why  \nstudent  \n  \nwhat can you do  what are we interested in when we compute a gradient  direction  so  \nif  the  magnitude  is  very  large  what  can  we  do   just  normalize  it  and  restricted  to  be  a \ncertain magnitude  so that is known as gradient clipping  so  exploding gradients in that \nsense  is  still  not  a  big  problem   but  vanishing  gradients  is  because   if  it  vanishes  you \ncannot do anything  because you could think of it that you already have a learning rate \nwhich is getting multiplied with the vector the gradient  now in addition to the learning \nrate  which  was  anyways  clipping  the  norm  of  the  gradient  right   so   you  are  doing  an \nexpressive clipping also  \nso  it just like a additional learning rate inductions right ok  \n \n\f\n \nso   here  the  intuition  and  then  will  go  to  the  more  rigorous  stuff   not  in  this  class \nprobably  so  during forward propagation  the gates control the flow of information right  \nthe gate decides how much of st one should be pass to st ok  and they prevent any relevant \ninformation from being returned to the next state  similarly during back propagation  the \ngates will regulate the flow of information  so  what i mean by that is that if at a certain \nstate  you have computed st   f t  st one   it\nt  right  \nso  this gate is  actually  deciding how much information flows  in  the positive direction \nok  and suppose this gate value was zero five  so the zero five of this information from st has been \ncarried on st one ok  now during back propagation what is the derivative of st with respect \nto  st one  going  to  be   partial  derivative   is  going  to  be  a  ft   think  of  st  and  st one  as  single \nvariables  like  you  know  n  dimension  variables   then  the  just  ft   of  course   you  are \nforgetting  that   what  kind  of  a  network  is  this  ordered  network  right   so   you  cannot \nread as till de t as a constant  \nwhere  s  tilde  t  also  somewhere  depends  on  s  t  minus  ok   but  just  this  assume  that  \nmaybe this vanishes and that is the worst case assumption right  because  i do not want it \nto vanish  but i am assuming that the second term vanishes  but even then with the first \nterm  i  will  have  a  gradient  which  is  proportional  to  the  gate   why  is  that  fine   so  \nremember  that  i  am  not  making  a  easy  assumption   i  am  making  a  worst  case \nassumption  this is not favourable to me  i am saying that the second term vanishes and i \ns \n\fdont want it to vanish  but i am just trying to prove that even in  the worst case  by the \nsecond term vanishes  you still have this gradient f t from the first term right  \nand why is that good  why is that ok  because f t decides how much flew in the forward \ndirection  and it is also deciding how much goes back in the backward direction  so  it is \na fair regulator with says that if i passed on only this much information in the forward \ndirection   then  during  backward  pass  also  i  should  only  make  a  responsible  by  this \nmuch  ok  now let us look at a situation where you had f  one  f two  f three upto f t and all of \nthese gates were zero five  now zero five seems a reasonable value  but when we have zero five raise to t \nand t is a large value  what is going to happen  this quantity is going to vanish  \nso   what  is  happening  is  that  sone  contribution  to  st  in  the  forward  direction  itself  had \nsone\u2019s  contribution  to  st  in  the  forward  direction  itself  was  had  already  vanished  right \nbecause  it was continuously getting multiplied by zero five  zero five  zero five  so it is like this chinese \nvespring problems right  so  this guy said something whereas  next guy added noise  the \nnext guy again added noise and so on  till the time it reach the tth guy this information \nwas  completely  lost  so   in  the  forward  pass  if  sone  did  not  contribute  to  s  t  in  the \nbackward pass should i make it responsible for the crimes of s t  no  \nso  what is happening in the backward pass  again the gradients are getting regulated by \nthe same forget  gates  so  again in the backward pass will have a situation that  by the \ntime  the  gradient  reach  is  sone  it  would  be  zero five  raise  to  t  and  that  is  fine   it  is  going  to \nvanish   but  that is  because even in  the forward pass  it vanished  so let it vanish  in  the \nbackward pass also  so  this kind of vanishing is ok  \n\f\n \nso  this is just the same thing written in words  so  if the stated time t minus one did not \ncontribute  much  to  the  state  at  time  t  because   ft  was  tending  to  zero  right   then  during \nbackpropagation the gradients flowing into st one minus one will also vanish because again \nduring backpropagation the gradients will get multiplied by ft and they will vanish   \nbut this kind of vanishing gradient is fine  this is fair because  if we did not contribute in \nthe  forward  direction  why  should  i  help  you  hold  your  responsible  in  the  backward \ndirection  right   so   that  is  fair   so   the  key  difference  from  rnns  is  that  the  flow  of \ngradients is now controlled by gates  which give the same regulation in the forward pass \nas well as the backward pass right  so  only if you contributed to something you will be \nheld responsible  if your contribution vanished your responsibility in the backward pass \nwill also vanish right  so  that is the intuition   \n \n\f\n \nand will next see an proof for this  a proof actually it s as based on the intuition  but i \njust make it more formal in terms of introducing the notations and so on  so that problem \nwe will do it in the next class ok   \n \n\f"}
{"audio_filepath": "lec011_009.wav", "duration": 1422.811, "text": "\n \n \nok  so   will  start  from  where  we  left  off   so   in  the  last  class  we  started  with  this \nmotivation  that  recurrent  neural  networks  have  this  problem  of  vanishing  and  grade \nexploding gradients  and we wanted to arrive with some principle way of avoiding this  \nso   you  have  first  started  with  this  intuition  that  in  many  real  life  situations  like  for \nexample  the human brain or the whiteboard  we tend to these to these three operations \ncalled selective read  selective write  and selective forget  and they essentially help us in \ndealing  with  these  finite  sized  memories  right  or  whether  it  is  a  whiteboard  which  is \nfinite sized or your brain or whatever it is right  \nso  can we is it possible to kind of improve rnn\u2019s which also suffer from this problem \nthat  they  have  this  finite  sized  memory   and  hence  if  you  are  trying  to  capture \neverything from time step one then by the time you reach time step t where say t is thirty or \nforty or so on  its quite natural that whatever you have learned earlier will get move off to \nan extent that it just is not recognisable anymore right  \n\fso  you wanted to deal with this problem and with that we motivated selective read write \nand forget  and then we introduced some equations or converted this into a model and \nthis is the diagram that you see is the model actually that is the lstm cell  yeah  and it \nhas  these  three  gates  output  gate   input  gate   and  forget  gate  and  which  perform  these \nthree functions of selective read write and forget  so  intuitively all these was fine  but \nwe need to be more technical in terms of you trying to deal with a problem of vanishing \nand grade exploding gradients  \nso  how does it solve that problem all that makes or the story seems fine  but how does \nthis  actually  relate  to  the  math   so   we  saw  some  intuition  for  that  and  the  intuition \nhinsed  on  this  observation  that   during  forward  pass  the  gates  control  how  much  of \ninformation passes from one state to another  and in particular if you have the situation \nthat from one time step to another say the forget gate tells you that keep forgetting point \nfive  of  the  previous  state   then  by  the  time  you  reach  say  the  one hundred  state  you  would  have \nforgotten zero five raise to one hundred of the first state  \nso  that means  even during forward pass the information from state one vanishes  so  if it \nvanishes during backward path that is also fine  because state one did not contribute to state \none hundred  and that was the intuition that all this hinsed on  now we are not going to do much \ndifferent from this intuition  we just going to see the corresponding equations for these \nintuitions and just make a more i would not call it rigorous but more mathematical proof \non why lstm solve the problem of vanishing gradients  \nand we are also sure that they actually do not solve the problem of exploding gradients \nand then we will see a simple trick of dealing with exploding gradient  that is what we \nwill  do  in  the  remainder  of  this  particular  lecture  and  then  will  move  on  to  the  next \nlecture in this lecture  \n\f\n \nso  we will now see an illustrative proof of how the gates control the flow of gradients \nright  \n\n \nso  we  call that this  is  the control this is  the  flow diagram  or the dependency diagram \nthat  you  had  for  rnn\u2019s   and  in  particular  because  you  are  dealing  with  an  ordered \nnetwork we add this explicit and implicit derivatives and finally  you came up with this \nmultiplicative form  and this term here is actually a matrix because it is a derivative of a \nvector with respect to a vector  \n \n \n\fand  then  this  same  matrix  was  getting  multiple  times  and  then  we  did  this  proof  it \nshowed  that  this  term  is  actually  lambda  gamma  ok   it  is  actually  proportional  to  this \nterm right and as if lambda into gamma is greater than one  then this will explode  if it is \nless than one then it will vanish given sufficient times that is  \n\n \nnow  in particular what is happening here is the following that you have this loss at time \nstep t you have the time step is four  now  what if this loss or this error occurred  because \nw was not  good enough to compute a good value for sone right  so  w was at  a certain \nconfiguration based on that you computed sone  and that sone was not good enough which \neventually led to the error at time step four all of you if you can imagine this situation that \nyou mean  you not being not be able to do something well at sone  now this needs to be \ntold to w so that it can improve right  and that information has to come through sone  that \ninformation  is  already  going  from  here   but  this  information  is  about  how  badly  it \nperformed in computing sfour  \nthis is not how badly it can perform in computing sone  so  that information has to travel \nto w all the way through sone and that was not happening because this path do not look at \nthe bullets this path was actually vanishing  and that is what this multiplicative term says \nthat as the number of times that increased that time that path would vanish ok  so  that is \nthe actual problem that we are trying to deal  \n \n\f\n \nso  now what is the general situation here right  the general principle is that the gradient \nof l theta at particular time step say here we are considering lfour so i will just call it lt \nwith respect to any parameter theta i  the parameters at w u v b and c with respect to \nany parameter it would vanish if all the paths leading to that parameter if it vanishes  so  \nwith respect to this particular path so that is the only path which leads to w through sone  \nif  there  were  multiple  paths  if  there  was  say  one  such  direct  path  right  if  we  had  you \nsome  other  kind  of  connection  which  gave  us  this  direct  path  then  it  would  still  have \nbeen fine  \nbut there was only one path leading to w through sone at the gradient vanishes along that \npath  then  the  gradient  will  vanish  ok   if  there  were  multiple  paths  then  only  if  the \ngradient varnishes across all the paths then the gradient would vanish  is it fine  what is \nthe corresponding  rule for exploding gradients  if there are multiple paths  the gradient \nwould explode if  \nstudent  \n  \nif it vanishes through any o ne   if it explodes though any one of the paths ok  \n \n\f\n \n  \nso  these are the two things that we need to consider ok  so  to prove that in the case of l \nst m this does not happen  for the first case will have to show that there are at least one \npath through which it does not vanish and for the second case because we are going to \nshow that it explodes we just have to show that there is at least one path through which it \ncan explode ok  so  these are the two things that we need to prove and the first thing that \nwe are going to focus on  is the vanishing gradient problem  \n\n \n \n\fso   will  start  with  the  dependency  graph  for  lstm\u2019s   that  means   i  want  to  draw \nsomething  similar  for  lstm\u2019s  involving  all  the  different  elements  in  lstm   so   what \nare these different elements the two rhyming things one being gates states ok  so  gates \nand states that the two things that we care about  so  let us look at all these  so  starting \nwith states at time step k   one  at time step k  one you have this two states sk  one and hk one ok  \nusing hk one you are going to compute the output gate at time step k  and it is also depends \non these parameters wo  uo and bo right which is obvious from the equation  \njust  to  make  sure  that  this  diagram  remains  tractable   i  am  going  to  get  rid  of  the \nparameters and i will come back to them later  so  right now will just focus on the states \nand  the  gates  ok  and  then  you  have  these  other  intermediate  states  and  the  other  gates \nright  so  you had fk  you had ik  so  add these three gates the temporary state and then \nwhat else what are the other two things at time step k  so  we saw this diagram about all \nthe computations which  happen at time step k right   how many  computations  happen  \nthree states and three gates right  \nso   you seen the three  gates and this one temporary  state  so  which are  the other two \nthings  there is no selective forget with you guys is early everything forget  hint look at \nthe grey cells and change the time step  what will you get away are you all i mean we \ndid  lstm\u2019s two days right  i  mean are  you all  with  that or should  i we need to  revise \nsomething  mean i do not need to revise it  but we going to is it fine ok  so  sk and the \nother thing hk remember that sk also depends on hk just stare at this for thirty seconds and \nmake  sure  that  you  are  with  it  right   all  the  equations  are  there  these  are  the  see  six \nequations that or the six computations which happen at time step k  \nthere  are  three  gates  and  three  states  and  the  dependency  graph  is  obvious  from  these \nequations  except for the fact that i have ignored the parameters  how many if  you are \ncomfortable  with  the  equations  and  the  graph  corresponding  graph  please  raise  your \nhands high  so  i think it should be right we have these six equations and we have this \ndependency graph  \n\f\n \nnow   starting  so   what  happened  in  the  graph  is   we  started  from  sk one  and  hk one  and  we \nreached sk and hk which were the outputs at the next state  now what will happen from \nhere  we were looking at recurrent neural networks recursion is the answer  what will \nhappen now  \nthe  same  graph  will  keep  recusing  right  for  the  next  time  step  and  up  to  the  last  time \nstep  right   does  that  makes  sense  ok   this  looks  much  more  complicated  than  the \ndependency graph that we had for rnn\u2019s right by just because there are so  many we in \nrnn we just had this one state and no gates so here but we have these three states and \nthree gates that is why this so many paths ok  now for simplicity what i will do is  i will \nnot  draw separate nodes for the parameters all the in  the case of the rnn dependency \ngraph i had drawn them separately  what i am going to do is  i am just going to put the \nparameters on the corresponding edges right  \nso  fk actually depends on wf  it also depends on uf  and it also depends on that bias  but \ni am just going to take a small set of parameter i am only going to focus on the w\u2019s not \nthe  u\u2019s  and  the  biases  ok   there  is  only  for  illustration  for  no  other  reason  right  and \nwhatever arguments or proof that we are going to see it holds for all the parameters  but \nwe  just  need  to  prove  it  with  respect  to  one  parameter  and  the  same  story  repeats  for \neverything ok  so  this is the dependency graph and these are the parameters  now what i \n \n\fam interested in knowing is that  there was some loss at time step t and maybe that loss \nhappened because wf was not good enough to compute sk  \nof course wf computes fk and then fk helps in computing sk  but maybe wf was not i am \njust  short  it  short  circuiting  it  and  saying  that  wf  was  not  good  enough  to  compute  sk \nright   and that  is  why  i  want  the  gradient to  reach to  wf through this sk that is  what  i \nwant do ok  \n\n \nand  this  exactly  what  i  said  i  am  interested  in  knowing  that  if  this  loss  can  reach  wf \nthrough  sk  right   so   all  the  three  highlighted  things  that  what  i  am  interested  in   i  am \ninterested in the path to wf through sk  of course  there are many other paths to wf  but \nthey do not account for the problem in sk  is that fine everyone is clear the setup ok  \nnow and we can ask similar questions about all the other parameters the w\u2019s the us the \nthe input gate parameters the output gate parameters and so on right  there is nothing so \nspecial  about  wf  the  same  question  holds  for  all  these  other  parameters  also  ok   now \nhow does lstm ensure that this does not vanish  so let us see that  \n \n\f\n \nas i argued earlier it is sufficient to show that this gradient does not vanish ok  if i can \nshow  that  this  gradient  does  not  vanish   then  i  am  pity  sure  there  is  only  there  is  no \nrecursive  connection  here  because  it  just  a  single  connection   so   there  is  no  recursive \nconnection here  so if i can show that the gradient reaches up to this point  then after that \ni can be sure that it is going to reach wf everyone buys that set up right that is what  i \nneed to show  \nso  to prove that the gradient reaches wf i just need to show that it reaches sk that is the \nonly thing that i need to show  and the first thing i am going to observe is that there are \nmultiple  paths  to  reach  to  sk  which  are  these  paths   one  through  sk one   because  sk \ncontributes to sk one the other through  \nstudent  hk  \nhk which is visible  but now also notice that how many paths are there to reach hk itself   \nfour  not four actually that is going to be combinatorial because there four outgoing edges \nfrom  here   but  then  again  there  will  be  four  next  stage  and  four  next  stage  and  so  on \nright   so   let  us  not  count  the  number  of  paths   but  let  us  just  convince  ourselves  that \nthere are many paths to reach to sk from lt \n   everyone is convinced about that we are \nnot counting the exact number of paths that is not very hard to do  but all we are saying \nis  that  we  know  that  there  is  one  path  through  sk one   one  path  through  hk  and  hk  itself \nseems to have many incoming path during back propagation  \n\uf071 \n\fso   there  are  many  paths  which  are  reaching  from  lt \n   to  sk   everyone  is  convinced \nabout that anyone who has a problem with that  now to show that the gradient does not \nvanish what do i need to show of all the paths the set there exist at least one path through \nwhich the gradient can flow that is what i need to show ok  even if i vanishes across all \nthe other paths i am still fine with it ok  \n\n \nso  now consider one such path which is this highlighted path that is a valid path to reach \nto sk  now let us denote the gradient along this path to be t naught and the total gradient \nis going to be a sum of many such paths right  so  i am calling this path as t naught and \nthis is what the gradient look like ok  so  this is simple just this red path the next red path \nand  then  the  series  of  problematic  multiplications  right  you  have  this  recursive \nmultiplications  again   so   everyone  agrees  that  red  is  good   the  red  path  there  is  no \nrecursion the gradient will flow right we just need to focus on the blue path everyone is \nconvinced about that right ok  \nso  that is good the first term is fine as i said because it directly connected to l t there is \nno recursive or no other intermediate nodes  so  the gradient will just flow through that \nthere is not a problem there and now we look at the other terms which is first is dht   st \nand the other is this ok  \n\uf071 \n\f\n \nso  let us look at  ht   st what is this going to be  tensor  vector  matrix scalar at this \npoint in the course i want a unanimous answer  \nstudent  matrix \nmatrix right and recall that in particular the equation was of this form ok  so  what is the \nderivative  going  to  look  like  even  without  computing  can  you  tell  me  something \nprofound  about  it   it  will  be  a  dash  matrix   big  matrix   how  many  if  you  say  diagonal \nmatrix   how  many  if  you  do  not  think  it  is  a  diagonal  matrix  please  raise  your  hands \ntotal sum is never one  so  remember that ht is equal to htone  httwo up to htd  and you have ot \nequal to otone ottwo otd and st equal to stone sttwo std  so  httwo depends only on ottwo and sttwo right it does \nnot depend on in particular does not depend on any of the other st\u2019s  \nso  we have already seen this before in such cases whats the i jth entry of this matrix of \nthe gradient matrix  derivative of hti with respect to stj which of these terms are going to \nbe zero wherever  \nstudent  i not equal to zero  \ni is not equal to zero  that means  it results in a  \nstudent  diagonal matrix \ndiagonal matrix  \n\uf0b6 \n\f\n \nso  that is exactly what is written here and the diagonal elements are going to be this  is \nthat fine  everyone with this  ok  so  now  this diagonal matrix which contains this on \nthe diagonal  i am going to represent it by the following  notation   is that ok   fine  so  \nthis  is  a  diagonal  matrix  where  every  element  is   i  mean  this  is  actually  a  vector  right \neveryone agrees this is a vector  so  this diagonal is this vector is along the diagonal of \nthis  matrix   how  many  if  you  get  this  notation   if  you  do  not  get  this  you  will  not \nunderstand anything else  \n\n \n \n \n\fnow let us consider \nst   \nst one  ok  this is what st is equal to  so  what is the derivative \nof \nst   \nst one   ft right ft right what else  why no  why are you rebelling  what the i mean \nst  only  right   if  it  is   can  you  treat  this  as  a  constant   no  why   because  this  is  a  dash \nnetwork  \nstudent  \n  \nso in an ordered network the derivative will have  \ntwo terms which are those  \nstudent  explicit  \nexplicit  and  implicit  in  the  explicit  term  what  you  assume   the  other  terms  to  be  a \nconstant right fine  so  st i mean s tilde t also depends on st one  so  we cannot treated as a \nconstant so once again this derivative is going to contain an explicit term and an implicit \nterm  now i am going to make a worst case assumption  i making this assumption that \nactually the implicit term vanishes  notice that this not favourable to me i am trying to \nprove that the gradient does not vanish the gradient is a sum of two terms i am saying it \nlet the worst case be that one of these terms vanishes ok  \nso   this  is  not  a  favourable  assumption  this  is  a  unfavourable  assumption  which  i  am \nmaking  so  let us fine  so  i making the assumption that the implicit term vanishes  so  \nwhat is the explicit term actually  \nstudent  ft  \nft and what kind of a matrix is that  \nstudent  diagonal matrix  \nif  you agree that it is a matrix first of all  it is a diagonal matrix again and what is the \ndiagonal  \nstudent  ft  \nft right  so  i am going to represent it as d of ft  is that fine   \n\uf0b6\uf0b6\uf0b6\uf0b6\f\n \nso  remember that the original equation  had three terms all of these the last blue once \nfor all identical  so  this is not problematic because this is a directly the last layer  this \nwe have already derived a form this is sum diagonal  and now for each of these we have \na form  do you get that these are the three paths that we have done so far  so  let me just \nsubstitute them  this is what it looks like ok  now this is a product of diagonal matrices \nwhat will the product look like  \nstudent  diagonal matrix  \na diagonal matrix and each element would be each element on the diagonal would be a  \nstudent  product \na product of all those things right  so  is it fair if i write it as this right  which i can write \nit  as  this  ok   now  just  stare  at  this  equation  and  tie  it  back  to  the  intuition  that  we \ndeveloped  something  about  the  gates  regulating  the  flow  of  information   you  have  a \nmultiplicative  term  here  right   whenever  there  is  a  multiplicative  term  we  have  a \nproblem  because remember these gates are between zero to one  \nso  there is a chance of vanishing  everyone sees that  you are multiplying t terms all of \nwhich are between zero to one  so  there is a chance of vanishing  but i am going to end this \nproof by saying that the gradient does not vanish  so  by what am i going to do now  ok  \n \n\fmake the statement the gradient could vanish  but this kind of vanishing is fair  what do \nyou mean by that  now when will the gradient vanish  \nstudent  product \nat this product of the forget gates vanishes  but if the product of the forget gate vanishes  \nthat means  what would have happened during the forward pass that information was not \ncarried all the way back  all the way front two times step t right do you see that ok  so  \nthat  is  the  main  reason  here  right   so   the  red  term  does  not  vanish   the  red  term  time \nzone vanish the blue term can vanish  but it will vanish only if during the forward pass \nalso this multiplicative term at cause the information to vanish by the time you are reach \nthe  time  step  t   how  many  if  you  get  this   and  this  exactly  what  i  meant  earlier  by \nsaying that  \n\n \nif during the forward pass st did not contribute much to st one  because the forget gate was \ntending to zero  then during backward pass there is no need to pass this information back to \nst  right  because  during  forward  pass  you  did  not  contribute   so   during  backward  pass \nwhy should i hold you responsible right  and this is absolutely fine to do this and this is \nexactly what the equation tells us that they gauze the gradient will vanish only if things \nvanished  in  the  forward  pass  ok   and  the  gates  are  doing  the  same  regulation  in  the \nforward pass as they will do in the backward pass so everything is fair  is that ok  \n \n\fand does there exist one path along which the gradients will not vanish when they do not \nneed  to  vanish   so   if  during  forward  pass  all  the  gates  were  on   that  means   the \ninformation  from  state  one  was  actually  carried  all  the  way  up  to  state  t  then  during \nbackward pass what will happen  the information will go all the way back right is that \nfine  so  the gradients flow back only when required as regulated by the forget gates and \nthis  is  fair  because  if  you  are  regulating  the  same  thing  in  the  forward  as  well  as  the \nbackward direction then you are not doing anything wrong ok  \n\n \nnow that is a proof for lstm solve the vanishing gradient problems  or in other words \nthe  gradients  vanish  only  when  required  and  not  unnecessarily  or  arbitrary  as  is  to \nhappen  in  the  case  rnn\u2019s   now  we  will  show  there  exist  one  path  along  which  the \ngradients can explode right  so  let us show that path  so  consider this path  ok  a this \npath is again also active so i if we consider the path to hk there is going to be active for \nall the gates and all the states right  so  in whatever gates or states you are considering \nthis paths would be there  \nand this is what this path looks like you have the derivative with respect to the last layer \nand then you have these guys ok these pairs ht by ot  ot by ht oneand so on  everyone fine \nwith this so far  what is the derivative of ht with respect to ot  we do not remember the \nequations so i will just tell you directly  so  based on whatever we have done so  far just \ntrust me that this is what each of the terms in the bracket looks like  we can go back and \n \n\fcheck this  this is just comes directly from the equations  now what is happening here \ndoes this look very similar to the situation that we had with rnn\u2019s  \nwe  had  a  diagonal  matrix  and  a  weight  matrix  and  a  repeated  multiplication  of  these \nright  and again the diagonal matrix is bounded the weight matrix is bounded  so  now  \nthe repeated multiplication could explode is that fine  so  it does not solve the problem of \nexploding gradients  but it solves the problem of vanishing gradients  but now still this is \nbad  for  us  right  whether  the  gradients  explode  or  vanish  our  training  is  going  to  get \nmessed up  so  how do we deal with this for exploding gradients what will you do  \nstudent  \n clipping  \nwe will do  \nstudent  clipping  \nclipping right  \n\n \nso  in  practice  the  way  of  dealing  with  this  is  gradient  clipping   if  the  norm  of  the \ngradient exceeds the certain value  then we are going to just clip it to a certain threshold  \nand this is fine because we care about the gradients only for the direction and not for the \nmagnitude  anyways when we introduce a learning rate  we are doing some kind of scale \n \n\fdown for the gradient magnitude  so  this is  just being more explicit and being careful \nthat if the gradients are non manageable in terms of their magnitude  \nthen we just  going  to  keep them to some manageable value  while being  faithful to  the \ndirection  and the direction is what  matters  so that is why exploding gradients is easy  \nbut in the case of vanishing gradients you do not have direction also because the entire \ngradient  becomes  zero   so  there  is  no  direction  there  right   so   that  is  why  vanishing \ngradients is more serious than exploding gradients  and as long as lstm solve that they \nare fine with it is that fine ok  so  that is the end of this lecture  \n\f"}
{"audio_filepath": "lec012_001.wav", "duration": 1304.888, "text": "\n\n\nand in this lecture  we are going to talk about  encoder decoder models and  attention \nmechanism  so  this is a very interesting lecture at least interesting to me because  this is \nvery  put  all  these  pieces  that  we  have  learnt  so  far  right   we  have  learnt  three  types  of \nnetworks   feed forward  networks   recurrent  neural  networks  and  convolutional  neural \nnetworks and  we have seen independent  applications  of each of these word to  vec and \nimage classification and so  on  now today  what  we are  going to  see is  how do we do \ndifferent combinations of these networks and come up with a wide range of applications \nlike apply them to a wide range of applications ok  so  let me start by an introduction to \nencoder decoder models and then we do various applications of encoder decoder models  \n\n \n\fso   what  we  are  going  to  do  is  we  will  start  by  revisiting  the  problem  of  language \nmodeling  so  the problem of language modeling was that you are given some t   one words \nor  characters  and  you  want  to  predict  the  tth  word  or  character  right   this  is  like  auto \ncomplete in short right  whenever we are typing something  you have type four words  you \nwant to predict the fifth word  or  you have typed four characters and  you want to predict \nthe fifth character ok   \nso  more formerly this is what we are interested in  how many of you get this equation \nthis  expression   so   we  are  given  a  sequence  of  t     one  words  and  you  want  to  find  out  \nwhat  the  value  of  y  would  be  at  time  step  t  and  we  want  to  find  out  that  value  which \nmaximizes this property  that is what this argmax equation means and now we will try \nto see  how to model this using a rnn   \nso  let us see  we  are  going to  start with  go that  is  that  we want to  start  generating a \nsentence and then we will produce the first word which is i  ok  and what is it that we are \npredicting  at  this  point   what  is  the  network  supposed  to  predict   what  is  the  output \nsupposed to predict actually  \nit  is  supposed  to  predict  a  dash  over  the  vocabulary   a  broadly  distribution  over  the \nvocabulary right  so  this is what is happening  we will of course come back to this on \nthe next few slides  but you have say words wone  wtwo up to w v in your vocabulary  at \nevery time step  you want to find a distribution over these words and then pick the word  \nwhich  had  the  maximum  probability  at  that  time  step  right   that  is  exactly   what  this \nquantity is that is what we want the rnn in to model and then we want to keep doing \nthis till  we reach the end of the sentence ok  so  that is the language modeling problem \nand as we had made a case for it earlier  the word produce the time step t depends on a \nfew previous words  how does a recurrent neural network ensure that  at any time step  \ni am going to give it only one word as the input   \nso  how does that  ensures that it depends on all the previous  words also  through the \nrecurrent connections and the gate and sorry it is not the gate the state st  ok  fine  \n\f\n \nso  we will see this of course  in more detail and we will write down the model equations \nand what is happening  so  we are interested in this quantity  which is the probability of \nthe word at the time at the t th time step  where this j belongs to vocabulary v and see a \nvocabulary of tenk words or twentyk words for english it is actually much higher  but say you \nare considering only tenk to twentyk words  then we want to predict a distribution over this \nvocabulary   so   using  an  rnn  what  are  you  going  to  do  at  the  output  layer  is  the \nfollowing  is this correct  how many if  you understand this equation  not many why  \nwhat does this equation compute  first of all softmax  softmax means  \nstudent  probability distribution  \nprobability  distribution   ok   what  does  it  take  as  input  at  every  time  step   the  state \nright  what does it do with the state  a linear transformation right and then a bias ok  so  \nwhat is this quantity  scalar vector matrix  vector of size  \nstudents  \n  \nthe \n  what is the gth element of the that  \nstudents  probability of the gth word  \n \n \n\fthe  probability  of  the  gth  word  right   so   i  just  have  to  explain  it  in  that  many  words  \neveryone gets it  now  everyone gets it  if you do not get it  you will not understand the \nrest of the lecture  i am very serious everyone gets it   \nso in other words  what we do this entire yone to yt one  which we were conditioning on we \nare just using st as a surrogate for that and that is fair because  st has actually captured all \nthe  previous  information  that  we  had   now  just  using  st  as  a  state  which  captures \neverything that happened so far  \nso  that is actually how we are modeling this and the recurrent connections ensures that \nst captures everything which has happened so far  \n\n \n so now  let us look at the five things that we have in a typical supervised machine learning \nset up which are those  data  model  \nstudents  parameters  \nparameters  \nstudents  objective function  \nobjective function  \nstudent  cross \n  \n \n\fvery  good   no  someone  said  objective  function  and  then  loss  function   learning \nalgorithm right ok  so  whats the model here  \nstudents  \n  \nyou  know   what  you  are  trying  to  model  which  is  a  property  distribution   what  is  the \nactual  so  here y is the probability distribution and your x is the input given to you  can \nyou tell me what\u2019s and we have already set always said in this course that  whatever be \nthe y whatever the be the x  we are interested in this function x sorry function f and we \nshould be actually expressively  we able to write this function  so  what is the function \nhere  can you actually write down the set of equation  just think of what the output is  \nhow you are going to reach the output given this network  what is yt going to be  what\u2019s \nthe equation for yt   \nand  then  try  to  go  back  all  the  way  back  to  xt   so   yt  depends  on  something  that \nsomething might depend on xt  so  how do you go all the way back  right  that is the \nthing   which  i  expect  you  to  do   ok   how  many  of  you  get  it  now   please  raise  your \nhands ok  so  let us see  at every time step  what am i interested in predicting  \nstudents  probability distribution  \na probability distribution  that means  i will have to compute which function  \nstudents  softmax  \nsoftmax  so  the green vector is what i am going to focus on  so  what\u2019s the equation for \nthe green vector  is this fine  now what does this contain apart from the parameters st  \nhow  do  i  get  st   is  it  fine   you  can  write   now  you  have  written  this  output  y  as  a \nfunction of x because  x appears here or other  yt as a function of xt is not it is straight \nforward  right  once   i  show  you  the  answer  is  should  be  how  many  of  you  get  it  now  \nplease raise your hands high up above  okay  what are the parameters  b and c right  \nso   these  are  the  parameters   what\u2019s  the  objective  function   cross  entropy  or  dash  of \ncross entropies  \nstudents  sum of cross entropies  \n\fsum of cross entropies right  so  the loss is going to be over all the time steps  at every \ntime  step  is  the  cross  entropy  loss  right   everyone  gets  this   ok   what\u2019s  the  learning \nalgorithm  back propagation  \nstudents  true time  \ntrue time  fine  so  that is what it is going to be right everyone is clear  so  you can see \nthat we have written the final output as a function of the input right and this is end to end \ntrainable   that  means   the  gradients  can  flow  modulo  this  vanishing  exploding  radiant \nproblem and we have a way of handling that  we can replace rns by lstms that is all \nright   so  that is what it is now  this  just make sure  you understand this properly  so  \nthat we are going to do various instantiations of this for different problems ok  \n\n \nnow here is one question  we all smartly wrote this xt  but why is the input at every time \nstep  when i am predicting home  the input was at  but how did i get at  that is what i \ndash at the previous time step  predicted at the previous time step right  \nso  this is what the input looks like  so  at time step one i predicted i as the output at the \nnext time step  i am going to feed that has the input  does it make sense  so  just see if \nyou are doing auto complete  you would select that i am fine with the word i at this time \nstep  so  it is going to take that as the input and then try to predict the next word that is \n \n\fwhat exactly is happening here and now you are predicted am at the next time step you \nare going to feed am as the input and continue this chain throughout ok  \nso   the  input  at  every  time  step  is  going  to  be  the  word  that  you  have  predicted  the \nprevious  time  step  and  i  am  just  going  to  represent  it  by  a  one  hot  vector  right   it  is  the \nindex of the jth word only that could be hot everything else would be zero and all of you are \nfine with this  no so at training time this is the inference time  at training time we will \nhave the real  inputs  no   that is  at  inference time  at  training time   we  will just use that \nthrough  because   training  time  you  know  what  the  inputs  are  right   you  know  the  true \nsentence you have the wikipedia sentence right and you know what the true sentence is \ngoing to be  \nwhat i am talking about  how will you generated at test time  at training time  you know \nall these things right  no about training time  how will you do that  you will know what \nthe next input is right  so now  ok  so  i said that the input is going to be a one hot vector  \nis everyone fine with that  one hot vectors are ok  what else could you use  \nstudents  word representation  \nthe word representation for that right  so  assume that you have already done the word \nto  vec  assignment  and  you  have  completed  all  the  word  representations  and  you  have \nthem with you now  but instead of feeding the one hot representation of the input  you \ncan just feed the word representation of it does that make sense  one hot representation \nis just one of the many representations possible for the world  so  why just do that  you \ncould  do  s  v  d   you  could  do  one  word  vec  or  whatever  you  want  right   so   that  is  in \npractice  what we will feed is the word to vec representation  \nso  everyone gets this what is happening at every time step   \n\f\n \nnow  one more thing  that you need to notice that szero  which is the input at time step one  \nthe  previous   so   sone one   so   that  we  do  not  know  what  it  is   so   we  just  keep  it  as  a \nparameter  we say that szero is also weight vector and you are going to learn it  along with \nall the other parameters in the network  does not make sense  because you do not know  \nwhat  szero means is a semantics of it is  not clear like  what was  generated  at  the zeroth time \nstep   we  do  not  really  know  right   so   will  just  make  it  a  learnable  parameter  and  that \nwould be trained along with all the other parameters of the network  \n\n \n \n \n\fso  before we move on what we are going to do is  we are going to see a very compact \nrepresentations  for  rnns   grus  and  lstms   so   remember  rnn  is  the  following \nequation rnn is defined by the following equation  the st is a recursive function of st one \nand xt right  so  i am just going to write it as that st is equal to rnn of st one  xt  instead of \nwriting  all  these  parameters  and  sigma\u2019s  and  all  that   i  am  just  going  to  write  it \ncompactly as this  now this is what  what is this  gru  so  how may going to write it as  \nstudents  gru  \ngru of  \nstudents  st one xt  \nst one  xt  \nwhat is this  \nstudents  lstm  \nlstm  how may going to write it  lstm of when the output of the lstm is both ht one \nand st oneright fine  so  in some sometimes i will just say st  sometimes i will say both ht \nminus ht and st as per whatever i needed right  so  this is i am not going to write these \nequations  and  parameters  again   i  will  just  say  that  lstm  of  this   assume  that  is  a \nfunction which does this calculation and gives you back ok  ok  \n\n \n \n\fso  far what  you have done is  we have seen  how to  model the conditional probability \ndistribution  given  the  previous  t  minus  one  words   now   let  me  give  you  a  different \napplication right  what if we want to generate a sentence given an image  so  this is what \ni am interested in doing  i am giving an image and i want to generate a sentence  can we \njust  think  of  it  formally   what  is  it  that  you  want  to  do   so   we  saw  that  in  this  case \nformally  we were interested in this conditional distribution  in this case  what is it that \nwe are formally interest in  \nif i were to write it as something formal  what would i write it as  ok i will give you a \nhint   what  kind  of  a  distribution  is  this   a  conditional  distribution  right   given  the \nprevious  sequences  previous  sequence  of  words  generate  the  tth  word   now  in  this \nsituation can you stated an similar words  given the  \nstudents  image  \nimage generate the  \nstudents  sentence  \nsentence or given the image and the description that are generated so far  because i am \ngoing to write the description  one word at a time given  the image and the description that \nhave written so far  generate the next word in the mission  so  what kind of conditional \ndistribution is that  p\n given  \nstudents  y one to t minus one  \nyone to t minus one  \nstudents  comma  \ncomma  \nstudents  image  \nimage  does that make sense  everyone gets that  ok  so what  so  this is what we want \nright  so here  now we are interested in this quantity as a post to this quantity  does that \nmake sense  ok and this is again a conditional distribution   \n\f\n \nso earlier  how did we model this  we just modeled it as the following we said that the \nwhole context of yone to yt minus one is just contained in that blue vector  which is st right  \nso  remove this variable and replace it by a vector does that make sense  ok  \nnow you have the image also  so  how are  you going to model this  so  what are  you \ngoing to write on the right hand side  ok let me give you a hint  we all agreed that this is \nthe  quantity  that   we  are  interested  in  right   we  also  agreed  that  the  following  is  fine \nreplacing  yone  to  t     one  by  st  is  fine   now  what  about  the  image   what  do  you  mean  by \nobjection in the image  you will supply the words  which are there the object names that \nman fine that is fair enough well  if want to make it more abstract more neural  so  what \nyou are saying is that whatever information is contained in the image should be passed \nhere  \nwhatever  information  is  contained  in  the  image  should  be  passed  here   how  do  you \nwhat\u2019s the way  that  you have learnt of computing the information in  the  image a dash \nneural network  \nstudents  \n  \na  \nstudents  convolutional neural network  \n \n\ffeedforward neural network  \nstudents  convolutional neural network  \nconvolutional  neural  network  ok   so   but  what  from  a  convolutional  neural  network  \nhow many representations that is a convolutional neural network learn  how many does \nv g g network learn  v g g sixteen  the last layer is a softmax layer fifteen right  so  which one \nwill you give now  one before the last one  that is called the  \nstudents  \n  \ndash layer   dash dash layer  fully dash layer  \nstudents  fully connected layer  \nfully connected layer ok  at least your language moral works fine ok  so  that is the fully \nconnected layer  remember that all the layers in the convolutional neural network  learn \nan  abstract  representation  of  the  image  and  as  she  was  trying  to  say   that  this  abstract \nrepresentation contains or at least you believe it contains all the information that is there \nin the original image  just as st contains all the information that was there in the sequence \nyone to t minus one  this abstract representation that we will get from a cnn  contains all the \nrepresentation all the information  that is there in the image we all believe that ok  \nand we also believe that any of these representation is fine  in practice  the convention is \nto use the fully connected layer that is called as f c seven  the seventh fully connected layer \nright and it is seven  because you also start the numbering from the convolution layer one  two  three  \nfour  five and then the seventh layer ok  so  that is what you will take ok  so  does this make \nsense and it is a very simple extension from what we were doing earlier  this is what  i \nhave circles is what we were doing earlier right  where we only had st  now  i am saying \nis just as you believe that st and codes all the information in the previous sequence  \n i am just asking you to stretch that a bit more and say that f c seven of the image contains all \nthe information that was there in the image is it fine   \n\f\n \nok  but still there are some issues and there are other ways of making this condition on f \nc  seven   in  particular  what  you  could  have  done  as  she  was  trying  to  suggest  initial  is  that \nmaybe you have a vocabulary of all the objects that are possible in your image right  so  \nmaybe in  your image there is man  woman there is flying desk  frisbee or there is dog  \ncat and all these things right  \nso  you do an object detection first  get out all the object which are there and then make \nthe distribution conditional on these objects right  so  you can say that i will allow for a \nten  words  to  describe  the  image   so   there  word  one  is  equal  to  man   because  i  have \ndetected the object man in the image word two is equal to frisbee  because i have detected \nthe object  frisbee  in the image that is all that is another way of doing it ok  so  i just \nwant to make it clear  that there are different ways of making the conditional distribution  \nconditional on the image itself  we are choosing to make it conditional on f c seven of i right \nthat is the neural way of doing it ok  \n \n\f\n \nso let us see  two such options  the first thing that we could do is we could set szero to fcseven of \nthe  image   what  is  s  zero   the  first  thing  that  was  passed  to  the  language  model  ok   so \nremember  we had this go symbol and we had this s zero  which was mysterious we did not \nknow how it comes  but now we know it that s zero could just be the image that is what my \nstarting  point  is   so   take  this  image  and  now  start  generating  the  representation \ngenerating  a  description  does  that  make  sense  ok   so   this  is  what  the  network  looks \nlike  \nso  what do you saying is that these things are of dimension d  the cns output was say \nof dimension four thousand and ninety six  so  this has to be converted to size d right  that means  what will you  \nhow will you do that  we have a four thousand and ninety six dimensional vector and you want to convert it to a \nd dimensional vector w belonging to  \nstudents  \n  \nfour thousand and ninety six was d  fine in general  any two vectors if you want to make them compatible  this is \nwhat you will do you will project  so  that they are of the same dimensions  x zero will be \nthe go symbol  go is the special word in your vocabulary  which says star generating the \nsentence  right   so   whatever  vocabularies  you  will  add  two special  words  right  one  is  go \nand  other  is  stop   so   whenever  you  generate  stop   you  stop  generating  after  that  fine  \nwhat  is the other way of  so  here now what  happens is   so  this is what  is  happening \ntechnically and that is why that is what i wanted you to understand this now s one depends \n \n\fon szero ok  what we are interested in is the following that yt should be conditional on yone to \nt minus one comma image ok  \nthey have make sure that i is s zero and this quantity is st  you have to find now since the \nfirst  time  step  depended  on  the  image   all  subsequent  time  steps  will  depend  on  the \nimage  is that ok  what is the other way of doing this  what now in this looks slightly \ninefficient   what\u2019s  the  other  option  that  you  could  have  used   just  feed  the  image  at \nevery time right  so  that is the one constant thing that this is the image  now whatever \nyou have generated so far  considered that  but in the addition to that also consider the \nimage  \n\n \nso  what would the diagram look like just passing  the input to every stage of the decoder \nok  \ni  have  already  started  using  terminology   which  have  not  introduced   but  i  will  just \nintroduce it shortly  \n \n\f\n \nso  let us look at what the full architecture looks like  there is something known as the \nencoder   which  takes  your  input  encodes  it  and  gives  you  a  representation  right   then \nyou have something known as a decoder because  given this  input  you  want  to  decode \nwhat the output is right  so  remember general terminology would be whatever input is \ngiven to you  you want to encode it and whatever is the output that needs to be decoded \nright  it  is  you  could  think  of  it  that   this  is  the  image   now   i  am  trying  to  decode  the \ndescription  for  there   is  that  fine   ok  and  then  you  have  an  rnn   which  is  used  to \ndecode the sentence from this input  \nso  such architectures are known as encoder decoder architecture and these are become \nextremely popular and we will see why they are so popular and why they have led to the \npopularity of deep learning in general ok  so  everyone understands this diagram  anyone \nwho does not see a problem with this diagram  there is actually  no problem  but i want \nyou   i  want  you  to  see  beyond  the  diagram  and  to  look  at  the  equations   what  do  you \nmean by that  what do i have here as the input  what is my x  so  this this looks fine  i \nhave taken one box and connected it to another box and everything is fine right  but that is \nnot what i am interested in  \nwhat am i interested in  can you write the input as a the output as the function of the \ninput in this case  is it possible to do that  so  that is what we need to make sure that we \nare able to do right  so  we look at various applications suggest lepted criptical here  but \n \n\fi am going to come back to it  so  i just the emphasis that look beyond the diagram  the \ndiagram looks very nice  i hope it does thanks to the ta\u2019s  but it does  and  but we need \nto  understand   what  is  the  what  is  the  set  of  equations  being  conveyed  through  this \ndiagram right  what is the function that we are trying to learn  we are going to write y \nas  a  function  of  x   are  we  able  to  write  that  function   because   now  we  are  suddenly \nthrown  in  a  convolution  neural  network  at  some  place   we  have  an  recurrent  neural \nnetwork then  we have the feed forward layer at the output  which is the green vectors  \nso  does all this combined together right   \n\f"}
{"audio_filepath": "lec012_002.wav", "duration": 1043.0829375, "text": "\n\n so  we are going to see a lot of applications of the encoder decoder models  \n\n \nand   for  all  these  applications  we  are  trying  to  answer  the  following  questions   what \nkind of network can we used to encode the input  in the previous application what do we \nuse  cnn  what kind of network can be used to decode the output  what did we use  \nstudent  rnn  \nrnn  what are the parameters of the model  we will see that and what is an appropriate \nloss function  right  \n\f\n \nso  let us again go back to this task which was image captioning  what is the input what \nis  the  training  data  given  to  you   what  is  x   what  is  y   x  is  the  image   y  is  the \ndescription  right  so  this is what is given to you given n such training pairs where x i is \nthe image and y i is the description and y i in itself is a sequence  right  so  you have y i \none to y i capital t  everyone gets the input and output  \nnow  what is the next thing  model  can you write down the model equations  i want an \nequation which starts from x and goes all the way up to y and since we have several time \nsteps i want an equation for y t  this is generic for every time step right say can you write \nthat equation and feel free to use shortcuts  so  you do not need to write the entire rnn \nequation just say rnn of something  dont even try to write the vgg sixteen cnn equations \njust say cnn ok so  we will go ahead  \nthe first thing that i am going to do is  i am going to write the equation for the encoder  \nso   the  encoder  gives  me  cnn  of  x  i  whatever  is  the  input  given  to  me   x  i  is  the  ith \ntraining image given to me  so  i will just pass it through cnn i will get a representation \nfor  that  and  i  am  just  being  cryptic  here  it  could  be  the  fcseven  representation  or  the  con  five \nrepresentation  or  the  max  fool  five  representation  or  whatever  you  want   right  and  it  is \ngoing to  denote all of this  as cnn of  x i   run this cnn take whichever  representation \nyou want to take  \n \n\fnow   what  is  the  decoder  going  to  be   decoder  is  the  following  rnn   remember  the \nequation of rnn was st one comma x t  what is the input of the tth time step  whatever we \nare  predicted  at  the  previous  time  step   just  the  embedding  of  that   so   e  means \nembedding   if  you  want  take  one  odd  embedding  if  you  want  take  word    to  vec \nembedding  is that fine  ok  \n and then what is the output  it is the soft max function of the following  how many of \nyou get this now please raise your hands  how many of you can say that y can be written \nas a function of x  is that pretty straight forward ok  so  you have an encoder  you have a \ndecoder and remember that this final y is a composite function of the original input x ok  \njust that you are doing too many computations along the way  but there is a path which \nexists ok  \nwhat is the loss function  everyone at this point should be able to say it  \nstudent  sum of cross entropies  \nsum of cross entropies  they just wait for me to say two more sentences  what are the \nparameters u v w b c  \nstudent  \n  \na b c d e f  laugher  everything  right  what is that  all the parameters of the  \nstudent  \n  \nconvolutional neural network  that means  all the filters that you have  all the parameters \nof the rnn which is w  u and the parameters of the output layer which is v  right  all of \nthese  is that fine  i am i may have missed some biases but ok  the objective function as \nyou said is a sum of cross entropies  where l t is the true character at time step true word \nat time step t  and what is the algorithm that you are going use back propagation through \ntime and with back propagate all the way through the cnn also which is an end to end \nthing   in  practice  of  course   you  do  not  do  that  yes  you  could  just  said  both  to  be  the \nsame  do you get that question  is that ok  ok  \n\f\n \nnow  let us look at another task we look at the task of textual entailment  what textual \nentailment does is that i give you a input or premise the premises that it is raining outside \nand you need to tell me a hypothesis  the hypothesis that the ground it is wet  ok  with \nbasically means that it is raining outside implies that the ground is wet ok  now  what is \nthe  encoder  decoder  architecture  that  you  will  use  for  this  problem   what  is  the  input \nhere   \nstudent  a sequence  \na sequence  what is the output  sequence  it is all the hint that i am going to give you  \nso  what will you do what is the encoder equation going to be  \nstudent  rnn  \nrnn  what is the decoder equation going to be  \nstudent  rnn  \nrnn  how will it become end to end by setting what to what  \nstudent  \n  \nszero of the decoder to  to what of the encoder  \n \n\fstudent  \n  \nlast time step of the encoder  how many if you get that  really we are on the same page \nfirst time in i do not know how many lectures by finally  it happened  so  here is what \ntraining  data  is   right  it  is  a  collection  of  premises  and  hypothesis  and  you  have  n  of \nthese  there are two options for the model the first option is that you encode the input \nusing  an  rnn  feel  free  to  replace  and  by  an  lstm  if  you  want   then  you  have  the \ndecoder where and you set the zero time step to whatever you got from the encoder  \nthen every time step you computed using the rnn where remember the input at every \ntime step is whatever you predicted at the previous time step and then the output is just \nthe  soft  max  function   is  that  fine   and  what  is  the  loss  function  going  to  be   loss \nfunction  \nstudent  \n  \nsum of cross entropies  training algorithm  \nstudent  back propagation  \nback propagation through time and really it is through time right all the way back ok so  \nwe will see that  let me see if i had any other question ha ok i will ask it parameters i am \nnot going to bother about ok  now  this was option one i have just clearly written  what is \noption two  what  is  the set of equations look  like for option two  which of these equations \nwill change and how  remember option two was maybe pass the input at every time step  \nwhich equation will change  \n st  what will it become  but s t can take only i mean rnn take only two inputs  right  s \nt minus when you need to gave embedding you need to gave so  how will you fit in the \nthird  input   this  animation  has  it  is  own  mind   so   this  is  how  back  propagation  will \nhappen  right so  let us it is finish that  so  will actually back propagate all the way back \nthrough time  fine really all the way back through time  \n\f\n \nand same task textually entailment i want model two option two  so  this is what will happen  \nwe will just concatenate h capital t which is this guy along with the input at every time \nstep  right  how many if you get this the rnn is still taking just two inputs  one is the \nprevious state the other is the concatenation of the current input as well as input that we \ngot from the encoder  everyone get this  ok  so  this is model two i am going forward i am \nnot going to do both model one and model two it is model two is just a very simple variation of \nmodel one  a parameters loss function training algorithm everything remains the same ok  \n\n \n \n \n \n\flet us look at machine translation  what is the input  an english sentence  what is the \noutput  a hindi sentence  what is the encoder going to be  \nstudent  rnn  \nrnn  what is the decoder going to be  \nstudent  rnnsixrnn  what is the loss function going to be  \nstudent  \n  \nsoft max  who said soft max  \nstudent  \n  \nwhat is the loss function going to be  \nstudent  sum of  \nsum of cross entropies  training algorithm all the way through time  right ok  so  let us \ncan  you  draw  can  you  write  the  equations   just  copy  it  from  the  previous  slide   right \nactually copy it from the previous slide  right  if you have the rnn you have the rnn as \na  decoder   again  in  option  when  you  will  set  s  zero  to  h  t  you  have  the  loss  function  the \nparameters and your training algorithm  ok  and for option two it is back propagation will  \nfine and for option two what will happen  option two what will happen  \nstudent  \n  \nthis will change  right  so  just focus on that we just passed in the last time say belong \nwith that   \n\f\n \nnow  transliteration  what is transliteration  what is transliteration  if you do not know it \nat least see it from the example what is it  \nstudent  \n  \nwriting the same word in another language  right   so  this is typically done for named \nentities very when you are  when you are translating from one  language to another you \ndo not often for thomas you do not come up with an indian translation right you just say \nthomas  in  devanagari   right  you  just  right  thomas  in  the  devanagari   right   so   for \nnames you typically just do a transliteration  that means  from the english script you just \nwrite  it  in  the  native  language  script  ok   what  is  the  input   one  word  the  input  is  the \nword right  what is it  a sequence of  \nstudent  characters  \ncharacters  what is the output  \nstudent  sequence of characters  \na sequence of characters  what will you use for the input  \nstudent  rnn  \nrnn  what for the output  \n \n\fstudent  rnn   \nrnn  so  i will becoming too easy  right  can you write the equations for this  yes  you \nwill copy it from the previous slide  yes ok  everything remains the same  right  so  you \nsee why this framework has become so powerful  you do not see it still maybe let us look \nat something else   \n\n \nimage question answering  tell me what is the data here  what is the input  image and  \nstudent  question  \nquestion and what is the answer  what is the output  answer  so  for simplicity we are \ngoing  to  assume  that  the  answer  here  is  a  finite  vocabulary   we  are  not  generating \ndescriptive answers  we are not being overly dramatic let us going to say one word what \nis the colour  white  we are not going to write i think the colour of the image is white \nnow  ok   just  white   so   all  these  outputs  are  going  to  be  single  words  and  we  have  v \npossibilities and we are going to predict one of those v possibilities ok  \nnow   give  me  a  model  for  this   now   things  are  getting  slightly  complicated  you  have \none image as the input one sequence as an input and a dash as the output  oh god  now \nthink why would you generate the sequence of characters as the answer  i said that the \nanswer is going to be come from a finite vocabulary  that means  you need a  \n \n\fstudent  probability distribution  \na  distribution   probability  distribution  is  here  enough  said   now   tell  me  what  is  the \nmodel  a model should connect the input to the output  you have two inputs here  i see \nsome people doing this  laugher  i do not know what that means  but let us just do it let \nus  make  a  train   simple  formula  simple  recipe  and  whatever  input  you  are  given  just \nencode it  depending on the type of input you know what is the encoding is going to be  \nfor images what is encoding  sequences  \nstudent  \n  \nnow what do you do with these two separate things  \nstudent  \n  \nconcatenate them  ok and then  \nstudent  \n  \nafter that  \nstudent  \n  \ncan you think of all the equations can imagine all the equations along the way  \nstudent  yes  \nof  course   yes   laugher   right  i  mean  imagination   laugher   you  can  always  do  that \n laughter   now  just think about it  can you write the output as a function of the input \nwhere  the  input  is  actually  a  pair  now   it  is  image  comma  question  what  is  the  model \ngoing to look like let us see  so  model will first have an encoder for the image let us call \nthat as \ni  it is going to have an encoding of the question let us call it as it as \n  i am \ngoing to concatenate these two as someone rightly gestured and then what am i going to \ndo after that pass it through a  \nstudent  feed forward network  \nfeed forward network and predict a probability distribution  what are the parameters of \nthis  network   parameters  of  the  feed  forward  network   the  parameters  of  the  recurrent \n\u02c6hh\fneural networks and the parameters of the cnn  right  so  everything that we have done \nso fine because ok  how do you train it  back propagate through time and space  ok  also \ngo back to the image also  fine  is that ok  ok  \n\n \ndocument summarisation  what is the input  sequence  what is the output  rnn  rnn \neverywhere  fine  i will not even bother to ask you  \n\n \nvideo captioning  sequence of images  i want to hear the choice of phrasing that you use  \ni just want to hear that i love hearing that every time  \n \n \n\fstudent  \n  \nrnn of cnns whatever that means  what rnn of cnn  every time i do this everyone \nsays rnn of cnn  laugher  i do not know what that means  but it is the right answer  \nwhat does it mean  what is the video  it is a sequence  what is the sequence of  \nstudent  images  \nimages  so  what will you do  encode every image and then pass it through a  \nstudent  rnn  \nrnn can you imagine the equations  ok  let us see  ok and in this case what is the output \nagain  the sequence  so  what is the decoder going to be  \nstudent  rnn  \nso here is the model  so  first what you do is for every time step you compute the cnn \nencoding of the frame  then you pass it through an rnn to get the final time step t and \nthen you feed it to a decoder and generate one word at a time  is that fine  so  this thing \napparently is called rnn of cnns ok  and so  that is and loss function would again be \nthe same sum of cross entropies and back propagation through time and space  ok  good  \nplease do not quote me on this thing also this is getting a recorded but ok  \n\n \n \n\fthe  next  one  video  classification   what  is  the  decoder   decoder  is  probability \ndistribution  okay  what is the decoder  \nstudent  feed forward neural network  \nfeed forward neural network   \n\n \nok  this one  dialogue how are you i am not fine  ok  input  \nstudent  rnn  \noutput  \nstudent  rnn  \nright   so   you  see  this  why  this  has  become  so   popular  we  took  a  wide  range  of \nproblems different modalities right we took images  we took videos  we took sequences  \na combination of these  right  image question answering has a combination of images \nand  sentences  and  the  output  you  have  a  probability  distribution   all  of  this  could  be \nmodel  by  this  unified  end  to  end  network   all  of  the  components  are  neural  network \nbased components whether it is a convolutional network or a feed forward network or a \nrecurrent neural network  right  \n \n\fnow   let  me  stretch  this  right  what  if  you  have  video  question  answering   what  is  the \ninput going to be sequence of images and sequence of  \nstudent  words  \nwords  the output is  \nstudent  \n  \nno  just a word right we will pick from a fixed vocabulary  how are you going to model \nthe input  \nstudent  \n  \nrnn for the question  rnn of cnn for the video  then  \nstudent  concatenate  \nconcatenate and  \nstudent  feed  \nfeed it to  your feed forward network right  so  all of these become to what extent that \nwork is a separate question  but all it was not even possible to model all of this as an end \nto  end  network   right   but   now  i  just  because  possible  to  model  it  as  an  end  to  end \nnetwork with all the components being neural components  right  and  this is this story \nstill  not  complete  everything  is  not  as  easy  as  it  looks  they  still  and  very  crucial \ncomponent that we have missing in the architectures that we have seen so far and we will \ntalk about that soon ok  \n\f\n \nnow  let us just continue that i challenge you to do this  pick up any problem there are \nstudent from relevant different departments  pickup any problem do not say that i want to \ndesign some gear for certain aeroplane and all that and i want to use neural network to do \nso   no   something  which  involves  machine  learning   right  not  problem  is  does  not \ninvolve machine learning and see if  you can model it using the encoder decoder frame \nwork  just try to do this  \ntake problem from biotech right for example  they given a sequence of genes and  you \nwant to predict whether this person is susceptible to a certain disease  what will you do  \nconduct a blood test ok  do not do not  laugher  do not go and do neural networks for \nthat  but if  you had to do that this is what  you will do it will take a sequence  you will \ntreat  the sequence of sequence  at  the  given dna as a sequence of  genes  and then  you \nwill try to  predict something as the output try to predict a probability  distribution over \ndisease it a possible  right  \nso   you  can  think  of  many  applications  from  many  domains  and  all  of  that  you  could \nproblems  involving  machine  learning  with  potentially  model  than  using  the  neural \nencoder decoder architecture  but  there is a very important part missing from this whole \nstory which is attention which is a very important idea and we will spend some time on \nthat in the remainder of the lectures  so  we will first motivate why do we need attention \n \n\fand from there we will see that how do you make how do you integrate attention with all \nthese encoder decoder architectures that you have seen so far   \n\f"}
{"audio_filepath": "lec012_003.wav", "duration": 1647.371, "text": "\nso  let us go on to the next module which is attention mechanism  \n\n \nso  let us motivate not the task of attention  let us motivate attention mechanisms with \nthe  help  of  machine  translation   ok   so   what  is  happening  in  the  models  that  we  have \nseen so far  the current model that we saw for machine translation  by the way all the \nmodels that i have shown you so far are wrong or rather incomplete we will complete all \nof them  right and that is where attention fits in  ok  that was for the camera  \na  encoder  reads  the  sentence  and  its  computes  the  encoding  once   right  we  read  the \nentire sentence and be encoded it  and then we have these two options either the pass the \nencoding at the zero time step or pass this encoding at every time step  is this how humans \ntranslate  a  sentence   what  is  the  human  analogy  for  this   you  have  read  the  sentence \nonce  done  and  now  we  are  going  to  remember  this  entire  thing  throughout  and  then \ntranslate  imagine if you doing this for sentences which have twenty five words which is a typical \nwikipedia sentence  what is wrong with this  we have read the input ones and we have \n\fencoded  it   what  is  likely  to  happen  you  will  forget  something  you  are  going  to  lose \ninformation  not just that is the entire sentence important that every time step  \nstudent  \n  \nonly  certain  words  are  important   you  see  this  conceptually  something  wrong  that  we \nare doing here  is is saying  ok  i have encoded the sentence and then start decoding from \nthere   ok   thats  the  conceptual  error  that  we  are  making   so   let  us  see  how  humans \nactually try translate it  right  \n\n \nso  when producing one word in the output suppose my input is the hindi sentence and i \nhave the output sentence  when i am trying to produce the first word i actually compute \nthis probability distribution which tells me which of the input words that i need to focus \non  at this point it is ok if i dont know what is the translation for  ghar or ja or raha or \nhoon  as long as i know the translation for mai i am done because that is the word which \ni first need to produce there  right   \nso  i am going to say that at this point i only need to pay attention with the first word in \nthe input and i can ignore everything else  what about the second time step  i just need \nto focus on the last word  what about the third time step  is it always going to be that i \nonly need to focus on one word at a time  \nstudent  no  \n \n\fno  what about the third time step  \nstudent   fl   \ni  am  sorry  i  am  assuming  everyone  understands  hindi   but  i  think  that  is  this  is  small \nsentence i can assume that  what will you focus on  \nstudent  ja rahi  \nja and rahi  right  you want to focus on both these things and not an anything else and \nwhat about the next one  hoon  right  so  just on ghar and not an anything else  is this \nwhat  the model encoder decoder model is  doing  what  is  it  doing actually  the every \ntime step is focusing on the entire sentence because that is the encoding that your feeding \nto every time step  that is the problem that we need to correct  we need to learn to pay \nattention to certain important parts of the sentence  is the setup clear to all of you  is the \nmotivation fine  not your motivation layers  is the motivation for this fine or not  ok  \nthe distribution actually tells us how much attention to pay to each input word at each \ntime  step   and  ideally  at  each  time  step  we  should  face  pay  attention  to  only  certain \nwords in the input  \n\n \nso   let  us  revisit  the  decoder  that  we  have  seen  so  far   this  is  what  the  decoder  looks \nlike  in fact  i also have the encoder there  now suppose  sorry  so  currently what we are \n \n\fdoing  is  we  are  either  feeding  s  zero  at  the  i  mean  we  have  either  feeding  the  input \nembedding or the encoder embedding at  time step zero or at  every time step   the suppose \nthere was an oracle which told us exactly which are the words important at time step t  \nright   so   in  our  example  at  time  step  three  suppose  it  told  us  that  the  word  going  is \nimportant  actually we need to flip the input and output here also  right  but you can still \nunderstand  right  \nso  i am saying at time step three certain words are important and suppose a oracle actually \ntold us that these are the words which are important  what would you do  assume that \nyou have already run the encoder  what will you do now  and say someone told you that \nonly  this  word  is  important  word  why  weighted  i  am  just  saying  binary  weigths   right \nonly this word is important  what would you do ideally  \nstudent  \n  \njust feed this blue vector to the decoder and do not feed everything else  does not make \nsense  suppose i told you that two words were important send those two words but how \nconcatenate   but  now  at  certain  time  steps  four  words  will  be  important   and  you  cannot \nconcatenate four words  right because then the dimensions will change  so  what do you do  \nstudent  \n  \na weighted  \nstudent  \n  \nweighted some of the important inputs is that make sense  at time stamp three we saw that \nja was  zero five important  and rahi  was zero five important   just a weighted combination of those \ntwo blue vectors and feed that to the decoder  so  you are not changing the dimensions at \neach time stamp  because the blue vectors have the same dimension   i am  just taking  a \nweighted  combination  of  those  i  am  going  to  give  you  the  same  dimension   does  that \nmake sense  ok  \n\f\n \nso  in fact what i am saying is that  i could just take a weighted combination of all the \nblue  vectors  that  i  have  at  the  encoder  and  the  weights  of  this  weighted  combination  \nright  now  i  am  assuming  that  someone  oracle  has  given  me  is  that   ok   if  i  had  his \nweights  does  this  makes  more  sense  then  having  the  vanilla  encoder  decoder  model  \neveryone agrees with that  ok  \nnow   the  question  of  course   us  who  is  going  to  give  us  these  weights   we  will  come \nback to that later  but at least given the weights this make sense  so  at every time step \nthey just going to focus on the words which are actually important  just take a weighted \ncombination of those words and we will just feed that to the decoder  and intuitively this \nshould work better because unlike before where we were overloading the decoder with \nthe entire sentence remember twenty five words  thirty words  entire sentence was  being passed to \nthe  decoder  now  you  are  just  overloading  it  with  the  amount  of  information  that  it \nactually needs to produce that particular word  hence  intuitively this should work better  \nright  ok  \nnow  how do you convert this intuition into a model  \n \n\f\n \nin practice of course  there is no angel who is going to coming give as these weights is \nno oracle  the machine will have to learn this from the data  whenever you need to learn \nsomething you need to  \nstudent  \n  \nintroduce parameters  so   i am  going to  now introduce a parametric  form for the  from \nthe figure which thing for those of we cannot see these are  alpha one  alpha two and so on  \nso  now  from the figure we are going to introduce a parametric form for  \nstudent  alphas  \nfor the alphas  ok  so  i am going to introduce i will come to alpha  but and what you \nthink this weight should depend on  what i am trying to say is that at the tth time step of \nthe  decoder   so  this  is  ejt  at  the  tth  time  step  of  the  decoder  i  want  to  find  out  how \nimportant is the jth word in the input  that is exactly what i am interested in at every time \nstep  of  the  decoder  of  all  the  input  words  i  want  to  see  which  of  them  is  the  most \nimportant  right  so  this is the quantity that  i am interested is  how important is the jth \ninput word at the tth time step  this should depend on what  what should it be a function \nof  \nfor one it should depend on what that word is  right the other is should depend on what \nhas happened in the decoder so far  what is the decoder produce  so  what is the input \n \n\fand what is the decoder state at so far  right  so  as the decoder has already decoded the \nword  ghar  or  home   it  does  not  need  to  look  back  at  home   right   that  is  why  need  to \nknow what is the state of the decoder  what captures the state of the decoder at time step \nt  ht  and what is the state of all the words that we have  it is captured by what  the \nhj\u2019s  right  this is h one  h two  h three  h four  does that make sense  how many of you have fine at \nthis  point   please  raise  your  hands  high  above   ok   how  many  of  you  have  questions  \nplease ask specific questions if you have a question  \nall i am saying is a couple of things one is at every time step instead of the oracle giving \nme these weights  i want to  learn these weights   whenever  i  want  to  learn something  i \nhave to introduce a parametric form and then i learn those parameters  ok  now  what is \nthe quantity that i am interested in  i am interested in this for all the input words  for the \njth input word i am interested in knowing how important it is for the tth time step  there \nare  several  ways  i  could  write  this  function   i  am  saying  that  the  two  things  that  are \nimportant is one what is this jth word which is captured by hj right and what is the state of \nthe decoder up to this time step which is captured by st one  \nyou could think of various other equations  at this point i am fine if you by the intuition \nthat this quantity should indeed depend on these two terms it should depend on what has \nhappened  in  the  decoder  so  far  and  what  is  my  current  word  actually  look  like   how \nmany of your fine with that please raise your hands up and high  ok  now also the other \nthing that i want is that across all the input words this should actually sum to one  right  i \njust  want  a  weighted  combination  i  do  not  want  arbitrary  weights  it  just  like  taking  a \nprobability distribution over what which word is important by how much  so  if i have \nthis ejt how will i convert it to a probability distribution  \nstudent  softmax  \nsoftmax  so  i will just compute the alpha j\u2019s as a softmax of ejt  e j\u2019s is that fine  did not \nget  this   ok   now   we  have  still  not  seen  what  the  exact  form  of  attention  is   of  the  f \nattention function is  \n\f\n \nso  this  is  what  the equation for the \njt is  that we had an alpha j  actually  denotes the \nprobability of focusing on the jth word at the tth time step  ok  now  we are now trying to \nlearn  these  alphas  instead  of  an  oracle  telling  us  what  these  alphas  are   so   learning  is \nalways going to involve some parameters  so  let us define a parametric form for alphas \nand just a couple of notations  \n\n \n\uf061 \n \n\fso  from now on we would not change this  we are going to refer to the decoder state as \nst and the encoder state as shj  ok  so  these blue vectors are s s and these blue vectors are \nhs  ok  \ngiven  these  new  notations  one  among  many  possible  choices  for  f  attention  is  the \nfollowing  i wanted it to depend on the current decoder state i am making a dependent on \nthe  current  decoder  state  but  i  am  also  adding  a  parameter  in  front  of  it   right   i  also \nwanted to make a dependent on h j i am making a dependent on that i am also adding a \nparameter here   \nwhy do i need this parameter  what is the dimension of this  let us assume this is also \nwhat  is  the  dimension  of  this   remember  after  multiplying  with  u  attention  and  after \nmultiplying with w attention the two vectors should be addable  is that fine  something \ncross d what about this same thing cross d  good  ok  so  let us call that same thing as d \none  what is this output then  the tanh output is vector scalar matrix vector of size  \nstudent  \n  \nyou said matrix or scalar  it is  \nask r raise to d one  what is the quantity on the left hand side  vector  scalar  matrix  \nvector  even though it has two indices it is a vector  what is this quantity capturing  at \ntime step t what is the importance of the jth input that is a i will keep asking till everyone \nreplies that is a  \nstudent  scalar  \nscalar  ok  now  you have scalar equal to something multiplied by rdone  so  why do you \nneed this something  \nstudent  \n  \nso  what is the dimension of that going to be  \nstudent  rdone  \nrdone  so  that is the dot product  so  you see why we have these parameters  ok  so  what \nwe have done is made it dependent on stone and hj  and also made sure that the output is a \n\fscalar that is what these three parameters are doing  ok  and these parameters will be learned \nalong with all the other parameters of the encoder and the decoder  \n\n \nso  now this is all fine  you would actually someone had given me the true alpha j\u2019s and \ni had predicted this alpha j\u2019s to that fancy equation which i just showed you and then i \nadded a dash function  softmax that is the  laughter  safest choice in this course  i want \nto learn something  so  what do i what should i add  \nstudent  loss function  \nloss function  what would the loss function be  \nstudent  \n  \nsay  a  squared  error  loss   and  then  i  want  to  adjust  the  parameters  to  minimize  this \nsquared  error  loss   then  all  of  this  makes  a  lot  of  sense   right  because  then  you  can \nimagine that your u attention  w attention and v attention will get tuned in a way that \nthe predicted weights are very close to the true weights this we all understand  \ngiven an objective function we understand that the weights will get adjusted so that you \nare there to the objective function  but the whole premise was that we do not have the \ntrue alphas because in the case of translation no one is going to tell you that the kth word \ncan come came from the jth word or the set of j words  do you all agree with that  so  if \n \n\fwe had the true alphas this makes a lot of sense because then we could have added a loss \nfunction which takes the loss of alpha true with respect to alpha prediction  and then an \naddition  to  our  lost  theta   which  was  the  sum  of  the  cross  entropy  errors  and  then  we \ncould  have jointly minimize this and we  could  have hope that the attention  parameters \nwould have been learnt accordingly  \n\n \nin practice we will not have this  in our translation example we would want someone to \nmanually annotate for every word in the output which is the set of input word from is this \nwhich it came is not going to be possible this  we cannot collect so much annotated data  \nso   what  do  we  do   why  should  this  model  then  work   they  does  not  have  any \nsupervision why should this model work in the absence of such data  \nhow many of you get the meaning of the question  how many of you see the problem \nplease raise your hands  we are not given the true alphas and that is what a problem is \nthen why should this work better  this works better because this is a better dash  better \ndash  choice  language  model  better  dash  choice   what  is  the  possibility  is  there   better \nmodeling  choice   why   ok   i  give  you  the  answer   this  was  better  because  this  is  a \nbetter  modeling  choice   why  so   so   i  will  give  you  analogy  and  the  reinforcement \nlearning fans will cringe but they can just go out  \nso  suppose i trying to learn a bicycle how to ride a bicycle  that\u2019s why i said they will \ncringe i already see some of you can see as if you guy have a copyright on bicycles  ok  \n \n\fso   suppose  you  trying  to  learn  a  bicycle   and  for  some  reason  you  in  your  infinite \nwisdom  you decide that you can learn how to ride it without holding the handle  and you \nstart trying to do it  its conceivable that you know few years or decades you will actually \nknow how to ride the bicycle  right even if you are not holding the handle  right people \ndo that and people can ride it before without that  \nnow  the only thing that i do is  i come and tell  you that instead of doing this why not \nyou try to  hold  the handle and then try to  ride the bicycle  right  that is  all  i am  telling \nyou   i  am  not  giving  you any other supervision  i have just given  you a better model   i \nhave said that instead of just trying to adjust the parameters  with respect your feet  and \nthe pedal and your back position why not  you also introduce this additional parameters \nwhere you are holding the bicycle which your hands and now try to figure out what kind \nof weights you need to put on your left hand  right hand and so on  \ni am not giving you any supervision for that that you need to still discover on your own  \nthat you will start riding it you might fall on one side you might fall on the other side  \nbut you will eventually figure out what these weights need to be  right  because a second \nmodel where you hold the handle is a better model than the first model where you are not \nholding the hand  \nin  the  second  model  you  have  additional  parameters  where  you  could  adjust  these \nparameters   so   that  you  could  learn  to  drive  better  that  some  more  natural  way  more \nclose to human way of learning how to ride a bicycle  the same thing is happening here  \nthe second model where you have a way of learning these attention on the weights even \nthough  i  am  not  all  i  am  telling  you  that  look  maybe  if  you  decided  every  time  step \nwhich  were  to  pay  attention  on  you  might  be  able  to  do  better  than  feeding  the \ninformation from the entire sentence at every time step  \nthat is all the information that i am giving you which is very similar to saying just hold \nthe hand  that is not going to teach you how to ride a bicycle  right  you still have to do \nthe  extra  work  of  learning  these  parameters   but  now  you  are  given  a  chance  you  are \ngiving the model a chance to learn these parameters they are telling it that  this is a better \nway of modeling it with this you should be able to learn better  right  \n\f\n \nso   there  is  a  hope  of  doing  better  because  now  the  model  is  actually  making  a  more \ninformed choice  right   it is  a more informed way  of learning how to do translation  by \nfocusing on certain words at every time step  \nand now these parameters how will they get adjusted  they will get adjusted because at \nevery at a given time step you produced a wrong output you did that maybe because this \nparameter  was  wrong  which  is  the  v  parameter  or  maybe  because  these  recurrent \nconnections  were  wrong  or  maybe  because  your  attention  weights  are  not  proper   so  \nnow  adjust the attention weights and that should given sufficient data it should be able \nto learn which words to focus on  just as humans learn how to do translation  right  \neven when we are doing learning how to translate or when we learn translating from one \nlanguage to another we are not given this word by word supervision  right  we just do a \nlot  of  translations  or  read  a  lot  of  translations  and  somehow  understand  that  while \ntranslating i need to focus on certain words and at every time step this is the word that i \nneed to focus on  so  given enough words it should be able to learn that at least someone \ngets the joke good  so  that is the hope and in practice indeed these models work better \nas compared to vanilla encode  you do not know where the statement comes from  \n \n\f\n \nso  now  let us what we will do is  so this entire thing hints on hope only  right that is all \nthat is all i am saying but it does makes sense  right because you have these additional \nparameters   which  you  can  learn  and  you  can  back  propagate  through  them   i  will  just \nnot stop there will actually prove what happens not proved by demonstrate what happens \nin practice  right  \nso  with this attention model in mind let us look back at the encoder decoder model that \nwe had for machine translation integrate the attention mechanism with it and then let us \nsee the end to end equation that we get  ok  \n \n\f\n \nso  this is what the diagram looks like  the input and output still remains the same we \nhave just given the source sentence and the target sentence in particular you are not given \nwhich  words  to  pay  attention  to  every  time  step   right  that  is  not  given   so   remember \nthat  my  input  is  not  changing  it  still  the  same  source  sentence  and  the  target  sentence  \nwhat is the encoder  \nnow  try to work out the equations i wanted to write the equation for y t which is going \nto be some composite function of x where x is a vector  it is a x one  x two  x three all the words \nin the input  and somewhere along the line  it also going to have this attention equation  \nit is going to take a while but at least try to imagine it there is some hints in the diagram \nitself you could take a look at it i am just asking you to convert the diagram to a set of \nequations   \nso  encoder part is fine  i have computed the representation of each word at time step t  \nso   this  is  a  contextual  representation  the  word  because  it  is  aware  of  what  the \nneighboring  words  are   right   now   what  is  the  decoder  going  to  be   what  is  the  first \nthing at time step one or a time step t in the decoder  what is the first thing that i am going \nto compute  the dash weights  the last time step of the decoder of the encoder  sorry  \nwhat is the first thing that we need to compute a time step t  t attention weights  speak \nup please  what is the first thing that you need to compute at every time step  how what \n \n\fkind of a combination i take off the inputs or rather which are the words that i need to \nfocus on from the input  who tells us this  \nstudent  attention weights  \nthe  attention  weights   how  will  you  compute  the  attention  weights   using  this  fancy \nequation that we have seen earlier  is this enough i need to convert this to a  \nstudent  probability distribution  \nprobability distribution  right that is just to ensure that everything is a neat combination  \nonce  i  know  the  attention  weights  what  do  i  need  to  feed  the  decoder   a  dash \ncombination  of  the  inputs   a  weighted  combination   how  do  i  take  a  weighted \ncombination of the inputs  summation i is equal to one to capital t  \nstudent  \njt  \njt into h j  right  no j  t is the decoder time step j is the input word  so  at the tth time \nstep of the decoder i am taking a weighted combination of all my inputs the index over \nthe inputs is going from j equal to one to t  by the way did that answer your question that is \nwhat you are asking  right  ok  is that fine  ok  \nnow  what next now i want to produce a word at the output  what is the decoder going \nto be  first thing that i am going to do is  decoder is a dash rnn  ok  what is the input \nto  the  rnn  every  time  step   the  previous  predicted  word  as  well  as  the  weighted \ncombination  input  that  you  have  given  it   does  that  make  sense   ok   and  then  finally  \nhow  do  i  get  the  probability  distribution  is  that  fine  yeah  i  think  this  should  be  a \ndistribution  right l t  does not make sense l t is r max of this  right  and what is the loss \nfunction  \nstudent  cross entropy  \ncross  entropy   there  is  no  change  in  the  loss  function   right   loss  and  the  algorithm \nremains the same  say seen these set of equations  now  how many of your confident of \ngoing back and modifying all the wrong encoder decoder modules that we have covered \nin  the  initial  part  of  lecture   modifying  them  to  add  the  attention  equations  in  it   how \n\uf061\uf061\fmany of you can do that  please raise your hands  i am not going to ask you just do it so \nthat i feel happy you can do  right  any questions at this point  very good  ok  \n\n \nso  you can go back and try adding attention mechanisms to all the models that we have \nseen  before   right   see  how  will  you  compute   so   remember  the  only  purpose  of   ok  \nwhat  kind  of  a  network  is  the  attention  network   it  is  a  single  feed  forward  neural \nnetwork  right  this is just transforming a simple linear transformation of the inputs and \nin a non linearity on top of that and then just again one more transformation  right  \nit is a simple feed forward neural network  only these three equation somehow need to be \nfitted  in  all  the  other  models  that  we  have  seen  so  far   right   this  is  a  very  generic \nframework just as the encoder decoder  framework or the very  generic  framework   the \nencode attend decode framework is  also very generic framework   you can go back and \nmodel all the applications that we saw and you can change them change them to at the \nother  case   ok   try  to  answer  the  same  set  of  questions  what\u2019s  the  data   what\u2019s  the \nencoder  what\u2019s the decoder  what\u2019s the loss  what\u2019s the training function  \nand in particular remember that in when you go back represent all the applications that \nyou have done the data is not going to change  no one is going to give us the supervision \nfor the alphas  that is one thing which is not going to change  ok  \n \n\f\n \nso  here is one more thing  so  this probably tie to this question  like how do we be sure \nthat the alphas actually learn something meaningful  now  what do i mean by this  if i \nhave to convince you that alphas are actually learning something meaningful  and let us \ntake the context of machine translation  what do i need to show you  suppose the model \nhas generated an output for a given input sentence it has generated a translation  what do \ni need to show you to convince that it is learn some kind of weights  at every time step \nwhat should i show you  \nstudent  \n  \nwhat does the attention weights look like  right  so  let us see  \n \n\f\n \nthis is a common trick or not a trick actually it is a probably a trick only  but this is the \ncommon thing which is used in several papers and that is why i call it a trick because it is \na trick to get a paper accepted that you actually show what the attention weights actually \nlook like  right  so  on this is the input document and this is the summary that you want \nto generate  ok  \nand what you see here is that at different time step  so look at the last time step terrorism \nit paid maximum attention to the word terrorism in the input  right  so  we can draw this \nmatrix suppose you had capital l time steps in the output and capital t time steps in the \ninput  so  you could draw this l cross t time step or t matrix which tells you what was \nthe attention paid to every input word at every output time step  do you get that  \nyou see what is matrix is this heat map is essentially a matrix of size l cross t and every \ncell here tells you how much attention you paid to a particular word at that time step and \nthe darker the cell that means  more the attention that you paid everyone get this  ok  so  \nwhat  this  is  saying  is  that  probably  see  when  you  wanted  to  generate  russia   the \nmaximum attention was paid to russian and maybe some other words also  sometimes it \ndoes not work very well but sometimes it does  right   \nso  for calls the maximum attention was paid to called  and then similarly for front with \nthe maximum attention was paid to front and so on  you see some meaningful patterns \nthat it is learning here  \n \n\fand here is another example for machine translation  so  roughly to quickly understand \nwhat  this  figure  is   right   so   this  is  i  think  english  to  french   is  it  french   yeah   its \nfrench or french to english translation which is largely monotonic  right  that means  at \nthe fourth  english word  you would end up paying  attention to  the fourth  french word   right \nthat means  you are almost doing a word by word translation   \nand that is exactly what you see the most of the attention is along the diagonal  right  so  \nit is  learning some meaningful  attention weights  as always helpful if  you are using if \nyou are using an attention mechanism to plot the sense see if it is actually learning any \nmeaningful attentions or attention weights or not  right  so  that is a common trick which \npeople use   \n\f"}
{"audio_filepath": "lec012_004.wav", "duration": 147.292, "text": "\n\n \nso  let us start  s last lecture we are looking at encoder decoder models and we saw that \na bunch of problems from different domains and different modalities images  text  videos \nand so on  and  even this cross modal or multi modal applications where you are taking \na  video  and  trying  to  describe  it   so  video  is  one  modality   description  texts  is  another \nmodality and so on  \nwe  were  able  to  propose  modals  for  all  of  these  using  this  encoder  \u2013  decoder \narchitecture   and   then  we  motivated  this  attention  mechanism  where  we  said  that \nencoder decoder is  trying to  do this  silly thing  where it tries  to  encode the entire input \nonce and that is what how humans do it  he do this back and forth thing where at every \ntime step if we are trying to produce a translation or a single word in the translation we \njust focus on certain words in the input sentence and kind of ignore the other  \nso   the  attention  mechanism  which  is  this  bunch  of  equations  that  you  see  here  that \nallowed you a neural way of modelling attention and the key thing to note here is a there \nwas a supervision for the attention  no one actually tells us that this is the portion of the \n\ftext which is important at time step t  but they still works better because this is the better \nmodelling  choice  and  i  give  you  that  bicycle  analogy  and  also  it  is  a  better  modelling \nchoice  we  are  able  to  no  one  has  given  you  these  supervisions  but   you  are  still  have \nmore parameters in the model to learn this kind of a behaviour  \n\n \nand   then  we  also  saw  that  we  could  actually  visualise  these  attention  based  and  from \nsome  experiments  on  some  papers  we  saw  that  actually  learn  some  meaningful \nattentions  in the particular case  on the figure on the on the right hand side  so  the one \nthat  clearly  shows  that  for  a  monotonic  kind  of  a  translation  scenario  between  english \nand french  most of the attentions weights are along a diagram and that is exactly what \nyou would expect  right  \nso  that is where we end it  \n \n\f"}
{"audio_filepath": "lec012_005.wav", "duration": 680.538, "text": "\nand  so  now   in  this  lecture  we  will  go  on  to  the  next  module  which  is  talking  about \nattention over images  so  let us first motivate why is it so different and what could be \ndone there right  \n\n \nso  the question is how do we model an attention mechanism for images  \n\f\n \nso  in the case of text we have a representation for every location of the input sequence \nright  so  every location in the input sequence in the case of text was a word  and then \nyou  are  looking  at  this  problem  of  transliteration  every  location  was  a  character   and \nwhether it is a character or a word  for everything it was discrete  \nso  we could just know that this is the important time step t  and then we know that along \nall the inputs at different time steps you want to pay attention to certain time steps right  \nso  that the definition they was very straightforward right  \n\n \n \n \n\fnow for images what do we do  so  for images we typically take the representation from \na cnn right   it could  be fc seven  or any of the convolution  layers or max pooling layers \nright  now  there is no concept of time step there right because the entire image is given \nto you at one go  \nso   now  how  do  you  decide  where  to  pay  attention  to   but  if  you  think  about  it  does \nmakes sense at least the motivation is very clear  so  for example  for this figure  if i am \ntrying to generate the description as man throwing a frisbee in a garden or in a park or \nsomething like that  right  \nso  when i am generating a word man  i would want to focus only on the man and not \nfocus on any other part of the image  similarly  when i am generating the word frisbee i \nwould like to focus on this  may be when i am saying throwing i would like to focus on \nhis  hand  action  or  something  like  that   and  in  a  park  i  would  like  to  focus  on  the \nbackground  and  so  on   so   it  does  make  sense  that  each  word  in  the  description  is \ncomplete covering from coming from a different space in the image  or different position \nin the image  \nbut the representation that we use say the fc seven representation that doesnot contain any \nlocation information it just a flat  and vector that we had  so  now how do we do this  \nhow  do  we  get  attention  on  locations   is  is  a  motivation  and  the  problem  clear   and \nmotivation is straight forward  the problem is that we are using fc seven representation is \njust a flat  vector   remember that was the fully connected vector and does not  have any \nlocation based encoding  \nso  if for example if the fully connected vector is of size five hundred and twelve i cannot say that the first twelve \nor  first  twenty four  of  these  five hundred and twelve  dimensions  correspond  to  this  set  of  pixels   the  next  twenty four \ncorresponds to this set of pixels and so on right  so that is the problem  how do i  what \ndo i attend to  how do i decide where to attend to  because that is what i am saying that \nthe  vector  the  elements  of  the  vector   or  the  dimension  of  the  vectors  do  not  have  any \nsemantic right that is not that the first dimension corresponds to first location  \nwhat you want an attention on this  locations in the image right  but the first dimension \nin the vector fc seven vector does not correspond to any specific location in the image you \nknow  that  was  a  fully  connected  vector  right   so   it  corresponds  to  everything  in  the \nimage  \n\fso  what something simpler than that  why do i say something simpler than that  object \ndetection is  itself is  a in itself is  a another convolution  neural  network which does this \nand so on right  and we saw this past or seen in past class seen in problems  \nnow   let  us  solve  the  problem  at  hand   the  problem  at  hand  is  that  i  want  i  will  just \nrephrase  a  problem  definition   so  that  the  answer  becomes  obvious   i  want  a \nrepresentation which allows to give which allows me to get some location information  \nno  but the fully connected layer if you back track it was fully connected by definition \nright  the answer is really straight forward  the problem only arise at the fully connected \nlayer  right   because  that  is  fully  connected   but  what  about  the  outputs  on  the \nconvolution layers  do they have position information   \nstudent  yeah sir yes  \n\n \nwe saw that suppose this is vgg whatever it is sixteen i guess  and this is what i am saying \nas a problem because this was fully connected  so  you do not know that this dimension \ncorresponds to location one  location two and so on  but if you look at the convolution layers \nwe know that everything here actually goes back to some location in the image  and if i \nlearn to pay attention to this guy maybe i am paying attention to some equivalent portion \nin the image does that make sense  right   \n \n\fnow  can you build on this intuition and tell me i want this as a solution right  so  in the \ncase of words these are the word vectors that i had at every location and i was learning to \npay  attention  to  them   let  us  learning  these  alphas  for  each  of  these   now  what  is  the \ncorresponding diagram for images  what is each of these box is going to be  so  that we \nlearn the attention weights  what do you mean by pass  \nstudent  \n  \nno  so  maybe i am not understanding your answer  what is the equivalent of this  this \nbox which i have highlighted there  between the image and some attention weight  so  \nyou  are  directly  going  to  operate  on  the  image   remember  attention  weights  are  never \ngiven  to  us  no  one  will  mark  that  man  is  this   frisbee  is  this   and  park  is  this   or \nwhatever  there is no supervision  same size as a convolution what  now what is the size \nof a convolution  \nlet  us  take  the  last  one  right   so   what  do  you  mean  by  size   you  mean  five hundred and twelve   or  you \nmean fourteen  or you mean the other fourteen  that is the size of the output five hundred and twelve cross fourteen cross fourteen  \nso  what do you mean by the size  five hundred and twelve or fourteen  or fourteen or five hundred and twelve cross fourteen cross fourteen  channel \ndoes not make sense because channels capture i mean you do not want to focus on the \nred part  or the green part  or the blue part in some cases you might that is correct ok   \nstudent  \n  \nthese  are  all  partially  correct  answers  going  in  the  right  direction   let  just  think  a  bit \nmore and see  so  probably between these two they gave some part of the answer  now \ncan you think of it  so  you want a representation for the image first of all of us are clear \nthat we do not want to work with the raw image right that is all of us are clear with that  \nthe second part is we are going to work with the convolution neural network  we want \nto  pick up a representation for the convolution neural  network  which  gives  us location \ninformation right  \nand we agree that the fully connected layer does not give us the convolution layers give \nthis  ok   now   i  am  asking  you  to  focus  on  one of  the  convolution  layers  which  is  five hundred and twelve \ncross fourteen cross fourteen  so  let us see from there how we will be try to get these attentions ok  \nso  the output of the fiveth convolution layer or five c this is i think this whole thing is five and \n\fthis  is  five  a   five  b  and  five  c  right   these  this  is  how  the  code  or  the  general  architecture  is \nnumbered  \nso  this guy has fourteen cross fourteen locations right  the five hundred and twelve cross fourteen cross fourteen output  so  it is five hundred and twelve \nchannels  but the number of locations is fourteen cross fourteen  and we have seen that each of these \nfourteen cross fourteen locations corresponds to certain portion in the image right  \n\n \nnow  for each of these fourteen cross fourteen locations  that means  i have one hundred and ninety six such locations  and \nfor each of this how many dimensional representation do i have  i want everyone to say \nthis  \nstudent  five hundred and twelve  \nfive hundred and twelve   because  we  are  reading  it  from  the  figure   no   what  you  are  taking  is  the  one \ntaking  this  pixel   can  you  see  what  i  am  highlighting   i  am  taking  it  across  the  depth \nright  so  that is why five hundred and twelve dimensional representation of one pixel in your output volume  \nand how many such pixels do you have  \nstudent  five hundred and twelve  \none hundred and ninety six   and  each  of  these  pixels  corresponds  to  some  real  location  in  your  image   that \nmeans   it  has  space  information  right  ok   so   now  these  are  the  one hundred and ninety six  locations  that  you \nhave  now this looks very much similar to that diagram that we had for words  so  now \n \n\fyou can think about that you have one hundred and ninety six items in your sequence  and now what will you \ndo  we will learn to pay   \nstudent  \n  \nso what would that look like  \n\n \nso  at every time step  we will have an equation for alphas right   let us try to write an \nabstract  equation  right  first  of  all  give  me  the  indices  of  alpha   what  does  alpha \ncompute   the  importance  of  the  of  the  jth  location  at  the  pth  time  step  fine  is  that  ok  \nwhat should this be a function of  second part is kind of obvious  so  let us call these as \nhone to hone hundred and ninety six  so these are the j\u2019s or the t\u2019s j\u2019s or the t\u2019s  \nstudent  j\u2019s  \nj\u2019s  so  what would the second parameter be  \nstudent  h j  \nh j  and what is the first parameter be  what is the decoder in this case  we are trying to \ngenerate  a  caption   that  means  we  are  trying  to  generate  a  sequence   that  means   what \nwill be the decoder be  \nstudent  rnn  \n \n\frnn  so what should i depend on st or st one  \nstudent  st one  \nst one  s t is the current thing right that we do not know yet  so  it will depend on st minus one  \ncomma h j  of course  you can make it depended on several other things also  but at the \nminimum  you  will  see  these  two  things  right  because  you  are  trying  to  understand  the \nimportance of these guys  so these better participate in the function  and you are trying \nto compute the importance at current time step  so  you better know what has happened \ntill time step t   one  it is not very different from the attention equation that we had written \nin fact  it is a same actually  \nand what was one form of this attention that we had seen  does anyone remember that  \nwe had carefully analysed the parameters and the dimensions of that form  what should \nthe output of this function be scalar  vector  matrix  scalar right  \nso   what  is  the  form  that  we  had  seen  for  this  function   vt  tan  h  of  something   you \nshould get comfortable in writing these equations right because that is what you will do \nif you are proposing your models and so on right  so  you will see okay  in the previous \nmodel  this  depended  on  the  following  two  quantities  i  think  it  should  depend  on  four \nmore quantities  so  i will write a new equation  just think about it there are two inputs s t \nminus one and h j  what will be doing with each of these inputs  do a introduce some  \nstudent  parameters  \nparameters  so  what will you do  \nstudent  w \n  \nw \n plus  \nstudent  plus u  \nv into sorry u into  \nstudent  h j  \nplus  some bias right   so   you just comfortable with  this right  i mean that is  all  i mean \nwhatever  we  have  seen   so   first  of  all  remember  that  the  attention  is  a  feed  forward \n\fneural  network   the  moment  i  tell  you  that  you  should  know  that  it  will  have  a  linear \ntransformation  followed  by  a  non linearity  right   so   that  should  have  been  very  clear \nthat it would have a linear transformation followed by a non linearity ok  \nand  this  is  the  non linearity  and  then  you  have  this  other  constraint  that  you  want  the \noutput to be a scalar  that is why you had this guy which was a vector multiplied by a \nvector which gives you a scalar  everyone is comfortable with this  right  so  we see at \nthe attention over images is not very different from attention over sequences  it is more \nor less the same once you figure out what is the correct representation to you so that you \nget  the  space  information  after  that  it  is  straightforward  right  ok   so   that  ends  the \nmodule   \n\f"}
{"audio_filepath": "lec012_006.wav", "duration": 1221.168, "text": "\nso  we will go on to the next one which is hierarchical attention  so  again something \nvery popular  in  nowadays  become  very  common  for  various  things   so  again  not  very \ndifficult idea to understand  \n\n \nso  let us first look at the motivation for this and then we will look at the solution  so  \nconsider  a  dialog  right   today  everyone  is  interested  building  chatbots   every  second \nstart up wants to build their own chatbot  and every second startup out of that they wants \nto build for the agriculture domain  or the banking domain  or the healthcare domain  so  \nhere  is  what  a  typical  dialog  looks  like  right   this  is  of  course   not  for  any  profound \npurpose this is  but you can see this is an important dialog right very relevant and very \nimportant  so  this is what a dialog looks like  \nso  let us try to break it down into the kind of entities that we deal with  so  can you tell \nme about a dialog  what is a dialog  it is a dash  think in terms of things that we have \ndiscussed so far   \n\fstudent  \n  \nsequence  good right  again the safest answer is sequence from now on  no  it is only \nfor one lecture  it is a  is it a just a sequence or sequence of sequences right  \nso  it contains a sequence of utterances  so each of these lines here is an utterance and \neach  utterance  in  turn  is  a  sequence  of  words  right  ok   so   what  we  have  here  is  a \nsequence of sequence as input and this is very common in many many applications right  \nso  can you think of an encoder for such a sequence of sequence  rnn of rnn\u2019s good \nthat is the answer right  \n\n \nso  we think of a two level hierarchical rnn encoder  so  first leveller will encode the \nutterances ok  let me ask you few questions  is there is a mistake in a diagram  should \nthis be connected  yes  no  maybe  do not care ok  second question   i will write some \nparameters here right  what is our notation  w  u  this is u  right and this is w right  is it \nfine   if  i  have  a  dialog  which  contains  one hundred  utterances   what  will  happen   that  is  a \npractical problem  \nbut more conceptually what is wrong here  what is each rnn trying to do  i encode a \nsentence   encode  an  utterance   so   why  should  it  be  different  for  the  first  utterance  \nsecond utterance  third utterance and so on right  all these rnn\u2019s should be the same  \ndoes that make sense  the u one is equal to u two is equal to u three and w one is equal to w two is \n \n\fequal to w three ok  is that fine  everyone agrees with that  so  now can you tell me if there \nis a correction should be there  or not  \nconceptually what is each rnn doing  encoding   \nstudent  \n   \none sentence right  then why should it be connected to the previous sentence  but then if \nyou  do  not  know  all  these  sentences  then  how  will  you  predict  the  utterance   the  next \nresponse rather  what is missing here  what kind of a what was the title of this module  \nso  what is missing  where is the hierarchy right  \n\n \nso  what we will have is this right  so  each of these green guys is presentation for one \nutterance  in  your  input   in  fact  the   red  red  guys  sorry   the  red  guys  are  the \nrepresentations for the utterances in your dialog and then the green guy is a sequence of \nutterances right  so  remember you have the sequence of sequence of words  \nso   the  red  guys  are  the  sequence  of  words   and  the  green  guys  are  the  sequence  of \nutterances   does  that  make  sense  ok   how  many  of  you  get  this   please  raise  your \nhands   ok   good   so   and  now  what  would  the  decoder  be   you  have  a  hierarchical \nencoder  what could the decoder be  what is the decoder have to do  it has to produce a \neveryone   \n \n\fstudent  sequence of words  \nsequence  of  words   so   what  kind  of  a  decoder  will  you  use   just  an  rnn  not  a \nhierarchical  rnn   the  input  is  hierarchical  why  should  not  the  output  be  hierarchical  \nhow it would look  unbalanced right  the diagram  will not  look  very neat  right  if  you \n\n  \ndo you need a hierarchical and decoder  no right  i mean just a simple decoder  because \nthe  decoders  has  to  produce  a  sequence  of  words   so  it  will  take  something  from  the \nencoder  what will it take from the encoder in the normal encoder decoder paradigm not \nthe attention based paradigm  what will be the input to the decoder  \nthe last  dash vector   your options are state that  is  very safe  answer here  the last  state \nvector what will that means  your options are orange  blue  green and red   \nstudent  green  \ngreen  the last green vector that is what the input is going to be  right  so  this is what is \ngoing to look like  this is option one  what is option two  feed it to every time step that is \nwhat option two is going to be ok  so  that is you have your hierarchical encoder decoder \nnetwork  \n\n \n \n\fwhat is missing here  attention  ok  so let us look at another example  consider the task \nof  document  classification   or  summarisation   what  is  the  difference  between  two   in \nclassification what the what were the decoder be  feed forward neural network with the \nsoftmax  what would be decoder be in the case of summarisation  \nstudents  rnn  \nrnn good  what is the document  sequence of sequence  not sequence of sequence of \nsequence  it can be a sequence of sequence of sequence also right  i did not think of that \nthen it could be a sequence of sequence of sequence of sequence ok  let us look at the \nnot so funny case which is sequence of sequence  what is the sequence of sequence of  \nit is the sequence of sentences  which in turn is a sequence of words  which in turn is a \nsequence  of  character   we  will  not  go  there   we  will  just  keep  it  till  words   so  it  is  a \nsequence of sequence  so  again you need some kind of a hierarchical encoder here right  \nso  will encode each sentence  then you will treat the sentence sequence as a sequence  \nencode that and then you will pass it to the classifier  \nnow  think  of  this  problem  right   now  if  we  want  to  do  document  classification   how \nwould you go about it actually  you want to classify whether this is a politics or sports \nor health or whatever \n  i think in terms of attention  what would you \ndo actually  first we will find the important words in the sentence  to find the important \nwords you will have to read all the  \nstudents  \n  \nif you want to find the important words you will have to read all the sentences  so  what \nwill you do  first find the  \nstudents  word in sentences  \nword in sentences and then  important words within the sentence  so  what kind of an \nattention mechanism do you need  \nstudent  hierarchy  \nhierarchical  right  \n\f\n \nso  let us look at this  so first let us look at the our data model paradigm  so  what is the \ndata given to  you  i  given a document and the class table for the document   and  your \nfirst thing is the word level encoder which looks like this  can i will not explain what this \nis i will just expect you to know what this is  why do i have two indices for h  what is i  \nand what is j  i is the dash id  sentence id and so it is the jth word or the ith sentence right \nthat  is  what  i  am  encoding   and  how  many  sentences  am  i  encoding   the  number  of \nsentences in the document ok  \nand what is the second encoder  so  diagram is  absolutely clear  but the equations are \nnot   the  diagram  is  absolutely  clear  right   there  is  no  just  need  to  map  the  red   blue  \norange  green  guys to the equations ok  so  let me ask  you this  what is hij  no  in the \ndiagram  blue  red  orange  \nstudent  \n  \nwhat  is  wij   how  did  you  write  the  rnn  equation   time  step  t  is  equal  to  rnn  of  t \nminus one and the input at time step t right  what is the input at time step t here  \nstudent  word  \nword  right  probably this was not a good choice maybe we should make it xij  w  i think \nwe might get a confused with the weight\u2019s we should not  but they are  so  fine  so wij is \n \n \n\factually the input word  what is h i j minus one  now tell me what colour is w i j  it is like \nan iq test at which colour map it  w i j maps to which colour  \nstudents  orange  \norange  good and h i j  \nstudents  blue  \nblue  but what about the red  that is which colour i mean sorry not which variable  \nstudents  \n  \n hi ti that is the last state of every sequence right  t i is a length of the ith sentence right \nok  now  what about hitwo  the green guys right and what is this htwok   \nstudents  \n   \nthe last green guy  this guy  ok  is it fine  \n\n \nand  then  the  decoder  is  just  a  softmax   we  do  not  need  to  go  with  that  and  loss  and \neverything  is  fine   so   this  again  whatever  it  is  we  should  always  be  comfortable  and \nwriting the end to end equations from x to y right and you can write it in this case  \n \n\f\n \nnow   let  us  make  it  a  bit  interesting   how  would  you  model  attention  in  such  a \nhierarchical  encoder  decoder  model   how  many  attention  functions  would  you  need  \ntwo  one for attention over sentences  the other for attention over   \nstudents  works  \nworks ok  can you think of these equations  not a very big stretch from what we have \ndone already right  i mean at level one it should be straight forward  at level two just ignore \nlevel one  \nhow many if you can imagine the equations  it is not very hard i am not joking i am i \nmean just think about it  and the level one should be straight forward because that is just \nthe same as ok  so first we need to attend to the most important words in a sentence and \nthen we need to attend to the most important sentence in a document ok  \n \n\f\n \nlet  us  have  see  how  to  model  this  so   we  have  document   again  the  same  input   then \nyou have the word level rnn ok  now what be the word level attention equation look \nlike  \n\n \ni am looking for the attention equation for words  what are the indices going to be  j  i j \nt  ok  what is i j t  that is i have put as superscript in w  this is the word level attention  \nso  at the tth time step i want the importance of the  \nstudent  \n  \n \n \n\fjth good  right  what would that equation depend on   \nstudents  the word   \nthe word should be straightforward right  it should depend on oh but oh ok sorry  this is \nonly for this guy right  ok  so  you have focusing on one of these  so you trying to find \nthe importance of these three words right which are there in the first sentence  \nso  you have computed h i j  that means   you have computed all the representation for \neach of these word  that means  you have computed the first three blue vectors that you \nsee ok  and then you are computing the attention  no  so this is  oh  so instead of having \nit here  this is how we have been writing at right  \nstudent  \n  \nok  i so sorry i should have check this  so  read this as u i j is equal to  or let me just \nexplain it  so  remember this is a vector and we wanted to do this operation to make it a \nscalar right  so  u w is that parameter which was getting multiplied earlier  so  we had \nthis attention equation as u w transpose tan h of something right  so  now  that u w has \nbeen removed from here in equation two and has been added as exponent to equation to the \nalpha equation  does is that ok  \nhow  many  of  you  completely  confused   please  raise  your  hands   how  many  of  you \nunderstand this completely  once can the sum the one  ok  aa shit  what did i do  ok let us \njust see if i can still salvage this  ok let us go one by one  so  what is let me just delete \nsome of these things  let us try to write it on our own right  so  this is what we are trying \nto do  \nso  i will i am ignoring the sentence id rights  so this is sentence one  two and three  so let us just \nfocus on one sentence and the same equations we hold for the other sentences also ok  so \nfirst  of  all  the  attention  weight  would  depend  on  what   it  would  depend   what  are  we \ntrying to pay attention to   \nstudent  words  \nwords  so it should have word in the input right  what else can you at have in the input  \nstudent  previous  \n\fprevious state  unfortunately for this problem do we have a previous state   \nstudent  \n  \nno  we just doing one prediction right  there is no rnn here  we just the feed forward \nnetwork  do you have any s t minus one in the output  that we have put here  so  this was \nthe importance of the jth word at the tth time step  so j belongs to the input and t belongs \nto  the decoder  right   in this  case does decoder have multiple time steps  there is  only \none time step of the decoder right that is the problem which we have run into  \nbut let us assume instead of classification we are trying to do summarisation  that means  \nwe are given this document  and we were trying to generate summary of this and let us \nsay  the  summary  was  the  following  ok   so   this  is  the  summary  that  you  are  trying  to \ngenerate  from  this  document  ok   and  now  this  summary  has  three  or  four  time  steps  if  you \ncount exclamation as the last time step  now  how is it going to be  what is the decoder \ngoing  to  be  in  that  case   rnn  right  and  the  decoder  will  have  some  k  time  steps  ok  \nnow  at every time step at a given time step t  what am i trying to do   \nin next assignment try to develop a better eraser for this  ok  \n\n \nso   we  want  to  compute   when  you  compute  the  attention  for  a  word   the  jth  word  in \nparticular at that tth time step  right  and we have for a minute understood that we do not \nhave a feed forward network at the decoder  we have a recurrent neural network at the \n \n\fdecoder  because  we  are  trying  to  generate  a  summary   ok   we  are  trying  to  generate  a \nsequence at the output  so  at the tth time step we are interested in understanding which \nof  the  document  was  to  pay  attention  to  ok   so  now   that  is  going  to  be  a  function  of \nwhat and i finded a bit irritating for the want of a better word that at least one input to \nthis function should be straight forward right  what is it  \nstudent  the word  \nthe word  that you are trying to learn how much attention to pay to right  so  that should \nbe  very  straightforward   so  that  should  be  w  one  j  because  i  am  considering  the  first \nsentence right now ok  is that fine  for the first sentence i am trying to find out which \nare  the  words  which  are  important   what  is  the  second  input  that  you  could  put  in   it \nshould depend on the index t right  so it should be the previous state of the decoder ok  \nand then of course  i will have a this is again actually not alpha  but e right   and then \nyou get how do you get alpha from there  how do you get the alphas  \nstudent  softmax  \nsoftmax  is that fine ok  so alpha will be some softmax of the e\u2019s  is that ok  fine  so  \nthis is for the word level  now  the equations that are written on the slide are slightly  so  \nthe equation has slices slightly differently written  so let me just go back and write our \nown equation  and so we want to write an equation for this  what was our equation  \n\n \n \n\fignore the equations on the slide  it was something like this  v transpose tan h of w  \nstudent  s t  \nst one  uwonej    \nstudent  b  \nb  ok   now   imagine  that  your  decoder  is  a  feed  forward  neural  network   what  will  be \nmissing  in  that  case   there  is  no  s  t  minus  one   there  is  no  previous  state  of  a  decoder \nbecause we just want to make a prediction once by paying attention to all the important \nwords and sentences in your document right  so  which part will go away  wst one ok  fine  \nnow  the other thing that you are doing is alpha was actually or other alpha i j is actually \nexponent of e i j divided by summation of other k sorry  alpha j t e j t  e k t  is that fine  \nit is just the softmax equation  is that ok right  now  the only thing that you see different \nand  these  two  equations  here  is   first  you  do  not  have  the  w  s  t  minus  one  because  the \ndecoder only has one time step  and second we have taken out this v transpose from here \nand  instead  we  have  added  it  here   is  that  ok   does  that  make  sense   this  is  just \ndifferent way of writing it  so  again you write the attention equation for the words now \nok  now what  about  the sentence  now first  of all  earlier what  were we using for the \ngreen  guy   what  was  the  representation  for  the  green  vector   it  was  the   what  was  the \ngreen vector  \nin the absence of  attention what  was the  green vector  h t  i  right   the last  time step of \nsentence one  is it fine everyone with me  please raise your hands  if you are with me ok  \nnow what would it be  it would be a dash sum of w vectors  a weighted sum  attention \nweighted sum right  so that is exactly what this equation is capturing  so  what did you \nsaying is there is representation of sentence i is a weighted sum of the representations of \nall the words in that sentence is it ok   \nso  that we will get a representation for s one  s two up to s capital k all the sentences that \nyou have  now what do you want to do for the second level what do we want  we want \nto compute the importance of that sentence for the tth time step right  so  let us call that \nbeta   so  i  am  interested  in  beta  if  we  need  to  really  read  out  this  i  said  again  alpha  is \nbeing used in both the places right  \n\fso  you want to find out the importance of the j th sentence at the t th time step  what is \nit  going  to  be  a  function  of   one  is  a  sentence  representation   what  is  the  sentence \nrepresentation given by   \nstudent  s j  \ns  j  and  what  else   the  decoder  state  at  the  previous  time  step  right   does  the  decoder \nhave a previous time step state here  no  so  it will just depend on s j and that is exactly \nwhat  this  equation  is  capturing  right   and  again  the  same  trick  that  i  have  added  this \nextra parameter to the  exponent   is that fine  and the final  representation being fed to \nthe feed forward network is a weighted sum of the sentence representations right  so  this \nagain has to be si\nsi ok   \ni really sorry about this  but i am pretty sure that once we correct the slides and then you \ngo  back  and  look  at  it  should  be  clear  right   it  just  two  sets  of  equation   one  set  of \nequation  sorry  os  sorry  that  is  correct  sorry   so   this  the  idea  is  there  are  two  sets  of \nattention mechanisms  for each you will have your own set of equations  the basic form \nif you can work out what the f attention would depend on  the actual form would depend \nfrom would differ from paper to paper or the toolbox to toolbox  that does not matter so \nmuch  you just need to know that you have these as the input  you are going to add some \nparameters  to  every  input  that  means   you  are  going  to  do  linear  transformation   and \nthen you just need to make sure that alphas eventually turn out to be scalars right  that is \nwhy you will have this additional vector getting multiplied at one point  \n\uf061\f"}
